#+LATEX_CLASS: book-noparts
#+LATEX_CLASS_OPTIONS: [a4paper]
#+LATEX_HEADER: \usepackage{shellesc}
#+LaTeX_HEADER: \usepackage{pdfpages}
# #+LaTeX_HEADER: \usepackage{tikz}

# Include our default LaTeX setup
#+SETUPFILE: ~/.emacs.d/default_latex_header.org

# 'externalize' all TikZ plots, i.e. cache them
# #+LaTeX_HEADER: \usepackage{pgfplots}
# #+LaTeX_HEADER: \usepgfplotslibrary{external} 
# #+LaTeX_HEADER: \tikzexternalize[prefix=cache/]

#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usetikzlibrary{external}
#+LATEX_HEADER: \tikzexternalize[prefix=cache/] % activate!

# got an error suddenly with the 'externalize' section above
# https://tex.stackexchange.com/questions/365777/cannot-run-tikz-externalize-with-lualatex-but-it-used-to-work

# for mini table of contents for each chapter
#+LATEX_HEADER: \usepackage{minitoc}

# Note: Code font is changed in default LaTeX header in emacs (Fira Code)
# config. This uses `fontspec`
# NOTE: At this point <2023-09-14 Thu 18:04> compiling using Computer
# Modern works fine! We don't actually use any fancy unicode in the
# context of the thesis.
#+LATEX_HEADER: \setmainfont{DejaVu Serif} % supports all unicode we care about as serif font

# STIX looks nice, but we have to set up the other versions (bold
# etc.) and decide on a good line spacing.
# #+LATEX_HEADER: \setmainfont[Path = "/usr/share/fonts/stix/static_otf/", Extension = ".otf"]{"STIXTwoText-Regular"}
# #+LATEX_HEADER:   #UprightFont    =  ,
# #+LATEX_HEADER:   #BoldFont       = *-Bold ,
# #+LATEX_HEADER:   #ItalicFont     = *-Italic ,
# #+LATEX_HEADER:   #BoldItalicFont = *-BoldItalic
# #+LATEX_HEADER: ]{"STIXTwoText-Regular"}


# The following is the approach using `ucharclasses` but that ruins
# code blocks of minted...
#   #+LATEX_HEADER: \usepackage{fontspec}
#   #+LATEX_HEADER: \usepackage[Latin,Mathematics,Punctuation,Symbols]{ucharclasses}
#
#   #+LATEX_HEADER: \newfontfamily{\mydefaultfont}{DejaVuSans}
#   #+LATEX_HEADER: \newfontfamily{\mymainfont}{CMU Serif}
#
#   #+LATEX_HEADER: \setTransitionsForPunctuation{\mymainfont}{\mydefaultfont}
#   #+LATEX_HEADER: \setTransitionsForLatin{\mymainfont}{\mydefaultfont}
#   #+LATEX_HEADER: \setTransitionsForSymbols{\mydefaultfont}{\mymainfont}
#   #+LATEX_HEADER: \setTransitionsForMathematics{\mydefaultfont}{\mymainfont}

# Bibliography related:
#+LATEX_HEADER: \usepackage[backend=biber]{biblatex}
#+LATEX_HEADER: \addbibresource{references.bib}

# Epigraphs
#+LATEX_HEADER: \usepackage{epigraph}


# With Dejavu Serif a linespacing of 1.2 is too tight. 1.5 looks nice,
# maybe 1.4 is optimal?
# The default *I think* is 1.2
#+LATEX_HEADER: \linespread{1.5} % change line spacing to be a bit larger. TODO: find good value!

# HTML Export
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="nimdoc.css" />
#+OPTIONS: html-style:nil

#+OPTIONS: toc:nil # turn off Table of Contents here and place it elsewhere

#+LATEX: \dominitoc % initialize the package

# Disable evaluation of Org babel source code blocks on export For
# reasons I don't understand we don't see any org code block
# evaluation regardless of any settings that I have. I don't get
# it.
#+PROPERTY: header-args :eval no-export



\begin{titlepage}

\begin{center}
  \huge Search for solar axions using a 7-GridPix IAXO prototype detector at CAST

  \vspace{2cm}
  \Large Sebastian Michael Schmidt
\end{center}

place funny logos and stuff

Doktorgrad
erworben 2021
Solingen


\end{titlepage}

#+TOC: headlines 2

# Part 0: Introduction

* Compile                                                          :noexport:
:PROPERTIES:
:CUSTOM_ID: sec:compile_thesis
:END:

Compilation at the moment is still a bit broken due to =biber=.

We need to generate the TeX file from Org =C-c C-e l l= to generate
the TeX file.

Then in terminal:
#+begin_src sh
lualatex --shell-escape thesis.tex
biber thesis
lualatex --shell-escape thesis.tex
#+end_src

Or better yet, let =latexmk= take care of it:
#+begin_src
latexmk -pvc -pdf -view=none -shell-escape -output-directory=texout -pdflatex=lualatex thesis.tex
#+end_src

it watches the file and automatically recompiles if the file changed
on disc.

* Start me                                                         :noexport:
:PROPERTIES:
:CUSTOM_ID: sec:latex_book_class
:END:

#+begin_src emacs-lisp
(add-to-list 'org-latex-classes
             '("book-noparts"
               "\\documentclass{book}"
               ("\\chapter{%s}" . "\\chapter*{%s}")
               ("\\section{%s}" . "\\section*{%s}")
               ("\\subsection{%s}" . "\\subsection*{%s}")
               ("\\subsubsection{%s}" . "\\subsubsection*{%s}")
               ("\\paragraph{%s}" . "\\paragraph*{%s}")
               ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
#+end_src

#+RESULTS:
| book-noparts | \documentclass{book}          | (\chapter{%s} . \chapter*{%s}) | (\section{%s} . \section*{%s})       | (\subsection{%s} . \subsection*{%s})       | (\subsubsection{%s} . \subsubsection*{%s}) | (\paragraph{%s} . \paragraph*{%s})         | (\subparagraph{%s} . \subparagraph*{%s}) |
| article      | \documentclass[11pt]{article} | (\section{%s} . \section*{%s}) | (\subsection{%s} . \subsection*{%s}) | (\subsubsection{%s} . \subsubsection*{%s}) | (\paragraph{%s} . \paragraph*{%s})         | (\subparagraph{%s} . \subparagraph*{%s})   |                                          |
| report       | \documentclass[11pt]{report}  | (\part{%s} . \part*{%s})       | (\chapter{%s} . \chapter*{%s})       | (\section{%s} . \section*{%s})             | (\subsection{%s} . \subsection*{%s})       | (\subsubsection{%s} . \subsubsection*{%s}) |                                          |
| book         | \documentclass[11pt]{book}    | (\part{%s} . \part*{%s})       | (\chapter{%s} . \chapter*{%s})       | (\section{%s} . \section*{%s})             | (\subsection{%s} . \subsection*{%s})       | (\subsubsection{%s} . \subsubsection*{%s}) |                                          |

Bibliography for org-ref:
#+begin_src emacs-lisp
(setq bibtex-completion-bibliography "~/phd/references.bib")
#+end_src

#+RESULTS:
: ~/phd/references.bib


#+begin_src emacs-lisp
(add-to-list 'org-latex-packages-alist '("outputdir=texout" "minted"))
#+end_src

#+RESULTS:
| outputdir=texout   | minted       |     |
| labelformat=simple | subcaption   |     |
| margin=2.5cm       | geometry     |     |
|                    | mhchem       |     |
|                    | amsmath      |     |
|                    | unicode-math |     |
|                    | fontspec     |     |
|                    | siunitx      |     |
|                    | pdfpages     |     |
|                    | longtable    |     |
|                    | booktabs     |     |
|                    | minted       |     |
|                    | minted       | t   |
|                    | booktabs     | nil |

In addition make sure you have an environment variable referring to
the ~TimepixAnalysis~ repository:
#+begin_src sh
# A short helper to reference the TPA directory
export TPA=~/CastData/ExternCode/TimepixAnalysis
#+end_src


** Customize exclude tags per backend

#+begin_src emacs-lisp
(defun my-custom-org-exclude-tags (backend)
  "Set tags to exclude depending on BACKEND."
  (cond ((eq backend 'latex)
         (setq-local org-export-exclude-tags '("noexport" "extended" "html")))
        ((eq backend 'html)
         (setq-local org-export-exclude-tags '("noexport" "extended" "latex")))))

(add-hook 'org-export-before-processing-hook 'my-custom-org-exclude-tags)
#+end_src

#+RESULTS:
| my-custom-org-exclude-tags | my/org-process-subfigure-dsl | my/org-html-export-figs | org-blackfriday--reset-org-blackfriday--code-block-num-backticks |


* Introduction                                                        :Intro:
:PROPERTIES:
:CUSTOM_ID: sec:introduction
:END:

#+begin_export latex
\epigraph{
Life before death. \\Strength before weakness. \\Journey before destination.
}{
\textit{The Stormlight Archive \\ by Brandon Sanderson}
}
#+end_export

Mathematics has long served as a guiding tool in theoretical
physics. Symmetries, mathematical 'beauty' and the notion of
'naturalness' have long been successfully used to predict new
phenomena in physics to be verified experimentally later. Although we
may never know whether the universe actually cares about our funny
intuitions, in the lack of empirical evidence to the contrary we tend
to stick to such approaches. Arguably in the infinite space of
mathematical avenues physics /could/ express itself, this may be
considered an application of Occam's razor to physics. In a sense it
underlines the intersection between philosophy, mathematics and
physics and indicates that the term 'philosophy of nature' is not
actually wholly inapplicable to modern physics. And so we arrive at
one of our current representations of this in the form of an angle $θ$
in quantum chromodynamics. Measurements seem to indicate this angle is
essentially zero. However, we tend to reject the idea that our
universe simply /is/ such that $θ$ happens to be close to
zero. Instead derive the simplest explanations as to why this might be
the case. And who can blame us when the result of accepting $θ = 0$ by
nature would lead us precisely nowhere? Finding an explanation spawns
a new hypothetical friend in our zoo of particles, the /axion/. The
point of this thesis is to continue the search for this zoo member by
way of staring into the core of the Sun. With the help of a very large
number of /virtual/ photons we will attempt to entice some axions to
become real X-rays, directly detectable by us. And if we fail in this
quest, we can put our philosopher's hat back on and muse about how
little our axion friends want to dance with our virtual photons.

# The one place where we want a little bit of space to go from
# a little bit less serious to a more serious language :)
#+begin_export latex
\vspace{1cm} 
#+end_export

After a short side note about this thesis as a document in chapter
[[About this thesis (structure)]], we introduce the theoretical foundation
of axion physics in chapter [[Theory of axions]]. From a historical
standpoint as to why axions were invented in the first place to the
avenues of detection and related the expected solar axion fluxes.

This leads to chapter [[Axion helioscopes]], which introduces the concept
of an axion helioscope as a way to potentially detect axions of a
solar origin. Other possible approaches will be shortly mentioned.

With an understanding of possible detection mechanisms for axions, we
will focus next on the required hardware to actually measure axions
indirectly, i.e. via gaseous detectors for X-ray detection in chapter
[[Gaseous detectors principles]].

As we finally wish to compute a limit on different axion coupling
constants, an understanding of basic statistics for limit calculations
is required. This we will cover in chapter [[Statistics & limit
calculations]].

Next we introduce our detector, the Septemboard based GridPix
detector, in chapter [[Septemboard detector]]. Here we will discuss both
the motivation behind why such a detector was built, the different
features of the detector and basic calibration principles as well as
covering the setup and software used to run the detector.

Stopping at the software running the detector, we will then transfer
over to the software suite built to analyze the data in chapter
[[Software]].

A further chapter about the analysis principles follows, which
explains the ideas of how the data reconstruction works, what kind of
calibrations are applied to the data and how it all fits together to
compute a background rate and limit. This is chapter [[Chapter about
analysis principle]].

Raytracing chapter will be moved somehow.

What follows in chapter [[Detector preparation / study /
characterization etc.]] is the explanation of all characterization
measurements, an introduction to the \cefe calibration measurements
and how the energy calibration works.

From there we go to the actual deployment of the detector at CAST in
chapter [[Detector installation at CAST & data taking]], in which we
describe the physical setup and give an overview over the different
data taking periods.

With the description of the data periods out of the way, we can make
use of the data to compute the background rate of the detector for
different cases in chapter [[Background rate computation]].

These background rates are combined with the expected signals from
chapter *TODO which one Raytracing + theory?* and the measured
candidates during tracking time to compute a limit in chapter [[Limit
calculation]].

As a final part we will give an outlook of what a future Timepix3
based detector might achieve in chapter [[Outlook]].

Afterwards we finally conclude in chapter [[Summary & conclusion]].

** Choice of quote                                                :noexport:

Why did I choose the quote from Stormlight Archive?

If you haven't read The Stormlight Archive either this won't mean
anything to you and/or it will spoil you somewhat. So read at your own
discretion.

'The First Ideal' quoted for the Knights Radian is sort of a metaphor
for many aspects of doing a PhD in our real world. Joining a PhD
program is a bit like joining one of the orders of the Knights
Radiant.

I suppose many PhD students can tell their own stories about the
struggles encountered during the PhD work. While different for each of
us, a common theme is struggling with "why am I even doing this?". To
many the notion of _having a PhD_ ends up becoming the dominant reason
not to just quit. So we forget "Journey before Destination". The point
of a PhD is not the title we get at the end, but being part of
academia and pushing science forward together.

Kaladin's mental struggles and bordering on breaking his oaths speaks
to me during my PhD. Questions of self worth, if my approach to my PhD
and general intelligence is enough to achieve what I want, my mental
health and whether anyone even cares about the research I do at many
times let me wonder about if doing a PhD is worth it. In that sense I
was close to breaking my own "oaths" of doing a PhD and doing it for
the right reasons. While I don't think I was ever really considering
to quit, I certainly wondered many times about the importance of my
work. And mentally I frequently visited the chasms, I can say that
much. 

To this day now I'm very torn about the concept of PhDs in our modern
society. I think [[https://www.quantamagazine.org/a-math-puzzle-worthy-of-freeman-dyson-20140326/][Freeman Dyson's thoughts]] deserve thought.

I surely could have picked many other quotes (or none at all) to
include in the beginning of the thesis. I like this one in particular
though because its meaning is a bit ambiguous. For most readers (who
will likely have _not_ read Cosmere novels), it will likely be a
mysterious, but very serious sounding quote that leaves much room for
personal interpretation. I hope that the quite lighthearted
introduction afterwards is a nice uplifting contrast for those readers
who interpret the quote in an ominous or dark way.

* About this thesis (structure)                                       :Intro:
:PROPERTIES:
:CUSTOM_ID: sec:about_thesis
:END:

*TODO:* (maybe) insert the essay written on my phone one night here
(into the full version of the thesis at least) 

Explanation about the thesis structure and introduction of the "full
thesis document".

The thesis you are reading right now is a shortened version of the
full document it is part of. To conform to the expectations of a PhD
thesis many parts are removed that are irrelevant for the basic
presentation of the work done during the thesis.

However, a fellow researcher who wishes to understand all the details,
in particular in terms of reproducibility of the results, the full
document should be read instead. If possible a PhD thesis should be a
tome of knowledge about the topic that allows the interested reader to
absorb as much of the authors knowledge as possible to help with the
continuation of the research.

Furthermore, every thesis, especially those relying on large pieces of
software, contains mistakes, bugs, wrong assumptions and more. Most of
these are not known to the researcher, possibly due to lack of
knowledge in a specific topic. Other times shortcomings _are_ known,
but left out for convenience. This thesis is about one thing:
*transparency*. There are bugs in the referenced code that I'm not
aware of, bad assumptions about certain things, etc. Where I *am*
aware of sketchy choices, I will highlight them honestly. 

The full document is found at:

https://phd.vindaar.de

*TODO*: insert link, probably to GitHub as well as some other source

Note that I'm always available for questions about my work either via
Matrix (@vindaar:matrix.org) or on other channels (Twitter / Github
@vindaar) or plainly by email phd@vindaar.de. Please do not hesitate
to contact me, even if it's several years since this thesis was
initially released.

The main difference between the regular thesis document and the
extended version are the following. The extended version contains:
- either inline code or links to the code that produces *every plot*
  (that is created by me); inline code is used if the required code to
  generate the plot is less than a certain amount of lines.
  For non-inlined code the used code is referenced (link to the code +
  correct git commit)
- access to *all* raw and reconstructed data to reproduce the results
- additional chapters that were not relevant enough / polished enough
  for inclusion into the thesis. This includes additional plots,
  investigations of detector behavior etc., theoretical calculations
  and more.

In essence the idea is to provide a fully reproducible thesis. The
extended version should ideally be read as a mix of the generated PDF
and the real Org file behind it.

The extended version includes many source code blocks that can be
extracted using [[https://github.com/OrgTangle/ntangle][ntangle]]:
#+begin_src sh
ntangle thesis.org
#+end_src

Ideally, all results can be reproduced with a single:
#+begin_src sh
./generateResults.sh
#+end_src
call (we'll see how that will work out).

The package versions for the code used in the extended version will be
frozen at a specific time. The list of version numbers will be found
below.

*TODO*: Add version numbers of all packages used for final plots.

*TODO*: Have specific marking in (sub)sections if they contain more
information in extended version?

*TODO*: It would be sick if we could do something like
#+begin_src sh
curl -s <backblaze link> | sh foo.sh
#+end_src
to download and generate everything in one go. Seems a bit insane
though. But who knows.

*TODO*: In =noexport= sections, possibly have a "Skip this section
if:" introduction? So that readers know exactly why a certain section
might be of interest to them.


Further, this thesis does not attempt to cover *every* aspect of the
theoretical foundation required to understand every part. For example
we will not introduce the Standard Model or explain certain detector
features, if they are not of importance for the understanding of our
data.
Good references, if available, will however be given for an interested
reader / a reader attempting to fill in gaps in knowledge.




** Notes for future PhD students and IAXO analyses                :extended:

Note that even if it's 2037 right now and you are a PhD student trying
to understand my analysis, because you are working on (Baby)IAXO data
that doesn't mean you cannot reach me to ask questions. Unless I died
(let's hope not!) I'll still be reachable via

phd@vindaar.de

no matter when.

But then again at that point you probably already told your personal
AGI to just reconstruct all I did, so well. Let's see how this ages,
shall we?


* TODO List of todos [0/9]                                   :Intro:noexport:
:PROPERTIES:
:CUSTOM_ID: sec:todos
:END:

** TODO Have reference to Firmware used at CAST in each run

** TODO Linking to thesis & plots

When finally creating the links to the thesis, the figures, the data
etc. we should additionally create links using
~vindaar.de/phd_related_link~. That way we can later still change the
hosting location without making the links in the written thesis outdated.

** DONE Run list (appendix)

** STARTED Include exact results from Geometer measurements

- [X] Find in EDH and include, even if we don't use it. Referenced in X-ray
finger measurements.

** TODO fix up schematic of V6 Septemboard connections
** TODO fix up schematic of MM working principle

The existing schematic is not very clear. Change the drift gap
behavior and amplification gap one by reversing their drawing
style. Add some alpha to different regions to highlight amount of
electrons drifting. Add a text label with O(magnitude)

** TODO insert the first LaTeX + Vega-lite based plot

This gives us an idea of how this will work. For a start I'd say base
the Vega-lite plots on Github gists. That allows for easy replacement
for the time being.

** TODO generating plots

Currently most plot we insert are either placeholders or generated by
hand. For plots that are already conveniently generated by running TPA
on data, we should probably do the following:

- add an option to ~config.toml~ to activate "pretty" (of some kind)
  plots, meaning TikZ + Vega-Lite backend for _all plots_ placed into
  the default output path when running TPA
- change the output path for all plots into a thesis local
  ~Figs/TPA_generated~ directory of sorts  

** TODO implement nothing ⇒ background rate as reproducible build

This one will be a bit ambitious, but maybe it's a day of work.

*If* we get this working we're at a point where generating other plots
is just a simple shell command (call script X with args Y), as we will
have all =Calibration/DataRunsX_Y.h5= files ready somewhere.

Steps:
*** Setup Nim + all packages of fixed versions (take versions from a TOML file)
*** Have config file storing paths of raw data + output paths
*** run raw data, reco, ...
*** generate CDL datasets
*** compute logL files
*** plot background

** TODO find way to host the raw data

Can also just use Zenodo https://zenodo.org


B2 maybe as an alternative?

We could start by a simple Backblaze B2 hosting.

Pricing is competitive:
Hosting: 0.005 $/Month/GB
Download: 0.01 $/GB
https://www.backblaze.com/b2/cloud-storage-pricing.html

Which is 1.5$ for 300 GB and 3$ to download it. Certainly cheap enough
to try!

Data to store
*** All 2017/18 data runs
**** Run 2
**** Run 3
*** Detector calibration files for Run 2, Run 3
*** FADC pedestal run
*** X-ray finger runs
*** All our notes + thesis
*** CAST log files
*** Nim code?                                                     :pending:

** TODO use some package for abbreviations

** TODO update all links to code

- [ ] There are especially many references (and needed) in the
  reconstruction and calibration chapters obviously.

Currently we use some links in footnotes to code on github.
1. replace the links to master branch by permalinks to a git tag for
   my thesis
2. add citations (*OR* find a way to add a "secondary" bibliography
   only for code references?)
   Apparently this is possible, either using biblatex directly or
   using a package called =multibib=
   https://www.overleaf.com/learn/latex/Questions/Creating_multiple_bibliographies_in_the_same_document

** TODO Adjust spacing in itemize etc. environments

Can be done using the ~enumitem~ package like here:
https://tex.stackexchange.com/questions/10684/vertical-space-in-lists

** TODO Use something like ~isodate~ to format dates

** TODO Define environment variables?

We could define a variable like ~TPA~ set to the path of the
TimepixAnalysis directory for convenience in the shell snippets that
appear, running code?

At the very least this should not really be needed for most things!

Instead what we should do is to make sure we:
a) before starting any work of the whole data analysis pipeline first
compile every program we will use. Check the code of this thesis for
shell code snippets that contain ~nim c~ commands!
b) add all binaries we use to the TPA ~bin~ directory to have them in
our PATH.

*Note*: we can even make use of ~set/getEnv~ in some of our tools
themselves to use the environmental variables.
This could be useful to put into ~projectDefs.nim~. Having a CT
variable is great, but adding some helpers to get runtime versions
from env variables could be very useful and simplify things.

** TODO Think about signal-like or X-ray like

We currently prefer "signal-like" in the background rate chapter. But
we should think about whether we actually prefer that over the whole
course of the thesis or not.

** TODO Language about likelihood method, probability density

Make sure our language is consistent about the relationship between
individual probability densities of each geometric property and the
full likelihood distribution. Do not call an individual property
distribution a likelihood distribution!

** TODO About gas gain variation and ingrid properties

Note: now that we have some understanding about where the changes in
the gas gain come from (and proof that it _is_ real gas gain change
and not electronics, via FADC amplitudes), a very important and
related concept is how such changes affects the properties of all
clusters.
In the CDL data we have those ridgeline plots (appendix
[[#sec:appendix:fit_by_run_justification]]) comparing the properties of
different CDL runs with very different gas gains showing very little
differences.

It *is worth a thought* whether we might want to have a very short
section in an earlier part (where we talk about variation and its
causes in the first place for example) where we provide some short
"proof" that gas gain variations leave the properties _mostly_ unchanged.

** Points of contention

At the moment <2021-07-31 Sat 11:36> my biggest point of uncertainty
is the whole detector calibration part + how this plays into a
software framework.

Difficult to come up with good layout at this point. Will be easier
once more notes are added in each part I think.

** Structure

I'm very lost <2022-08-22 Mon 13:53> about how to structure the thesis
at this point. :(

Maybe it's easier to think of the ingredients for the limit
calculation as tips of strands. Follow each back to its
introduction. But: should each of these simply be its own chapter?

E.g. axion image:
- raytracing
- solar axion flux
- axion models

So therefore have a chapter "Deriving the expected axion image" that
starts from:
- pick a solar model & axion model
- compute expected flux
- use raytracing, explain, to compute the image?
- but: needs average absorption depth to know at what point to even
  compute something!
  Well, this _can_ work, as long as the setup & detector are explained
  _before_ this chapter.

However: none of that makes any sense without the context of the
limit calculation! Why else would one need to compute such an image
etc?

Instead could also start part 2 (or whatever) of the thesis as "limit
calculation" and have this be a huge part that first introduces the
math of what & how to compute a limit, and *then* introduces how one
ends up at the necessary inputs?

If I do it this way, then the first part of the thesis is purely:
- axion theory generically
- axion helioscopes  
- gaseous detector physics & micromegas
- the septemboard detector
- deployment at CAST
- data analysis to an extent? to what extent though?

Part 2:
limit calculation
- how to compute limit, method
- ingredients, show them.
- then: each ingredient, how to derive it
- finally:
  - put all ingredients together, short overview
  - compute
  
** Current thoughts about structure

<2022-11-03 Thu 10:32>:
So, as I'm currently finishing up the chapter about TOS and the
Timepix calibrations, I'm unclear about how to structure the next
steps:

- real detector calibrations used, Septemboard FSRs, Thresholds
  etc. When performed and link to appendix containing all of them.
- scintillator calibrations
- FADC pedestal runs

- Data reconstruction must have an introduction that motivates why we
  even compute geometric properties and so on. Comparison of events
  etc.

- reconstruction before deployment at CAST?

- 


** DONE Use ~booktabs~ everywhere

Simply done by adding ~:booktabs t~ to the ~#+ATTR_LATEX:~ above a
table!

https://orgmode.org/manual/Tables-in-LaTeX-export.html

(Note that it isn't really clear from the documentation that it needs
an argument, as it is otherwise interpreted as receiving ~nil~ I presume)

** TODO HTML export of the thesis

Examples of websites with great HTML layout that I might want to copy:
- https://mpv.io/manual/master
- a typical mdbook / nimibook
- ar5ix (the HTML5 access to arxiv)
- https://news.ycombinator.com/item?id=34050835
  There are some interesting links here, e.g. the Peter Scholze page
  about the computer math proof website
- This is also a great looking site:
  https://cpu.land/how-to-run-a-program
- Another simple but nice looking page:
  https://jaylittle.com/  

Idea:
In the HTML export version of the thesis, it might be a good idea to
have all :noexport: sections by default folded. So if one goes to the
page of a chapter (or section) all sections are by default not folded
_except_ the :noexport: ones. That keeps things clean by default,
especially given all the code sections (which should probably be in an
_extra_ fold by default).

Another note:
For the HTML version for certain references having direct inline links
is a plus of course. No need to hide them behind a citation for
example. E.g. when linking to the relevant parts of the code, we can
just make some piece of text clickable!

The HTML export also requires better handling of subfigures that are
side by side.

In general we made good progress on the HTML export today thanks to
GPT4 helping us to auto convert all PDFs to SVGs on export!

- [ ] Extend the HTML PDF->SVG conversion such that any PDF larger
  than some cutoff will be converted to PNG instead (i.e. for those
  that freeze brave which are 19MB SVGs)

- [ ] Fix PDF->SVG conversion logic to handle ~file:~ type links!

In terms of table of contents:
It would be nice to have the ToC for the entire thesis on the left,
but something like this:
https://agraphicsguynotes.com/posts/fiber_in_cpp_understanding_the_basics/
on the right for the minitoc for each chapter! 

*** Bibliography for HTML

See for example:
https://emacs.stackexchange.com/questions/62236/org-ref-exporting-org-file-to-html-with-its-style-exactly-same-as-a-specific-sc
and
https://github.com/jkitchin/org-ref/issues/319
and the manual:
https://raw.githubusercontent.com/jkitchin/org-ref/master/org-ref.org


*** Multiple file from thesis

For the HTML export we'll likely want to have all sections to be on
separate pages.

I found the following code here:
https://stackoverflow.com/a/65428989
which is slightly modified to kill the correct buffer (thanks GPT)
#+begin_src emacs-lisp
(defun my-org-export-each-level-1-headline-to-html (&optional scope)
  (interactive)
  (org-map-entries
   (lambda ()
     (let* ((title (car (last (org-get-outline-path t))))
            (dir (file-name-directory buffer-file-name))
            (filename (concat dir title ".html"))
            (current-buffer (current-buffer)))
       (org-narrow-to-subtree)
       (org-html-export-as-html)
       (write-file filename)
       ;; switch to current export buffer and kill it
       (switch-to-buffer (other-buffer current-buffer 1))
       (kill-current-buffer)
       (switch-to-buffer current-buffer)
       (widen)))
   "LEVEL=1" scope))
#+end_src

While this works (aside from having issues with all the figures if we
just copy this file elsewhere to test), it leaves open the issue about
links to other sections that thus end up in other documents! How
should we handle this? By hand by just parsing the HTML and replacing
all ~[BROKEN LINK: section]~ by an ~href~ ?

GPT4 has the following to say about it:



and it proposed the following code to automatically perform the
replacement:
#+begin_src emacs-lisp
(defun my-org-export-each-level-1-headline-to-html (&optional scope)
  (interactive)
  ;; 1. Build a mapping of CUSTOM_ID to filenames
  (let ((id-to-filename 
         (mapcar (lambda (headline)
                   (let ((custom-id (org-entry-get (point) "CUSTOM_ID")))
                     (when custom-id
                       (cons custom-id
                             (concat (file-name-directory buffer-file-name)
                                     (car (last (org-get-outline-path t headline)))
                                     ".html")))))
                 (org-map-entries
                  (lambda () (org-heading-components))
                  "LEVEL=1" scope))))
    (org-map-entries
     (lambda ()
       (let* ((title (car (last (org-get-outline-path t))))
              (dir (file-name-directory buffer-file-name))
              (filename (concat dir title ".html"))
              (current-buffer (current-buffer)))
         (org-narrow-to-subtree)
         ;; 2. Replace internal links with corresponding filenames
         (goto-char (point-min))
         (while (re-search-forward org-link-bracket-re nil t)
           (let* ((desc (match-string 4))
                  (path (match-string 2))
                  (new-filename (cdr (assoc path id-to-filename))))
             (when new-filename
               (replace-match (format "[[%s][%s]]" new-filename (or desc path))))))
         (org-html-export-as-html)
         (write-file filename)
         (switch-to-buffer (other-buffer current-buffer 1))
         (kill-current-buffer)
         (switch-to-buffer current-buffer)
         (widen)))
     "LEVEL=1" scope)))
#+end_src

Alternatively it proposed to write a custom Org export backend that
derives from the HTML backend. That looks pretty elegant actually.

#+begin_src emacs-lisp
(org-export-define-derived-backend 'custom-html 'html
  :translate-alist '((link . custom-html-link-transcoder)))

(defun custom-html-link-transcoder (link contents info)
  "Transcode a LINK from Org to custom HTML."
  (let ((type (org-element-property :type link))
        (path (org-element-property :path link))
        (raw-path (org-element-property :raw-link link)))
    (cond
     ;; For CUSTOM_ID links, replace them with corresponding filenames.
     ((and (string= type "id") 
           (assoc path (org-export-get-id-to-filename-alist info)))
      (format "<a href=\"%s.html\">%s</a>"
              (cdr (assoc path (org-export-get-id-to-filename-alist info)))
              contents))
     ;; Default handling for other links
     (t (org-html-link link contents info)))))

(defun custom-html-export-to-separate-files ()
  "Export all level 1 headings in the current buffer to separate HTML files."
  (interactive)
  ;; Build a mapping of CUSTOM_IDs to filenames based on level 1 headlines.
  (let* ((base-dir (file-name-directory (buffer-file-name)))
         (id-to-filename 
          (mapcar (lambda (headline)
                    (let ((id (org-element-property :CUSTOM_ID headline)))
                      (when id
                        (cons id (concat base-dir
                                         (org-element-property :raw-value headline)
                                         ".html")))))
                  (org-element-map (org-element-parse-buffer) 'headline
                    (lambda (hl) hl)
                    nil nil 'headline t))))
    (org-map-entries
     (lambda ()
       (let ((title (nth 4 (org-heading-components))))
         (org-narrow-to-subtree)
         (org-export-to-file 'custom-html (concat base-dir title ".html"))
         (widen)))
     "LEVEL=1")))

(defun custom-html-export-menu-entry ()
  "Menu entry for the custom HTML export."
  (interactive)
  (org-export--dispatch-ui
   '((?H "To separate files by level 1 heading" custom-html-export-to-separate-files))
   "Export with Custom HTML"))

(define-key org-mode-map (kbd "C-c C-e H") 'custom-html-export-menu-entry)
#+end_src
*NOTE*: This is currently broken: Adding it to the Org export dispatch
window doesn't work yet, but more importantly:
- [ ] The links are not actually working right now. So we'll need to fix it!
  But it's a good start and gives us the idea on how to handle this!
- [ ] We must use the CUSTOM_ID as a file name instead of the title of
  the section! The title is useless, as it contains ~/~ etc that cause
  trouble producing files!
- [ ] The produced HTML files do not have a table of content on the
  left hand side! Anyway we want a table of content for the full
  thesis on the left! 



Here is the Org manual about adding a custom backend:
https://orgmode.org/manual/Adding-Export-Back_002dends.html

See also the syntax for referencing something in a separate file:
https://www.gnu.org/software/emacs/manual/html_node/org/Search-Options.html

*Example*:
[[file:~/org/Doc/StatusAndProgress.org::#sec:list_different_uncertainties]]


*** Bibliography

We should use ~org-ref~ to use do our citations and cross links in
this document:
https://github.com/jkitchin/org-ref

It handles exporting a bibliography to HTML too!

*** Org mode subfigures for LaTeX

I found this here (https://www.mail-archive.com/emacs-orgmode@gnu.org/msg140190.html):
#+begin_src
#+name: fig:fig
#+caption: plots of....
#+begin_figure

#+name: fig:sfig1
#+attr_latex: :caption \subcaption{1a}
#+attr_latex: :options {0.5\textwidth}
#+begin_subfigure
#+attr_latex: :width 0.8\linewidth
[[~/s/test/mip.png]]
#+end_subfigure

#+name: fig:sfig2
#+attr_latex: :options {0.5\textwidth}
#+attr_latex: :caption \subcaption{1b}
#+begin_subfigure
#+attr_latex: :width 0.8\linewidth
[[~/s/test/mip.png]]
#+end_subfigure

#+end_figure
#+end_src

Does it work to produce a subfigure? If so, what happens on HTML
export?

Also relevant:
https://kitchingroup.cheme.cmu.edu/blog/2016/01/17/Side-by-side-figures-in-org-mode-for-different-export-outputs/


And another one which looks very simple:

https://list.orgmode.org/87mty1an66.fsf@posteo.net/#t

#+begin_quote
Hi,

I have come up with a way to export subfigures to LaTeX (with the subfigure package) by
defining a new link type. The 'subcaption' of the subfigure would be the description of
the link. If we want to add parameters such as width, scale, etc., we can put them next
between the marks '>( ... )'

The code:

#+begin_src emacs-lisp
  (org-link-set-parameters
   "subfig"
   :follow (lambda (file) (find-file file))
   :face '(:foreground "chocolate" :weight bold :underline t)
   :display 'full
   :export (lambda (file desc backend)
	     (when (eq backend 'latex)
	       (if (string-match ">(\\(.+\\))" desc)
		   (concat "\\subfigure[" (replace-regexp-in-string "\s+>(.+)" "" desc) "]"
			   "{\\includegraphics"
			   "["
			   (match-string 1 desc)
			   "]"
			   "{"
			   file
			   "}}")
		 (format "\\subfigure[%s]{\\includegraphics{%s}}" desc file)))))
#+end_src

Example:

#+begin_src org
  ,#+CAPTION: Lorem impsum dolor
  ,#+ATTR_LaTeX: :options \centering
  ,#+begin_figure
  [[subfig:img1.jpg][Caption of img1 >(width=.3\textwidth)]]

  [[subfig:img2.jpg][Caption of img2 >(width=.3\textwidth)]]

  [[subfig:img3.jpg][Caption of img3 >(width=.6\textwidth)]]
  ,#+end_figure
#+end_src

Results:

#+begin_src latex
  \begin{figure}\centering
    \subfigure[Caption of img1]{\includegraphics[width=.3\textwidth]{img1.jpg}}

    \subfigure[Caption of img2]{\includegraphics[width=.3\textwidth]{img2.jpg}}

    \subfigure[Caption of img3]{\includegraphics[width=.6\textwidth]{img3.jpg}}
    \caption{Lorem impsum dolor}
  \end{figure}
#+end_src

If we want to export to HTML it would be something more tricky. In this case, the export
function could be like this (a width parameter would be enclosed between >{ ... }):

#+begin_src emacs-lisp
  (lambda (file desc backend)
    (cond
     ((eq backend 'latex)
      (if (string-match ">(\\(.+\\))" desc)
	  (concat "\\subfigure[" (replace-regexp-in-string "\s*>.+" "" desc) "]" "{\\includegraphics" "[" (match-string 1 desc) "]" "{"  file "}}")
	(format "\\subfigure[%s]{\\includegraphics{%s}}" (replace-regexp-in-string "\s*>.+" "" desc) file)))
     ((eq backend 'html)
      (if (string-match "&gt;{\\(.+\\)}" desc)
	  (concat "<td><img src=\"" file "\" alt=\"" file "\"" " style=\"width:"
		  (match-string 1 desc)
		  "\""
		  "/><br>"
		  (replace-regexp-in-string "\s*&gt;.+" "" desc)
		  "</td>")
	(format "<td><img src=\"%s\" alt=\"%s\"/><br>%s</td>"
		file file
		(replace-regexp-in-string "\s*&gt;.+" "" desc))))))
#+end_src

Example:

#+begin_src org
  ,#+CAPTION: Lorem impsum dolor
  ,#+ATTR_LaTeX: :options \centering
  ,#+begin_figure
  @@html:<div class="org-center"><table style="margin-left:auto;margin-right:auto;"><tr>@@

  [[subfig:img1.jpg][Caption of img1 >(width=.3\textwidth) >{300px}]]

  [[subfig:img2.jpg][Caption of img2 >(width=.3\textwidth) >{300px}]]

  @@html:</tr></table><p> </p><table style="margin-left:auto;margin-right:auto;"><tr>@@

  [[subfig:img3.jpg][Caption of img3 >(width=.6\textwidth) >{600px}]]

  @@html:</tr></table><br>Lorem ipsum dolor</div>@@
  ,#+end_figure
#+end_src

As you can see, it is not the panacea, and you have to apply some direct format...

Happy holidays

Juan Manuel 
#+end_quote

**** Our implementation

*UPDATE*: The below is now essentially finished. We've finalized our
subfigure DSL and it produces the correct code for both backends. With
some CSS and JS we have also implemented being able to click on
subfigures in HTML to resize them.
We use ~org-ref~ to have custom references ~sref~ and ~ssubref~ to
reference figures / subfigures within the generated code.



We started an implementation of the former (the DSL) here
[[file:~/org/Misc/side_by_side_subfigure_elisp_dsl.org]]

Once starting to work on the HTML version I realized that it will
likely be tricky to get the figure counters working. They are
hardcoded by the HTML export logic of Org here
[[file:/usr/share/emacs/29.1/lisp/org/ob-C.el.gz::3417]]

I just learned about CSS counters though:

https://tympanus.net/codrops/2013/05/02/automatic-figure-numbering-with-css-counters/

which might be perfect. Maybe we need to replace the current Org logic
by something custom for it (i.e. replace the
~"<span class=\"figure-number\">"~
by something that uses ~<figure>~).


*UPDATE* <2023-09-15 Fri 22:12>: *Ohhhh!* There already *is* support
for HTML5 ~<figure>~ environments! That should take care of one
aspect. We can activate it using
Ref: https://emacs.stackexchange.com/questions/27691/org-mode-export-images-to-html-as-figures-not-img
#+begin_src emacs-lisp
(setq org-html-html5-fancy t
      org-html-doctype "html5")
#+end_src
or alternatively set it only for a single file using
#+begin_src 
:html-doctype "html5"
:html-html5-fancy t
#+end_src

*However*, this still inserts the hard coded ~Figure %d~ into the
code. So our rebinding the function is still necessary.

For *referencing* our custom IDs, it should be fine to set the
~org-html-prefer-user-labels~!
#+begin_src emacs-lisp
(setq org-html-prefer-user-labels t)
#+end_src
which then perfectly leaves the labels we assign!


Maybe we can disable the counting for the ~figure-number~ span classes
though!
https://emacs.stackexchange.com/a/17625
#+begin_src css
.figure-number {
    display: none;
}
#+end_src

And to get our custom counting logic, we use CSS counters:
#+begin_src css
/* Initialize the counter */
body {
    counter-reset: fig-counter;
}

figure {
    /* Increment the counter for every figure */
    counter-increment: fig-counter;
}

figcaption::before {
    /* Display the counter value before each figcaption */
    content: "Figure " counter(fig-counter) ": ";
}

#+end_src

Let's try it out by merging it into
[[file:~/org/Doc/SolarAxionConversionPoint/nimdoc.css]]

*IT WORKS PERFECTLY!!!!!* :partying_face:

Ok, the HTML export now also more or less works! Had some small
issues, but all good. It is to be noted that we need to emit
~#+begin_export html~ blocks.

Also we need to be careful about the order in which we add both our
HTML export hooks! This one and the one that converts the PDFs to SVG!

Also: we probably want a different CSS counter for the subfigures
internally. That we can generate the correct counters for *within* the
subcaption and using letters instead. So outer counter + a, b etc.

In particular, the final problem (outside of potentially having
trouble aligning the figures?) is the conversion of PDFs to SVGs.
Ahhh! We can just call the conversion and copy function from *this*
hook!
-> YUP, that works!

Now all that is left:
- [ ] Resize the images to the desired size (currently no size given!
  nor CSS sets size!)
- [ ] Change the subfigure counter logic and text so that it's better.  





Ref: https://orgmode.org/manual/HTML-doctypes.html

*** Inline images

Found this interesting article. He inserts all SVG images directly as
base64 data into the HTML file! That's pretty neat. It relies on
~cl-letf~ to rebind the function ~org-html--format-image~ to do that.
https://niklasfasching.de/posts/org-html-export-inline-images/


** TODO TikZ backend / useTeX

When the legend is too large to fit onto the regular plot, TikZ or
rather latex with extend the figure.
But the rectangle for the background is then too small!

We should apply the background color of the background rectangle to
the entire document! That way this cannot happen.

** TODO In context of determining gas diffusion [0/1]

- [ ] As mentioned in the relevant section, don't forget to clear up
  the distinction between:
  - σ_T, the actual transverse diffusion coefficient
  - D_T the diffusion constant
  - transverse RMS, our actual value computed from the cluster, which
    is related to D_T(z), but D_T(z) is the sigma of the deviation
    after z drift distance of the *full population*, but the
    transverse RMS data is the sigma of a *small sample of the
    population*!

-> Explaining it like this in the text and coming up with good
terminology for transverse RMS should help.    

** TODO Fixup pygmentize support of unicode for Nim [/]

Nim allows unicode characters. The Pygmentize lexer however highlights
those as errors in the code.

There is some stackoverflow question about something similar for a
different language (maybe even python?)

- [ ] Need to fix that.

- [ ] Also change the font colors for certain cases, e.g. ~sh~
  -> ~sh~ uses black text. For the monokai colors that's super
  broken. Text should be white!
  - [ ] Google on how to change default colors if possible

** TODO Create a Docker image for the software? [/]

At least for all CPU related stuff (i.e. *not* training the MLP) we
could create a docker image that we can distribute. That way at least
there is *one guaranteed* way to run the software by others without
running into struggles building on certain platforms.

https://stackoverflow.com/questions/36808396/how-to-create-new-docker-image-based-on-existing-image

My idea would be:
- download a bare docker image we like
- install the entire software stack in the image
- save the image as a new image

At least that seems simpler than creating a full VM image.
https://askubuntu.com/questions/308897/convert-ubuntu-physical-machine-to-virtual-machine
https://askubuntu.com/questions/34802/convert-my-physical-operating-system-to-a-virtualbox-disk


** TODO Cross check our usages of the differential, e.g. dx

Define a ~\dd~ operator like:
https://tex.stackexchange.com/questions/14821/whats-the-proper-way-to-typeset-a-differential-operator/637613#637613
and then make sure we use that everywhere instead of our manual mathrm
usage!
- [X] Defined
- [ ] Check its usage!  

** TODO Copy other documents to hoster (for now Backblaze B2)

*** Index file for PhD /docs

- [ ] !

*** Axion mass - buffer gas calculation

Note: here we simply don't copy the whole directory as there are so
many files in the directory that don't really need to be on the remote.
#+begin_src
cd ~/org/Code/CAST/babyIaxoAxionMassRange/
rclone copy axionMass.org b2:vindaarNotes/phd/docs/bufferGasIAXO/v1/
rclone copy axionMass.nim b2:vindaarNotes/phd/docs/bufferGasIAXO/v1/
rclone copy axionMass.pdf b2:vindaarNotes/phd/docs/bufferGasIAXO/v1/
rclone copy axionMass.html b2:vindaarNotes/phd/docs/bufferGasIAXO/v1/
rclone copy figs b2:vindaarNotes/phd/docs/bufferGasIAXO/v1/figs/
rclone copy nimdoc.css b2:vindaarNotes/phd/docs/bufferGasIAXO/v1/
rclone copy mass_attenuation_nist_data.txt b2:vindaarNotes/phd/docs/bufferGasIAXO/v1/
rclone copy polypropylene_window_10micron.txt b2:vindaarNotes/phd/docs/bufferGasIAXO/v1/
rclone copy index.html b2:vindaarNotes/phd/docs/bufferGasIAXO/v1/
#+end_src

*** SolarAxionConversionPoint

#+begin_src
cd ~/org/Doc/SolarAxionConversionPoint/
rclone copy SolarAxionConversionPoint/ b2:vindaarNotes/phd/docs/SolarAxionConversionPoint/
#+end_src

** TODO Fix alignment of the table of contents overlapping section numbers

Ref:
- https://tex.stackexchange.com/questions/296545/overlapping-numbers-and-titles-in-toc
- https://www.reddit.com/r/LaTeX/comments/24ao1g/section_numbers_overlapping_titles_in_table_of/

* Theory of axions                                                   :Theory:
:PROPERTIES:
:CUSTOM_ID: sec:theory
:END:
#+LATEX: \minitoc

A deep understanding of the of the axion requires a diverse set of
knowledge of different aspects of the Standard Model, quantum field
theory and QCD. The \cpt invariance of the SM and related \cp
violation of the weak force, the common algebraic structure of the
weak and strong force via $\mathrm{SU}(2)$ and $\mathrm{SU}(3)$,
anomalies in QFT, in particular the Adler-Bell-Jackiw anomaly, the
structure of the QCD vacuum and the related $\mathrm{U}(1)$ problem,
which is solved by instantons and the concepts of Goldstone's
theorem. To do justice to all these aspects in the context of a non
theory PhD thesis is not possible [fn:for_me]. As such the theory part
of this thesis will be kept short and instead a focus is placed on
referencing useful material for an interested reader. In particular
see sec. 

The standard model of particle physics at low energies can be
described by a combination of three different forces, the
electromagnetic, the weak and the strong force. These can be
represented mathematically by an internal group structure of
$\mathrm{U}(1) \times \mathrm{SU}(2) \times \mathrm{SU}(3)$,
respectively. [fn:groups] The weak force, represented by
$\mathrm{SU}(2)$, has long since been known to exhibit a \cp
violation [fn:cp_violation] *CITE REFERENCE WEAK CP PROBLEM -> PDG
chapters*. Due to the similar structure between the weak and the
strong force ($\mathrm{SU}(2)$ vs. $\mathrm{SU}(3)$) many parallels
exist between the mathematical descriptions of the two forces in the
standard model. In particular the following term is allowed [fn:theta_parameter]

\[
\mathcal{L}_θ = θ \frac{g_s²}{32π²} G^{μν}_a \tilde{G}_{aμν} 
\]

under the requirements for a Standard Model conform term (i.e. gauge
invariant, Lorentz invariant and so on). As a matter of fact, this
term is even required if the instanton solution to the $\mathrm{U}(1)$
problem is considered cite:hooft1986instantons,Peccei2008 and is a
result of the complex vacuum structure of QCD cite:tHooftU1. This
term violates $\mathrm{P}$ and $\mathrm{T}$ transformations and as a
result of \cpt symmetry also violates \cp.  Peculiarly, any effect
expected from this \cp violation has still not been observed. One such
effect is an expected electric dipole moment of the neutron
cite:CREWTHER_NEDM,CREWTHER_NEDM_ERRATA,Baluni_NEDM. Such a dipole
moment may naively be expected plainly from the fact that the
constituent quarks of a neutron are charged after all. However, very
stringent limits place an extremely low upper bound on it at
cite:NEDM_Limit,Revised_NEDM_Limit

\[
d_{\text{NEDM}} \leq \SI{3e-26}{\elementarycharge \cm},
\]

where $e$ is the electron charge. Nature's deviation from our
expectation in this context is coined the _strong \cp problem_ of
particle physics. One possible solution to the strong \cp problem
would be a massless up or down quark, in which case the QCD Lagrangian
would feature a global $\mathrm{U}(1)$ axial shift symmetry, which
could shift $θ ↦ 0$. Even in the late 1980's this was not entirely
ruled out, despite the already understood mass ratio $m_u / m_d = 5/9$
cite:Weinberg1977_mass, due to $2^{\text{nd}}$ order chiral effects
cite:PhysRevLett.56.2004. However, nowadays based on lattice QCD
calculations this has been ruled out
cite:AndrewG_Cohen_1999,PhysRevD.92.054004,10.1093/ptep/ptac097.

While it is possible that our universe is simply one in which the
effect of the strong \cp-violation is suppressed (or even exactly
zero) "by chance", Helen Quinn and Roberto Peccei realized in 1977
cite:PecceiQuinn1977_1, PecceiQuinn1977_2 that this behavior can be
explained in the presence of an additional scalar field. Shortly after
both Weinberg and Wilczek cite:AxionWeinberg, AxionWilczek realized
the implication of such an additional field, namely a pseudo
Nambu-Goldstone boson, which Wilczek named the _axion_, after a
washing detergent as it "washes the standard model clean" of the
strong \cp problem. The most straightforward axion model based on the
work by Wilczek and Weinberg yields a coupling of the axion to matter
that is already excluded, because they associate the spontaneous
symmetry breaking with the electroweak scale.

Models for an 'invisible axion' manage to unify the solution to the
strong \cp problem with the current lack of experimental evidence for
an axion-like particle. There are two main models for the invisible
axion, the KSVZ (Kim-Shifman-Vainshtein-Zakharov)
cite:Kim_KSVZ,SHIFMAN_KSVZ and the DFSZ
(Dine-Fischler-Srednicki-Zhitnitskii)
cite:DINE_DFSZ,Zhitnitskii_DFSZ models.

For the most comprehensive modern overview of the theory of axions,
overview of models, best bounds on different axion models and general
axion reference, make sure to look into the aptly named "landscape of
QCD axion models" cite:DILUZIO20201!

[fn:for_me] At least for me.  

[fn:groups] $\mathrm{U}(1)$ refers to the "circle group", i.e. the
group that describes rotations on a unit circle (consider a phase
shift on the complex plane). The group operation as such can be
considered as multiplication by a complex phase. $\mathrm{SU}(n)$ is
the special unitary group, which means the group of unitary matrices
of rank $n$ with determinant 1, where the group operation is matrix
multiplication of these matrices (for $\mathrm{SU}(2)$ the Pauli
matrices multiplied by $\frac{i}{2}$ are a possible set of
infinitesimal generators for example).

[fn:theta_parameter] The parameter $θ$ as written in the equation is
actually a compound of a pure $θ$ from the QCD $θ$ vacuum and an
electroweak contribution. A chiral transformation is required to go to
the physical mass eigenstates to diagonalize the quark mass
matrix. This adds a term to $θ$, $\overline{θ} = θ + \arg \det M$. We
simply drop the bar over $\overline{θ}$.

[fn:cp_violation] $C$ refers to the discrete transformation of charge
conjugation and $P$ for parity transformation. Both refer to the
idea of studying a physical system with either (or both) of these
transformations applied. A \cp conserving theory (or system) would
behave exactly the same under the combined transformation. The
standard model is mathematically \cpt invariant ($T$ being time
reversal). As such, if a system exhibits different behavior under time
reversal it implies a violation of \cp to achieve a combined \cpt
invariance.


[fn:axion_overview] To be brutally honest, as a combination of the
significant growth of the axion community (both experimentally and
theoretically) and my lack of theory work in the last years, I cannot
do an overview of axion theory justice. Fortunately, there are a huge
number of amazing reviews of the current axion landscape out there! In
particular "landscape of QCD axion models".


** TODOs for this section [/]                                     :noexport:

- [X] *ADD SENTENCE ABOUT CPT INVARIANCE OF SM*
  -> Done I think.

- [X] *SHOW WEAK CP VIOLATING TERM*
- [ ] *GET BEST CURRENT LIMIT ON NEDM*
- [X] *WRITE THETA TERM HERE TO REFERENCE IT IN QUINN PECCEI*


*REFERENCE REVIEW BY PECCEI, REVIEW BY SIKIVIE*

*FIND REFERENCE TO ORIGINAL AXION BEING VISIBLE*
*FIND REFERENCE TO INVISIBLE AXION*

*ADD REMARKS ABOUT*:
- theta term
- axion mass
  -> Mass is relevant for exclusion plot, if we make one!
- axion coupling constants
  -> It's what we search for / limit on



From a historical standpoint including the strong CP problem, we go
over to the Peccei-Quinn solution. Another way to look at it is from a
modern standpoint asking why does the neutron not have a dipole
moment?

*Take a look at lectures for Axion School*
*QCD AXION LANDSCAPE PAPER*
*PAPER RINGWALD LIKED SO MUCH*


*NOTE*: I think after introducing the axion the way I've done up there
now, maybe it's a good idea to just review the relevant parts that
will show up in the thesis? Mass, coupling, etc?
  


** Useful reading material                                        :optional:

This will become a written section, maybe with bullet points, to be
shown in extended version about the reading material I think is valuable. 

- Original papers by Peccei & Quinn
- Original paper by Wilczek and Weinberg
- t'Hooft paper about instantons as solution to U(1) problem
- t'Hooft review about renormalization etc
- Landscape of QCD and Igor's paper for current intro and overview of
  axions
- NEDM review paper for QCD vacuum
- My master thesis for an attempt to pull together all relevant
  aspects for students at the MSc level
- Axions and the strong CP problem by Kim
- Review by Peccei
- Landscape of QCD axion models
- ...? Check my MSc for other references

  
- Review by Sikivie
- Redondo paper about axion-electron flux    

** Illustration of the \cpt symmetry [/]                         :optional:

- [ ] *Change* the structure of the extended schematic to follow what
  Cristina proposed here:
  [[file:~/org/Figs/CPT_explanation/idea_cristina_better_structure.png]].
  Also, this section is likely not going to remain in the thesis, only
  in the extended version.

Fig. [[fig:theory:cpt_symmetry_schematic]] is an illustration of the three
discrete transformations part of the \cpt symmetry in a 1 dimensional
spacetime. The three transformations are:
- $C$ - charge conjugation: replaces each particle by its anti
  particle and thus reversing the charge $q ↦ -q$
- $P$ - parity transformation: mirrors all positions at some origin,
  $\vec{x} ↦ -\vec{x}$.
- $T$ - time reversal: reverses the arrow of time, $t ↦ -t$

The \cpt symmetry of the Standard Model states that a hypothetical
mirror universe to ours - obtained by the three transformations
together - follows the exact same physical laws and thus 'evolves'
identically (due to the time reversal 'evolution' in quotation marks).

#+CAPTION: Schematic showcasing the three transformations $C, P$ and $T$ in a 1 dimensional
#+CAPTION: spacetime. Each of the three are discrete transformations essentially reversing
#+CAPTION: the value along its axis. The combination of all three operations yields the
#+CAPTION: schematic on the right. 
#+NAME: fig:theory:cpt_symmetry_schematic
[[~/org/Figs/CPT_explanation/cpt_explanation_extended.pdf]]

** Historical origins                                             :noexport:

- [X] DONE IN INTRODUCTION

From electroweak theory we know about CP violation. Standard model for
strong force is just SU(3) vs. SU(2) for electroweak.

Lagrangian allows mostly the same terms for both forces. This implies
there should be a strong CP violation. This isn't observed and even
today theh neutron electric dipole moment is restricted to values
smaller $d_N \leq 1e-26 \text{? some units}$.

Merge the next section into this one and change title?

** Strong CP problem                                              :noexport:

- [X] DONE IN INTRODUCTION

More info here?  

Use the schematic I created for Hendrik's presentation?

** Peccei-Quinn solution                                          :noexport:

- [X] DONE IN INTRODUCTION

Main Peccei-Quinn paper citation.

Solution by introducing another global U(1) symmetry that is
spontaneously broken below some energy scale. 

** The axion                                                      :noexport:

- [X] DONE IN INTRODUCTION

Leads to a pseudo Nambu-Goldstone boson that Wilzcek named the Axion
(ref a pic of axion detergent), as it washes the standard model clean
of an ugly stain.

** Invisible axion models and axion couplings

- Kim-Shifman-Vainshtein-Zakharov model :: The so called KSVZ model
  cite:Kim_KSVZ cite:SHIFMAN_KSVZ model is the simplest invisible
  axion model. It adds a scalar field $σ$ and a superheavy quark $Q$,
  which $σ$ couples to via a Yukawa coupling. The main problem with
  the standard axion is that its energy scale is the electroweak scale
  $v_F \approx \SI{250}{GeV}$ resulting in too strong
  interactions. The KSVZ model effectively achieves a symmetry
  breaking scale $f_a \gg v_F$ and thus results in an 'invisible
  axion'. It contains a tree-level axion-photon coupling $g_{aγ}$, but
  no axion-electron couplings. The latter can be found at one-loop level
  cite:SREDNICKI1985689.

- Dine-Fischler-Srednicki-Zhitnitskii model :: The DFZS axion model
  cite:DINE_DFSZ, Zhitnitskii_DFSZ is another axion model, in which
  the scalar field $σ$ couples to two Higgs doublet fields, $H_u$ and
  $H_d$. It does not require an extra superheavy quark, in this case
  the coupling of the scalar to the doublets achieves the decoupling
  of the axion symmetry breaking scale $f_a$ from the electroweak
  scale $v_F$. The end result is the same, an 'invisible axion' which
  is very light and has only light interactions with other
  matter. However, in contrast to the KSVZ model axion-lepton
  couplings appear at tree level!

- Generalizations :: Further generalizations of axion models over the
  KSVZ and DFSZ models are possible, resulting in more flexible
  couplings. From a practical standpoint of an axion experiment it is
  usually better to consider axion interaction from the standpoint of
  effective couplings, as the limiting factor is detecting _something_
  rather than determining its properties. In particular because the
  two models above yield very small coupling constants in regions of
  axion masses easily accessible via laboratory
  experiments. Therefore, an effective Lagrangian like the following
  #+NAME: eq:theory:axion:general_axion_couplings
  \begin{equation}
    \mathcal{L}_{a,\text{eff}} = \frac{1}{2} \partial^{μ}
    a \partial_{μ} a - \frac{1}{2} m_a^2 a^2 -
    \frac{g_{aγ}}{4} \widetilde{F}^{μν} F_{μν} a -
    g_{ae} \frac{\partial_{μ} a}{2 m_e} \overline{ψ}_e γ^5
    γ^{μ} ψ_e,
  \end{equation}
  which contains an axion-photon coupling $g_{aγ}$ and an
  axion-electron coupling $g_{ae}$ is useful for experimental
  searches. [fn:other_couplings] Limits given on one of these
  parameters can always be converted to the specific couplings of one
  of the existing models if needed.

  In principle other couplings exist, for example the axion-nucleon
  coupling $g_N$. For brevity we ignore these as they are relevant in
  the context of this thesis. Future helioscope like IAXO may be able
  to be sensitive to at least $g_N$ though cite:di2022probing.
  
[fn:other_couplings] This Lagrangian can of course be extended by
other couplings, like $g_{aN}$ an axion-nucleon coupling and
others. We restrict ourselves here to those that are considered in the
remainder of the thesis.

*** Notes on KSVZ, DFSZ                                          :extended:

- KSVZ ::   The Lagrangian for this model can be written
  cite:SHIFMAN_KSVZ
  \begin{equation}
    \mathcal{L}_{\text{KSVZ}} = \overline{Q}\slashed{D}Q - h \left(
        σ \overline{Q}_R Q_L + σ^{\dag} \overline{Q}_L Q_R
      \right) + \partial^{μ} σ^{\dag} \partial_{μ} σ + m^2
      σ^{\dag} σ - λ \left( σ^{\dag} σ
    \right)^2,
  \label{eq:theory:axion:KSZV_lagrangian}
  \end{equation}
  with the dimensionless Yukawa coupling $h$. The vacuum expectation
  value for the field $σ$ calculates to
  \begin{equation}
    f_a = \langle σ \rangle \equiv σ_0 = \frac{m}{\sqrt{2 λ}}.
  \end{equation}
  The mass of this axion turns out to be exactly as for the standard
  axion eq. \ref{eq:theory:axion:effective_axion_mass}, with the replacement
  $\nu_{\text{EW}} \rightarrow \langle σ \rangle$. This means, the
  KSVZ model can be seen as the simplest extension of the standard
  axion, which allows for an arbitrary symmetry breaking scale $f_a$.

- DFSZ :: While not needing an additional superheavy quark it relies
  on two scalar Higgs doublet fields. $\Phi_u$ has hypercharge $-1$
  and couples to $u$ type right-handed quarks, whereas $\Phi_d$ has
  hypercharge $+1$ and couples to $d$ type right-handed quarks as well
  as leptons. The different Higgs field have different vacuum
  expectation values, with the requirement
  \begin{equation}
    \nu^2_{\text{EW}} = \nu^2_u + \nu^2_d,
  \end{equation}
  while the additional degree of freedom allows for $\sqrt{2}\langle
  σ \rangle \equiv f_{σ} \gg \nu_{\text{EW}}$.
  The mass term is again similar to the standard axion and KSVZ axion
  \begin{equation}
    m_a = m_{a0} / N_g,
  \end{equation}
  where $N_g$ is the number of quark generations of the theory. Again
  $\nu_{\text{EW}}$ is replaced by $\langle σ \rangle$. One
  interesting property of the DSFZ axion is its coupling directly to
  electrons (and other leptons [[cite:kim2010axions,kim2010axions_erratum]]), given by
  cite:Redondo_2013,Peccei2008:
  \begin{equation}
    \mathcal{L}_{al} = -i \frac{\nu_d^2}{\nu^2_{\text{EW}}}
    \frac{m_l}{f_a} a \overline{l} \gamma^5 l.
  \end{equation}
  where $l$ refers to lepton.
  This type of coupling may allow for easier detection of axions than
  models only including axion photon couplings at tree level.
  

** Implications for axion interactions [/]
:PROPERTIES:
:CUSTOM_ID: sec:theory:axion_interactions
:END:

Starting with the Lagrangian in
eq. [[eq:theory:axion:general_axion_couplings]] and extending it by the
Lagrangian for a free photon

\[
\mathcal{L} = \mathcal{L}_{a,\text{eff}} + \mathcal{L}_γ =
  \mathcal{L}_{a,\text{eff}} - \frac{1}{4} F_{μν} F^{μν} 
\]

and noting that

\[
\mathcal{L}_{aγγ} = \frac{1}{4}g_{aγγ} \tilde{F}^{μν} F_{μν} a =
-g_{aγγ} a \vec{E}·\vec{B}, 
\]

we can apply the Euler Lagrange equations to both the axion $a$ and
photon $A_ν$ to derive a modified Klein-Gorden equation for the axion,

\[
\left(\Box + m_a²\right) a = \frac{1}{4}g_{aγγ} F_{μν} \tilde{F}^{μν},
\]

which has a photon source term. Similarly, for the photon equation of
motion we derive the homogeneous Maxwell equations with an axion
source term,

\[
∂_μ F^{μν} = g_{aγγ} (∂_μ a) \tilde{F}^{μν}.
\]

Without going into too much detail, let's shortly sketch how one
derives the axion-photon conversion probability from here.

By choosing a suitable gauge and specifying directions of electric
and magnetic fields in a suitable coordinate system, we can then
derive the mixing between photon and axion states. For example if the
propagation of particles is along the $z$ axis and we fix the two
degrees of freedom of $A_ν$ by the Lorenz gauge ($∂_μ A^μ = 0$) and
Coulomb gauge ($\vec{\nabla}·\vec{A} = 0$) we can derive a single
equation of motion for the axion and $A_ν$ field by starting with a
plane wave approach. We obtain

\[
\left[ (ω² + ∂²_z) \mathbf{1} - \mathbf{M} \right] 
\vektor{A_{\perp}(z) \\ A_{\parallel}(z) \\ 0} = 0,
\]

with the parallel and orthogonal polarization of the photon
$A_{\parallel}$ and $A_{\perp}$, respectively and matrix $\mathbf{M}$:

\[
\mathbf{M} = \mtrix{
  m²_γ & 0    & 0 \\
  0    & m²_γ & -ω g_{aγγ} B_T \\
  0    & -ω g_{aγγ} B_T & m²_a \\
  }
  \text{ where } m²_γ = ω²_p.
\]

Here effects of QED vacuum polarization and other polarization effects
are ignored. The mass $m_γ$ refers to an effective photon mass that
can appear in media, $ω$ is the frequency and $B_T$ the transverse
magnetic field. The constant magnetic field $B_T$ appears, because we
assume for our purpose the magnetic field is constant along $z$, the
propagation direction of the photon and electric fields.

By recognizing that the orthogonal component $A_{\perp}$ is decoupled
from the other two, the problem reduces to a 2 dimensional
equation. Note that a side effect of this decoupling is that photons
produced from an incoming axion in a magnetic field are always
linearly polarized in the direction of the external magnetic field!
Further, this equation can be linearized in the ultra relativistic
limit $m_γ² \ll ω²$ to

\[
\left[ \left( ω + i∂_z \right) \mathbf{1} - \frac{\mathbf{M}}{2ω} \right] \vektor{ A_{\parallel}(z) \\ a(z) } = 0.
\]

As the mass matrix $\mathbf{M}$ is non diagonal, the fields
$A_{\parallel}$ and $a$ are interaction eigenstates and not
propagation eigenstates. If we wish to compute the axion to photon
conversion probability, we need the propagation eigenstates
however. Transforming from one to the other is done by a regular
rotation matrix $\mathbf{R}$, which diagonalizes $\mathbf{M}/2ω$. In the
basis of the propagation eigenstates the fields $A'_{\parallel}$ and
$a'$ are then decoupled and can be easily solved by a plane wave
solution. The fields we can _measure_ in an experiment are those of
the interaction eigenstates of course. [fn:eigenstates]

The interaction eigenstates after a distance $z$ can therefore be
expressed by

\[
\vektor{ A_{\parallel}(z) \\ a(z) } = \mathbf{R}^{-1}
\mathbf{M_{\text{diag}}} \mathbf{R} \vektor{ A_{\parallel}(0) \\ a(0) },
\]

where $\mathbf{M_{\text{diag}}}$ is the diagonalized mass matrix,

\[
\mtrix{ e^{-i λ_+ z} & 0 \\ 0 & e^{-λ_- z} },
\]

where $λ_{+, -}$ are its eigenvalues and coefficients in the
exponential of the plane wave solutions of the propagation eigenstate
fields $A'_{\parallel}$ and $a'$ given by

\[
λ_{+,-} = \pm \frac{1}{4ω} \sqrt{ \left(ω²_P - m²_a\right)² + \left(2 ω g_{aγγ} B_T\right)²}.
\]

Then finally, one can compute the conversion probability by starting
from initial conditions where no electromagnetic field is present,
$A_{\parallel}(0) = 0, a(0) = 1$. Computing the resulting
$A_{\parallel}(z)$ with these conditions yields the expression which
needs to be squared for the probability to measure a photon at
distance $z$ when starting purely from axions in an external,
transverse magnetic field $B_T$

\[
P_{a↦γ}(z) = |{A_{\parallel}(z)}|² = \left( \frac{g_{aγγ} B_T z}{2} \right)² \left(\frac{\sin(\frac{q z}{2})}{\frac{q z}{2}}\right)²,
\]

with $q = \frac{m²_γ - m²_a}{2ω}$ and we dropped additional terms $∝
g_{aγ} B_T$ as arguments to $\sinc$, because they are extremely small
compared to $q z$ for reasonable axion masses, magnetic fields and
coupling constants. 

If further the coherence condition $qL < π$ is such that $qL \ll π$,
the $\sin(x)/x$ term approaches 1 and the relevant conversion
probability is finally:

#+NAME: eq:theory:conversion_prob
\begin{equation}
P_{a↦γ, \text{vacuum}} = \left(\frac{g_{aγ} B L}{2} \right)^2 
\end{equation}

This is the case for long magnets and/or low axion masses. In the
context of this thesis generally assume it to be the case. The point
at which this condition does not strictly hold anymore is the axion
mass at which helioscope experiments start to lose
sensitivity.

Note that the above conversion probability is given in natural
units. Arguments ($B, L$) need either be converted to natural units as
well or the missing factors need to be added. The same equation in SI
units is given by:

\[
P_{a↦γ} = ε_0 \hbar c^3 \left( \frac{g_{aγ} B L}{2} \right)^2.
\]

A detailed derivation for the above can be found in
cite:masaki2017photon. [fn:biljana_kreso_doc] An initial derivation
for the first axion helioscope prototype is found in
cite:vanBibber1989 based on cite:raffelt1988mixing. Sikivie gives
expected rates in his groundbreaking papers about axion experiments,
cite:PhysRevLett.51.1415,PhysRevD.32.2988 but is extremely short on
details. Another source in the form of cite:raffelt1996stars in
which G. Raffelt covers a very large number of topics relevant to axion
searches.

- [ ] *FIND OUT IF WE CAN LINK AXION DOCUMENT OF BILJANA & KRESO!*

[fn:biljana_kreso_doc] There is a more detailed derivation written by
Biljana Lakić and Krešimir Jakovčić available internally in the IAXO
collaboration. If you don't have access to it, reach out!

[fn:eigenstates] For all practical purposes the terms interaction
eigenstates and propagation eigenstates (the latter often also called
mass eigenstates) are a convenient tool to work with. A field $X$ in
the interaction eigenstate corresponds to the field we can actually
measure in an experiment. However, if fields interact, say with
another field $Y$, then along their space and time evolution they may
mix. Therefore, it is useful (and convenient) to introduce a
propagation eigenstate $X'$ in which that new field (which is
different from the physical field $X$!) propagates without any
interaction. The nature of the field interactions have been absorbed
into the time and space evolution of the field itself - it is a
superposition of the $X$ and $Y$ interaction eigenstates. In the
simplest case a field $X'$ in the propagation state may just be oscillating
between $X$ and $Y$, for example.
  
*** Effects of a buffer gas [/]
:PROPERTIES:
:CUSTOM_ID: sec:theory:buffer_gas
:END:

As seen in the conversion probability above, there is a term for an
effective photon mass $m_γ$ as part of $q$. And indeed, $q$ becomes
zero if $m_γ = m_a$, which means the suppressing effect of the $\sinc$
term disappears. This is something that can be used to increase the
conversion probability inside of a magnet, by filling it with a buffer
gas (for example helium), as introduced in
cite:raffelt1988mixing,vanBibber1989. However, one also needs to
account for the attenuation effect of the gas on the produced
X-rays. As such the derivation above needs to include this as part of
the evolution of the field $\vec{A}$ [fn:refractive_index]. By doing this and following the rest of the
derivation, the conversion probability in the presence of a buffer gas
comes out to:

#+NAME: eq:theory:full_conversion_prob
\begin{equation}
P_{a\rightarrow\gamma} = \left(\frac{g_{a\gamma} B}{2}\right)^2 \frac{1}{q^2 + \Gamma^2 / 4} \left[ 1 + e^{-\Gamma L} - 2e^{-\frac{\Gamma L}{2}} \cos(qL)\right],
\end{equation}

where $\Gamma$ is the inverse absorption length for photons (or
attenuation length), $B$ the transverse magnetic field, $L$ the length
of the magnetic field and $q$ the axion-photon momentum transfer given by:

\[
q = \left|\frac{m_{\gamma}^2 - m_a^2}{2E_a}\right|
\]

One can easily see that this reduces to the vacuum case we discussed
before, where the attenuation length $Γ$ and effective photon
mass $m_γ$ are zero.

Filling the magnet with a buffer gas to induce an effective photon
mass and thus minimizing $q$ has been done at CAST with $\ce{He4}$ and
$\ce{He3}$ fillings, as we will mention in chapter
[[#sec:helioscopes:cast]].

#+begin_quote
Note: for a potential buffer gas run in BabyIAXO I did some
calculations about the required gas pressure steps and the effect of
different possible filling configurations. These are not really
relevant for this thesis, but can be found in [fn:axion_mass_link]
both as a PDF as well as the original Org file plus the tangled source code.
#+end_quote

[fn:refractive_index] This can be done easily by treating the buffer
gas as a refractive medium with complex refractive index $n_γ = 1 -
m²_γ / (2ω²) - iΓ/2ω$, which then produces attenuation via the $Γ$
attenuation term. This is useful to know as it relates to the X-ray
properties as discussed later in sec. [[#sec:theory:xray_matter_gas]]
and sec. [[#sec:theory:xray_reflectivity]].

[fn:axion_mass_link] https://phd.vindaar.de/docs/bufferGasIAXO/v1/index.html
contains the ~axionMass.pdf~, ~axionMass.nim~ and finally
~axionMass.org~, the document from which both other files are
generated. *FIX LINK*

**** TODOs for this section [/]                                 :noexport:

- [X] *VAN BIBBER cite:vanBibber1989 IS SOURCE OF CONVERSION WITH
  ATTENUATION*
  -> Based on cite:raffelt1988mixing !

- [ ] *INSERT ~axionMass.org~ CALCULATIONS SOMEWHERE*
  -> We need to link to the 
- [ ] *INSERT OUR DOCUMENT ABOUT AXION MASS BUFFER GAS HERE*
  -> maybe in the end it might be a good idea to mention the buffer
  gas in the actual thesis in a short paragraph with a couple of words
  on how it works, but mention that this is not immediately relevant.
  Then we can link to the appendix of the extended version. Due to the
  length of this part it makes more sense to have it in the appendix
  than here where it does affect the flow otherwise.
  -> Put them into a ~docs~ directory? 

**** Simplification                                             :extended: 
The conversion probability simplifies to:

\begin{align}
P_{a\rightarrow\gamma, \text{vacuum}} &= \left(\frac{g_{a\gamma} B}{2}\right)^2 \frac{1}{q^2} \left[ 1 + 1 - 2 \cos(qL) \right] \\
P_{a\rightarrow\gamma, \text{vacuum}} &= \left(\frac{g_{a\gamma} B L}{2} \right)^2 \left(\frac{\sin\left(\frac{qL}{2}\right)}{ \left( \frac{qL}{2} \right)}\right)^2
\end{align}

*** TODOs for the section                                        :noexport:

- [X] *Mention* that A field is decoupled from background field B

- [X] Interaction eigenstates vs propagation eigenstates
- [X] Compute result for propagation eigenstates easily, because
  matrix becomes diagonal
- [X] In laboratory we measure interaction eigenstates, so the result
  of the propagation eigenstates must be rotated back
- [X] calculate transition amplitude from axion to photon by simply
  taking $A_{\parallel}²$ of rotated interaction eigenstate. As it has
  axion coupled into it, gives result if we start with initial
  conditions A|| = 0, a = 1.
- [X] How to compute version with gas attenuation of photons? Start
  not with plane wave solution of photon, but plane wave + decay? I
  suppose.
- [X] Explain origin of the conversion probability by starting with
  modified Maxwell equations and then solving the Klein-Gordon
  equation. The resulting mixing of $A_{\parallel}$ and $a$ leads to
  it.

*** Dropping an additional $Δ$ term                              :extended:

Notation:

\[
q = \frac{m²_γ - m²_a}{2ω}
\]

and

\[
Δ = \frac{-g_{aγγ} B_T}{2}
\]

In principle the conversion probability contains

\[
\sinc²(\frac{πz}{L_{\text{osc}}})
\]

with the osciallation length

\[
L_{\text{osc}} = \frac{π}{\sqrt{ \left( \frac{q}{2} \right)² + Δ² }
\]

I assume we can safely drop the term $Δ²$, as it is likely orders of
magnitude smaller than the first term. Let's check that:

#+begin_src nim
import unchained

defUnit(GeV⁻¹)
proc q(m_a: meV, ω: keV): keV =
  result = (m_a*m_a / (2 * ω)).to(keV)

proc Δ(g_aγ: GeV⁻¹, B: Tesla): keV =
  result = (-g_aγ * B.toNaturalUnit() / 2.0).to(keV)

echo "q² = ", q(1.meV, 3.keV)^2
echo "Δ² = ", Δ(1e-12.GeV⁻¹, 9.T)^2
#+end_src

#+RESULTS:
| q² | = | 2.77778e-26 keV² |
| Δ² | = | 7.72795e-43 keV² |

Yeah, as expected that's the reason. 

*** Deriving the missing constants in the conversion probability [/] :optional:

- [ ] *Move this into an appendix?*

The conversion probability is given in natural units. In order to plug
in SI units directly without the need for a conversion to natural
units for the magnetic field and length, we need to reconstruct the
missing constants.

The relevant constants in natural units are:

\begin{align*}
ε_0 &= \SI{8.8541878128e-12}{A.s.V^{-1}.m^{-1}} \\
c &= \SI{299792458}{m.s^{-1}} \\
\hbar &= \frac{\SI{6.62607015e-34}{J.s}}{2π} 
\end{align*}

which are each set to 1.

If we plug in the definition of a volt we get for $ε_0$ units of:

\[
\left[ ε_0 \right] = \frac{\si{A^2.s^4}}{\si{kg.m^3}}
\]

The conversion probability naively in natural units has units of:

\[
\left[ P_{aγ, \text{natural}} \right] = \frac{\si{T^2.m^2}}{J^2} = \frac{1}{\si{A^2.m^2}}
\]

where we use the fact that $g_{aγ}$ has units of $\si{GeV^{-1}}$ which
is equivalent to _units_ of $\si{J^{-1}}$ (care has to be taken with
the rest of the conversion factors of course!) and Tesla in SI units:

\[
\left[ B \right] = \si{T} = \frac{\si{kg}}{\si{s^2.A}}
\]

From the appearance of $\si{A^2}$ in the units of $P_{aγ,
\text{natural}}$ we know a factor of $ε_0$ is missing. This leaves the
question of the correct powers of $\hbar$ and $c$, which come out to:

\begin{align*}
\left[ ε_0 \hbar c^3 \right] &= \frac{\si{A^2.s^4}}{\si{kg.m^3}} \frac{\si{kg.m^2}}{\si{s}}  \frac{\si{m^3}}{\si{s^3}} \\
  &= \si{A^2.m^2}.
\end{align*}

So the correct expression in SI units is:

\[
P_{aγ} = ε_0 \hbar c^3 \left( \frac{g_{aγ} B L}{2} \right)^2
\]

where now only $g_{aγ}$ needs to be expressed in units of
$\si{J^{-1}}$ for a correct result using tesla and meter.


*** Full simplification of conversion probability in vacuum      :extended:

The full simplification for the vacuum case from the buffer gas
conversion probability is as follows:

\begin{align*}
P_{a\rightarrow\gamma} &= \left(\frac{g_{a\gamma} B}{2}\right)^2 \frac{1}{q^2 + \Gamma^2 / 4} \left[ 1 + e^{-\Gamma L} - 2e^{-\frac{\Gamma L}{2}} \cos(qL)\right] \\
\text{for vacuum } Γ &= 0, m_γ = 0 \text{ and thus} \\
P_{a\rightarrow\gamma, \text{vacuum}} &= \left(\frac{g_{a\gamma} B}{2}\right)^2 \frac{1}{q^2} \left[ 1 + 1 - 2 \cos(qL) \right] \\
P_{a\rightarrow\gamma, \text{vacuum}} &= \left(\frac{g_{a\gamma} B}{2}\right)^2 \frac{2}{q^2} \left[ 1 - \cos(qL) \right] \\
P_{a\rightarrow\gamma, \text{vacuum}} &= \left(\frac{g_{a\gamma} B}{2}\right)^2 \frac{2}{q^2} \left[ 2 \sin^2\left(\frac{qL}{2}\right) \right] \\
P_{a\rightarrow\gamma, \text{vacuum}} &= \left(g_{a\gamma} B\right)^2 \frac{1}{q^2} \sin^2\left(\frac{qL}{2}\right) \\
P_{a\rightarrow\gamma, \text{vacuum}} &= \left(\frac{g_{a\gamma} B L}{2} \right)^2 \left(\frac{\sin\left(\frac{qL}{2}\right)}{ \left( \frac{qL}{2} \right)}\right)^2 \\
P_{a\rightarrow\gamma, \text{vacuum}} &= \left(\frac{g_{a\gamma} B L}{2} \right)^2 \left(\frac{\sin\left(\delta\right)}{\delta}\right)^2 \\
\end{align*}

*** Details on the polarization of axion induced photons         :extended:

These notes refer to the notes by Bijlana and Kreso
[[file:~/org/Papers/Axion-photon_conversion_report_biljana.pdf]]

*UPDATE*: <2023-09-08 Fri 20:14> I just noticed the following sentence
in Biljana's notes:
#+begin_quote
The Faraday rotation term $∆_R$ , which depends on the energy and the
longitudinal component of the external magnetic field, couples the
photon polarization states A⊥ and Ak . While Faraday rotation is
important when analyzing polarized sources of photons, it plays no
role in the problem at hand.
#+end_quote
which refers to off diagonal elements $Δ_R$, which were dropped in
their calculation! While we don't consider polarized *sources*, do we
still need to consider this?


See eq. (3.4) (reproduced here)

\[
a(z, t) = a(z) e^{-i ω t} \:\: , \:\: \vec{A}(z, t) =
\vektor{A_{\perp}(z) \\ A_{\parallel}(z) \\ 0} e^{-iωt}
\]

for the definition of the axion and photon in the
context. The equation of motion is eq. (3.8)

\[
\left[ (ω² + ∂²_z) \mathbf{1} - \mathbf{M} \right] =
\vektor{A_{\perp}(z) \\ A_{\parallel}(z) \\ 0} = 0
\]

and with the definition of eq. (3.14)

\[
\mathbf{M} = \mtrix{
  m²_γ & 0    & 0 \\
  0    & m²_γ & -ω g_{aγγ} B_T \\
  0    & -ω g_{aγγ} B_T & m²_a \\
  }
  \text{ where } m²_γ = ω²_p
\]

we can (as mentioned in the text) see that indeed the orthogonal
$\vec{A}$ component is independent of the axion field (the first row
of $\mathbf{M}$ only has an entry in the first column, i.e. the
product only yields an equation for $A_{\perp}$ alone without the
axion field.

Due to the difference between the interaction ($A_{\parallel}, a$) and
propagation eigenstates ($A'_{\parallel}, a'$) (connected via a
rotation matrix) the initial separate eigenstates $A_{\parallel}$ and
$a$ end up mixing in the propagation eigenstates (see page
9). Eq. 3.34

\[
i∂_z \vektor{A'_{\parallel}(z) \\ a'(z)} = \mathbf{H_D} \vektor{A'_{\parallel}(z) \\ a'(z)}
\]

is then the equation of motion for the propagation eigenstates. The
solutions are then eq. 3.38

\begin{align*}
A_{\parallel}(z) &= A_{\parallel}(0) \left( \cos² θ e^{-iλ_+z} + \sin² θ e^{-iλ_-z}\right) +
  a(0) \frac{\sin 2θ}{2} \left( e^{-iλ_+z} - e^{-iλ_-z}\right) \\
a(z) &= A_{\parallel}(0) \frac{\sin 2θ}{2} \left( e^{-iλ_+z} - e^{-iλ_-z}\right) +
  a(0) \left( \sin² θ e^{-iλ_+z} + \cos² θ e^{-iλ_-z}\right) \\
\end{align*}

which - with our assumption in our experiment

\[
A_{\parallel}(0) = 0 \text{ and } a(0) = 1
\]

i.e. we start with purely axions and no photons before the magnet -
can then be simplified to 3.39 and 3.40

\begin{align*}
A_{\parallel}(z) &= a(0) \frac{\sin 2θ}{2} \left( e^{-iλ_+z} - e^{-iλ_-z}\right) \\
a(z) &= \sin² θ e^{-iλ_+z} + \cos² θ e^{-iλ_-z} \\
\end{align*}

What this implies is that the photon contribution after mixing that
can end up as a detected physical photon is only of $A_{\parallel}$
type, which (again if my rusty understanding is not failing me)
implies that the produced photons all have the same polarization, the
one parallel to the constant $\vec{B}$ field (compare with fig. 1).


** Solar axion flux
:PROPERTIES:
:CUSTOM_ID: sec:theory:solar_axion_flux
:END:

The effective Lagrangian as shown in
eq. [[eq:theory:axion:general_axion_couplings]] allow for multiple
different axion interactions, which allow for production of axions in
the Sun. For KSVZ-like axion models (models with only $g_{aγ}$) the
only interaction allowing for axion production in the Sun is the
Primakoff effect [fn:primakoff_effect] for axions. For DFSZ models
with an axion-electron coupling $g_{ae}$ multiple other production
channels are viable. All relevant axion production channels are:
- ($P$) Primakoff production via $g_{aγ}$ in both KSVZ and
  DFSZ axion models
  \[
  γ + γ ↦ a
  \]
- ($\text{ff}$) electron ion bremsstrahlung (in radio astronomy low
  energetic cases with photons are also called free-free radiation)
  \[
  e + Z \longrightarrow e + Z + a
  \]
- ($ee$) electron electron bremsstrahlung cite:raffelt1986astrophysical,
  \[
  e + e \longrightarrow e + e + a
  \] 
- ($\text{fb}$) electron capture (also called recombination or free-bound
  electron transitions)
  \[
  e + Z \longrightarrow a + Z^-
  \]
- ($C$) Compton scattering cite:raffelt1986astrophysical
  \[
  e + \gamma \longrightarrow e + a
  \]
- ($\text{bb}$) and de-excitation (bound-bound electron transitions) via
  an axion
  \[
  Z^* \longrightarrow Z + a
  \]
See fig. [[fig:theory:axion:axion_couplings]] for the corresponding
Feynman diagrams.

#+CAPTION: Feynman diagrams of all contributing axion production
#+CAPTION: channels in the Sun for non-hadronic models. In hadronic models only
#+CAPTION: the Primakoff has meaningful contributions, because axion-electron
#+CAPTION: couplings only arise at loop level. Taken from cite:Redondo_2013.
#+NAME: fig:theory:axion:axion_couplings
[[~/phd/Figs/axion_prod_channels_javi.pdf]]

With these production channels we can write down the production rate
per volume in the Sun as the integral
\begin{equation}
  \frac{d Φ_a}{dω} = \frac{1}{4π R²_{\text{Earth}}}
    \int_{\text{Sun}} \mathrm{d}V\, \frac{4π ω²}{ (2π)³ } Γ_a(ω).
\end{equation}
where
\begin{equation}
Γ_a(\omega) = Γ^{\text{ff}}_a + Γ^{\text{fb}}_a + Γ^{\text{bb}}_a + Γ^C_a + Γ^{ee}_a + Γ^P_a
\end{equation}
are all contributing axion production channels. The superscripts
correspond to the bullet points above.

#+begin_src subfigure
(figure ()
        (subfigure (linewidth 0.5) (caption "Differential axion flux") (label "fig:theory:solar_axion_flux:differential_flux")
                   (includegraphics (list (cons 'width (linewidth 0.95)))
                                    "~/phd/Figs/axions/differential_solar_axion_flux_by_type.pdf"))
        (subfigure (linewidth 0.5) (caption ($ (SI "15e6" "K")) " blackbody spectrum") (label "fig:theory:solar_axion_flux:blackbody")
                   (includegraphics (list (cons 'width (linewidth 0.95)))
                                    "~/phd/Figs/blackbody_spectrum_solar_core.pdf"))
        (caption
         (subref "fig:theory:solar_axion_flux:differential_flux")
         "Differential solar axion flux based on the different interaction types using "
         ($ "g_{ae} = " (num 1e-13) ", g_{aγ} = " (SI 1e-12 "GeV^{-1}"))
         ". The Primakoff contribution was scaled up by a factor 100 to make it visible, as at these coupling constants
the $g_{ae}$ contributions dominate. "
         (subref "fig:theory:solar_axion_flux:blackbody") " show a blackbody spectrum corresponding to "
         ($ (SI "15e6" "K")) ", roughly the temperature at the solar core. Up to a scaling factor it is essentially the Primakoff flux.")
        (label "fig:theory:solar_axion_flux:flux_blackbody"))
#+end_src

One of the first papers to look at the implications of the axion in
terms of astrophysical phenomena is cite:PhysRevD.18.3605 by
K. Mikaelian. Raffelt expanded on this later in
cite:raffelt1986astrophysical with calculations for the Compton and
Bremsstrahlung production rates for DFSZ axion models. In
cite:raffelt1988plasmon he further calculates the production rate
for the Primakoff effect (and later reviews the physics
cite:raffelt1996stars). J. Redondo combined all production processes
in cite:Redondo_2013 to compute a full solar axion flux based on the
axion-electron coupling $g_{ae}$ using numerical calculations of the
expected metallicity contents at different points in the Sun. In
addition making use of the opacities for different elements at
different pressures and temperatures as tabulated by the 'Opacity
Project'
cite:team1995opacity,hummer1988equation,seaton1987atomic,seaton1994opacities,badnell2005updated,seaton2005mnras.

As a result of the expected extremely low mass of the axion, the
expected solar axion spectrum for Primakoff only models is essentially
a blackbody spectrum corresponding to the temperatures near the core
of the Sun,
$\mathcal{O}(\SI{15e6}{K})$. Fig. sref:fig:theory:solar_axion_flux:flux_blackbody
shows such a blackbody spectrum next to the expected axion
spectra. [fn:blackbody_differences] Partially for this reason, in many
cases analytical expressions are given to describe the Primakoff axion
flux, which are solutions obtained for specific solar models.

In cite:Redondo_2013 these production rates are expressed by
relating them to the corresponding photon production rates for these
processes, which are well known. For a detailed look at them, see
cite:Redondo_2013 and the master thesis of Johanna von Oy
cite:vonOy_MSc in which she - among other things - reproduced the
calculations done by Redondo. Her work is used as part of this thesis
to compute the axion production as required for the expected axion
flux in the limit calculation (and provides the data for the plots in
this section). The code responsible for computing the emission rates
for different axion models is cite:JvO_axionElectron, in particular
the ~readOpacityFile~ program. It also uses the Opacity Project as the
basis to compute the opacities for different elements.

Fig. \ref{fig:theory:solar_axion_flux:flux_vs_energy_and_radius} shows
how the solar axion flux (for DFSZ models) depends both on the energy
and the relative radius in the Sun. We can see clearly that the major
part of the axion flux comes from a region between
$\SIrange{7.5}{17.5}{\%}$ of the solar radius. The reason is the cubic
scaling of the associated volumes per radius on the lower end and
dropping temperatures and densities at the upper end. Interesting
substructure due to the details of the axion-electron coupling is
visible. The radial component alone comparing it to KSVZ models is
seen in
fig. \ref{fig:theory:solar_axion_flux:radial_dependence_ksvz_dfsz},
where we can see that the DFSZ flux drops off significantly at a
specific radius resulting in the net flux from slightly smaller radii.

#+begin_src subfigure
(figure ()
        (subfigure (linewidth 0.5) (caption "Flux") (label "fig:theory:solar_axion_flux:flux_vs_energy_and_radius")
                   (includegraphics (list (cons 'width (linewidth 0.95)))
                                    "~/phd/Figs/axions/flux_by_energy_vs_radius_axion_electron.pdf"))
        (subfigure (linewidth 0.5) (caption "Radial emission") (label "fig:theory:solar_axion_flux:radial_dependence_ksvz_dfsz")
                   (includegraphics (list (cons 'width (linewidth 0.95)))
                                    "~/phd/Figs/axions/solar_axion_radial_emission.pdf"))
        (caption
         (subref "fig:theory:solar_axion_flux:flux_vs_energy_and_radius")
         "Solar axion flux for DFSZ models showing the flux dependence on both the energy and relative solar radius.
Dominant contributien below " ($ "0.3 R_{\\odot}") "."
         (subref "fig:theory:solar_axion_flux:radial_dependence_ksvz_dfsz")
         "Difference in the radial contributions of axion flux for KSVZ models against DFSZ models.
DFSZ model production is constrained to slightly smaller radii.")
        (label "fig:theory:solar_axion_flux:flux_radial"))
#+end_src

The used solar model is an important part of the input for the
calculation of the emission rate and thus differential
flux. cite:Hoof_2021 conclude - based on a similar code
cite:Hoof_SolarAxionFlux_2021 - that the statistical uncertainty of
the solar models in $\mathcal{O}(\SI{1}{\%})$, while the systematic
uncertainty can reach up to $\mathcal{O}(\SI{5}{\%})$. This is an
important consideration for the systematic uncertainties later on.

[fn:primakoff_effect] The Primakoff effect - named after Henry
Primakoff - is the production of neutral pions in the presence of a
atomic nucleus. Due to the pseudoscalar nature and coupling to photons
of both neutral pions and axions the equivalent process is allowed for
axions.

[fn:blackbody_differences] The difference of the peak position of the
blackbody spectrum and the Primakoff flux is due to the axion
production being dominantly from $\SIrange{7.5}{17.5}{\%}$ of the
solar radius (compare
fig. \ref{fig:theory:solar_axion_flux:flux_radial}) where temperatures
on average are closer to $\sim\SI{12e6}{K}$.

# [fn:sikivie_effect] The Primakoff effect for axions is sometimes also called the Sikivie
# effect, after Pierre Sikivie who realized

*** TODOs for this section [/]                                   :noexport:

- [ ] Mention Sikivie effect?

- [X] Mention axion-nucleon coupling cite:di2022probing
  -> Mentioned in the context of the effective Lagrangian


- [X] Axion flux directly follows from couplings. Show the Feynman
  diagrams from Redondo
- [ ] Show the basic gist from Redondo, i.e. Γ of the different
  contributions
  -> Not sure!
- [X] Primakoff is effectively just Blackbody spectrum *!!! WRITE ME*

- [ ] *INSERT PLOTS!!!!!!!!!*
  For Primakoff relate to blackbody? Show axion-electron + primakoff
  left, then pure blackbody at solar core on right?
- [X] Primakoff has a analytical expressions for the entire flux
  - [ ] can of course also be computed numerically from a solar model

- [X] Explain that Γ axion is being related to Γ photon
- [X] Raffelt derived a whole bunch of those Γ for axion electron
  coupling!!!
  -> cite:raffelt1986astrophysical
  There is also cite:PhysRevD.18.3605 from earlier!


*KEEP IN MIND:*
[[file:~/org/Papers/CAST/cast_phase_I_results_andriamonje2007.pdf]]
Mention it as a very succinct derivation / explanation of origin solar
axion flux etc.


Important for us? How do we detect them.

Interaction tells us conversion is proportional to B and L. Where are
strong Bs for long Ls? Solar core.
Take modern solar model to plot the density profile & especially
temperature. Density + temperature allows us to compute:
- number of photons
- at various photon energies

By wrapping blackbody radiation (ref, 3 sentences about it) present in
solar core with Primakoff coupling, we get an effective axion flux
equivalent to:

$dΦ/dE ∝ g_{aγ}² · \text{black body radiation}$

*CHECK CAST PHASE I RESULT PAPER FOR OVERVIEW* (contains physics +
integration over solar model!)
Refer to that paper in particular to answer the question: "do axions
escape from the sun?"

*BIBBER* cite:vanBibber1989 contains derivation of axion flux based
on black body radiation. First CAST paper bases their flux on this,
with a modification from some other paper & a newer solar model from
2001 ("reference" 15 in that CAST paper). This reference *also*
contains a derivation of axion equations of motion etc. via KG
equation.

\begin{equation}
  \frac{d N_a}{dV\, dt} = \int \frac{\mathrm{d}^3
    \mathbf{k}}{(2\pi)^3} \Gamma^P_a(\omega) = \int^{\infty}_0
  \frac{\omega^2 \mathrm{d}\omega}{2\pi^2} \Gamma^P_a(\omega),
\end{equation}

*** Primakoff flux                                               :extended:

Including analytical equation for flux... :)

#+begin_src nim :tangle /tmp/solar_axion_flux.nim :results silent
import unchained, ggplotnim, math, chroma, ginger
defUnit(keV⁻¹•m⁻²•yr⁻¹)
defUnit(keV⁻¹•cm⁻²•s⁻¹)
defUnit(GeV⁻¹)

proc axionFluxPrimakoff(E_a: keV, g_aγ: GeV⁻¹): keV⁻¹•cm⁻²•s⁻¹ =
  ## dΦ_a/dE taken from paper about first CAST results cite:PhysRevLett.94.121301
  let g₁₀ = g_aγ / 1e-10.GeV⁻¹ # * 10e10.GeV¹ #
  result = g₁₀^2 * 3.821e10.cm⁻²•s⁻¹•keV⁻¹ * (E_a / 1.keV)^3 / (exp(E_a / (1.103.keV)) - 1)

proc axFluxPerYear(E_a: keV, g_aγ: GeV⁻¹): keV⁻¹•m⁻²•yr⁻¹ =
  result = axionFluxPrimakoff(E_a, g_aγ).to(keV⁻¹•m⁻²•yr⁻¹)

proc axionFluxPrimakoffMasterThesis(ω: keV, g_ay: GeV⁻¹): keV⁻¹•m⁻²•yr⁻¹ =
  # axion flux produced by the Primakoff effect
  # in units of m^(-2) year^(-1) keV^(-1)
  # From the CAST 2013 paper on axion electron coupling, eq 3.1
  result = 2.0 * 1e18.keV⁻¹•m⁻²•yr⁻¹ * (g_ay / 1e-12.GeV⁻¹)^2 * pow(ω / 1.keV, 2.450) * exp(-0.829 * ω / 1.keV)

let E = linspace(1e-3, 14.0, 1000)
let df = seqsToDf(E)
  .mutate(f{float: "Flux" ~ axionFluxPrimakoff(`E`.keV, 1e-11.GeV⁻¹).float})
  .mutate(f{float: "FluxYr" ~ axFluxPerYear(`E`.keV, 1e-11.GeV⁻¹).float})
  .mutate(f{float: "FluxMSc" ~ axionFluxPrimakoffMasterThesis(`E`.keV, 1e-11.GeV⁻¹).float})    
ggplot(df, aes("E", "Flux")) +
  geom_line() +
  #geom_line(aes = aes(y = "FluxMSc"), color = some(parseHex("0000FF"))) + 
  ggtitle("Solar axion flux due to Primakoff production, g_aγ = 10⁻¹¹·GeV⁻¹") +
  xlab("Energy [keV]") +
  #ylab("Axion flux [keV⁻¹·cm⁻²·s⁻¹]") +
  ylab("Axion flux [keV⁻¹·m⁻²·yr⁻¹]") +  
  ggsave("/tmp/primakoff_axion_flux.pdf")

ggplot(df.mutate(f{"Flux" ~ `Flux` / 1e8}), aes("E", "Flux")) +
  geom_line() +
  #xlab("Energy [keV]", tickFont = font(12.0), margin = 1.5) +
  xlab(r"\fontfamily{lmss}\selectfont Energy [$\si{\keV}$]", margin = 2.0, font = font(16.0),
       tickFont = font(16.0)) +
  xlim(0, 14) + 
  #ylab("Axion flux [10¹⁰ keV⁻¹·cm⁻²·s⁻¹]", margin = 1.5) +
  ylab(r"\fontfamily{lmss}\selectfont Axion flux [\SI[print-unity-mantissa=false]{1e11}{\keV^{-1} \cm^{-2} \second^{-1}}]",
       margin = 2.0,
       font = font(16.0)) + 
  #     tickFont = font(12.0)) +
  #ggtitle(r"Expected solar axion flux, g_aγ = 10⁻¹⁰ GeV⁻¹", titleFont = font(12.0)) +
  annotate(r"\fontfamily{lmss}\selectfont Expected solar axion flux" &
    r"\\$g_{aγ} = \SI[print-unity-mantissa=false]{1e-11}{\GeV^{-1}}$", #10⁻¹⁰ GeV⁻¹",
           x = 6.2, y = 6.2, 
           font = font(16.0),
           backgroundColor = transparent) +
  #ggtitle(r"Expected solar axion flux, $g_{aγ} = \SI{1e-11}{\GeV^{-1}}$", titleFont = font(12.0)) + 
  #ggsave("/tmp/cristina_primakoff_axion_flux.pdf", width = 400, height = 300) #, useTeX = true, standalone = true)
  ggsave("/tmp/cristina_primakoff_axion_flux.pdf", useTeX = true, standalone = true)
  

defUnit(m⁻²•yr⁻¹)  
echo 1.cm⁻²•s⁻¹.to(m⁻²•yr⁻¹)
#+end_src

There are different analytical expressions for the solar axion flux
for Primakoff production. These stem from the fact that a solar model
is used to model the internal density, temperature, etc. in the Sun to
compute the photon distribution (essentially the blackbody radiation)
near the core. From it (after converting via the Primakoff effect) we
get the axion flux.

Different solar models result in different expressions for the
flux. The first one uses an older model, while the latter ones use
newer models.

*** Axion-electron flux                                          :noexport:

*citations*: Redondo 2013, maybe (Johanna + Sebastian Hoof something?)
*Keep in mind errors in Redondo 2013*! *possibly write a mail to Sebastian Hoof*

Expected axion flux combined.

Reference to file storing the results for specific coupling constants.

Much more complicated.

ABC components.

B and C can be expressed analytically.

A cannot, needs opacity project.

Show plot of differential axion flux.

For a derivation of this, consider section about ray tracing. Custom
computation of A done by Johanna in code developed by her & me in
*LINK*.

*** Generate figures for Primakoff and axion flux from ~readOpacityFile~ output :extended:

- Use logic we use in ~TrAXer~ to compute differential fluxes as a
  function of radii.
- [X] Do it for each type of flux independently
- [X] Compute a smooth version of the *radial distribution* comparing
  axion-electron to Primakoff!
- [X] Black body spectrum
  -> Produced below in a separate plot!
- [X] Radial emission vs energy heatmap!!

First the differential flux by type:
#+begin_src nim :results drawer :flags -d:experimentalSDL2 -d:QuietTikZ=true
import ggplotnim
const fluxPath = "~/org/resources/solar_axion_flux_differential_g_ae_1e-13_g_ag_1e-12_g_aN_1e-15.csv"
let df = readCsv(fluxPath)
  .filter(f{string -> bool: `type` notin ["LP Flux", "TP Flux", "57Fe Flux"]})
  .mutate(f{float: "diffFlux" ~ (if (idx("type", string) == "Primakoff Flux"): `diffFlux` * 100 else: `diffFlux`)})
  .mutate(f{string: "type" ~ (if (`type` == "Primakoff Flux"): "Primakoff·100" else: `type`)})
ggplot(df, aes("Energy","diffFlux", color = "type")) +
  geom_line() +
  xlab(r"Energy [$\si{keV}$]") + ylab(r"Flux [$\si{keV^{-1}.m^{-2}.yr^{-1}}$]") +
  xlim(0, 15) + 
  ggtitle("Differential axion flux arriving on Earth by type") + 
  # ggshow(800, 480)
  ggsave("~/phd/Figs/axions/differential_solar_axion_flux_by_type.pdf", useTeX = true, standalone = true, width = 600, height = 360)
#+end_src

#+RESULTS:
:results:
[INFO] TeXDaemon ready for input.
shellCmd: command -v xelatex
shellCmd: xelatex -output-directory /home/basti/phd/Figs/axions /home/basti/phd/Figs/axions/differential_solar_axion_flux_by_type.tex
Generated: /home/basti/phd/Figs/axions/differential_solar_axion_flux_by_type.pdf
:end:


Now the radial distribution of the flux by coupling constant.
#+begin_src nim :flags -d:danger :results none
import std / [sequtils, algorithm]
import ggplotnim, unchained
import ggplotnim/ggplot_sdl2
type
  FluxData* = object
    fRCdf*: seq[float]
    diffFluxR*: seq[seq[float]]
    fluxesDf*: DataFrame
    radii*: seq[float]
    energyMin*: float
    energyMax*: float

const
  alpha = 1.0 / 137.0
  g_ae = 1e-13 # Redondo 2013: 0.511e-10
  gagamma = 1e-12 #the latter for DFSZ  #1e-9 #5e-10 #
  ganuclei = 1e-15 #1.475e-8 * m_a #KSVZ model #no units  #1e-7
  m_a = 0.0853 #eV
  m_e_keV = 510.998 #keV
  e_charge = sqrt(4.0 * PI * alpha)#1.0
  kB = 1.380649e-23
  r_sun = 696_342_000_000.0 # .km.to(mm).float # SOHO mission 2003 & 2006
  hbar = 6.582119514e-25 # in GeV * s
  keV2cm = 1.97327e-8 # cm per keV^-1
  amu = 1.6605e-24 #grams
  r_sunearth = 150_000_000_000_000.0

const factor = pow(r_sun * 0.1 / (keV2cm), 3.0) /
               (pow(0.1 * r_sunearth, 2.0) * (1.0e6 * hbar)) /
               (3.1709791983765E-8 * 1.0e-4) # for units of 1/(keV y m²)

import ggplotnim
proc getFluxRadiusCDF*(path: string): FluxData =
  var emRatesDf = readCsv(path)
  # get all radii and energies from DF so that we don't need to compute them manually (risking to
  # messing something up!)
  # sort both just to make sure they really *are* in ascending order
  let radii = emRatesDf["Radius"]
    .unique()
    .toTensor(float)
    .toSeq1D
    .sorted(SortOrder.Ascending)
  let energies = emRatesDf["Energy [keV]"]
    .unique()
    .toTensor(float)
    .toSeq1D
    .sorted(SortOrder.Ascending)
  var emRates = newSeq[seq[float]]()
  ## group the "solar model" DF by the radius & append the emission rates for all energies
  ## to the `emRates`
  for tup, subDf in groups(emRatesDf.group_by("Radius")):
    let radius = tup[0][1].toFloat
    #doAssert subDf["Energy [keV]", float].toSeq1D.mapIt(it.keV) == energies
    #doAssert radius == radii[k], "Input DF not sorted correctly!"
    emRates.add subDf["emRates", float].toSeq1D
  var
    fluxRadiusCumSum: seq[float] = newSeq[float](radii.len)
    diffRadiusSum = 0.0

  template toCdf(x: untyped): untyped =
    let integral = x[^1]
    x.mapIt( it / integral )

  var fluxesDf = newDataFrame()
  var diffFluxR = newSeq[seq[float]](radii.len)
  var rLast = 0.0
  for iRad, radius in radii:
    # emRates is seq of radii of energies
    let emRate = emRates[iRad]
    var diffSum = 0.0
    var diffFlux = newSeq[float](energies.len)
    for iEnergy, energy in energies:
      let dFlux = emRate[iEnergy] * (energy.float*energy.float) * radius*radius * (radius - rLast) * factor
      diffFlux[iEnergy] = dFlux
      diffSum += dFlux
    fluxesDf.add toDf({"Energy" : energies.mapIt(it.float), "Flux" : diffFlux, "Radius" : radius})      
    diffRadiusSum += diffSum
    fluxRadiusCumSum[iRad] = diffRadiusSum
    diffFluxR[iRad] = diffFlux
    rLast = radius
  result = FluxData(fRCdf: fluxRadiusCumSum.toCdf(),
                    fluxesDf: fluxesDf,
                    diffFluxR: diffFluxR,
                    radii: radii,
                    energyMin: energies.min,
                    energyMax: energies.max)

proc fluxToDf(data: FluxData, typ: string): DataFrame =
  result = toDf({ "Radius" : data.radii,
                  "fluxPerRadius" : data.diffFluxR.mapIt(it.sum),
                  "Type" : typ })
    .mutate(f{"fluxPerRadius" ~ `fluxPerRadius` / col("fluxPerRadius").max})
proc main =
  const ksvzPath = "~/CastData/ExternCode/AxionElectronLimit/resources/solar_model_dataframe_fluxKind_fkAxionPhoton_0.989AU.csv"
  const dfszPath = "~/CastData/ExternCode/AxionElectronLimit/resources/solar_model_dataframe_fluxKind_fkAxionElectronPhoton_0.989AU.csv"
  let ksvz = getFluxRadiusCdf(ksvzPath)
  let dfsz = getFluxRadiusCdf(dfszPath)
  var df = newDataFrame()
  df.add ksvz.fluxToDf("KSVZ"); df.add dfsz.fluxToDf("DFSZ")
  ggplot(df, aes("Radius", "fluxPerRadius", color = "Type")) +
    geom_line() +
    xlab("Relative solar radius", margin = 1.25) + ylab("Relative emission", margin = 1.75) +
    ggtitle("Radial emission profile of KSVZ ($g_{aγ}$) and DFSZ ($g_{aγ}, g_{ae}$) axion models", titleFont = font(11.0)) +
    #ggshow() 
    ggsave("/home/basti/phd/Figs/axions/solar_axion_radial_emission.pdf", useTeX = true, standalone = true, width = 800, height = 480)

  echo dfsz.fluxesDf
  let dfFlux = dfsz.fluxesDf
    .filter(f{`Energy` <= 10.0 and `Radius` <= 0.5})
  ggplot(dfFlux, aes("Energy", "Radius", fill = "Flux")) +
    geom_raster() +
    scale_fill_continuous(scale = (0.0, percentile(dfFlux["Flux", float], 99))) +
    ggtitle("Flux by energy and fraction of solar radius") +
    xlab(r"Energy [$\si{keV}$]", margin = 1.25) +
    ylab(r"Relative solar radius", margin = 1.75) +
    xlim(0, 10) + ylim(0, 0.5) +
    ggsave("/home/basti/phd/Figs/axions/flux_by_energy_vs_radius_axion_electron.pdf", useTeX = true, standalone = true)
    #ggshow() # 640, 480)
    
main() 
#+end_src

Hmm, this looks a bit bizarre. But comparing with
- [[~/org/Figs/statusAndProgress/axionProduction/sampled_radii_axion_electron.pdf]]
- [[~/org/Figs/statusAndProgress/axionProduction/sampled_radii_primakoff.pdf]]
the weird structure of the axion-electron emission is actually visible
a bit (in the form of sharp drops, in particular near 0.2). The question is really "why", but well. If it was some kind of
sampling issue, I would assume it comes out of the solar model.

*** Black body radiation in solar core                           :extended:

Let's compute the black body radiation for the solar core and see if
it matches the energy spectrum we expect for axions.

Planck's law is defined as *CITE SOMETHING*:

\[
B_ν(ν, T) = \frac{2hν³}{c²} \frac{1}{e^{hν/kT} - 1}
\]

where $ν$ is the frequency of the photon and $T$ the temperature in
Kelvin. $k$ is of course the Boltzmann constant and $h$ the Planck
constant. Let's see what this looks like for $T =
\SI{15}{\mega\kelvin}$.

[[~/phd/Figs/blackbody_spectrum_solar_core.pdf]]


#+begin_src nim :tangle /home/basti/phd/code/black_body_sun_core.nim :results drawer :flags -d:QuietTikZ=true
import ggplotnim, unchained, sequtils
import ggplotnim / ggplot_sdl2

#defUnit(s⁻¹)
#defUnit(μs⁻¹)
defUnit(Watt•Steradian⁻¹•Meter⁻²•NanoMeter⁻¹)
defUnit(Joule•Meter⁻²•Steradian⁻¹)

let T_sun = 15.MegaKelvin.to(Kelvin)

proc blackBody(ν: s⁻¹, T: Kelvin): Joule•Meter⁻²•Steradian⁻¹ =
  result = (2 * hp * ν^3 / c^2 / (exp(hp * ν / (k_B * T)) - 1)).to(Joule•Meter⁻²•Steradian⁻¹)

proc xrayEnergyToFreq(E: keV): s⁻¹ = 
  ## converts the input energy in keV to a correct frequency
  result = E.to(Joule) / hp
echo 1.keV.xrayEnergyToFreq
echo "Solar core temperature ", T_Sun, " in keV : ", T_Sun.toNaturalUnit().to(keV)
echo blackBody(1.μHz.to(Hz), T_sun)
echo blackBody(1.keV.xrayEnergyToFreq, T_sun)

let energies = linspace(0.01, 15.0, 1000)
let radiance = energies.mapIt(blackBody(it.keV.xrayEnergyToFreq, T_sun).float)
let df = seqsToDf(energies, radiance)
ggplot(df, aes("energies", "radiance")) + 
  geom_line() + 
  ggtitle(r"Black body radiation @ $T = \SI{15e6}{K}$") +
  xlab(r"Energy [$\si{keV}$]") +
  ylab(r"Radiance [$\si{J.m^{-2}.sr^{-1}}$]") +
  xlim(0, 15) +
  #ggshow() 
  ggsave("/home/basti/phd/Figs/blackbody_spectrum_solar_core.pdf", useTeX = true, standalone = true, width = 600, height = 360)
#+end_src

#+RESULTS:
:results:
2.41799e+17 Hz
Solar core temperature 1.5e+07 K in keV : 1.2926 keV
inf J•sr⁻¹•m⁻²
178.526 J•sr⁻¹•m⁻²
:end:


** TODO possibly add chameleons?

?? will depend on whether we do a chameleon limit (which we should, as
our detector is much better here!)

Should be easy after all, as everything is the same as for axions,
except different flux, raytracing and thus limit calc (from a number
perspective; concept is the same).

** Current bounds on axion couplings
:PROPERTIES:
:CUSTOM_ID: sec:theory:current_bounds
:END:

The field of axion searches is expanding rapidly in recent years,
especially in haloscope experiments. A thorough overview of all the
different possible ways to constrain axion couplings and best limits
in each of them is not in the scope of this thesis. We will give a
succinct overview of the general ideas and reference the best current
limits on the relevant coupling constants in the regions of interest
for this thesis. A great, frequently updated overview of the current
best axion limits is maintained by Ciaran O'Hare, available here
cite:ciaran_o_hare_2020_3932430.

#+begin_quote
Note that in the below mentioned coupling constants technically are
squared, due to the conversion probability being the squared
conversion amplitude. 
#+end_quote

Generally, axion couplings can be probed by three main avenues:

- Pure, indirect astrophysical constraints :: Different astrophysical phenomena
  can be used to study and constrain axion couplings. One example is
  the cooling rate of stars. If axions were produced inside of stars
  and generally manage to leave the star without interaction, they
  carry energy away. Similar to neutrinos they would therefore
  contribute to stellar cooling. From observed cooling rates and solar
  models constraints can be set on potential axion contributions. Many
  other astrophysical sources can be probed in similar ways. In all
  cases these constraints are indirect in nature. Which coupling can
  constrained depends on the physical processes considered.
- Direct astrophysical constraints :: Certain types of laboratory
  experiments attempt to measure axions directly and produce
  constraints due to non-detection. Solar helioscopes attempt to
  directly measure axions produced in the Sun - more on these in
  chapter [[#sec:helioscopes]]. Haloscope experiments utilize microwave
  cavities in an attempt to tune to the frequency resonant with the
  axion mass of cold axions part of the dark matter halo. While
  relying on astrophysically produced axions, the intent is direct
  detection. Recent interest in axions also means data from WIMP
  experiments like the XENON collaboration
  [[cite:&aprile17_xenon_dark_matter_exper]] is being analyzed for axion
  signatures. Haloscopes and helioscopes depend on the axion-photon
  coupling $g_{aγ}$, while WIMP experiments may consider the
  axion-electron $g_{ae}$ or axion-nucleon $g_{aN}$ couplings. For the
  astrophysical production mechanism an additional dependency on the
  coupling producing the axion source is added, which may result in
  only being able to give constraints on products of different
  couplings.
- Direct production constraints :: The final approach is full
  laboratory experiments, which both attempt to first _produce_ axions
  and then _detect_ them. This idea is commonly done in so called
  'light shining through the wall' (LSW) experiments like the ALPS
  experiment at DESY [[cite:&baehre13_any_light_partic_searc_ii]]. Here, a
  laser cavity in a magnetic field is intended as an axion production
  facility. These produced axions would leave the cavities, propagate
  through some kind of wall (e.g. lead) and enter a second set of
  equivalent cavities, just without an active laser. Produced axions
  could convert back into photons in the second set of cavities. Here
  one deals with a $g_{aγ}$ coupling, but at the disadvantage of also
  producing at that coupling. [fn:amount_axions_lsw]
  
The bounds of interest for this thesis are the axion-photon coupling
$g_{aγ}$ for masses below around $\SI{100}{meV}$ [fn:mass_expl] and
the product of the axion-photon and axion-electron couplings
$g_{ae}·g_{aγ}$ in similar mass ranges. The latter for the case of
dominant axion-electron production $g_{ae}$ in the Sun and detection
via $g_{aγ}$ in the CAST magnet.

The current best limit on the axion-photon coupling from laboratory
experiments is from the CAST Nature paper in 2017 cite:cast_nature,
providing a bound of

\[
g_{aγ} = \SI{0.66e-10}{GeV^{−1}} \text{ at } \SI{95}{\%} \text{ CL}.
\]

Similarly, for the product on the axion-electron and axion-photon
couplings, the best limit from a helioscope experiment is also from
CAST in 2013 [[cite:Barth_2013]]. This limit is

\[
g_{ae}·g_{aγ} \lesssim \SI{8.1e-23}{GeV^{-1}} \text{ for } m_a \leq \SI{10}{meV}
\]

and acts as the main comparison to the the results presented in this thesis.

From astrophysical processes the brightness of the tip of the
red-giant branch (TRGB) stars is the most stringent way to restrict
the axion-electron coupling $g_{ae}$ alone. This is because axion
production would induce more cooling, which would lead to a larger
core mass at helium burning ignition, resulting in brighter TRGB
stars. [[cite:&capozzi20_axion_neutr_bound_improv_with]] calculate a limit
of $g_{ae} = \num{1.3e-13} \text{ at } \SI{95}{\%} \text{ CL}$. A
similar limit is obtained in
cite:&straniero20_rgb_tip_galac_globul_clust. cite:&bertolami14_revis_axion_bound_from_galac
compute a comparable limit from the White Dwarf luminosity
function. However, for purely astrophysical coupling constraints the
strong assumptions needing to be made about the underlying physical
processes imply these limits are by themselves not sufficient. In fact
there is reason to believe at least some of the current astrohpysical
bounds are overestimated. In cite:&dennis2023tip the authors use a
machine learning (ML) model to predict the brightness of TRGB stars to
allow for much faster simulations of the parameter space relevant for
such bounds. Using Markov Chain Monte Carlo models based on the ML
output, they show values up to $g_{ae} = \num{5e-13}$ are not actually
excluded if the full uncertainty of stellar parameters is included. As
their calculations only went up to such values, even larger couplings
may still be allowed.

Finally, observations from X-ray telescope can also be used for limits
on the product of $g_{ae}·g_{aγ}$. This has been done in
cite:&PhysRevLett.123.061104 based on data from the Suzaku mission and
followed up on by the same authors in
cite:&dessert22_no_eviden_axion_from_obser using data from the Chandra
mission. In the latter they compute an exceptionally strong limit of

\[
g_{ae} · g_{aγ} < \SI{1.3e-25}{GeV^{-1}} \text{ at } \SI{95}{\%} \text{ CL},
\]

valid for axion masses below $m_a \lesssim \SI{5e-6}{eV}$.


[fn:amount_axions_lsw] While astrophysical sources of course also
introduce an additional $g²$ from their production (resulting in all
experiments depending on $g⁴$ effectively), the advantage is that in
absolute terms an astrophysical source produces orders of magnitude
more axion flux than a LSW experiment. In that sense LSW experiments
deal with a squared suppression over those depending on astrophysical
sources.

[fn:mass_expl] $\SI{100}{meV}$ is the range in which the CAST
experiment is not in the fully coherent regime of the $\sinc$ term of
the conversion probability anymore, resulting in significant
sensitivity loss.

*** Potential solar axion hints

For completeness a few words about previous results which do not
actually provide a limit, but rather show small hints of possible
axion signals. 

One of the first credible hints of such a solar axion signal comes
from the XMM Newton telescope. Seasonal variation in the X-ray flux at
a level of $11σ$ is observed. The explanation provided in
[[cite:&fraser14_poten_solar_axion_signat_x]] is axion reconversion into
X-rays in Earth's magnetic field. While criticism exists
[[cite:&roncadelli15_no_axion_from_sun]], there is anyhow recent interest
in this signal [[cite:&ge22_x_ray_annual_modul_obser]], this time due to
axion quark nuggets (AQN). An AQN explanation would produce a signal
up to \SI{100}{keV} outside the XMM Newton sensitive range. The
authors propose to check archival data of the Nuclear Spectroscopic
Telescope Array (NuSTAR) or the Gamma-Ray Burst Monitor of the FERMI
telescope for such seasonal variations.

Further, while the CAST Nature result [[cite:&cast_nature]] provides the
current best limit on the axion-photon coupling, its axion candidate
dataset actually shows a signal excess at \SI{3}{keV} at a $3σ$
level. Statistical effects or possibly not perfectly accounted for
systematic variation resulting in slightly more Argon fluorescence
during tracking data is more likely. 

In 2020 the XENON collaboration announced having seen an excess in
their electron recoil counts at energies compatible with a solar axion
signal [[cite:aprile20_exces_elect_recoil_event_xenon]]. A possible
explanation not requiring solar axions was given as trace amounts of
tritium below their sensitivity threshold. This garnered a lot of
attention, because while only of $3.4σ$ significance it was the first
hint of a potential solar axion signal published by a large
collaboration as such. However, combining the resulting axion coupling
with astrohysical results indicates a more likely non axion origin for
the signal
[[cite:&athron21_global_fits_axion_like_partic;cite:&luzio20_solar_axion_cannot_explain_xenon_exces]].
With the release the first XENONnT results in 2022
[[cite:&aprile22_searc_new_physic_elect_recoil]] about new physics in
which no excess is visible, the old result is ruled out. Add to that,
the LUX-ZEPLIN collaboration, a similar xenon filled experiment,
recently published their results on new physics. Although not as
sensitive as XENONnT, but more sensitive than XENON1T, no excess was
observed there either [[cite:&lux_zeppelin_2023]]. 


*** A few more words on haloscopes                               :optional:

A haloscope is a type of axion experiment consisting of a (typically
microwave) cavity placed in a magnetic field. It intends to detect
axions of the dark matter halo of our galaxy. Axions that are part of
the dark matter component are necessarily very low energy as they
decoupled long ago and underwent cooling ever since. Thus, their
energies are in the microwave range. If a cavity has a resonance
frequency matching the axion mass (the kinetic energy is negligible,
so the majority of the energy is in the mass), the conversion probability is
enhanced by the quality factor $Q$ of the cavity (effectively the
number of reflections in the cavity). The upside of such experiments
are the strong enhancements possible, which allow to reach very low
coupling constants. However, a cavity has a single resonance
frequency, limiting the mass range to be studied to a very narrow
range. Most experiments use cavities that can be tuned to expand the
mass range. At each tuned frequency data is taken for a fixed amount
of time to reach a certain coupling constant. As such a tunable cavity
experiment can scan a narrow band of axion masses over the course of
its data taking campaign. Due to the simplicity of the setup these
type of experiments are very popular nowadays.


*** TODO about this section [/]                                  :noexport:

Astronomical axion bounds.

Cavity bounds.

Helioscope bounds.

-


- [X] Mention CAST Nature excess
- [X] Mention XMM Newton 2014 signal, mention 2022 update 
- [X] Mention that TIP paper somewhat brings in doubt direct
  rejections of axion + astrohysical rejections of XENON result

*Talk about:*
- [X] cite:dennis2023tip
*!!!*

*CITE*: X-Ray Signatures of Axion Conversion in Magnetic White Dwarf
Stars cite:PhysRevLett.123.061104 with limit g_ae·g_aγ = 2e-24!
-> They also show a plot of g_ae g_aγ exclusion
- [X] *TODO*: include newest Chandra results for coupling constant if
  they exist? +I don't think so.+ Yes, it does cite:dessert22_no_eviden_axion_from_obser.
  

- [X] *TODO*: include newest Chandra results for coupling constant
  -> Mostly for ultra light <1e-12 eV masses!
  https://iopscience.iop.org/article/10.3847/1538-4357/ab6a0c/meta
  https://academic.oup.com/mnras/article/510/1/1264/6448485
  But this:
  https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.128.071102
  goes to around m_a < 5e-6 and g_aγ < 5.5e-13.
  -> Not these, but the above is included and more relevant.
  

- [X] *TODO*: include Xenon-1T results

- [X] *TODO*: include the new result I sent to Cristina of that other
  XenonNT like experiment showing limit of g_ae ~ 4e-12 or whatever it was.
  -> Can also find it by looking for tweet of Will cosmology guy
  commenting something like "suprise they didn't find anything"
  -> Found it:
  https://arxiv.org/2307.15753 !!

- [X] Mention Ciaran O'Hare's github page about axion limits!  
  
* Axion helioscopes                                                  :Theory:
:PROPERTIES:
:CUSTOM_ID: sec:helioscopes
:END:
#+LATEX: \minitoc

As discussed in the previous chapter in section
[[#sec:theory:solar_axion_flux]] stars are strong axion factories. In 1983
Pierre Sikivie proposed cite:PhysRevLett.51.1415,PhysRevD.32.2988
multiple methods to potentially detect axions, one of these making use
of this solar axion production. The fundamental realization is that a
transverse magnetic field can act as a means to reconvert axions back
into photons, as discussed in
sec. [[#sec:theory:axion_interactions]]. This allows to design a kind of
telescope consisting of a magnet, which tracks the Sun. One expects a
small fraction of the axions produced in the Sun to reconvert to
photons inside of the magnetic field via the inverse Primakoff
effect. These photons will carry the energy of the original particles
that produced the axions, namely the energy corresponding to the
temperature in the solar core. Some kind of X-ray detector is
installed behind the magnet. Ideally, an X-ray telescope can be added
to focus any potential X-rays from the full magnet volume onto a
smaller area of the detector to massively increase the signal to noise
ratio.

The first implementation of the helioscope idea was the
Rochester-Brookhaven-Florida experiment
cite:vanBibber1989,PhysRevLett.69.2333. It was followed by the SUMICO
experiment in Tokyo cite:MORIYAMA1998147,INOUE200218,INOUE200893. The
third and latest helioscope is the CERN Axion Solar Telescope (CAST),
which we will present in more detail in section
[[#sec:helioscopes:cast]]. In the final section we will introduce the next
generation of axion helioscopes, the International AXion Observatory
(IAXO), section [[#sec:helioscopes:iaxo]].

*** TODOs for this section [/]                                   :noexport:

*EXPLAIN WHY TRANSVERSE MAGNETIC FIELD IN THEORY*
- [X] Explained in theory section, derivation via KG eq

*PUT PRIMAKOFF FEYNMAN DIAGRAM*
- [X] Done
*POSSIBLY MOVE TO THEORY ITSELF AND REFERENCE*
- [ ] Not everything referenced
  
*DIFFERENTIATE BETWEEN PRIMAKOFF AND INVERSE PRIMAKOFF*


*INSERT FIG BLACKBODY HERE OR IN SOLAR AXION FLUX SECTION*
Essentially black body radiation of $\sim\mathcal{O}(\SI{15}{\mega\kelvin}$.
This means the reconverted photons are mostly in the soft X-ray range
between \SIrange{1}{7}{\keV}.
- [X] In theory about solar axion flux


- [ ] This paragraph can go, redundant.
From the theory on axions (ref. section
[[#sec:theory:axion_interactions]]) we know there is an effective coupling
to the photon $g_{aγ}$. This coupling is an equivalent to the
Primakoff effect, which describes a resonant production of mesons via
a Fermion loop in strong electromagnetic fields when interacting with
a nucleus. In the Primakoff effect two photons are present, an
incoming real photon and a virtual photon of the electromagnetic
interaction of the nucleus. Axions can take the place of the physical
photon, either in the initial state or in the final state. In the
former case we have an axion to photon conversion and in the latter a
photon to axion conversion.

As it turns out, the relevant aspect for the Primakoff effect is not
the presence of a nucleus, but simply the fact that the nucleus
provides an electromagnetic field. This means the nucleus can also be
replaced by - for example - a transverse, constant magnetic field.


** TODO small section about other kinds of experiment?  :noexport:

** CERN Axion Solar Telescope (CAST)
:PROPERTIES:
:CUSTOM_ID: sec:helioscopes:cast
:END:

The CERN Axion Solar Telescope (CAST) was proposed in 1999
cite:ZIOUTAS1999480 and started data taking in 2003
cite:PhysRevLett.94.121301. Fig. [[fig:helioscopes:cast:cast]] shows a
panorama view of the experiment in its final year of data taking
during some maintenance work.

#+CAPTION: Panorama view of the CAST experiment during some maintenance work.
#+NAME: fig:helioscopes:cast:cast
[[~/phd/Figs/CAST_panorama_mine.jpg]]

Using a \SI{9.26}{m} long Large Hadron Collider (LHC) prototype dipole
magnet that was available from the developments for the LHC, CAST
features an \SI{8.8}{T} [fn:magnetic_field] strong transverse magnetic
field for axion-photon conversion produced by a current of \SI{13}{kA}
in the superconducting $\ce{Nb Ti}$ wires cite:bona1992design at
\SI{1.8}{K} cite:bona1994performance. It is placed on a movable
platform that allows for solar tracking both during sunrise as well as
sunset. The vertical range of movement is about $\sim\pm\SI{8}{°}$,
but practically within $\sim\SIrange{-7}{7.7}{°}$ [fn:angles_origin]. This range of
motion allows for solar tracking of approximately \SI{90}{\minute}
per day, the exact duration depending on time of the year. Due to the
incredibly feeble interactions of axions, solar tracking can already
start before sunrise and stop after sunset as they easily traverse
through large distances of Earth's mantle.

An LHC dipole magnet has two bores for the two proton beams running in
reverse order. Being a prototype magnet it is *not* bent to the
curvature required by the LHC. These two bores have a diameter of
\SI{4.3}{cm}. In total then, two bores on each side allow for 4
experiments to be installed at CAST, two for data taking during
sunrise and two during sunset. [fn:confusion_bore_diameter]

The first data taking period (often referred to as 'phase I') took
place in 2003 for 6 months between May and November and was a pure
vacuum run with 3 different detectors. On the side observing during
sunset was a Time Projection Chamber (TPC) that covered both bores. On
the 'sunrise' side a Micromegas detector (Micromesh Gaseous Detector)
and a Charged Coupled Device (CCD) detector were installed. The CCD
was further behind a still in place X-ray telescope originally
designed as a spare for the ABRIXAS X-ray space telescope
cite:ABRIXAS_0a,ABRIXAS_0b. cite:PhysRevLett.94.121301

The full first phase I data taking period comprises of data taken in
2003 and 2004 and achieved a best limit of $g_{aγ} < \SI{8.8e-11}{\GeV^{-1}}$ cite:Andriamonje_2007.

In what is typically referred to as 'phase II' of the CAST data
taking, the magnet was filled with helium as a buffer gas to increase
the sensitivity for higher axion masses by inducing an effective
photon mass (as mentioned in sec. [[#sec:theory:buffer_gas]]). First
between late 2005 and early 2007 with $^4\text{He}$. From March 2008 a
run with $^3\text{He}$ was started, which ran until 2011
cite:Arik_2009,PhysRevD.92.021101. \num{160} steps of different gas
pressures were used to optimize sensitivity against time per step. In
2012 another $^4\text{He}$ data run took place
cite:PhysRevD.92.021101.

From 2013 on the CAST experiment has only taken under vacuum
configuration cite:&cast_nature. Further, the physics scope has been
extended to include searches for chameleons
cite:krieger2018search,krieger_chameleon_jcap,chameleons_sdd_cast,justin_phd,kwisp_first_results
and axions in the galactic halo via cavity experiments
cite:sergio_phd,rades_2021,marios_phd,cast_capp_nature.

A video showing the CAST magnet during a typical solar tracking can be
found under the link in this [fn:cast_video] footnote.


[fn:confusion_bore_diameter] There is some confusion about the
diameter and length of the magnet. The original CAST proposal
cite:ZIOUTAS1999480 talks about the prototype dipole magnets as
having a bore diameter of \SI{42.5}{mm} and a length of
\SI{9.25}{m}. However, ever CAST publication afterwards uses the
numbers \SI{43}{mm} and \SI{9.26}{m}. Digging into references about
the prototype dipole magnets is inconclusive. For better compatibility
with all other CAST related publications, we will use the same
\SI{43}{mm} and \SI{9.26}{m} values in this thesis. Furthermore,
measurements were done indicating values around $\SI{43}{mm}$.

[fn:magnetic_field] The magnetic field of $\SI{8.8}{T}$ corresponds to
the actual field at which the magnet was operated at CAST, as taken
from the magnet slow control data.

[fn:angles_origin] These numbers are from CAST's slow control
logs. See the extended thesis.

[fn:cast_video] [[https://www.youtube.com/watch?v=XY2lFDXz8aQ]]

*** TODOs about section                                          :noexport:

- [ ] Mention mini timeline in 2 sentences about GridPix 1 and Septemboard?

- [X] *CROSS SECTION OF LHC DIPOLE MAGNET*
- [X] *NAME SUPERCONDUCTING MATERIAL OF THESE MAGNETS*
- [X] *CITE PAPER ABOUT LHC PROTOTYPE MAGNET*
  
- [ ] Since about 2019 the movement of the magnet was slightly
  restricted due to the belt issues iirc. We can check the angles
  ourselves by looking at the slow control data!

- [ ] A cross section can be seen in fig. *INSERT ME*.
  -> Do we want such a picture? Not really needed, no?

- [ ] *2 ANNOTATED PICTURES OF CAST W/ HIGHLIGHT OF SUNRISE, SUNSET,
  AIRPORT, JURA* 
  *INTRODUCE THESE IN TEXT*
  -> Neither is really needed, no? I'm not sure if we ever mention
  "sunrise", "airport" etc. in the thesis.


- [ ] In addition, with the MicroMegas dataset taken in *CHECK EXACT* phase I a
  limit on the axion electron coupling was computed *CITE 2013*....


- [X] *160 STEPS WERE PERFORMED WITH BUFFER GAS* cite:Arik_2009

*BETTER SEPARATE X-ray OPTICS*

- [X] *MENTION COHERENCE CONDITION* (here or in theory?)
  -> THEORY!

*CAST PROPOSAL MENTIONS 9.25m and 42.5mm DIAMETER!! CHECK*

Data taking periods.

- [X] *INSERT VIDEO IN FOOTNOTE*

- [ ] *CAN WE REFERENCE THE MEASUREMENTS THEODOROS SENT US?*



*** Calculate the maximum tilting angles of the magnet           :extended:

The tilting angles of the CAST magnet are stored (among others, the
tracking files also contain angle data, as well as the "angles" file
{that existed at some point at least?} ) in the slow control log
files.

We'll use our log file parser to parse all the log files between 2016
and

#+begin_src sh
cd ~/CastData/ExternCode/TimepixAnalysis/LogReader/
./cast_log_reader sc -p ../resources/LogFiles/SCLogs -s Version.idx
#+end_src

This yields fig. [[fig:cast:vertical_angles_data_taking]] where we can see
that the real angles are between $\sim\SIrange{-7}{7.7}{°}$.

#+CAPTION: Vertical angle of the CAST magnet during the data taking discussed in this
#+CAPTION: thesis. The magnet moves in a range of $\sim\SIrange{-7}{7.7}{°}$ during
#+CAPTION: trackings.
#+NAME: fig:cast:vertical_angles_data_taking
[[~/phd/Figs/CAST_magnet_vertical_angle.pdf]]


We also have _all_ of the slow control logs in
[[file:~/CastData/ExternCode/TimepixAnalysis/resources/LogFiles/AllLogs/logfiles/]]. Feel
free to run the command on this directory. But don't expect amazing
performance given the amount of data!

A PNG of the full data in fig. [[fig:cast:vertical_angle_all_data]]
showing the same range as the figure only showing the data taking
period discussed in this thesis.

#+CAPTION: Vertical angle of the CAST magnet during (almost) the entire data taking period from 2005
#+CAPTION: to end of 2021. Angles go from roughty $\SI{-7}{°}$ to about $\SI{7.7}{°}$.
#+NAME: fig:cast:vertical_angle_all_data
[[~/org/Figs/statusAndProgress/cast_magnet_vertical_angles_2005_2021.png]]


*** CAST X-ray optics
:PROPERTIES:
:CUSTOM_ID: sec:helioscopes:cast:xray_optics
:END:

The first X-ray telescope used at CAST as a focusing optics for the
expected axion induced X-ray flux was a Wolter I type X-ray telescope
cite:wolter_1_type originally built for a proposed German space
based X-ray telescope mission, ABRIXAS cite:ABRIXAS_0a,ABRIXAS_0b. The telescope
consists of 27 gold coated parabolic and hyperbolic shells and has a
focal length of \SI{1.6}{m}. Due to the small size of the dipole
magnet's bores of only \SI{43}{mm} only a single section of the
telescope can be exposed. The telescope is thus placed off-axis from
the magnet bore to expose a single mirror section. An image of the
mirror system with a rough indication of the exposed section is shown
in fig. sref:fig:cast:abrixas_mirrors.

The telescope is owned by the Max Planck Institut für
extraterrestrische Physik in Garching. For that reason it will often
be referred to as the 'MPE telescope' in the context of CAST.

The efficiency of the telescope reaches about \SI{48}{\%} as the peak
at around \SI{1.5}{keV}, drops sharply at around \SI{2.3}{keV} to only
about \SI{30}{\%} up to about \SI{7}{keV}. From there it continues to
drop until about \SI{5}{\%} efficiency at \SI{10}{keV}. The efficiency
is shown in a comparison with another telescope in the next section
[[#sec:helioscopes:llnl_telescope]] in
fig. [[fig:cast:telescope_efficiency_comparison_mpe_llnl]].

A picture of the telescope installed at CAST behind the magnet on the
'sunrise' side of the magnet is shown in fig. sref:fig:cast:abrixas_installed.

This telescope was used for the data taking campaign in 2014 and 2015 using a GridPix
based detector discussed in cite:krieger2018search and serves as a
comparison for certain aspects in this thesis.

#+begin_src subfigure
(figure ()
        (subfigure (linewidth 0.5) (caption "Side view") (label "fig:cast:abrixas_installed")
                   (includegraphics (list (cons 'width (linewidth 0.95)))
                                    "~/org/Figs/thesis/CAST/cast_abrixas_telescope_image_clear.png"))
        (subfigure (linewidth 0.5) (caption "Mirrors") (label "fig:cast:abrixas_mirrors")
                   (includegraphics (list (cons 'width (linewidth 0.95)))
                                    "~/org/Figs/thesis/CAST/abrixas_cast_telescope_system.png"))
        (caption
         (subref "fig:cast:abrixas_installed")
          "Image of the Abrixas telescope installed at CAST on the 'sunrise' side.
           The image is taken from " (cite "CAST_telescope_ccd") " as it provides a 
           relatively clear image of the telescope, which is hard to take nowadays."
         (subref "fig:cast:abrixas_mirrors")
          "Image of the Abrixas telescope mirror system. The different shells of the 
           Wolter I type telescope system are visible. One section is exposed to the 
           magnet bore, the white line indicating roughly the extent of the bore. The 
           sproke like structure is the support for the mirror shells.
           Image taken from " (cite "CAST_telescope_ccd") ".")
        (label "fig:theory:abrixas"))
#+end_src

*** Lawrence Livermore National Laboratory (LLNL) telescope
:PROPERTIES:
:CUSTOM_ID: sec:helioscopes:llnl_telescope
:END:

Up to 2014 there was only a single X-ray telescope in use at CAST. In
August 2014 a second X-ray optics was installed on the second bore
next to the ABRIXAS telescope. This telescope using technologies
originally developed for the space based NuSTAR telescope by NASA
cite:Harrison_2013,Harrison2006,nustar_design_performance,nustar_fabrication,nustar_overview_status,
but purpose built for axion searches and in particular the CAST
experiment. Contrary to the ABRIXAS telescope only a single telescope
section of $\SI{30}{°}$ wide mirrors of the telescope was built as the
small bore cannot expose more area anyway. It uses a cone
approximation to a Wolter I optic, meaning the hyperbolic and
parabolic mirrors are replaced by cone sections cite:Petre:85. It
consists of 13 platinum / carbon coated glass shells in each telescope
section, for a total of 26 mirrors. Each mirror uses one of four
different depth graded multilayer coating recipes to improve
reflectivity over a larger energy and angle range. Further the focal
length was shortened to \SI{1.5}{m}. The telescope section is rotated
such that the focal point is pointing away from the other magnet bore
to make more space for two detectors side by
side. [fn:focal_point_llnl] This can be seen in the render of the
2017/18 detector setup in fig. [[llnl_telescope_setup_2017_render]], seen
from the top. The development process of the telescope is documented
in the PhD thesis by A. Jakobsen
cite:anders_phd. cite:llnl_telescope_first_cast_results gives an
overview and shows preliminary results from CAST. The telescope was
characterized and calibrated at the PANTER X-ray test facility in
Munich.

#+begin_center
#+CAPTION: Render of the setup of the GridPix septemboard detector in 2017/18 showing the 
#+CAPTION: LLNL telescope on the left side. The diversion away from the extension of the
#+CAPTION: bore is visible, to have more space for detector installation. The
#+CAPTION: lead shielding and veto scintillator are not shown in the render. Render created by Tobias Schiffer.
#+NAME: llnl_telescope_setup_2017_render
[[~/phd/Figs/llnl_cast_gridpix_render_small_annotated.png]]
#+end_center

This telescope achieves a significantly higher effective area than the
MPE telescope in the energy ranges most relevant for solar axion
searches, fig. [[fig:cast:telescope_efficiency_comparison_mpe_llnl]] (compare with
fig. sref:fig:theory:solar_axion_flux:differential_flux).

The precise understanding of the position, size and shape of the focal
point as well as the effective area is essential for the limit
calculation later in the thesis. In appendix [[#sec:appendix:raytracing]]
we discuss the raytracing simulation of this telescope as well as the
calculation for its effective area in detail.

- [ ] *REVISE THIS SECTION ABOUT EFFECTIVE AREA ONCE TALKED TO JAIME AGAIN*

#+begin_center
#+CAPTION: Comparison of the efficiency between the two telescopes, the MPE (ABRIXAS) as the 
#+CAPTION: original CAST telescope and the LLNL telescope purpose built for axion searches.
#+CAPTION: The LLNL telescope has superior efficiency in the energy range where the axion
#+CAPTION: flux is assumed to dominate, but falls off sharper at high energies.
#+CAPTION: The data for the LLNL telescope is extracted from fig. 3 in cite:llnl_telescope_first_cast_results,
#+CAPTION: whereas for the ABRIXAS telescope it is extracted from the red line in fig. 4
#+CAPTION: of cite:CAST_telescope_ccd.
#+NAME: fig:cast:telescope_efficiency_comparison_mpe_llnl
[[~/phd/Figs/telescopes/effective_area_mpe_llnl.pdf]]
#+end_center

#+begin_comment
Note: Refer to DTU thesis
[[/home/basti/org/Papers/CAST_IAXO_telescopes/llnl_telescope_optimizations_phdthesis_for_DTU_orbit.pdf]]
around page 65 (and shortly before for effective area definition; and
another eff area def on page 7). 
#+end_comment

[fn:focal_point_llnl] Because this telescope only consists of one
section, its focal point is 'parallel' to the magnet bore. In a full
Wolter I like optic the focal point is exactly behind the center of
the optic. With only a portion of a telescope this implies an
offset. Further, the telescope is not actually rotated exactly
$\SI{90}{°}$ to move the focal spot perfectly parallel to the two
bores, but only about $\SI{87}{°}$.


**** TODOs for this section [/]                                 :noexport:

- [ ] https://doi.org/10.1007/s10686-006-9068-8 <-- Possible citation
  for PANTER?

- [ ] Insert an image of the LLNL telescope internals
- [ ] Refer to later section. Raytracing? About way more infos about
  the telescope.

- [ ] In the section where we talk about our raytracing of this
  telescope include:
  - table of parameters of the telescope
  - numbers for telescope design 'as built'
  - Wolter equation with correct radius to use
  - the multilayer recipes used  

*BETTER INTRODUCE 2 LENGTH WISE SECTION THING OF WOLTER TELESCOPES*

**** Generate effective area plot for LLNL and MPE telescopes [/] :noexport:

We'll now create the comparison plot of the effective areas for the
two X-ray telescopes used at CAST.

For the MPE telescope: As mentioned in the caption of the figure
above, the data for the MPE telescope is extracted from fig. 4 of
cite:CAST_telescope_ccd, also found here
[[~/phd/resources/MPE/mpe_xray_telescope_cast_effective_area.pdf]]. This
is done using [[file:~/phd/code/other/extractDataFromPlot.nim]] using a
simplified version of fig. 4 as an input (i.e. crop the plot to
exactly the area of the actual plot and remove any lines that are not
the data to be extracted. This plot looks like
file:~/phd/resources/MPE/mpe_xray_telescope_cast_effective_area_cropped_no_axes.png.

Then run
#+begin_src sh
./extractDataFromPlot \
    -f ~/phd/resources/MPE/mpe_xray_telescope_cast_effective_area_cropped_no_axes.png \
    --xLow 0.305 --xHigh 10.0 \
    --yLow 0.0 --yHigh 8.0
#+end_src
which produces a plot from the extracted data and writes a CSV file,
which 

- [X] Make sure to add the effective area files to a ~resources~
  directory of the thesis repo!

- [ ] Ask Jaime if this really looks sensible, because to me it does
  not! That's barely better than the MPE telescope...  

#+begin_src nim :flags -d:QuietTikZ=true :tange code/plot_mpe_llnl_effective_areas.nim
import ggplotnim, ggplotnim/ggplot_sdl2
import unchained
const mpe = "~/phd/resources/MPE/mpe_xray_telescope_cast_effective_area.csv"
const llnl = "~/phd/resources/LLNL/EffectiveArea.txt"
let dfMpe = readCsv(mpe)
let dfLLNL = readCsv(llnl, sep = ' ')
  .rename(f{"Energy[keV]" <- "E(keV)"},
          f{"EffectiveArea[cm²]" <- "Area(cm^2)"})
let df = bind_rows([("MPE", dfMpe), ("LLNL", dfLLNL)], "Telescope")
const areaBore = (4.3.cm / 2.0)^2 * π ## Area of the CAST bore in cm²
ggplot(df, aes("Energy[keV]", "EffectiveArea[cm²]", color = "Telescope")) +
  geom_line() +
  xlab(r"Energy [\si{keV}]") + ylab(r"EffectiveArea [\si{cm^2}]") +
  scale_y_continuous(secAxis = secAxis(f{1.0 / areaBore.float}, name = r"Transmission [\si{\%}]")) +
  legendPosition(0.83, -0.2) + 
  ggshow("~/phd/Figs/telescopes/effective_area_mpe_llnl.pdf", width = 600, height = 360, useTeX = true, standalone = true)
#+end_src

#+RESULTS:
| 14.522 cm²        |                                                             |                   |                                 |                                                             |
| Producing        | ~/phd/Figs/telescopes/effective_area_mpe_llnl.pdf           |                   |                                 |                                                             |
| [INFO]           | TeXDaemon                                                   | ready             | for                             | input.                                                      |
| shellCmd:        | command                                                     | -v                | xelatex                         |                                                             |
| shellCmd:        | xelatex                                                     | -output-directory | /home/basti/phd/Figs/telescopes | /home/basti/phd/Figs/telescopes/effective_area_mpe_llnl.tex |
| Generated:       | /home/basti/phd/Figs/telescopes/effective_area_mpe_llnl.pdf |                   |                                 |                                                             |
| AudioDeviceAdded |                                                             |                   |                                 |                                                             |
| AudioDeviceAdded |                                                             |                   |                                 |                                                             |
| AudioDeviceAdded |                                                             |                   |                                 |                                                             |
| AudioDeviceAdded |                                                             |                   |                                 |                                                             |
| AudioDeviceAdded |                                                             |                   |                                 |                                                             |
| AudioDeviceAdded |                                                             |                   |                                 |                                                             |

*** Best limits set by CAST                                      :optional:

In the many years of data taking and countless detectors taking data
at the CAST experiment, it has put the most stringent limits on
different coupling constants over the years.

Specifically, CAST sets the current best helioscope limits on the:
- Axion-photon coupling $g_{aγ}$
- Axion-electron coupling $g_{ae}$
- Chameleon-photon coupling $β_γ$

For the axion-photon coupling the best limit is from
cite:cast_nature in 2017 based on the full MicroMegas dataset
including the data behind the LLNL telescope and constricts the coupling to $g_{aγ} <
\SI{6.6e-11}{\GeV^{-1}}$. 

For the axion-photon coupling the best limit is still from 2013 in
cite:Barth_2013 using the theoretical calculations for an expected
solar axion flux done by J. Redondo in cite:Redondo_2013 for a limit
on the product of the axion-electron and axion-photon coupling of
$g_{ae} g_{aγ} < \SI{8.1e-23}{\GeV^{-1}}$. The limit calculation was
based on data taken in CAST phase I in 2003 - 2005 with a pn-CCD
detector behind the MPE telescope.

CAST was also used to set a limit on the hypothetical chameleon
particle. The best current limit on the chameleon-photon coupling
$β_γ$ is based on a single GridPix based detector with data taken in
2014 and 2015 by C. Krieger in cite:krieger2018search,krieger_chameleon_jcap, limiting the
coupling to $β_γ < \num{5.74e10}$, which is the first limit below the
solar luminosity bound.

**** TODOs about this section [/]                               :noexport:
- [ ] *Mention the limit method with foreshadowing to statistics chapter
  that we will use the same?*
  -> No, I don't think so

- [ ] Subsection about gaseous phase, affecting conversion            
  Extract parts of the axionMass.org file and place it here. Essentially
  the:
  - conversion probability in gas
  - how to compute that
  - one step showing conversion prob outside coherent condition
  -> We've discussed the buffer gas stuff in the axion theory and
  mentioned our calculations. I think that's enough.

** International AXion Observatory (IAXO)
:PROPERTIES:
:CUSTOM_ID: sec:helioscopes:iaxo
:END:

Barring a revolution in detector development or a lucky find of a non
QCD axion, the CAST experiment was unlikely to detect any signals. A
fourth generation axion helioscope to possibly reach towards the QCD
band in the mass-coupling constant phase space is a natural idea.

The first proposal for a next generation axion helioscope was
published in 2011 cite:Irastorza_2011, with the name International AXion
Observatory (IAXO) first appearing in 2013 cite:vogel2013iaxo. A
conceptual design report (CDR) was further published in 2014
cite:Armengaud_2014. 

The proposed experiment is supposed to have a total magnet length of $\SI{25}{m}$
length with $\num{8}$  $\SI{60}{cm}$ bores with an average transverse
magnetic field of $\SI{2.5}{T}$ [fn:magnetic_field_iaxo]. With a cryostat and magnet design
specifically built for the experiment, much larger tilting angles of
the magnet of about $\pm\SI{25}{°}$ are proposed to allow for solar
tracking for $\SI{12}{\hour}$ per day for a 1:1 data split between
tracking and background data. cite:Armengaud_2014

In cite:Irastorza_2011 a 'figure of merit' (FOM) is introduced
to quantify the improvements possible by IAXO over CAST, defined by

\[
f = f_M f_{\text{DO}} f_T = (B² L² A)_M \left(\frac{ε_d ε_o}{\sqrt{b a}}\right)_{\text{DO}} (\sqrt{ε_t t})_T
\]

which is split into individual FOMs for the magnet $f_M$ the detector
and optics $f_{\text{DO}}$ and the total tracking time $f_T$ ($B$:
magnetic field, $L$: magnet length, $A$: total bore area of all bores,
$ε_d$: detector efficiency, $ε_o$: X-ray optic efficiency, $b$:
background in counts per area and time, $a$: area of the X-ray optic
focal spot, $ε_t$: data taking efficiency, $t$: total solar tracking
time). The biggest improvements would come from the magnet FOM (MFOM),
due to the much larger magnet volume. Compared to the CAST MFOM
$f_{M,\text{CAST}} = \SI{19.3}{T^2.m^4}$ IAXO would achieve a relative
improvement of $f_{M,\text{CAST}} / f_{M,\text{IAXO}} \approx 300$
with its $f_{M,\text{IAXO}} = \SI{5654.9}{T^2.m^4}$. As the number of
expected counts in a helioscope scales with
$g^4$ [fn:coupling_notation], the figure of merit directly relates
possible limits of a helioscope in case of a non-detection. The
aspirational target for IAXO would be a full $f_{\text{CAST}} /
f_{\text{IAXO}} > \num{10000}$  for a possible improvement on $g$
bounds by an order of magnitude.

A schematic of the proposed design can be seen in
fig. sref:fig:helioscopes:iaxo. Given the comparatively large budget
requirements for such an experiment, a compromise was envisioned to
prove the required technologies, in particular the magnet design. This
intermediate experiment called BabyIAXO will be discussed in the next
section, [[#sec:helioscopes:baby_iaxo]].

#+begin_src subfigure
(figure ()
        (subfigure (linewidth 0.5) (caption "IAXO") (label "fig:helioscopes:iaxo")
                   (includegraphics (list (cons 'width (linewidth 0.95)))
                                    "~/phd/Figs/IAXO/iaxo_annotated_schematic.png"))
        (subfigure (linewidth 0.5) (caption "BabyIAXO") (label "fig:helioscopes:baby_iaxo")
                   (includegraphics (list (cons 'width (linewidth 0.95)))
                                    "~/phd/Figs/IAXO/babyIAXO_schematic_annotated_ringwald23.png"))
        (caption
         (subref "fig:helioscopes:iaxo")
         "An annotated schematic of a potential IAXO design with 8 magnet bores. Taken from the IAXO CDR "
         (cite "Armengaud_2014") ". "
         (subref "fig:helioscopes:baby_iaxo")
         "An annotated schematic of the current BabyIAXO design, showing the two bores. One bore intended
          for an XMM-Newton optic of " (SI 7.5 "m") " focal length, and a shorter " (SI 5 "m") " focal
          length custom optic behind the other bore. Taken from " (cite "ringwald23_axions_hamburg") "."
          )
        (label "fig:helioscopes:iaxo_babyiaxo_schematics"))
#+end_src

[fn:magnetic_field_iaxo] Due to the envisioned toroidal magnet design
and much larger diameter, the magnetic field inside such a bore would
not be as homogeneous as in the LHC dipole magnets. The peak field
would be about $\SI{5.4}{T}$. This also means a correct $f_M$
calculation for IAXO must take into account the actual magnetic field
map.

[fn:coupling_notation] $g⁴$ is a placeholder for $g⁴_{aγ}$ or
$g²_{aγ}·g²_{ae}$. The flux scales with this product as explained in
sec. [[#sec:theory:current_bounds]] due to axion production and reconversion
happening via $g²$ each.

*** Calculate magnet figure of merit                             :noexport:

For more details on the figure of merit calculation, see sec. 3.1 in
cite:Irastorza_2011.

For completeness, the figure of merit is:

\[
f = f_M f_{\text{DO}} f_T = (B² L² A)_M (\frac{ε_d ε_o}{\sqrt{b a}})_{\text{DO}} (\sqrt{ε_t t})_T
\]

The full relationship with the axion-photon coupling is:

\begin{align*}
  N_γ ∝ N_a · g⁴ &= f · g⁴ \\
  N_b &= b a ε_t t
\end{align*}

So 10,000 more "produced" (i.e. not produced) photons due to f'/f =
10,000 for IAXO over CAST would imply the bounds on $g$ could be
improved by a factor of 10, one order of magnitude.

Let's calculate the figure of merit for the CAST magnet as well as the
IAXO / BabyIAXO magnets:
#+begin_src nim
import unchained

defUnit(T²•m⁴)
proc fMagnet(B: Tesla, L: Meter, d: CentiMeter): T²•m⁴ = B^2 * L^2 * (d / 2.0)^2 * π
let fCAST = fMagnet(8.8.T, 9.26.m, 4.3.cm)
echo "CAST magnet, single bore f_M = ", fCAST, " at 8.8 T"
echo "CAST, both bores f_M = ", fCAST * 2

let fIAXO = fMagnet(2.5.T, 20.m, 60.cm)
echo "IAXO magnet, single bore f_M = ", fIAXO, " at 2.5 T *average* field, 20 m length."
echo "IAXO, all bores f_M = ", fIAXO * 8

echo "IAXO FOM over CAST: ", fIAXO * 8 / (fCAST * 2)
#+end_src

#+RESULTS:
| CAST  | magnet, | single | bore  | f_M            | =            | 9.64304 T²•m⁴ | at | 8.8 | T |           |        |    |   |         |
| CAST, | both    | bores  | f_M   | =              | 19.2861 T²•m⁴ |              |    |     |   |           |        |    |   |         |
| IAXO  | magnet, | single | bore  | f_M            | =            | 706.858 T²•m⁴ | at | 2.5 | T | *average* | field, | 20 | m | length. |
| IAXO, | all     | bores  | f_M   | =              | 5654.87 T²•m⁴ |              |    |     |   |           |        |    |   |         |
| IAXO  | FOM     | over   | CAST: | 293.21 UnitLess |              |              |    |     |   |           |        |    |   |         |


Time scaling:

#+begin_src nim
import unchained, math
let g = 8e-11
let g4 = g^4
let t = 1
# g⁴ = √t
# g⁴ / √t = g'⁴ / √t'
let tp = 2
let g4p = g4 * sqrt(2.0)
echo "New g for twice the time: ", pow(g4p, 0.25)
#+end_src
#+begin_src nim
import unchained, math
let g = 8e-11
let g4 = g^4
let t = 1
# g⁴ · √t = N # <- some number of photons: we observe 0
# g⁴' · √t' = N' # <- new number of photons: we still observe 0
# So
# g⁴ · √t = g⁴' · √t'
# should hold, meaning
# g⁴ · √1 = g⁴' · √2
# g⁴' = g⁴ / √2
let g4p = g4 / sqrt(2.0)
echo "New g for twice the time: ", pow(g4p, 0.25)
#+end_src

#+RESULTS:
: New g for twice the time: 7.336032345637369e-11

*** TODOs for this section                                       :noexport:

- Make use of PRC (?) mainly for data, citation both that and first
  proposal.

- [X] *MAYBE PICTURE OF IAXO LEFT, BABYIAXO RIGHT*

- [ ] *FIGURE OF MERIT*
  -> Including definition.

- [ ] *EXPECTED LIMIT*



*** BabyIAXO
:PROPERTIES:
:CUSTOM_ID: sec:helioscopes:baby_iaxo
:END:

The major difference between full grown IAXO and BabyIAXO is
restricting the setup to 2 bores instead of 8 with a magnet length of
only \SI{10}{m} to prove the magnet design works, before building a
larger version of said design.

Since the first conceptual design of IAXO cite:Armengaud_2014 the
bore diameter for the two bores of BabyIAXO has increased from
\SI{60}{cm} to \SI{70}{cm}. cite:abeln2021conceptual 

The BabyIAXO design was approved by the Deutsches
Elektronen-Synchrotron (DESY) for construction onsite. The project has
suffered multiple delays most notably due to the magnet
construction. First COVID19 due to its severe effects on supply chains
and in 2022 the horrific Russian invasion of Ukraine have caused multi
year delays. The latter in particular was problematic as the only two
companies being able to supply the type of superconducting cable
needed for the magnet are from Russia. The magnet situation is still
in flow as of writing this thesis.

For the two bores two different X-ray telescopes are planned to be
operated. One bore will be used with a flight spare of the XMM-Newton
X-ray satellite mission with a focal length of $\SI{7.5}{m}$. The
second bore would receive a custom built X-ray optic based on a hybrid
design of a NuSTAR-like optic for the inner part - similar to the LLNL
telescope introduced in sec. [[#sec:helioscopes:llnl_telescope]] (just as
a full telescope) - and a cold slumped glass design following
cite:civitani16_cold_hot_slump_glass_optic for the outer part. This
optic would have a focal length of only $\SI{5}{m}$.

An annotaded schematic of the BabyIAXO design can be seen in
fig. sref:fig:helioscopes:baby_iaxo. The magnet figure of merit for
BabyIAXO is aimed at being at least a factor 10 higher than for CAST,
with cite:abeln2021conceptual listing values between
$\SIrange{232}{326}{T^2.m^4}$ depending on how it is calculated.

**** TODOs for this section                                     :noexport:

Any?


* Gaseous detectors principles                                       :Theory:
:PROPERTIES:
:CUSTOM_ID: sec:theory_detector
:END:

#+LATEX: \minitoc

Gaseous detectors, keep a bit short. Before writing properly read
Lucian. Best if read Lucian and then write a couple of weeks later.

This chapter will be kept reasonably short. Instead of introducing all
physics relevant for gaseous detectors, we will focus on the things
that are relevant for the understanding in the context of the
thesis. For better general overview of the physics of gaseous
detectors, read some of the following references: *Lucian, Markus MSc;
Lupberger, Krieger PhD, Elisa PhD, PDG, some book?...*

*Highlight which reference for what*

The theory sections covered in the following parts all have in common
that their understanding is required to make certain assumptions in
the data analysis or *???* 

It should be noted though that no part will be thorough enough to
stand on its own. Further reading is required in many places. This
theory section is supposed to serve as a reference for the later parts
of the thesis.

Of particular interest are all sections that give the theoretical
foundation for different kinds of background we might measure or the
understanding of our calibration data.

** Particle interactions with matter

We will now describe a few of the laws governing how particles
interact with matter, to the extent as it will be useful in the
context of the rest of this thesis.

*WRITE SUMMARY*

On the one hand we will discuss how X-rays interact with matter. Both
in terms of solids as well as gases, focused on their attenuation,
because this is required to describe signal attenuation due to a
detector window of a gaseous detector as well as for the absorption of
X-rays in the detector gas. In addition, X-ray reflectivity will be
discussed briefly as it is of interest for the behavior of X-ray
telescopes.

On the other hand the interaction of highly energetic charged
particles with matter will be discussed, its relation to cosmic
radiation as a source of background for an axion helioscope.

Finally, X-ray fluorescence will be covered as it is another major
source of background in an axion helioscope experiment, in particular
for gaseous detectors.

*** X-rays in solid matter & gases
:PROPERTIES:
:CUSTOM_ID: sec:theory:xray_matter_gas
:END:

Lambert-Beer's law cite:bouguer1729essai,lambert1760photometria,beer1852bestimmung

#+NAME: eq:theory:beer_lambert_law
\begin{equation}
I(z) = I_0 e^{-μz},
\end{equation}

gives the intensity of radiation $I(z)$ after traversing through a
medium with constant attenuation $μ$ of length $z$, given a starting
intensity of $I_0$. Directly related is of course the absorption
length $l_{\text{abs}} = 1/μ$ (or mean free path), which is a useful
property when considering typical absorption depths.

This law is of vital importance for the behavior of X-rays traversing
through matter, which is needed to compute the efficiency of a gaseous
detector with an entrance window.

In addition it is also related to the mean free path of X-rays in a
gas, which is an important parameter in gaseous detectors to
understand the absorption efficiency of X-rays at different energies
and the resulting expected diffusion.

For a more detailed overview of the remaining section, see the X-ray data
booklet cite:williams2001x.

In the context of X-rays the factor $μ$ is typically rewritten via the
'mass attenuation coefficient' $μ_m = μ · ρ$ with $ρ$ the density of
the material, commonly in $\si{g.cm^{-3}}$. $μ_m$ is then defined by

\[
μ_m = \frac{N_A}{M} σ_A,
\]

where $N_A$ is Avogadro's number, $M$ the molar mass of the medium in
units of $\si{g\per\mol}$ and $σ_A$ is the photoabsorption cross section
in units of $\si{cm^2}$. Thus, the mass attenuation coefficient is
usually given in $\si{cm^2.g^{-1}}$ such that $μ = μ_m · ρ$ is of inverse length
as expected. This directly yields the definition of the absorption
length,

\[
l_{\text{abs}} = \frac{1}{μ}.
\]

Further, the photoabsorption cross section can be described via the
atomic scattering factor $f₂$

\[
σ_A = 2 r_e λ f₂,
\]

where $r_e$ is the classical electron radius and $λ$ the wavelength of
the X-ray. $f₂$ is the imaginary part of the forward scattering factor
$f$

\[
f = f₁ - i f₂
\]

which itself is the simplification of the general atomic scattering
factor that describes the atom specific part of the scattering cross
section.

This way of expressing it has the nice property of relying on a well
tabulated parameter $f₂$. Together with $f₁$ these tabulated values
can be used to compute everything from the refractive index at a
specific X-ray energy of a compound to the attenuation coefficient and
even reflectivity of a multi-layer substrate.

It generalizes from single element to compounds easily by

\[
μ_m = \frac{N_A}{M_c} \sum_i n_i σ_{A,i},
\]

with $M_c$ the molar weight of the compound and $n_i$ the number of
atoms of kind $i$.

X-ray absorption and transmission properties can be calculated from
this only requiring the atomic scattering factors, which can be found
tabulated for different elements, for example by [[https://www.nist.gov/pml/x-ray-form-factor-attenuation-and-scattering-tables][NIST]] and
[[https://henke.lbl.gov/optical_constants/asf.html][Henke]]. 

There is an online calculator for calculations of X-ray transmission
found under [fn:henke_gov] cite:henke1993x, as well as a library
implementation developed during the course of this thesis
under [fn:scinim_xrayAttenuation] cite:Schmidt_xrayAttenuation_2022
for this purpose. 

Fig. [[fig:theory:transmission_examples]] shows an example of X-ray
transmission through a $\SI{300}{nm}$ thick layer of \ccsini as well
as transmission through $\SI{3}{cm}$ of argon at normal temperature and
pressure (NTP), $\SI{1}{atm}$, $\SI{20}{°C}$. All information about
the absorption lines and general transmission is encoded in $f₂$. 

#+CAPTION: X-ray transmission through a \SI{300}{nm} thick layer of \ccsini
#+CAPTION: and \SI{3}{cm} of argon calculated with cite:Schmidt_xrayAttenuation_2022. 
#+CAPTION: Calculation of the transmission based on tabulated scattering form factors.
#+NAME: fig:theory:transmission_examples
[[~/phd/Figs/theory/transmission_example.pdf]]

[fn:henke_gov] https://henke.lbl.gov/optical_constants/ 

[fn:scinim_xrayAttenuation] https://github.com/SciNim/xrayAttenuation

**** Absorption length of Argon                                 :extended:

Fig. [[fig:theory:absorption_length_argon]] also shows the absorption
length for Argon at NTP.

#+CAPTION: Absorption length of \SI{3}{cm} of argon calculated with cite:Schmidt_xrayAttenuation_2022. 
#+CAPTION: Calculation based on tabulated scattering form factors.
#+NAME: fig:theory:absorption_length_argon
[[~/phd/Figs/theory/absorption_length_example.pdf]]

**** TODOs for this section                                     :noexport:

- [ ] *FIND REFERENCE TO MODERN LAW IN SOMETHING LIKE DEMTRÖDER*
  -> Is this needed? It seems like the Demtröder doesn't mention
  it. Nor does the PDG.

- [X] *INSERT ABSRPTION LENGTH!!!*

**** Generation of \ccsini transmission figure                 :noexport:

Let's compute an example transmission plot using the Lambert-Beer law
as presented above based on =xrayAttenuation= now, on the one hand for
\ccsini as well as argon (common detector gas).

*TODO*: update ginger to use =-output-directory= to put the plot in
the right path & turn it into a TikZ plot.

#+begin_src nim :tangle /home/basti/phd/code/transmission_example.nim :flags -d:QuietTikZ=true
import std / strutils
import xrayAttenuation, ggplotnim
# generate a compound of silicon and nitrogen with correct number of atoms
let Si₃N₄ = compound((Si, 3), (N, 4))
# instantiate an Argon instance
let ar = Argon.init()
# compute the density using ideal gas law at 1 atm
let ρ_Ar = density(1013.25.mbar.to(Pascal), 293.15.K, ar.molarMass)

# define energies in which to compute the transmission
# (we don't start at 0, as at 0 energy the parameters are not well defined)
let energies = linspace(1e-2, 10.0, 1000)
proc compTrans[T: AnyCompound](el: T, ρ: g•cm⁻³, length: Meter): DataFrame =
  result = toDf({ "Energy [keV]" : energies })
    .mutate(f{float: "μ" ~ el.attenuationCoefficient(idx("Energy [keV]").keV).float},
            f{float: "Trans" ~ transmission(`μ`.cm²•g⁻¹, ρ, length).float},
            f{float: "l_abs" ~ absorptionLength(el, ρ, idx("Energy [keV]").keV).to(cm).float},
            f{"Compound" <- el.name})
var df = newDataFrame()
# compute transmission for Si₃N₄ (known density and desired length)
df.add Si₃N₄.compTrans(3.44.g•cm⁻³, 300.nm.to(Meter))
# and for argon 
df.add ar.compTrans(ρ_Ar, 3.cm.to(Meter))
# create a plot for the transmissions
echo df
let dS = pretty(300.nm, 3, short = true)
let dA = pretty(3.cm, 1, short = true)
let si = r"$\mathrm{Si}₃\mathrm{N}₄$"
ggplot(df, aes("Energy [keV]", "Trans", color = "Compound")) +
  geom_line() +
  xlab(r"Energy [$\si{keV}$]") + ylab("Transmission") +
  ggtitle("Transmission examples of $# $# and $# Argon at NTP" % [dS, si, dA]) +
  ggsave("/home/basti/phd/Figs/theory/transmission_example.pdf",
         useTex = true, standalone = true, width = 600, height = 360)

let dff = df.filter(f{float -> bool: classify(`l_abs`) != fcInf},
                    f{`Compound` == "Argon"})
echo dff
ggplot(dff, aes("Energy [keV]", "l_abs")) +
  geom_line() +
  xlab(r"Energy [$\si{keV}$]") + ylab(r"Absorption length [$\si{cm}$]") +
  ggtitle("Absorption length of $# Argon at NTP" % [dA]) +
  ggsave("/home/basti/phd/Figs/theory/absorption_length_example.pdf",
         useTex = true, standalone = true, width = 600, height = 360)
  
#+end_src


*** X-ray reflectivity & scattering [/]
:PROPERTIES:
:CUSTOM_ID: sec:theory:xray_reflectivity
:END:

The same atomic scattering factors $f₁$ and $f₂$ introduced in section
[[#sec:theory:xray_matter_gas]] for the attenuation can also be used to
compute the reflectivity of X-rays under shallow angles. A great
overview of the relevant physics for X-ray reflectivity is found in
cite:windt98_imd, which introduces the ~IMD~ program for the
simulation of multilayer coatings for X-rays.

By defining the combined scattering factor

\[
f(E) = f₁(E) + i f₂(E)
\]

at energy $E$, the refractive index $n$ of a medium can be computed using

\[
n(E) = 1 - r_e \frac{λ²}{2π} \sum_i n_{ai} f_i(E)
\]

where $n_{ai}$ is the number density of the $i\text{-th}$ compound of the
medium. By expressing the refractive index for X-rays in this fashion,
reflectivity can be expressed using the Fresnell equations, just like
for visible light. The reflectivity for s-polarization is calculated
by
#+NAME: eq:theory:fresnell_reflectance_s
\begin{equation}
r^s_{ij} = \frac{n_i · \cos(θ_i) - n_j · \cos(θ_j)}{n_i · \cos(θ_i) + n_j · \cos(θ_j}
\end{equation}
while for p-polarization it is done via
#+NAME: eq:theory:fresnell_reflectance_p
\begin{equation}
r^p_{ij} = \frac{n_i · \cos(θ_j) - n_j · \cos(θ_i)}{n_i · \cos(θ_j) + n_j · \cos(θ_i)}.
\end{equation}

Here $θ_i$, $θ_j$ are the incident and refracted angles and $n_i$, $n_j$ the
refractive indices on the incident and outgoing
side $i$ and $j$, respectively. [fn:practical_calculation]

The total reflected energy, the reflectance $R$, is expressed as

\[
R = \frac{1}{2}\left( \left|r^s\right|² + \left|r^p\right|²\right)
\]

for unpolarized light. This can be generalized to multiple layers of
material on a substrate and including a surface roughness. Combined
these provide the essence for a realistic computation of the
efficiency of an X-ray telescope mirror shell. 

This is also implemented in [fn:scinim_xrayAttenuation]
cite:Schmidt_xrayAttenuation_2022 and [fn:henke_gov]
cite:henke1993x also provides an online calculator for such
reflectivities.

- Depth graded multilayers :: One particular kind of surface, the
  depth graded multilayer, is used
  in certain kinds of modern X-ray telescopes, for example the LLNL
  telescope at CAST following the NuSTAR design. In such a multilayer
  repeating layers of a low $Z$ material and a high $Z$ material are
  stacked at decreasing thicknesses.
  
  A depth-graded multilayer is described by the equation:
  \[
  d_i = \frac{a}{(b + i)^c}
  \]
  where $d_i$ is the depth of layer $i$ (out of $N$ layers),
  \[
  a = d_{\text{min}} (b + N)^c
  \]
  and
  \[
  b = \frac{1 - N k}{k - 1}
  \]
  with
  \[
  k = \left(\frac{d_{\text{min}}}{d_{\text{max}}}\right)^{\frac{1}{c}}
  \]
  where $d_{\text{min}}$ and $d_{\text{max}}$ are the thickness of the
  bottom and top most layers, respectively.

  For example for the the LLNL telescope a Pt/C depth graded
  multilayer is used, in which a top layer of carbon is stacked on top
  of a platinum layer. Between 2 to 5 repetitions of decreasing
  thickness are stacked with a ratio of $\SIrange{40}{45}{\%}$ carbon
  to platinum in thickness. More details on this will be discussed in
  appendix [[#sec:appendix:raytracing]], as it is of vital importance to
  calculate the axion image required for the limit calculation
  correctly.

  The reflectivity for a depth-graded multilayer is computed
  recursively from the bottom of the stack to the top layer using
  
  \[
  r_i = \frac{r_{ij} + r_j \exp(2 i β_i)}{1 + r_{ij} r_j \exp(2 i β_i)}
  \]
  
  with

  \[
  β_i = 2π · d_i · \frac{\cos(θ_i)}{λ},
  \]

  where $θ_i$ as seen from the normal axis and with the wavelength of
  the incoming X-rays $λ$. The $r_{ij}$ values are computed following
  equations [[eq:theory:fresnell_reflectance_s]] and
  [[eq:theory:fresnell_reflectance_p]]. Such multilayers work by summing
  the reflecting contributions from the different layer
  transitions. Different thicknesses of the different multilayers mean
  X-rays at different energies and angles are best reflected from
  different layers. Thus, a much improved overall reflectivity over a
  wider energy and angle range can be achieved compared to a normal
  single layer on a substrate (e.g. a gold coating as used for the
  XMM-Newton or ABRIXAS optics).

[fn:practical_calculation] In practice care must be taken to compute
these. Instead of attempting to explicitly compute refracted and
reflected angles, one should work with the complex $\sin$ or $\cos$
expressions. *CHECK ME AGAIN, I DONT THINK THIS IS REALLY NEEDED!!* 

[fn:henke_gov] https://henke.lbl.gov/optical_constants/ 

[fn:scinim_xrayAttenuation] https://github.com/SciNim/xrayAttenuation

**** TODOs for this section                                     :noexport:

- [ ] *FIX UP REFERENCE TO APPENDIX WHEN WRITTEN TO LINK EXPLICIT LLNL
  SECTION ABOVE!*

- [ ] *IN APPENDIX ABOUT MULTILAYERS* explain how the reflectance for
  depth graded multilayer is calculated!  

*TODO: FIX THIS UP LIKELY TO INCLUDE SURFACE ROUGHNESS. ALSO LOOK AT
XRAY DATA BOOKLET FOR IT AGAIN*
- [ ] *REWRITE THIS IN KNOWLEDGE OF MULTILAYER CODE* I.e. Fresnell
  equations & complex refractive indices!!!

**** Other notes                                                :noexport:

\[
R = \left| \frac{k_m - k_p}{k_m + k_p} \right|²
\]

where $k_m$ and $k_p$ are

\[
k_m = \sqrt{k² - (k \cos{θ})²}
\]

and

\[
k_p = \sqrt{ k² n² - (k \cos{θ})² }
\]

defined via the wave number $k$, which itself is computed via

\[
k = 2π \sin{θ} / λ.
\]



*** Bethe-Bloch equation
:PROPERTIES:
:CUSTOM_ID: sec:theory:bethe_bloch
:END:

Another relevant aspect for gaseous detectors is the energy deposition
of charged particles. In particular for experiments that sit near
Earth's surface, a major source of background is due to cosmic
radiation, with cosmic muons making up more than \SI{95}{\percent}
cite:Zyla:2020zbs of radiation (aside from neutrinos) at the surface,
see sec. [[#sec:theory:cosmic_radiation]].

These muons lose energy according to the Bethe-Bloch equation, which
describes the average energy loss per distance for a charged particle
with charge $z$ in a homogeneous medium with charge carriers
$Z$. cite:Zyla:2020zbs [fn:bethe_equation_form]

#+NAME: eq:theory:bethe_bloch_eq
\begin{equation}
  \left\langle -\frac{\mathrm{d}E}{\mathrm{d}x}\right\rangle = 
    K z² \frac{Z}{A} \frac{1}{β²} \left[ 
      \frac{1}{2} \ln\frac{2m_e c² β² γ² W_{\text{max}}}{I²} - β² - \frac{δ(βγ)}{2} 
    \right]
\end{equation}
where the different variables are as follows:
- $K = 4π N_A r_e² m_e c² = \SI{0.307075}{MeV.mol^{-1}.cm^2}$
- $W_{\text{max}}$: maximum possible energy transfer to an electron in
  a single interaction
- $I$: mean excitation energy of the absorber material in \si{\eV}
- $δ(βγ)$: density-effect correction to energy loss  
- and $r_e = \frac{e²}{4π ε_0 m_e c²}$ the classical electron radius,
  $N_A$ Avogadro's number, $m_e$ electron mass, $c$ speed of light
  in vacuum, $z$ charge number of incident particle, $Z$ atomic number
  of absorber material, $A$ atomic mass of absorber material, $β =
  v/c$ speed of incident particle, $γ$ Lorentz factor

This interaction behavior of muons leads to a specific, expected
energy loss per distance. For argon gas at normal conditions this is shown in
fig. [[fig:theory:muon_argon_3cm_bethe_loss]].

As the Bethe formula was derived from quantum mechanical perturbation
theory, higher order corrections can be computed. For our purposes
here the leading order is enough. It is important to keep in mind that
the Bethe-Bloch equation gives the /mean energy/ per distance. When
considering short distances as typically encountered in particle
detectors, this mean is skewed by rare interactions that deposit large
amounts of energy (towards $W_{\text{max}}$). The energy deposition
along short distances is typically described by a Landau-Vavilov
distribution (similar, but different from a normal Landau
distribution) [[cite:Zyla:2020zbs,BICHSEL2006154]]. The most probable
energy loss is often a more appropriate number to look at. It can be
expressed as

#+NAME: eq:theory:most_probable_loss
\begin{equation}
Δ_p = ξ \left[ \ln{ \frac{2 m_e c² β² γ²}{I}} + \ln{\frac{ξ}{I}} + j - β² - δ(βγ) \right],
\end{equation}

where $ξ$ is

\[
ξ = \frac{1}{2} K z² \left\langle \frac{Z}{A} \right\rangle \frac{x}{β²} \, \si{MeV},
\]

for a detector in which the material column the particle travels
through is expressed as $x = d · ρ$ of a distance $d$ in
$\si{g.cm^{-2}}$. $j = \num{0.200}$ is an empirical constant
cite:Zyla:2020zbs,bichsel1988straggling. Further, $\langle Z / A
\rangle$ is simply the average $Z/A$ for a material compound $\langle
Z/A \rangle = \sum_i w_i Z_i / A_i$.

The large difference typically encountered between the most probable
and the mean value for the energy loss in particle detectors, makes
studying the expected signals a complicated topic. For a detailed
description relevant for thin gaseous detectors, see especially
cite:BICHSEL2006154. 

Fig. [[fig:theory:muon_argon_3cm_bethe_loss]] shows the comparison of the
most probable energy loss via equation [[eq:theory:most_probable_loss]]
and the mean energy loss via the Bethe-Bloch equation
[[eq:theory:bethe_bloch_eq]] for muons of different energies traversing
$\SI{3}{cm}$ of argon gas.

# \input{~/phd/Figs/muonStudies/ar_energy_loss_cast.tex}
#+CAPTION: Mean energy loss via Bethe-Bloch (purple) equation of muons in \SI{3}{\cm} of argon at
#+CAPTION: conditions in use in GridPix detector at CAST. \SI{1050}{mbar} of chamber pressure at room 
#+CAPTION: temperature. Note that the mean is skewed by events that transfer a large amount of energy,
#+CAPTION: but are very rare! As such care must be taken interpreting the numbers. Green shows the most 
#+CAPTION: probable energy loss, based on the peak of the Landau-Vavilov distribution underlying the 
#+CAPTION: Bethe-Bloch mean value. Interactive Vega-Lite version available at cite:vega_fig:theory:muon_argon_3cm_bethe_loss.
#+NAME: fig:theory:muon_argon_3cm_bethe_loss
[[~/phd/Figs/muonStudies/ar_energy_loss_cast.pdf]]


[fn:bethe_equation_form] Note that there are different common
parametrizations of the Bethe-Bloch equation.

**** TODOs for this section [/]                                 :noexport:

- [ ] Maybe restructure bethe bloch parameter explanation?

- [X] *TODO: ADD MOST PROBABLE LOSS TO PLOT BELOW!*

- [ ] Either replace referencing figure from bibliography and simply
  put mini description into reference or something like this:
  https://chat.openai.com/share/cdec7de9-40d3-4dfa-a0ff-0f5e43263ec3

Landau distribution!

Also check out this $f$ function that is mentioned here:
https://doi.org/10.1016/j.nima.2006.03.009

as a better way to compute the actual energy loss per distance?

- [ ] Also: read again PDG part about PDG and later in chapter the average
energy loss. Of course cannot take the mean of the Landau distribution
due to the long tail. We don't really do that in our muon simulation
though.

- [ ] *Mention* higher order corrections and names?
  -> The next corrections proportional to $Z³$ and $Z⁴$ are called /??/ and
  /shell correction/ respectively.  At higher energies also the density
  correction by Fermi *CITE* needs to be accounted for. These higher
  order corrections are mainly relevant for very low energies. *SEE PDG
  "Energy loss at low energies" section* *SHOW WITH OR WITHOUT. EQUATION
  WITH, BUT DROP IN CALCS*
  -> From cite:sauli2014gaseous on page 30:
  #+begin_quote
The expression shows that the differential energy loss depends only on the
particle’s velocity β and not on its mass; the additional term C/Z represents the
so-called inner shell corrections, that take into account a reduced ionization
efﬁciency on the deepest electronic layers due to screening effects, and δ/2 is a
density effect correction arising from a collective interaction between the medium
and the Coulomb ﬁeld of the particle at highly relativistic velocities; its contribu-
tion is small for non-condensed media. It should be noted, however, that in thin
absorbers electrons produced with high momentum transfer might escape from the
layer, thus reducing the effective yield.
  #+end_quote
  So:
  - C/Z term is the "inner shell correction" -> reduced ionization
    efficiency at deepest layers due to screening
  - δ/2 density effect correction from collective interaction medium
    and Coulomb field at highly relativistic speeds

- [ ] *Add C/Z term to Bethe-Bloch equation!*
- [ ] *Explain δ/2 term!*

***** Muons

The below is not really needed here, right? We have an entire section
about muons after all!



*REPHRASE* instead focus on fact that they lose > 2 GeV instead of
talking about typical muon energies.

Muons arriving at the surface have energies typically above
\SI{100}{\MeV}. For that reason the higher order corrections are not
of importance for the study of muons in gaseous detectors.

At each point the formula gives the *expectation value* for the energy
loss after a distance large enough to include many interactions. In
each interaction the particle loses energy according to a Landau
distribution *CITE WHAT*, shown in fig. *LANDAU PLOT*. 
*EXPLANATION NOT QUITE CORRECT*

*MOVE FOLLOWING TO SEPARATE SECTION LATER (noexport about muon studies?)*
By taking into account the Bethe formula and a Landau distribution for
each point, we can compute an expectation for the energy loss for
muons under typical conditions met in a gaseous detector.



**** Parameters Bethe-Bloch long                                :noexport:

- $K = 4π N_A r_e² m_e c² = \SI{0.307075}{MeV.mol^{-1}.cm^2}$
- $r_e = \frac{e²}{4π ε_0 m_e c²} = \SI{2.817 940 3227(19)}{fm}$: classical  
- $N_A = \SI{6.022 140 857(74)e23}{\mol^{-1}}$: Avogadro's number

  electron radius
- $m_e = \SI{9.1093837015(28)e-31}{\kg}$: electron mass
- $c = \SI{299792458}{\meter\per\second}$: speed of light in vacuum
- $z$: charge number of incident particle
- $Z$: atomic number of absorber material
- $A$: atomic mass of absorber material
- $β = \frac{v}{c}$: speed of incident particle
- $γ = \frac{1}{\sqrt{1 - β²}}$: Lorentz factor
- $W_{\text{max}}$: Maximum possible energy transfer to an electron in
  a single interaction
- $I$: mean excitation energy of the absorber material in \si{\eV}
- $δ(βγ)$: density-effect correction to energy loss

**** Bethe equation for muons traversing \SI{3}{\cm} of argon gas :noexport:

We will now compute the energy loss for muons traversing the
\SI{3}{\cm} of argon gas that are seen by a muon traversing
orthogonally to the readout plane (i.e. such that it may look like a
photon).

#+begin_src nim :results silent :tangle /home/basti/phd/code/bethe_bloch.nim
import math, macros, unchained, ggplotnim, sequtils, strformat, strutils
import thesisHelpers
import ggplotnim / ggplot_vegatex

let K = 4 * π * N_A * r_e^2 * m_e * c^2 # usually in: [MeV mol⁻¹ cm²]

defUnit(cm³•g⁻¹)
defUnit(J•m⁻¹)
defUnit(cm⁻³)
defUnit(g•mol⁻¹)
defUnit(MeV•g⁻¹•cm²)
defUnit(mol⁻¹)
defUnit(keV•cm⁻¹)
defUnit(g•cm⁻³)
defUnit(g•cm⁻²)

proc I[T](z: float): T =
  ## use Bloch approximation for all but Argon (better use tabulated values!)
  result = if z == 18.0: 188.0.eV.to(T) 
           else: (10.eV * z).to(T)

proc calcβ(γ: UnitLess): UnitLess =
  result = sqrt(1.0 - 1.0 / (γ^2))

proc betheBloch(z, Z: UnitLess, A: g•mol⁻¹, γ: UnitLess, M: kg): MeV•g⁻¹•cm² =
  ## result in MeV cm² g⁻¹ (normalized by density)
  ## z: charge of particle
  ## Z: charge of particles making up medium
  ## A: atomic mass of particles making up medium
  ## γ: Lorentz factor of particle
  ## M: mass of particle in MeV (or same mass as `m_e` defined as)
  let β = calcβ(γ)
  let W_max = 2 * m_e * c^2 * β^2 * γ^2 / (1 + 2 * γ * m_e / M + (m_e / M)^2)
  let lnArg = 2 * m_e * c^2 * β^2 * γ^2 * W_max / (I[Joule](Z)^2)
  result = (K * z^2 * Z / A * 1.0 / (β^2) * (
   0.5 * ln(lnArg) - β^2
  )).to(MeV•g⁻¹•cm²)

proc mostProbableLoss(z, Z: UnitLess, A: g•mol⁻¹, γ: UnitLess,
                      x: g•cm⁻²): keV =
  ## Computes the most probable value, corresponding to the peak of the Landau
  ## distribution, that gives rise to the Bethe-Bloch formula.
  ##
  ## Taken from PDG chapter 'Passage of particles through matter' equation
  ## `34.12` in 'Fluctuations in energy loss', version 2020).
  ##
  ## `x` is the "thickness". Density times length, `x = ρ * d`. The other parameters
  ## are as in `betheBloch` above.
  let β = calcβ(γ)
  let ξ = K / 2.0 * Z / A * z*z * (x / (β*β))
  const j = 0.200
  let I = I[Joule](Z)
  result = (ξ * ( ln((2 * m_e * c^2 * β^2 * γ^2).to(Joule) / I) + ln(ξ.to(Joule) / I) + j - β^2)).to(keV) # - δ*(β*γ)

proc density(p: mbar, M: g•mol⁻¹, temp: Kelvin): g•cm⁻³ =
  ## returns the density of the gas for the given pressure.
  ## The pressure is assumed in `mbar` and the temperature (in `K`).
  ## The default temperature corresponds to BabyIAXO aim.
  ## Returns the density in `g / cm^3`
  let gasConstant = 8.314.J•K⁻¹•mol⁻¹ # joule K^-1 mol^-1
  let pressure = p.to(Pa) # pressure in Pa
  result = (pressure * M / (gasConstant * temp)).to(g•cm⁻³)

proc E_to_γ(E: GeV): UnitLess =
  result = E.to(Joule) / (m_μ * c^2) + 1

type
  Element = object
    name: string
    Z: UnitLess
    M: g•mol⁻¹
    A: UnitLess # numerically same as `M`
    ρ: g•cm⁻³

proc initElement(name: string, Z: UnitLess, M: g•mol⁻¹, ρ: g•cm⁻³): Element =
  Element(name: name, Z: Z, M: M, A: M.UnitLess, ρ: ρ)

let M_Ar = 39.95.g•mol⁻¹ # molar mass. Numerically same as relative atomic mass
#let ρAr = density(1050.mbar, M_Ar, temp = 293.15.K)
let ρAr = density(1013.mbar, M_Ar, temp = 293.15.K)
let Argon = initElement("ar", 18.0.UnitLess, 39.95.g•mol⁻¹, ρAr)

proc intBethe(e: Element, d_total: cm, E0: eV, dx = 1.μm): eV =
  ## integrated energy loss of bethe formula after `d` cm of matter
  ## and returns the energy remaining
  var γ: UnitLess = E_to_γ(E0.to(GeV))
  var d: cm
  result = E0
  var totalLoss = 0.eV
  while d < d_total and result > 0.eV:
    let E_loss: MeV = betheBloch(-1, e.Z, e.M, γ, m_μ) * e.ρ * dx
    result = result - E_loss.to(eV)
    γ = E_to_γ(result.to(GeV))
    d = d + dx.to(cm)
    totalLoss = totalLoss + E_loss.to(eV)
  result = max(0.float, result.float).eV

func argonLabel(): string = "fig:theory:muon_argon_3cm_bethe_loss"

## TODO: add in the most probable value calc!  
func argonCaption(): string = 
  result = r"Mean energy loss via Bethe-Bloch (purple) equation of muons in \SI{3}{\cm} of argon at " &
    r"conditions in use in GridPix detector at CAST. \SI{1050}{mbar} of chamber pressure at room " &
    r"temperature. Note that the mean is skewed by events that transfer a large amount of energy, " &
    r"but are very rare! As such care must be taken interpreting the numbers. Green shows the most " &
    r"probable energy loss, based on the peak of the Landau-Vavilov distribution underlying the " &
    r"Bethe-Bloch mean value." &
    interactiveVega(argonLabel())

proc plotDetectorAbsorption(element: Element) =
  let E_float = logspace(-2, 2, 1000)
  let energies = E_float.mapIt(it.GeV)
  let E_loss = energies.mapIt((it.to(eV) - intBethe(element, 3.cm, it.to(eV))).to(keV).float)
  let E_lossMP = energies.mapIt(mostProbableLoss(-1, element.Z, element.M, E_to_γ(it), ρ_Ar * 3.cm).float)
  let df = seqsToDf({E_float, "Bethe-Bloch (BB)" : E_loss, "Most probable (MP)" : E_lossMP})
    .gather(["Bethe-Bloch (BB)", "Most probable (MP)"], "Type", "Value")
  ggplot(df, aes("E_float", "Value", color = "Type")) +
    geom_line() +
    #xlab(r"μ Energy [\si{\GeV}]") + ylab(r"$-\left\langle \frac{\mathrm{d}E}{\mathrm{d}x}\right\rangle$ [\si{\keV}]") +
    xlab(r"μ Energy [\si{\GeV}]") +
    ylab(r"$-\left\langle \frac{\mathrm{d}E}{\mathrm{d}x}\right\rangle$ (BB), $Δ_p$ (MP) [\si{\keV}]") +
    scale_x_log10() + scale_y_log10() +
    #theme_latex() +
    margin(right = 5.5) + 
    ggtitle(r"Energy loss of Muons in \SI{3}{\cm} " & &"{element.name.capitalizeAscii} at CAST conditions") +
    #ggsave(&"/home/basti/phd/Figs/muonStudies/{element.name}_energy_loss_cast.pdf", useTeX = true, standalone = true)
    ggvegatex(&"/home/basti/phd/Figs/muonStudies/{element.name}_energy_loss_cast",
              onlyTikZ = false,
              caption = argonCaption(),
              label = argonLabel(),
              width = 600, height = 360)
plotDetectorAbsorption(Argon)

proc plotMostProbable(e: Element) =
  let E_float = logspace(-1.5, 2, 1000)
  let energies = E_float.mapIt(it.GeV)
  let E_loss = energies.mapIt(mostProbableLoss(-1, e.Z, e.M, E_to_γ(it), ρ_Ar * 3.cm))
  let df = toDf({"E_loss" : E_loss.mapIt(it.float), E_float})
  ggplot(df, aes("E_float", "E_loss")) +
    geom_line() +
    scale_x_log10() + 
    xlab("Energy [GeV]") + ylab("Most probable loss [keV]") +
    ggsave("/tmp/most_probable_loss.pdf")
plotMostProbable(Argon)
#+end_src

*** X-ray fluorescence
:PROPERTIES:
:CUSTOM_ID: sec:theory:xray_fluorescence
:END:

Cosmic muons in their interactions with matter can ionize atoms,
leading to the possible emission of X-rays if the removed electron is
part of an inner shell, mostly K (and some L) shell electrons. This
leads to a form of background based on real X-rays and thus represents
a kind of background that is impossible to distinguish from any kind
of axion signal unless external scintillator based vetoes are used. Of
course to be relevant as a form of detector background the material
must be close to the detector, as the X-rays will otherwise be
absorbed. This makes the detector material, the gas itself and all
material in the direction of the detectors' sensitivity a candidate
for X-ray fluorescence background.

Table [[tab:theory:xray_fluorescence]] lists the X-ray fluorescence lines
for elements one commonly encounters in the context of gaseous
detectors for a helioscope experiment. Table
[[tab:theory:binding_energies]] lists the atomic binding energies of the
same elements. This is intended as a useful reference to understand
possible lines in background, potential targets for an X-ray tube for
reference X-ray data and understand the absorption edges for
materials.

The difference between the binding energy and the energies of the most
likely fluorescence lines is the explanation for why commonly a
material is more transparent for a fluorescence X-ray than energies
slightly above it. This (among other effects) explains the 'escape
peak' seen in many types of gaseous detectors and is also the
effectively the reason the typical absorption edges in materials. For
example see fig. [[fig:theory:transmission_examples]] for the Argon
transmission. The Argon $K 1s$ binding energy of $\SI{3.2}{keV}$ is
visible in the form of the sudden drop in transmission (which is of
course directly proportional to the absorption length!). At the same
time an X-ray produced by an ionized Argon atom from the $K 1s$
electron via the $Kα$ line yields an X-ray of only $\SI{2.95}{keV}$
and thus Argon gas is extremely transparent for such X-rays (this is
the cause of so called 'escape photons', see
sec. [[#sec:theory:escape_peaks_55fe]]).

#+NAME: tab:theory:xray_fluorescence
#+CAPTION: Excerpt of X-ray fluorescence energies of K, L and M emission lines for different elements
#+CAPTION: mostly relevant for the context of this thesis in in \si{eV}. 
#+CAPTION: Taken from the X-ray data book cite:williams2001x, specifically https://xdb.lbl.gov/Section1/Table_1-2.pdf.
#+ATTR_LATEX: :booktabs t :font \footnotesize :float sidewaystable
|----+---------+-----------+----------+----------+----------+----------+----------+----------+----------+---------|
|  Z | Element | Kα1       | Kα2      | Kβ1      | Lα1      | Lα2      | Lβ1      | Lβ2      | Lγ1      | Mα1     |
|----+---------+-----------+----------+----------+----------+----------+----------+----------+----------+---------|
|  6 | C       | 277       |          |          |          |          |          |          |          |         |
|  7 | N       | 392.4     |          |          |          |          |          |          |          |         |
|  8 | O       | 524.9     |          |          |          |          |          |          |          |         |
| 13 | Al      | 1,486.70  | 1,486.27 | 1,557.45 |          |          |          |          |          |         |
| 14 | Si      | 1,739.98  | 1,739.38 | 1,835.94 |          |          |          |          |          |         |
| 18 | Ar      | 2,957.70  | 2,955.63 | 3,190.5  |          |          |          |          |          |         |
| 22 | Ti      | 4,510.84  | 4,504.86 | 4,931.81 | 452.2    | 452.2    | 458.4    |          |          |         |
| 25 | Mn      | 5,898.75  | 5,887.65 | 6,490.45 | 637.4    | 637.4    | 648.8    |          |          |         |
| 26 | Fe      | 6,403.84  | 6,390.84 | 7,057.98 | 705.0    | 705.0    | 718.5    |          |          |         |
| 28 | Ni      | 7,478.15  | 7,460.89 | 8,264.66 | 851.5    | 851.5    | 868.8    |          |          |         |
| 29 | Cu      | 8,047.78  | 8,027.83 | 8,905.29 | 929.7    | 929.7    | 949.8    |          |          |         |
| 47 | Ag      | 22,162.92 | 21,990.3 | 24,942.4 | 2,984.31 | 2,978.21 | 3,150.94 | 3,347.81 | 3,519.59 |         |
| 54 | Xe      | 29,779    | 29,458   | 33,624   | 4,109.9  | —       | —       |          | —       |         |
| 78 | Pt      | 66,832    | 65,112   | 75,748   | 9,442.3  | 9,361.8  | 11,070.7 | 11,250.5 | 12,942.0 | 2,050.5 |
| 79 | Au      | 68,803.7  | 66,989.5 | 77,984   | 9,713.3  | 9,628.0  | 11,442.3 | 11,584.7 | 13,381.7 | 2,122.9 |
| 82 | Pb      | 74,969.4  | 72,804.2 | 84,936   | 10,551.5 | 10,449.5 | 12,613.7 | 12,622.6 | 14,764.4 | 2,345.5 |

#+CAPTION: Excerpt of electron binding energies of elements mostly relevant for the context of
#+CAPTION: this thesis in \si{eV}. Taken from the X-ray data book cite:williams2001x,
#+CAPTION: specifically https://xdb.lbl.gov/Section1/Table_1-1.pdf.
#+NAME: tab:theory:binding_energies
#+ATTR_LATEX: :booktabs t :font \footnotesize :float sidewaystable
|----+---------+--------+----------+----------+----------+----------+----------+----------+----------+----------+----------+
|  Z | Element |   K 1s |    L1 2s | L2 2p1/2 | L3 2p3/2 |    M1 3s | M2 3p1/2 | M3 3p3/2 | M4 3d3/2 | M5 3d5/2 |          |
|----+---------+--------+----------+----------+----------+----------+----------+----------+----------+----------+----------+
|  6 | C       |  284.2 |          |          |          |          |          |          |          |          |          |
|  7 | N       |  409.9 |     37.3 |          |          |          |          |          |          |          |          |
|  8 | O       |  543.1 |     41.6 |          |          |          |          |          |          |          |          |
| 13 | Al      | 1559.6 |    117.8 |    72.95 |    72.55 |          |          |          |          |          |          |
| 14 | Si      |   1839 |  149.7   |    99.82 |    99.42 |          |          |          |          |          |          |
| 18 | Ar      | 3205.9 |    326.3 |    250.6 |    248.4 |     29.3 |     15.9 |     15.7 |          |          |          |
| 22 | Ti      |   4966 |    560.9 |    460.2 |    453.8 |     58.7 |     32.6 |     32.6 |          |          |          |
| 25 | Mn      |   6539 |    769.1 |    649.9 |    638.7 |     82.3 |     47.2 |     47.2 |          |          |          |
| 26 | Fe      |   7112 |    844.6 |    719.9 |    706.8 |     91.3 |     52.7 |     52.7 |          |          |          |
| 28 | Ni      |   8333 |   1008.6 |    870.0 |    852.7 |    110.8 |     68.0 |     66.2 |          |          |          |
| 29 | Cu      |   8979 |   1096.7 |    952.3 |    932.7 |    122.5 |     77.3 |     75.1 |          |          |          |
| 47 | Ag      |  25514 |     3806 |     3524 |     3351 |    719.0 |    603.8 |    573.0 |    374.0 |    368.3 |          |
| 54 | Xe      |  34561 |     5453 |     5107 |     4786 |   1148.7 |   1002.1 |    940.6 |    689.0 |    676.4 |          |
| 78 | Pt      |  78395 |    13880 |    13273 |    11564 |     3296 |     3027 |     2645 |     2202 |     2122 |          |
| 79 | Au      |  80725 |    14353 |    13734 |    11919 |     3425 |     3148 |     2743 |     2291 |     2206 |          |
| 82 | Pb      |  88005 |    15861 |    15200 |    13035 |     3851 |     3554 |     3066 |     2586 |     2484 |    891.8 |
|----+---------+--------+----------+----------+----------+----------+----------+----------+----------+----------+----------+
|  Z | Element |  N1 4s | N2 4p1/2 | N3 4p3/2 | N4 4d3/2 | N5 4d5/2 | N6 4f5/2 | N7 4f7/2 |    O1 5s | O2 5p1/2 | O3 5p3/2 |
|----+---------+--------+----------+----------+----------+----------+----------+----------+----------+----------+----------+
| 47 | Ag      |   97.0 |     63.7 |     58.3 |          |          |          |          |          |          |          |
| 54 | Xe      |  213.2 |    146.7 |    145.5 |     69.5 |     67.5 |      --- |      --- |     23.3 |     13.4 |     12.1 |
| 78 | Pt      |  725.4 |    609.1 |    519.4 |    331.6 |    314.6 |     74.5 |     71.2 |    101.7 |     65.3 |     51.7 |
| 79 | Au      |  762.1 |    642.7 |    546.3 |    353.2 |    335.1 |     87.6 |     84.0 |    107.2 |     74.2 |     57.2 |
| 82 | Pb      |  761.9 |    643.5 |    434.3 |    412.2 |    141.7 |    136.9 |      147 |    106.4 |     83.3 |     20.7 |

**** TODOs for this section [/]                                 :noexport:

- [ ] *HOW DOES THIS CORRESPOND TO AUGER ELECTRONS?*

- [X] *TOO MUCH DETAIL HERE?* *CHECK THE SHELL STUFF, GIVE A MINI TABLE OF
  IMPORTANT ATOMIC LINES!*

Important for our 3 keV Argon line + 8 keV copper line mainly.

- [X] Tab. *TABLE INSERT* contains the different lines of plausible
  materials used for detector construction / etc. *...*
  *ASK TOBI IF TO ADD SOME MATERIAL*

- [X] *ADD RELEVANT TABLE FOR BINDING ENERGY AS WELL!*  
- [X] *TODO: REMOVE UNNECESSARY LINES*

- [ ] *REPHRASE THE BELOW IN PARTICULAR REFERENCE TO ABSORPTION EDGE*
                 
**** Full tables for X-ray fluorescence lines and binding energies :noexport: 

#+NAME: tab_all_xray_fluorescence
#+CAPTION: Photon energies of K, L and M emission lines for different elements in \si{eV}. 
#+CAPTION: Taken from cite:williams2001x, specifically https://xdb.lbl.gov/Section1/Table_1-2.pdf.
|----+---------+-----------+-----------+----------+----------+-----------+----------+----------+----------+---------|
|  Z | Element | Kα1       | Kα2       | Kβ1      | Lα1      | Lα2       | Lβ1      | Lβ2      | Lγ1      | Mα1     |
|----+---------+-----------+-----------+----------+----------+-----------+----------+----------+----------+---------|
|  3 | Li      | 54.3      |           |          |          |           |          |          |          |         |
|  4 | Be      | 108.5     |           |          |          |           |          |          |          |         |
|  5 | B       | 183.3     |           |          |          |           |          |          |          |         |
|  6 | C       | 277       |           |          |          |           |          |          |          |         |
|  7 | N       | 392.4     |           |          |          |           |          |          |          |         |
|  8 | O       | 524.9     |           |          |          |           |          |          |          |         |
|  9 | F       | 676.8     |           |          |          |           |          |          |          |         |
| 10 | Ne      | 848.6     | 848.6     |          |          |           |          |          |          |         |
| 11 | Na      | 1,040.98  | 1,040.98  | 1,071.1  |          |           |          |          |          |         |
| 12 | Mg      | 1,253.60  | 1,253.60  | 1,302.2  |          |           |          |          |          |         |
| 13 | Al      | 1,486.70  | 1,486.27  | 1,557.45 |          |           |          |          |          |         |
| 14 | Si      | 1,739.98  | 1,739.38  | 1,835.94 |          |           |          |          |          |         |
| 15 | P       | 2,013.7   | 2,012.7   | 2,139.1  |          |           |          |          |          |         |
| 16 | S       | 2,307.84  | 2,306.64  | 2,464.04 |          |           |          |          |          |         |
| 17 | Cl      | 2,622.39  | 2,620.78  | 2,815.6  |          |           |          |          |          |         |
| 18 | Ar      | 2,957.70  | 2,955.63  | 3,190.5  |          |           |          |          |          |         |
| 19 | K       | 3,313.8   | 3,311.1   | 3,589.6  |          |           |          |          |          |         |
| 20 | Ca      | 3,691.68  | 3,688.09  | 4,012.7  | 341.3    | 341.3     | 344.9    |          |          |         |
| 21 | Sc      | 4,090.6   | 4,086.1   | 4,460.5  | 395.4    | 395.4     | 399.6    |          |          |         |
|----+---------+-----------+-----------+----------+----------+-----------+----------+----------+----------+---------|
|  Z | Element | Kα1       | Kα2       | Kβ1      | Lα1      | Lα2       | Lβ1      | Lβ2      | Lγ1      | Mα1     |
|----+---------+-----------+-----------+----------+----------+-----------+----------+----------+----------+---------|
| 22 | Ti      | 4,510.84  | 4,504.86  | 4,931.81 | 452.2    | 452.2     | 458.4    |          |          |         |
| 23 | V       | 4,952.20  | 4,944.64  | 5,427.29 | 511.3    | 511.3     | 519.2    |          |          |         |
| 24 | Cr      | 5,414.72  | 5,405.509 | 5,946.71 | 572.8    | 572.8     | 582.8    |          |          |         |
| 25 | Mn      | 5,898.75  | 5,887.65  | 6,490.45 | 637.4    | 637.4     | 648.8    |          |          |         |
| 26 | Fe      | 6,403.84  | 6,390.84  | 7,057.98 | 705.0    | 705.0     | 718.5    |          |          |         |
| 27 | Co      | 6,930.32  | 6,915.30  | 7,649.43 | 776.2    | 776.2     | 791.4    |          |          |         |
| 28 | Ni      | 7,478.15  | 7,460.89  | 8,264.66 | 851.5    | 851.5     | 868.8    |          |          |         |
| 29 | Cu      | 8,047.78  | 8,027.83  | 8,905.29 | 929.7    | 929.7     | 949.8    |          |          |         |
| 30 | Zn      | 8,638.86  | 8,615.78  | 9,572.0  | 1,011.7  | 1,011.7   | 1,034.7  |          |          |         |
| 31 | Ga      | 9,251.74  | 9,224.82  | 10,264.2 | 1,097.92 | 1,097.92  | 1,124.8  |          |          |         |
| 32 | Ge      | 9,886.42  | 9,855.32  | 10,982.1 | 1,188.00 | 1,188.00  | 1,218.5  |          |          |         |
| 33 | As      | 10,543.72 | 10,507.99 | 11,726.2 | 1,282.0  | 1,282.0   | 1,317.0  |          |          |         |
| 34 | Se      | 11,222.4  | 11,181.4  | 12,495.9 | 1,379.10 | 1,379.10  | 1,419.23 |          |          |         |
| 35 | Br      | 11,924.2  | 11,877.6  | 13,291.4 | 1,480.43 | 1,480.43  | 1,525.90 |          |          |         |
| 36 | Kr      | 12,649    | 12,598    | 14,112   | 1,586.0  | 1,586.0   | 1,636.6  |          |          |         |
| 37 | Rb      | 13,395.3  | 13,335.8  | 14,961.3 | 1,694.13 | 1,692.56  | 1,752.17 |          |          |         |
| 38 | Sr      | 14,165    | 14,097.9  | 15,835.7 | 1,806.56 | 1,804.74  | 1,871.72 |          |          |         |
| 39 | Y       | 14,958.4  | 14,882.9  | 16,737.8 | 1,922.56 | 1,920.47  | 1,995.84 |          |          |         |
| 40 | Zr      | 15,775.1  | 15,690.9  | 17,667.8 | 2,042.36 | 2,039.9   | 2,124.4  | 2,219.4  | 2,302.7  |         |
| 41 | Nb      | 16,615.1  | 16,521.0  | 18,622.5 | 2,165.89 | 2,163.0   | 2,257.4  | 2,367.0  | 2,461.8  |         |
| 42 | Mo      | 17,479.34 | 17,374.3  | 19,608.3 | 2,293.16 | 2,289.85  | 2,394.81 | 2,518.3  | 2,623.5  |         |
| 43 | Tc      | 18,367.1  | 18,250.8  | 20,619   | 2,424    | 2,420     | 2,538    | 2,674    | 2,792    |         |
| 44 | Ru      | 19,279.2  | 19,150.4  | 21,656.8 | 2,558.55 | 2,554.31  | 2,683.23 | 2,836.0  | 2,964.5  |         |
| 45 | Rh      | 20,216.1  | 20,073.7  | 22,723.6 | 2,696.74 | 2,692.05  | 2,834.41 | 3,001.3  | 3,143.8  |         |
| 46 | Pd      | 21,177.1  | 21,020.1  | 23,818.7 | 2,838.61 | 2,833.29  | 2,990.22 | 3,171.79 | 3,328.7  |         |
| 47 | Ag      | 22,162.92 | 21,990.3  | 24,942.4 | 2,984.31 | 2,978.21  | 3,150.94 | 3,347.81 | 3,519.59 |         |
| 48 | Cd      | 23,173.6  | 22,984.1  | 26,095.5 | 3,133.73 | 3,126.91  | 3,316.57 | 3,528.12 | 3,716.86 |         |
| 49 | In      | 24,209.7  | 24,002.0  | 27,275.9 | 3,286.94 | 3,279.29  | 3,487.21 | 3,713.81 | 3,920.81 |         |
| 50 | Sn      | 25,271.3  | 25,044.0  | 28,486.0 | 3,443.98 | 3,435.42  | 3,662.80 | 3,904.86 | 4,131.12 |         |
| 51 | Sb      | 26,359.1  | 26,110.8  | 29,725.6 | 3,604.72 | 3,595.32  | 3,843.57 | 4,100.78 | 4,347.79 |         |
| 52 | Te      | 27,472.3  | 27,201.7  | 30,995.7 | 3,769.33 | 3,758.8   | 4,029.58 | 4,301.7  | 4,570.9  |         |
| 53 | I       | 28,612.0  | 28,317.2  | 32,294.7 | 3,937.65 | 3,926.04  | 4,220.72 | 4,507.5  | 4,800.9  |         |
| 54 | Xe      | 29,779    | 29,458    | 33,624   | 4,109.9  | —         | —        | —        | —        |         |
| 55 | Cs      | 30,972.8  | 30,625.1  | 34,986.9 | 4,286.5  | 4,272.2   | 4,619.8  | 4,935.9  | 5,280.4  |         |
| 56 | Ba      | 32,193.6  | 31,817.1  | 36,378.2 | 4,466.26 | 4,450.90  | 4,827.53 | 5,156.5  | 5,531.1  |         |
| 57 | La      | 33,441.8  | 33,034.1  | 37,801.0 | 4,650.97 | 4,634.23  | 5,042.1  | 5,383.5  | 5,788.5  | 833     |
| 58 | Ce      | 34,719.7  | 34,278.9  | 39,257.3 | 4,840.2  | 4,823.0   | 5,262.2  | 5,613.4  | 6,052    | 883     |
| 59 | Pr      | 36,026.3  | 35,550.2  | 40,748.2 | 5,033.7  | 5,013.5   | 5,488.9  | 5,850    | 6,322.1  | 929     |
| 60 | Nd      | 37,361.0  | 36,847.4  | 42,271.3 | 5,230.4  | 5,207.7   | 5,721.6  | 6,089.4  | 6,602.1  | 978     |
| 61 | Pm      | 38,724.7  | 38,171.2  | 43,826   | 5,432.5  | 5,407.8   | 5,961    | 6,339    | 6,892    | —       |
| 62 | Sm      | 40,118.1  | 39,522.4  | 45,413   | 5,636.1  | 5,609.0   | 6,205.1  | 6,586    | 7,178    | 1,081   |
|----+---------+-----------+-----------+----------+----------+-----------+----------+----------+----------+---------|
|  Z | Element | Kα1       | Kα2       | Kβ1      | Lα1      | Lα2       | Lβ1      | Lβ2      | Lγ1      | Mα1     |
|----+---------+-----------+-----------+----------+----------+-----------+----------+----------+----------+---------|
| 63 | Eu      | 41,542.2  | 40,901.9  | 47,037.9 | 5,845.7  | 5,816.6   | 6,456.4  | 6,843.2  | 7,480.3  | 1,131   |
| 64 | Gd      | 42,996.2  | 42,308.9  | 48,697   | 6,057.2  | 6,025.0   | 6,713.2  | 7,102.8  | 7,785.8  | 1,185   |
| 65 | Tb      | 44,481.6  | 43,744.1  | 50,382   | 6,272.8  | 6,238.0   | 6,978    | 7,366.7  | 8,102    | 1,240   |
| 66 | Dy      | 45,998.4  | 45,207.8  | 52,119   | 6,495.2  | 6,457.7   | 7,247.7  | 7,635.7  | 8,418.8  | 1,293   |
| 67 | Ho      | 47,546.7  | 46,699.7  | 53,877   | 6,719.8  | 6,679.5   | 7,525.3  | 7,911    | 8,747    | 1,348   |
| 68 | Er      | 49,127.7  | 48,221.1  | 55,681   | 6,948.7  | 6,905.0   | 7,810.9  | 8,189.0  | 9,089    | 1,406   |
| 69 | Tm      | 50,741.6  | 49,772.6  | 57,517   | 7,179.9  | 7,133.1   | 8,101    | 8,468    | 9,426    | 1,462   |
| 70 | Yb      | 52,388.9  | 51,354.0  | 59,370   | 7,415.6  | 7,367.3   | 8,401.8  | 8,758.8  | 9,780.1  | 1,521.4 |
| 71 | Lu      | 54,069.8  | 52,965.0  | 61,283   | 7,655.5  | 7,604.9   | 8,709.0  | 9,048.9  | 10,143.4 | 1,581.3 |
| 72 | Hf      | 55,790.2  | 54,611.4  | 63,234   | 7,899.0  | 7,844.6   | 9,022.7  | 9,347.3  | 10,515.8 | 1,644.6 |
| 73 | Ta      | 57,532    | 56,277    | 65,223   | 8,146.1  | 8,087.9   | 9,343.1  | 9,651.8  | 10,895.2 | 1,710   |
| 74 | W       | 59,318.24 | 57,981.7  | 67,244.3 | 8,397.6  | 8,335.2   | 9,672.35 | 9,961.5  | 11,285.9 | 1,775.4 |
| 75 | Re      | 61,140.3  | 59,717.9  | 69,310   | 8,652.5  | 8,586.2   | 10,010.0 | 10,275.2 | 11,685.4 | 1,842.5 |
| 76 | Os      | 63,000.5  | 61,486.7  | 71,413   | 8,911.7  | 8,841.0   | 10,355.3 | 10,598.5 | 12,095.3 | 1,910.2 |
| 77 | Ir      | 64,895.6  | 63,286.7  | 73,560.8 | 9,175.1  | 9,099.5   | 10,708.3 | 10,920.3 | 12,512.6 | 1,979.9 |
| 78 | Pt      | 66,832    | 65,112    | 75,748   | 9,442.3  | 9,361.8   | 11,070.7 | 11,250.5 | 12,942.0 | 2,050.5 |
| 79 | Au      | 68,803.7  | 66,989.5  | 77,984   | 9,713.3  | 9,628.0   | 11,442.3 | 11,584.7 | 13,381.7 | 2,122.9 |
| 80 | Hg      | 70,819    | 68,895    | 80,253   | 9,988.8  | 9,897.6   | 11,822.6 | 11,924.1 | 13,830.1 | 2,195.3 |
| 81 | Tl      | 72,871.5  | 70,831.9  | 82,576   | 10,268.5 | 10,172.8  | 12,213.3 | 12,271.5 | 14,291.5 | 2,270.6 |
| 82 | Pb      | 74,969.4  | 72,804.2  | 84,936   | 10,551.5 | 10,449.5  | 12,613.7 | 12,622.6 | 14,764.4 | 2,345.5 |
| 83 | Bi      | 77,107.9  | 74,814.8  | 87,343   | 10,838.8 | 10,730.91 | 13,023.5 | 12,979.9 | 15,247.7 | 2,422.6 |
| 84 | Po      | 79,290    | 76,862    | 89,800   | 11,130.8 | 11,015.8  | 13,447   | 13,340.4 | 15,744   | —       |
| 85 | At      | 81,520    | 78,950    | 92,300   | 11,426.8 | 11,304.8  | 13,876   | —        | 16,251   | —       |
| 86 | Rn      | 83,780    | 81,070    | 94,870   | 11,727.0 | 11,597.9  | 14,316   | —        | 16,770   | —       |
| 87 | Fr      | 86,100    | 83,230    | 97,470   | 12,031.3 | 11,895.0  | 14,770   | 14,450   | 17,303   | —       |
| 88 | Ra      | 88,470    | 85,430    | 100,130  | 12,339.7 | 12,196.2  | 15,235.8 | 14,841.4 | 17,849   | —       |
| 89 | Ac      | 90,884    | 87,670    | 102,850  | 12,652.0 | 12,500.8  | 15,713   | —        | 18,408   | —       |
| 90 | Th      | 93,350    | 89,953    | 105,609  | 12,968.7 | 12,809.6  | 16,202.2 | 15,623.7 | 18,982.5 | 2,996.1 |
| 91 | Pa      | 95,868    | 92,287    | 108,427  | 13,290.7 | 13,122.2  | 16,702   | 16,024   | 19,568   | 3,082.3 |
| 92 | U       | 98,439    | 94,665    | 111,300  | 13,614.7 | 13,438.8  | 17,220.0 | 16,428.3 | 20,167.1 | 3,170.8 |
| 93 | Np      | —         | —         | —        | 13,944.1 | 13,759.7  | 17,750.2 | 16,840.0 | 20,784.8 | —       |
| 94 | Pu      | —         | —         | —        | 14,278.6 | 14,084.2  | 18,293.7 | 17,255.3 | 21,417.3 | —       |
| 95 | Am      | —         | —         | —        | 14,617.2 | 14,411.9  | 18,852.0 | 17,676.5 | 22,065.2 | —       |
|----+---------+-----------+-----------+----------+----------+-----------+----------+----------+----------+---------|

X-Ray Data Booklet Table 1-1. Electron binding energies, in electron
volts, for the elements in their natural forms.  
https://xdb.lbl.gov/Section1/Table_1-1.pdf

#+CAPTION: Electron binding energies of all elements up to uranium in \si{eV}.
#+CAPTION: Taken from the X-ray data book cite:williams2001x,
#+CAPTION: specifically https://xdb.lbl.gov/Section1/Table_1-1.pdf.
#+NAME: tab_all_atomic_binding_energies
|----+---------+----------+----------+----------+----------+---------+----------+----------+----------+----------+--------+----------+----------|
|  Z | Element |     K 1s | L1 2s    | L2 2p1/2 | L3 2p3/2 | M1 3s   | M2 3p1/2 | M3 3p3/2 | M4 3d3/2 | M5 3d5/2 | N1 4s  | N2 4p1/2 | N3 4p3/2 |
|----+---------+----------+----------+----------+----------+---------+----------+----------+----------+----------+--------+----------+----------|
|  1 | H       |     13.6 |          |          |          |         |          |          |          |          |        |          |          |
|  2 | He      |    24.6* |          |          |          |         |          |          |          |          |        |          |          |
|  3 | Li      |    54.7* |          |          |          |         |          |          |          |          |        |          |          |
|  4 | Be      |   111.5* |          |          |          |         |          |          |          |          |        |          |          |
|  5 | B       |     188* |          |          |          |         |          |          |          |          |        |          |          |
|  6 | C       |   284.2* |          |          |          |         |          |          |          |          |        |          |          |
|  7 | N       |   409.9* | 37.3*    |          |          |         |          |          |          |          |        |          |          |
|  8 | O       |   543.1* | 41.6*    |          |          |         |          |          |          |          |        |          |          |
|  9 | F       |   696.7* |          |          |          |         |          |          |          |          |        |          |          |
| 10 | Ne      |   870.2* | 48.5*    |    21.7* |    21.6* |         |          |          |          |          |        |          |          |
| 11 | Na      |  1070.8† | 63.5†    |    30.65 |    30.81 |         |          |          |          |          |        |          |          |
| 12 | Mg      |  1303.0† | 88.7     |    49.78 |    49.50 |         |          |          |          |          |        |          |          |
| 13 | Al      |   1559.6 | 117.8    |    72.95 |    72.55 |         |          |          |          |          |        |          |          |
| 14 | Si      |     1839 | 149.7*b  |    99.82 |    99.42 |         |          |          |          |          |        |          |          |
| 15 | P       |   2145.5 | 189*     |     136* |     135* |         |          |          |          |          |        |          |          |
| 16 | S       |     2472 | 230.9    |   163.6* |   162.5* |         |          |          |          |          |        |          |          |
| 17 | Cl      |   2822.4 | 270*     |     202* |     200* |         |          |          |          |          |        |          |          |
| 18 | Ar      |  3205.9* | 326.3*   |   250.6† |   248.4* | 29.3*   | 15.9*    | 15.7*    |          |          |        |          |          |
| 19 | K       |  3608.4* | 378.6*   |   297.3* |   294.6* | 34.8*   | 18.3*    | 18.3*    |          |          |        |          |          |
| 20 | Ca      |  4038.5* | 438.4†   |   349.7† |   346.2† | 44.3    | †        | 25.4†    | 25.4†    |          |        |          |          |
| 21 | Sc      |     4492 | 498.0*   |   403.6* |   398.7* | 51.1*   | 28.3*    | 28.3*    |          |          |        |          |          |
| 22 | Ti      |     4966 | 560.9†   |   460.2† |   453.8† | 58.7†   | 32.6†    | 32.6†    |          |          |        |          |          |
| 23 | V       |     5465 | 626.7†   |   519.8† |   512.1† | 66.3†   | 37.2†    | 37.2†    |          |          |        |          |          |
| 24 | Cr      |     5989 | 696.0†   |   583.8† |   574.1† | 74.1†   | 42.2†    | 42.2†    |          |          |        |          |          |
| 25 | Mn      |     6539 | 769.1†   |   649.9† |   638.7† | 82.3†   | 47.2†    | 47.2†    |          |          |        |          |          |
| 26 | Fe      |     7112 | 844.6†   |   719.9† |   706.8† | 91.3†   | 52.7†    | 52.7†    |          |          |        |          |          |
| 27 | Co      |     7709 | 925.1†   |   793.2† |   778.1† | 101.0†  | 58.9†    | 59.9†    |          |          |        |          |          |
| 28 | Ni      |     8333 | 1008.6†  |   870.0† |   852.7† | 110.8†  | 68.0†    | 66.2†    |          |          |        |          |          |
| 29 | Cu      |     8979 | 1096.7†  |   952.3† |    932.7 | 122.5†  | 77.3†    | 75.1†    |          |          |        |          |          |
| 30 | Zn      |     9659 | 1196.2*  |  1044.9* |  1021.8* | 139.8*  | 91.4*    | 88.6*    | 10.2*    | 10.1*    |        |          |          |
| 31 | Ga      |    10367 | 1299.0*b |  1143.2† |  1116.4† | 159.5†  | 103.5†   | 100.0†   | 18.7†    | 18.7†    |        |          |          |
| 32 | Ge      |    11103 | 1414.6*b | 1248.1*b | 1217.0*b | 180.1*  | 124.9*   | 120.8*   | 29.8     | 29.2     |        |          |          |
| 33 | As      |    11867 | 1527.0*b | 1359.1*b | 1323.6*b | 204.7*  | 146.2*   | 141.2*   | 41.7*    | 41.7*    |        |          |          |
| 34 | Se      |    12658 | 1652.0*b | 1474.3*b | 1433.9*b | 229.6*  | 166.5*   | 160.7*   | 55.5*    | 54.6*    |        |          |          |
| 35 | Br      |    13474 | 1782*    |    1596* |    1550* | 257*    | 189*     | 182*     | 70*      | 69*      |        |          |          |
| 36 | Kr      |    14326 | 1921     |  1730.9* |  1678.4* | 292.8*  | 222.2*   | 214.4    | 95.0*    | 93.8*    | 27.5*  | 14.1*    | 14.1*    |
| 37 | Rb      |    15200 | 2065     |     1864 |     1804 | 326.7*  | 248.7*   | 239.1*   | 113.0*   | 112*     | 30.5*  | 16.3*    | 15.3*    |
| 38 | Sr      |    16105 | 2216     |     2007 |     1940 | 358.7†  | 280.3†   | 270.0†   | 136.0†   | 134.2†   | 38.9†  | 21.3     | 20.1†    |
| 39 | Y       |    17038 | 2373     |     2156 |     2080 | 392.0*b | 310.6*   | 298.8*   | 157.7†   | 155.8†   | 43.8*  | 24.4*    | 23.1*    |
| 40 | Zr      |    17998 | 2532     |     2307 |     2223 | 430.3†  | 343.5†   | 329.8†   | 181.1†   | 178.8†   | 50.6†  | 28.5†    | 27.1†    |
| 41 | Nb      |    18986 | 2698     |     2465 |     2371 | 466.6†  | 376.1†   | 360.6†   | 205.0†   | 202.3†   | 56.4†  | 32.6†    | 30.8†    |
| 42 | Mo      |    20000 | 2866     |     2625 |     2520 | 506.3†  | 411.6†   | 394.0†   | 231.1†   | 227.9†   | 63.2†  | 37.6†    | 35.5†    |
| 43 | Tc      |    21044 | 3043     |     2793 |     2677 | 544*    | 447.6    | 417.7    | 257.6    | 253.9*   | 69.5*  | 42.3*    | 39.9*    |
| 44 | Ru      |    22117 | 3224     |     2967 |     2838 | 586.1*  | 483.5†   | 461.4†   | 284.2†   | 280.0†   | 75.0†  | 46.3†    | 43.2†    |
| 45 | Rh      |    23220 | 3412     |     3146 |     3004 | 628.1†  | 521.3†   | 496.5†   | 311.9†   | 307.2†   | 81.4*b | 50.5†    | 47.3†    |
| 46 | Pd      |    24350 | 3604     |     3330 |     3173 | 671.6†  | 559.9†   | 532.3†   | 340.5†   | 335.2†   | 87.1*b | 55.7†a   | 50.9†    |
| 47 | Ag      |    25514 | 3806     |     3524 |     3351 | 719.0†  | 603.8†   | 573.0†   | 374.0†   | 368.3    | 97.0†  | 63.7†    | 58.3†    |
|----+---------+----------+----------+----------+----------+---------+----------+----------+----------+----------+--------+----------+----------|
|  Z | Element |     K 1s | L1 2s    | L2 2p1/2 | L3 2p3/2 | M1 3s   | M2 3p1/2 | M3 3p3/2 | M4 3d3/2 | M5 3d5/2 | N 4s   | N2 4p1/2 | N3 4p3/2 |
|----+---------+----------+----------+----------+----------+---------+----------+----------+----------+----------+--------+----------+----------|
| 48 | Cd      |    26711 | 4018     |     3727 |     3538 | 772.0†  | 652.6†   | 618.4†   | 411.9†   | 405.2†   | 109.8† | 63.9†a   | 63.9†a   |
| 49 | In      |    27940 | 4238     |     3938 |     3730 | 827.2†  | 703.2†   | 665.3†   | 451.4†   | 443.9†   | 122.9† | 73.5†a   | 73.5†a   |
| 50 | Sn      |    29200 | 4465     |     4156 |     3929 | 884.7†  | 756.5†   | 714.6†   | 493.2†   | 484.9†   | 137.1† | 83.6†a   | 83.6†a   |
| 51 | Sb      |    30491 | 4698     |     4380 |     4132 | 946†    | 812.7†   | 766.4†   | 537.5†   | 528.2†   | 153.2† | 95.6†a   | 95.6†a   |
| 52 | Te      |    31814 | 4939     |     4612 |     4341 | 1006†   | 870.8†   | 820.0†   | 583.4†   | 573.0†   | 169.4† | 103.3†a  | 103.3†a  |
| 53 | I       |    33169 | 5188     |     4852 |     4557 | 1072*   | 931*     | 875*     | 630.8    | 619.3    | 186*   | 123*     | 123*     |
| 54 | Xe      |    34561 | 5453     |     5107 |     4786 | 1148.7* | 1002.1*  | 940.6*   | 689.0*   | 676.4*   | 213.2* | 146.7    | 145.5*   |
| 55 | Cs      |    35985 | 5714     |     5359 |     5012 | 1211*b  | 1071*    | 1003*    | 740.5*   | 726.6*   | 232.3* | 172.4*   | 161.3*   |
| 56 | Ba      |    37441 | 5989     |     5624 |     5247 | 1293*b  | 1137*b   | 1063*b   | 795.7†   | 780.5*   | 253.5† | 192      | 178.6†   |
| 57 | La      |    38925 | 6266     |     5891 |     5483 | 1362*b  | 1209*b   | 1128*b   | 853*     | 836*     | 274.7* | 205.8    | 196.0*   |
| 58 | Ce      |    40443 | 6549     |     6164 |     5723 | 1436*b  | 1274*b   | 1187*b   | 902.4*   | 883.8*   | 291.0* | 223.2    | 206.5*   |
| 59 | Pr      |    41991 | 6835     |     6440 |     5964 | 1511    | 1337     | 1242     | 948.3*   | 928.8*   | 304.5  | 236.3    | 217.6    |
| 60 | Nd      |    43569 | 7126     |     6722 |     6208 | 1575    | 1403     | 1297     | 1003.3*  | 980.4*   | 319.2* | 243.3    | 224.6    |
| 61 | Pm      |    45184 | 7428     |     7013 |     6459 | ---     | 1471     | 1357     | 1052     | 1027     | ---    | 242      | 242      |
| 62 | Sm      |    46834 | 7737     |     7312 |     6716 | 1723    | 1541     | 1420     | 1110.9*  | 1083.4*  | 347.2* | 265.6    | 247.4    |
| 63 | Eu      |    48519 | 8052     |     7617 |     6977 | 1800    | 1614     | 1481     | 1158.6*  | 1127.5*  | 360    | 284      | 257      |
| 64 | Gd      |    50239 | 8376     |     7930 |     7243 | 1881    | 1688     | 1544     | 1221.9*  | 1189.6*  | 378.6* | 286      | 271      |
| 65 | Tb      |    51996 | 8708     |     8252 |     7514 | 1968    | 1768     | 1611     | 1276.9*  | 1241.1*  | 396.0* | 322.4*   | 284.1*   |
| 66 | Dy      |    53789 | 9046     |     8581 |     7790 | 2047    | 1842     | 1676     | 1333     | 1292.6*  | 414.2* | 333.5*   | 293.2*   |
| 67 | Ho      |    55618 | 9394     |     8918 |     8071 | 2128    | 1923     | 1741     | 1392     | 1351     | 432.4* | 343.5    | 308.2*   |
| 68 | Er      |    57486 | 9751     |     9264 |     8358 | 2207    | 2006     | 1812     | 1453     | 1409     | 449.8* | 366.2    | 320.2*   |
| 69 | Tm      |    59390 | 10116    |     9617 |     8648 | 2307    | 2090     | 1885     | 1515     | 1468     | 470.9* | 385.9*   | 332.6*   |
| 70 | Yb      |    61332 | 10486    |     9978 |     8944 | 2398    | 2173     | 1950     | 1576     | 1528     | 480.5* | 388.7*   | 339.7*   |
|----+---------+----------+----------+----------+----------+---------+----------+----------+----------+----------+--------+----------+----------|
|  Z | Element | N4 4d3/2 | N5 4d5/2 | N6 4f5/2 | N7 4f7/2 | O1 5s   | O2 5p1/2 | O3 5p3/2 | O4 5d3/2 | O5 5d5/2 | P1 6s  | P2 6p1/2 | P3 6p3/2 |
|----+---------+----------+----------+----------+----------+---------+----------+----------+----------+----------+--------+----------+----------|
| 48 | Cd      |    11.7† | l0.7†    |          |          |         |          |          |          |          |        |          |          |
| 49 | In      |    17.7† | 16.9†    |          |          |         |          |          |          |          |        |          |          |
| 50 | Sn      |    24.9† | 23.9†    |          |          |         |          |          |          |          |        |          |          |
| 51 | Sb      |    33.3† | 32.1†    |          |          |         |          |          |          |          |        |          |          |
| 52 | Te      |    41.9† | 40.4†    |          |          |         |          |          |          |          |        |          |          |
| 53 | I       |     50.6 | 48.9     |          |          |         |          |          |          |          |        |          |          |
| 54 | Xe      |    69.5* | 67.5*    |      --- |      --- | 23.3*   | 13.4*    | 12.1*    |          |          |        |          |          |
| 55 | Cs      |    79.8* | 77.5*    |      --- |      --- | 22.7    | 14.2*    | 12.1*    |          |          |        |          |          |
| 56 | Ba      |    92.6† | 89.9†    |      --- |      --- | 30.3†   | 17.0†    | 14.8†    |          |          |        |          |          |
| 57 | La      |   105.3* | 102.5*   |      --- |      --- | 34.3*   | 19.3*    | 16.8*    |          |          |        |          |          |
| 58 | Ce      |     109* | ---      |      0.1 |      0.1 | 37.8    | 19.8*    | 17.0*    |          |          |        |          |          |
| 59 | Pr      |   115.1* | 115.1*   |      2.0 |      2.0 | 37.4    | 22.3     | 22.3     |          |          |        |          |          |
| 60 | Nd      |   120.5* | 120.5*   |      1.5 |      1.5 | 37.5    | 21.1     | 21.1     |          |          |        |          |          |
| 61 | Pm      |      120 | 120      |      --- |      --- | ---     | ---      | ---      |          |          |        |          |          |
| 62 | Sm      |      129 | 129      |      5.2 |      5.2 | 37.4    | 21.3     | 21.3     |          |          |        |          |          |
| 63 | Eu      |      133 | 127.7*   |        0 |        0 | 32      | 22       | 22       |          |          |        |          |          |
| 64 | Gd      |      --- | 142.6*   |     8.6* |     8.6* | 36      | 28       | 21       |          |          |        |          |          |
| 65 | Tb      |   150.5* | 150.5*   |     7.7* |     2.4* | 45.6*   | 28.7*    | 22.6*    |          |          |        |          |          |
| 66 | Dy      |   153.6* | 153.6*   |     8.0* |     4.3* | 49.9*   | 26.3     | 26.3     |          |          |        |          |          |
| 67 | Ho      |     160* | 160*     |     8.6* |     5.2* | 49.3*   | 30.8*    | 24.1*    |          |          |        |          |          |
| 68 | Er      |   167.6* | 167.6*   |      --- |     4.7* | 50.6*   | 31.4*    | 24.7*    |          |          |        |          |          |
| 69 | Tm      |   175.5* | 175.5*   |      --- |      4.6 | 54.7*   | 31.8*    | 25.0*    |          |          |        |          |          |
| 70 | Yb      |   191.2* | 182.4*   |     2.5* |     1.3* | 52.0*   | 30.3*    | 24.1*    |          |          |        |          |          |
|----+---------+----------+----------+----------+----------+---------+----------+----------+----------+----------+--------+----------+----------|
|  Z | Element |     K 1s | L1 2s    | L2 2p1/2 | L3 2p3/2 | M1 3s   | M2 3p1/2 | M3 3p3/2 | M4 3d3/2 | M5 3d5/2 | N1 4s  | N2 4p1/2 | N3 4p3/2 |
|----+---------+----------+----------+----------+----------+---------+----------+----------+----------+----------+--------+----------+----------|
| 71 | Lu      |    63314 | 10870    |    10349 |     9244 | 2491    | 2264     | 2024     | 1639     | 1589     | 506.8* | 412.4*   | 359.2*   |
| 72 | Hf      |    65351 | 11271    |    10739 |     9561 | 2601    | 2365     | 2108     | 1716     | 1662     | 538*   | 438.2†   | 380.7†   |
| 73 | Ta      |    67416 | 11682    |    11136 |     9881 | 2708    | 2469     | 2194     | 1793     | 1735     | 563.4† | 463.4†   | 400.9†   |
| 74 | W       |    69525 | 12100    |    11544 |    10207 | 2820    | 2575     | 2281     | 1872     | 1809     | 594.1† | 490.4†   | 423.6†   |
| 75 | Re      |    71676 | 12527    |    11959 |    10535 | 2932    | 2682     | 2367     | 1949     | 1883     | 625.4† | 518.7†   | 446.8†   |
| 76 | Os      |    73871 | 12968    |    12385 |    10871 | 3049    | 2792     | 2457     | 2031     | 1960     | 658.2† | 549.1†   | 470.7†   |
| 77 | Ir      |    76111 | 13419    |    12824 |    11215 | 3174    | 2909     | 2551     | 2116     | 2040     | 691.1† | 577.8†   | 495.8†   |
| 78 | Pt      |    78395 | 13880    |    13273 |    11564 | 3296    | 3027     | 2645     | 2202     | 2122     | 725.4† | 609.1†   | 519.4†   |
| 79 | Au      |    80725 | 14353    |    13734 |    11919 | 3425    | 3148     | 2743     | 2291     | 2206     | 762.1† | 642.7†   | 546.3†   |
| 80 | Hg      |    83102 | 14839    |    14209 |    12284 | 3562    | 3279     | 2847     | 2385     | 2295     | 802.2† | 680.2†   | 576.6†   |
| 81 | Tl      |    85530 | 15347    |    14698 |    12658 | 3704    | 3416     | 2957     | 2485     | 2389     | 846.2† | 720.5†   | 609.5†   |
| 82 | Pb      |    88005 | 15861    |    15200 |    13035 | 3851    | 3554     | 3066     | 2586     | 2484     | 891.8† | 761.9†   | 643.5†   |
| 83 | Bi      |    90524 | 16388    |    15711 |    13419 | 3999    | 3696     | 3177     | 2688     | 2580     | 939†   | 805.2†   | 678.8†   |
| 84 | Po      |    93105 | 16939    |    16244 |    13814 | 4149    | 3854     | 3302     | 2798     | 2683     | 995*   | 851*     | 705*     |
| 85 | At      |    95730 | 17493    |    16785 |    14214 | 4317    | 4008     | 3426     | 2909     | 2787     | 1042*  | 886*     | 740*     |
| 86 | Rn      |    98404 | 18049    |    17337 |    14619 | 4482    | 4159     | 3538     | 3022     | 2892     | 1097*  | 929*     | 768*     |
| 87 | Fr      |   101137 | 18639    |    17907 |    15031 | 4652    | 4327     | 3663     | 3136     | 3000     | 1153*  | 980*     | 810*     |
| 88 | Ra      |   103922 | 19237    |    18484 |    15444 | 4822    | 4490     | 3792     | 3248     | 3105     | 1208*  | 1058     | 879*     |
| 89 | Ac      |   106755 | 19840    |    19083 |    15871 | 5002    | 4656     | 3909     | 3370     | 3219     | 1269*  | 1080*    | 890*     |
| 90 | Th      |   109651 | 20472    |    19693 |    16300 | 5182    | 4830     | 4046     | 3491     | 3332     | 1330*  | 1168*    | 966.4†   |
| 91 | Pa      |   112601 | 21105    |    20314 |    16733 | 5367    | 5001     | 4174     | 3611     | 3442     | 1387*  | 1224*    | 1007*    |
| 92 | U       |   115606 | 21757    |    20948 |    17166 | 5548    | 5182     | 4303     | 3728     | 3552     | 1439*b | 1271*b   | 1043†    |
|----+---------+----------+----------+----------+----------+---------+----------+----------+----------+----------+--------+----------+----------|
|  Z | Element | N4 4d3/2 | N5 4d5/2 | N6 4f5/2 | N7 4f7/2 | O1 5s   | O2 5p1/2 | O3 5p3/2 | O4 5d3/2 | O5 5d5/2 | P1 6s  | P2 6p1/2 | P3 6p3/2 |
|----+---------+----------+----------+----------+----------+---------+----------+----------+----------+----------+--------+----------+----------|
| 71 | Lu      |   206.1* | 196.3*   |     8.9* |     7.5* | 57.3*   | 33.6*    | 26.7*    |          |          |        |          |          |
| 72 | Hf      |   220.0† | 211.5†   |    15.9† |    14.2† | 64.2†   | 38*      | 29.9†    |          |          |        |          |          |
| 73 | Ta      |   237.9† | 226.4†   |    23.5† |    21.6† | 69.7†   | 42.2*    | 32.7†    |          |          |        |          |          |
| 74 | W       |   255.9† | 243.5†   |    33.6* |    31.4† | 75.6†   | 45.3*b   | 36.8†    |          |          |        |          |          |
| 75 | Re      |   273.9† | 260.5†   |    42.9* |    40.5* | 83†     | 45.6*    | 34.6*b   |          |          |        |          |          |
| 76 | Os      |   293.1† | 278.5†   |    53.4† |    50.7† | 84*     | 58*      | 44.5†    |          |          |        |          |          |
| 77 | Ir      |   311.9† | 296.3†   |    63.8† |    60.8† | 95.2*b  | 63.0*b   | 48.0†    |          |          |        |          |          |
| 78 | Pt      |   331.6† | 314.6†   |    74.5† |    71.2† | 101.7*b | 65.3*b   | 51.7†    |          |          |        |          |          |
| 79 | Au      |   353.2† | 335.1†   |    87.6† |     84.0 | 107.2*b | 74.2†    | 57.2†    |          |          |        |          |          |
| 80 | Hg      |   378.2† | 358.8†   |   104.0† |    99.9† | 127†    | 83.1†    | 64.5†    | 9.6†     | 7.8†     |        |          |          |
| 81 | Tl      |   405.7† | 385.0†   |   122.2† |   117.8† | 136.0*b | 94.6†    | 73.5†    | 14.7†    | 12.5†    |        |          |          |
| 82 | Pb      |   434.3† | 412.2†   |   141.7† |   136.9† | 147*b   | 106.4†   | 83.3†    | 20.7†    | 18.1†    |        |          |          |
| 83 | Bi      |   464.0† | 440.1†   |   162.3† |   157.0† | 159.3*b | 119.0†   | 92.6†    | 26.9†    | 23.8†    |        |          |          |
| 84 | Po      |     500* | 473*     |     184* |     184* | 177*    | 132*     | 104*     | 31*      | 31*      |        |          |          |
| 85 | At      |     533* | 507      |     210* |     210* | 195*    | 148*     | 115*     | 40*      | 40*      |        |          |          |
| 86 | Rn      |     567* | 541*     |     238* |     238* | 214*    | 164*     | 127*     | 48*      | 48*      | 26     |          |          |
| 87 | Fr      |     603* | 577*     |     268* |     268* | 234*    | 182*     | 140*     | 58*      | 58*      | 34     | 15       | 15       |
| 88 | Ra      |     636* | 603*     |     299* |     299* | 254*    | 200*     | 153*     | 68*      | 68*      | 44     | 19       | 19       |
| 89 | Ac      |     675* | 639*     |     319* |     319* | 272*    | 215*     | 167*     | 80*      | 80*      | ---    | ---      | ---      |
| 90 | Th      |   712.1† | 675.2†   |   342.4† |   333.1† | 290*a   | 229*a    | 182*a    | 92.5†    | 85.4†    | 41.4†  | 24.5†    | 16.6†    |
| 91 | Pa      |     743* | 708*     |     371* |     360* | 310*    | 232*     | 232*     | 94*      | 94*      | ---    | ---      | ---      |
| 92 | U       |   778.3† | 736.2†   |   388.2* |   377.4† | 321*ab  | 257*ab   | 192*ab   | 102.8†   | 94.2†    | 43.9†  | 26.8†    | 16.8†    |
|----+---------+----------+----------+----------+----------+---------+----------+----------+----------+----------+--------+----------+----------|

*** Bremsstrahlung                                               :noexport:

Talk about Bremsstrahlung as a requirement for the CDL data?

Well yes, but that is really not of any importance or my work on the
thesis. In particular the X-ray fluorescence chapter is pretty much
that already.

** Cosmic rays
:PROPERTIES:
:CUSTOM_ID: sec:theory:cosmic_radiation
:END:

Cosmic rays, or cosmic radiation refers to two aspects of a related
phenomenon. Primary cosmic radiation is the radiation arriving at
Earth from the Sun, galactic and extragalactic sources. The main
contribution are highly energetic protons, but other long lived
elementary particles and nuclei also contribute to a lesser
extent. Cosmic rays are isotropic at most energies, because of the
influence of galactic magnetic fields. Their energies range from
$\SI{1e9}{eV}$ up to $\SI{1e21}{eV}$. It is generally assumed that
particles below $\SI{1e18}{eV}$ are of mainly galactic origin, whereas
the above is dominated by extragalactic sources. The flux of the
primary cosmic rays generally follows a power law
distribution. Different contributions follow a generally similar power
law cite:Zyla:2020zbs (chapter on cosmic rays).

When cosmic rays interact with the molecules of Earth's atmosphere,
mesons are produced, mainly pions. Neutral pions generate showers of
photons and electron-positron pairs. Charged pions on the other hand
decay into muons and anti muon-neutrinos. Muons are produced over
electrons in this case, due to chirality. As they are more massive
than electrons they have a larger component of the opposite chirality
to the neutrino, which is necessary for this 'forbidden' decay due to
angular momentum conservation.

They are produced at an altitude of roughly $\SI{15}{km}$. A large
fraction of them reaches the surface as they are highly
relativistic. Their spectrum is described by a convolution of the
production energy, their energy loss due to ionization in the
atmosphere and possible decay.

Muons are of interest in the context of helioscope experiments, as
they present a dominant source of background, especially in gaseous
detectors (directly and indirectly due to fluorescence). And because
current helioscope experiments are built near Earth's surface, little
attenuation of muon flux happens. Therefore, a good understanding of
the expected muon flux is required.

Above $\SI{100}{GeV}$ muon decay is negligible. At those energies the
muon flux at the surface strictly follows the same power law as the
primary cosmic ray flux. Following Gaisser cite:gaisser2016cosmic,
in this regime it can be described by

#+NAME: eq:theory:muon_flux_gaisser
\begin{equation}
\frac{\mathrm{d}N_μ}{\mathrm{d}E_μ \mathrm{d}Ω} \approx \frac{0.14
E_μ^{-2.7}}{\si{\centi\meter\squared \second \steradian \giga\electronvolt}} \times \left[ \frac{1}{1 + \frac{1.1
E_μ \cos ϑ}{\SI{115}{GeV}}} + \frac{0.054}{1 + \frac{1.1 E_μ \cos
ϑ}{\SI{850}{GeV}}} \right]
\end{equation}

where the first term in parenthesis is the pion and the second the
kaon contribution.

For lower energies, cite:doi:10.1142/S0217751X18501750 provide a set
of fitted functions based on [[eq:theory:muon_flux_gaisser]] with a
single power law

\[
I(E, θ) = I_0 N (E_0 + E)^{-n} \left(1 + \frac{E}{ε}\right)^{-1} D(θ)^{-(n - 1)},
\]

where $I_0$, the intensity under zenith angle, and
$ε$ is another fit parameter for the replacement of the separate meson
masses in eq. [[eq:theory:muon_flux_gaisser]]. $D(θ)$ is the path length
through the atmosphere under an angle $θ$ from the zenith. $N$ is a
normalization constant given by

\[
N = (n - 1) (E_0 + E_c)^{n-1},
\]

where $n$ corresponds to the effective power of the cosine behavior
and is the final fit parameter. $E_0$ accounts for the energy loss due
to interactions in the atmosphere and $E_c$ is the lowest energy given
in a data set.

If the Earth is assumed flat, it is $D(θ) = 1/\cosθ$ (which is often
assumed for simplicity and is a reasonable approximation as long as
only angles close to $θ = 0$ are considered). To describe a trajectory
through Earth's curved atmosphere $D(θ)$ can be written as:

\[
D(θ) = \sqrt{
  \left(
    \frac{R²}{d²} \cos² θ + 2\frac{R}{d} + 1
  \right)
} -
  \frac{R}{d}\cos θ
\]
where $R$ is the Earth radius, $d$ the vertical path length (i.e. the
height at which the muon is created) and $θ$ the zenith angle.

While this parametrization is very useful to describe the few specific
datasets shown in cite:doi:10.1142/S0217751X18501750 and provides a
way to fit any measured muon flux at a specific location, it is
limited in applicability to arbitrary locations, altitudes and
angles. For that an approach that does not require a fit to a dataset
is preferable, namely by utilizing a combination of the approximation
by Gaisser, eq. [[eq:theory:muon_flux_gaisser]], and the interaction of
muons with the atmosphere. As such, we modify the equation for the
intensity $I$ to the following:

\[
I(E, θ) = I_0 (n-1) (E_θ(E, θ) + E_c)^{n-1} (E_θ(E, θ) + E)^{-n} \left[ \frac{1}{1 + \frac{1.1
E_μ \cos ϑ}{\SI{115}{GeV}}} + \frac{0.054}{1 + \frac{1.1 E_μ \cos
ϑ}{\SI{850}{GeV}}} \right] D(θ)^{-(n - 1)},
\]

where we take $n = 3$ exactly. One could put in the best fit for the
general cosine behavior under zenith angles, $n = n_{\text{fit}} + 1$,
but for simplicity we just use 3 here. Take $E_θ(E, θ)$ to be the
energy of a muon left from initial energy $E$ at generation in the
upper atmosphere after transporting it through the atmosphere under
the angle $θ$. The transport must take into account the density change
using the barometric height formula of the atmosphere. Transport is
done using the Bethe-Bloch equation as introduced in
sec. [[#sec:theory:bethe_bloch]] assuming an atmosphere made up of
nitrogen, oxygen and argon. As such we remove all parameters except an
initial intensity $I_0$, which can be set to the best fit of the
integrated muon flux at the zenith angle at sea level. In the
following figures we simply use $I_0 = \SI{90}{\per\meter\squared
\per\steradian \per\second}$. Figure [[sref:fig:theory:muon_flux_surface]]
shows the expected differential muon flux using these parameters for
different angles at sea level. In
sref:fig:theory:muon_flux_surface:initial the initial energy of the
muons is shown before transporting through the atmosphere. For each
angle the lines cut off at the energy below which the muon would
likely be stopped by the atmosphere according to its energy loss per
distance or reaches the surface with less than $\SI{300}{MeV}$. On the
right we see the final energy of the same muons at the surface. The
lines in sref:fig:theory:muon_flux_surface:final are ragged, because
muon decay is simulated using Monte Carlo. [fn:note_on_flux]

These numbers match reasonably well with different datasets for
different locations under different angles, but they should /not/ be
considered as more than a starting point for a general expectation.

#+begin_src subfigure
(figure ()
        (subfigure (linewidth 0.5) (caption "Initial energy") (label "fig:theory:muon_flux_surface:initial")
                   (includegraphics (list (cons 'width (linewidth 0.95)))
                                    "~/phd/Figs/muons/initial_energy_vs_flux_and_angle_cosmic_muons.pdf"))
        (subfigure (linewidth 0.5) (caption "Final energy") (label "fig:theory:muon_flux_surface:final")
                   (includegraphics (list (cons 'width (linewidth 0.95)))
                                    "~/phd/Figs/muons/final_energy_vs_flux_and_angle_cosmic_muons.pdf"))
        (caption
          "Differential muon flux at sea level for different zenith angles. "
          (subref "fig:theory:muon_flux_surface:initial") " shows the initial energy of the muon. The cut-off corresponds to the
          lowest energy transported through the atmosphere; muons still arrive at the surface without decay or stopping. "
          (subref "fig:theory:muon_flux_surface:final") " shows the final muon energy at the surface, with the lowest muon "
          ($ (SI 300 "MeV")) " at the surface. ")
        (label "fig:theory:muon_flux_surface"))
#+end_src

[fn:note_on_flux] The calculation is a "hybrid" Monte Carlo
approach. We don't sample a large number of muons and transport them
through the atmosphere. We just compute the loss through the
atmosphere and allow the muon to decay randomly, dropping a single
data point. This is for simplicity on the one hand and the fact that
there is a very sharp transition of 'most muons traverse' to
'essentially no muons traverse' the atmosphere at a given energy. For
that reason the flux in sref:fig:theory:muon_flux_surface:final at low
energies is to be taken with a grain of salt. The fluxes are literally
mappings from data points in sref:fig:theory:muon_flux_surface:initial
to the corresponding energy left at the surface (i.e. assuming no
muons decay). Further the $\SI{300}{MeV}$ energy left is already well
inside the energy range where a large number of muons would already decay.

*** TODOs for this section                                       :noexport:

Because of their large energies, muons behave as minimally ionizing
particles (MIPS), which means their mean energy loss is more or less
independent of the muon's energy. They are in the trough of the
Bethe-Bloch equation, see sec. [[#sec:theory:bethe_bloch]]. This means the
exact energy of each muon is irrelevant and for background studies
only the actual rate of muons is important.

*PLOT OF PRIMARY RADIATION*

*SECONDARY MUON PRODUCTION*

- [X] *MUON FLUX AT ZENITH*, *MUON FLUX UNDER ANGLE* (cos²)

- [X] *REFERENCE TO MUON CHIRALITY OVER ELECTRON PROD*


*** Calculation of muon angular / energy dependence at surface   :noexport:
:PROPERTIES:
:CUSTOM_ID: sec:theory:calc_muon_angular_flux
:END:

The code here is directly based on code written in my notes
[[file:~/org/Doc/StatusAndProgress.org::#sec:muons:expected_muon_flux]] being tangled into a file
=/tmp/muon_flux.nim=.

Aside from the figures included in the main thesis, this also produces
fig. [[fig:theory:flux_at_88deg_CAST]].

#+CAPTION: Expected muon flux under an angle of $\SI{88}{°}$ at CAST.
#+NAME: fig:theory:flux_at_88deg_CAST
[[~/phd/Figs/muons/flux_at_cast_88_deg.pdf]]

#+begin_src nim :results silent :tangle /home/basti/phd/code/muons.nim
import math, macros, unchained
import seqmath, ggplotnim, sequtils, strformat

let K = 4 * π * N_A * r_e^2 * m_e * c^2 # usually in: [MeV mol⁻¹ cm²]

defUnit(cm³•g⁻¹)
defUnit(J•m⁻¹)
defUnit(cm⁻³)
defUnit(g•mol⁻¹)
defUnit(MeV•g⁻¹•cm²)
defUnit(mol⁻¹)
defUnit(keV•cm⁻¹)
defUnit(g•cm⁻³)
defUnit(g•cm⁻²)

proc electronDensity(ρ: g•cm⁻³, Z, A: UnitLess): cm⁻³ =
  result = N_A * Z * ρ / (A * M_u.to(g•mol⁻¹))

proc I[T](z: float): T =
  ## use Bloch approximation for all but Argon (better use tabulated values!)
  # 188.0 eV from NIST table 
  result = if z == 18.0: 188.0.eV.to(T) 
           else: (10.eV * z).to(T)

proc calcβ(γ: UnitLess): UnitLess =
  result = sqrt(1.0 - 1.0 / (γ^2))

proc betheBloch(z, Z: UnitLess,
                   A: g•mol⁻¹,
                   γ: UnitLess,
                   M: kg): MeV•g⁻¹•cm² =
  ## result in MeV cm² g⁻¹ (normalized by density)
  ## z: charge of particle
  ## Z: charge of particles making up medium
  ## A: atomic mass of particles making up medium
  ## γ: Lorentz factor of particle
  ## M: mass of particle in MeV (or same mass as `m_e` defined as)
  let β = calcβ(γ)
  let W_max = 2 * m_e * c^2 * β^2 * γ^2 /
    (1 + 2 * γ * m_e / M + (m_e / M)^2)
  let lnArg = 2 * m_e * c^2 * β^2 * γ^2 * W_max / (I[Joule](Z)^2)
  result = (K * z^2 * Z / A * 1.0 / (β^2) * (
    0.5 * ln(lnArg) - β^2
  )).to(MeV•g⁻¹•cm²)

proc mostProbableLoss(z, Z: UnitLess, A: g•mol⁻¹, γ: UnitLess,
                      x: g•cm⁻²): keV =
  ## Computes the most probable value, corresponding to the peak of the Landau
  ## distribution, that gives rise to the Bethe-Bloch formula.
  ##
  ## Taken from PDG chapter 'Passage of particles through matter' equation
  ## `34.12` in 'Fluctuations in energy loss', version 2020).
  ##
  ## `x` is the "thickness". Density times length, `x = ρ * d`. The other parameters
  ## are as in `betheBloch` above.
  let β = calcβ(γ)
  let ξ = K / 2.0 * Z / A * z*z * (x / (β*β))
  const j = 0.200
  let I = I[Joule](Z)
  result = (ξ * ( ln((2 * m_e * c^2 * β^2 * γ^2).to(Joule) / I) + ln(ξ.to(Joule) / I) + j - β^2)).to(keV) # - δ*(β*γ)

proc density(p: mbar, M: g•mol⁻¹, temp: Kelvin): g•cm⁻³ =
  ## returns the density of the gas for the given pressure.
  ## The pressure is assumed in `mbar` and the temperature (in `K`).
  ## The default temperature corresponds to BabyIAXO aim.
  ## Returns the density in `g / cm^3`
  let gasConstant = 8.314.J•K⁻¹•mol⁻¹ # joule K^-1 mol^-1
  let pressure = p.to(Pa) # pressure in Pa
  # factor 1000 for conversion of M in g / mol to kg / mol
  result = (pressure * M / (gasConstant * temp)).to(g•cm⁻³)

proc E_to_γ(E: GeV): UnitLess =
  result = E.to(Joule) / (m_μ * c^2) + 1

proc γ_to_E(γ: UnitLess): GeV =
  result = ((γ - 1) * m_μ * c^2).to(GeV)

type
  Element = object
    Z: UnitLess
    M: g•mol⁻¹
    A: UnitLess # numerically same as `M`
    ρ: g•cm⁻³
proc initElement(Z: UnitLess, M: g•mol⁻¹, ρ: g•cm⁻³): Element =
  Element(Z: Z, M: M, A: M.UnitLess, ρ: ρ)

# molar mass. Numerically same as relative atomic mass
let M_Ar = 39.95.g•mol⁻¹
let ρAr = density(1050.mbar, M_Ar, temp = 293.15.K)
let Argon = initElement(18.0.UnitLess, 39.95.g•mol⁻¹, ρAr)

proc intBethe(e: Element, d_total: cm, E0: eV, dx = 1.μm): eV =
  ## integrated energy loss of bethe formula after `d` cm of matter
  ## and returns the energy remaining
  var γ: UnitLess = E_to_γ(E0.to(GeV))
  var d: cm
  result = E0
  var totalLoss = 0.eV
  while d < d_total and result > 0.eV:
    let E_loss: MeV = betheBloch(-1, e.Z, e.M, γ, m_μ) * e.ρ * dx
    result = result - E_loss.to(eV)
    γ = E_to_γ(result.to(GeV))
    d = d + dx.to(cm)
    totalLoss = totalLoss + E_loss.to(eV)
  result = max(0.float, result.float).eV

proc plotDetectorAbsorption() =
  let E_float = logspace(-2, 2, 1000)
  let energies = E_float.mapIt(it.GeV)
  let E_loss = energies.mapIt(
    (it.to(eV) - intBethe(Argon, 3.cm, it.to(eV))).to(keV).float
  )
  let df = toDf(E_float, E_loss)
  ggplot(df, aes("E_float", "E_loss")) +
    geom_line() +
    xlab("μ Energy [GeV]") + ylab("ΔE [keV]") +
    scale_x_log10() + scale_y_log10() +
    ggtitle("Energy loss of Muons in 3 cm Ar at CAST conditions") +
    ggsave("/home/basti/phd/Figs/muons/ar_energy_loss_cast.pdf")
plotDetectorAbsorption()

let Atmosphere = @[(0.78084, initElement(7.0.UnitLess, 14.006.g•mol⁻¹, 1.2506.g•dm⁻³.to(g•cm⁻³))), # N2
                   (0.20964, initElement(8.0.UnitLess, 15.999.g•mol⁻¹, 1.429.g•dm⁻³.to(g•cm⁻³))),  # O2
                   (0.00934, initElement(18.0.UnitLess, 39.95.g•mol⁻¹, 1.784.g•dm⁻³.to(g•cm⁻³)))]  # Ar

proc plotMuonBethe() =
  let E_float = logspace(-2, 2, 1000)
  let energies = E_float.mapIt(it.GeV)
  var dEdxs = newSeq[float]()
  for e in energies:
    var dEdx = 0.0.MeV•g⁻¹•cm²
    for elTup in Atmosphere:
      let (w, element) = elTup
      let γ = E_to_γ(e)
      dEdx += w * betheBloch(-1, element.Z, element.M, γ, m_μ)
    dEdxs.add dEdx.float
  let df = toDf(E_float, dEdxs)
  ggplot(df, aes("E_float", "dEdxs")) +
    geom_line() +
    xlab("μ Energy [GeV]") + ylab("dE/dx [MeV•g⁻¹•cm²]") +
    scale_x_log10() + scale_y_log10() +
    ggtitle("Energy loss of Muons in atmosphere") +
    ggsave("/home/basti/phd/Figs/muons/energy_loss_muons_atmosphere.pdf")  
plotMuonBethe()
#if true: quit()
import math, unchained, ggplotnim, sequtils

const R_Earth = 6371.km
func distanceAtmosphere(θ: Radian, d: KiloMeter = 36.6149.km): UnitLess =
  ## NOTE: The default value for `d` is not to be understood as a proper height. It.s an
  ## approximation based on a fit to get `R_Earth / d = 174`!
  result = sqrt((R_Earth / d * cos(θ))^2 + 2 * R_Earth / d + 1) - R_Earth / d * cos(θ)

defUnit(cm⁻²•s⁻¹•sr⁻¹)  
defUnit(m⁻²•s⁻¹•sr⁻¹)
proc muonFlux(E: GeV, θ: Radian, E₀, E_c: GeV,
              I₀: m⁻²•s⁻¹•sr⁻¹,
              ε: GeV): m⁻²•s⁻¹•sr⁻¹ =
  const n = 3.0
  let N = (n - 1) * pow((E₀ + E_c).float, n - 1)
  result = I₀ * N * pow((E₀ + E).float, -n) *
    #pow((1 + E / ε).float, -1) *
    ( ( 1.0 / (1 + 1.1 * E * cos(θ) / 115.GeV).float) + (0.054 / (1 + 1.1 * E * cos(θ) / 850.GeV).float) ) * 
    pow(distanceAtmosphere(θ), -(n - 1))

from numericalnim/integrate import simpson
proc plotE_vs_flux(θ: Radian, E₀, E_c: GeV, I₀: m⁻²•s⁻¹•sr⁻¹, ε: GeV,
                   suffix = "") =
  let energies = linspace(E_c.float, 100.0, 1000)
  let E = energies.mapIt(it.GeV)
  let flux = E.mapIt(muonFlux(it, θ, E₀, E_c, I₀, ε).float) # .to(cm⁻²•s⁻¹•sr⁻¹)
  let df = toDf(energies, flux)

  echo "Integrated flux: ", simpson(flux, energies)
  
  ggplot(df, aes("energies", "flux")) +
    geom_line() +
    xlab("Energy [GeV]") + ylab("Flux [m⁻²•s⁻¹•sr⁻¹]") +
    scale_x_log10() + scale_y_log10() +
    ggtitle(&"Flux dependency on the energy of muons at θ = {θ.to(°)}{suffix}") +
    ggsave(&"/home/basti/phd/Figs/muons/energy_vs_flux_cosmic_muons{suffix}.pdf")
plotE_vs_flux(0.Radian,
              2.5.GeV, #4.29.GeV,
              0.5.GeV, 70.7.m⁻²•s⁻¹•sr⁻¹, 854.GeV)


let E₀ = 25.0.GeV
let I₀ = 90.0.m⁻²•s⁻¹•sr⁻¹
let E_c = 1.GeV
let ε = 2000.GeV

proc plotFlux_at_CAST() =
  let energies = linspace(0.5, 100.0, 1000)
  let E = energies.mapIt(it.GeV)
  let flux = E.mapIt(muonFlux(it, 88.0.degToRad.Radian, E₀, E_c, I₀, ε).float)
  let df = toDf(energies, flux)
  ggplot(df, aes("energies", "flux")) +
    geom_line() +
    xlab("Energy [GeV]") + ylab("Flux [m⁻²•s⁻¹•sr⁻¹]") +
    scale_x_log10() + scale_y_log10() +
    ggtitle("Flux dependency on the energy at θ = 88° at CAST altitude") +
    ggsave("/home/basti/phd/Figs/muons/flux_at_cast_88_deg.pdf")
plotFlux_at_CAST()

proc computeMeanEnergyLoss() =
  let energies = linspace(0.5, 100.0, 1000)
  let E = energies.mapIt(it.GeV)
  let flux = E.mapIt(muonFlux(
    it, 88.0.degToRad.Radian, E₀, E_c, I₀, ε).float
  )
  let E_loss = E.mapIt(
    (it.to(eV) - intBethe(Argon, 3.cm, it.to(eV))).to(keV).float
  )
  let fluxSum = flux.sum
  let df = toDf(energies, E_loss, flux)
      .mutate(f{"flux" ~ `flux` / fluxSum},
              f{"AdjFlux" ~ `E_loss` * `flux`})
  echo "Mean energy loss: ", df["AdjFlux", float].sum
computeMeanEnergyLoss()

proc computeHeight(S: Meter, θ: Radian): KiloMeter =
  ## For given remaining distance distance along the path of a muon
  ## `S` (see fig. 1 in 1606.06907) computes the remaining height above
  ## ground. Formula is the result of inverting eq. 7 to `d` using quadratic
  ## formula. Positive result, because negative is negative.
  result = (-1.0 * R_Earth + sqrt(R_Earth^2 + S^2 + 2 * S * R_Earth * cos(θ)).m).to(km)

import algorithm
defUnit(K•m⁻¹)
proc barometricFormula(h: KiloMeter): g•cm⁻³ =
  let hs = @[0.0.km, 11.0.km]
  let ρs = @[1.225.kg•m⁻³, 0.36391.kg•m⁻³]
  let Ts = @[288.15.K, 216.65.K]
  let Ls = @[-1.0 * 0.0065.K•m⁻¹, 0.0.K•m⁻¹]
  let M_air = 0.0289644.kg•mol⁻¹
  let R = 8.3144598.N•m•mol⁻¹•K⁻¹
  let g_0 = 9.80665.m•s⁻²
  let idx = hs.mapIt(it.float).lowerBound(h.float) - 1
  case idx
  of 0:
    # in Troposphere, using regular barometric formula for denities
    let expArg = g_0 * M_air / (R * Ls[idx])
    result = (ρs[idx] * pow(Ts[idx] / (Ts[idx] + Ls[idx] * (h - hs[idx])), expArg)).to(g•cm⁻³)
  of 1:
    # in Tropopause, use equation valid for L_b = 0
    result = (ρs[idx] * exp(-1.0 * g_0 * M_air * (h - hs[idx]) / (R * Ts[idx]))).to(g•cm⁻³)
  else: doAssert false, "Invalid height! Outside of range!"

import random
randomize(430)
proc intBetheAtmosphere(E: GeV, θ: Radian, dx = 10.cm): eV =
  ## integrated energy loss using Bethe formula for muons generated at
  ## `15.km` under an angle of `θ` to the observer for a muon of energy
  ## `E`.
  # Main contributions in Earth's atmosphere
  const τ = 2.19618.μs # muon half life
  let elements = Atmosphere
  var γ: UnitLess = E_to_γ(E.to(GeV))
  result = E.to(eV)
  var totalLoss = 0.eV
  let h_muon = 15.km # assume creation happens in `15.km`
  let S = h_muon.to(m) * distanceAtmosphere(θ.rad, d = h_muon)
  var S_prime = S
  while S_prime > 0.m and result > 0.eV:
    let h = computeHeight(S_prime, θ)
    let ρ_at_h = barometricFormula(h)
    var E_loss = 0.0.MeV
    for eTup in elements: # compute the weighted contribution of the element fraction
      let (w, e) = eTup
      E_loss += w * betheBloch(-1, e.Z, e.M, γ, m_μ) * ρ_at_h * dx

    ## Add step for radioactive decay of muon.
    ## - given `dx` compute likelihood of decay
    ## - eigen time of muon: dx / v = dt. dτ = dt / γ
    ## - muon decay is λ = 1 / 2.2e-6s
    let β = calcβ(γ)
    # compute effective time in lab frame
    let δt = dx / (β * c)
    # compute eigen time
    let δτ = δt / γ
    # probability of a decay in this time frame
    let p = pow(1 / math.E, δτ / τ)
    # decay with likelihood `p`
    #echo "γ = ", γ, " yields ", p, " in δτ ", δτ, " for energy ", E
    if rand(1.0) < (1.0 - p):
      echo "Particle decayed after: ", S_prime
      return 0.eV
          
    result = result - E_loss.to(eV)
    S_prime = S_prime - dx
    γ = E_to_γ(result.to(GeV))
    totalLoss = totalLoss + E_loss.to(eV)
  echo "total Loss ", totalLoss.to(GeV)
  result = max(0.float, result.float).eV

block MuonLimits:
  let τ_μ = 2.1969811.μs
  # naively this means given some distance `s` the muon can
  # traverse `s = c • τ_μ` (approximating its speed by `c`) before
  # it has decayed with a 1/e chance
  # due to special relativity this is extended by γ
  let s = c * τ_μ
  echo s
  # given production in 15 km, means
  let h = 15.km
  echo h / s
  # so a reduction of (1/e)^22. So 0.
  # now it's not 15 km but under an angle `θ = 88°`.
  let R_over_d = 174.UnitLess
  let n = 3.0
  let E₀ = 25.0.GeV
  let I₀ = 90.0.m⁻²•s⁻¹•sr⁻¹
  let E_c = 1.GeV
  let ε = 2000.GeV

  # distance atmospher gives S / d, where `d` corresponds to our `h` up there
  let S = h * distanceAtmosphere(88.0.degToRad.rad)
  # so about 203 km
  # so let's say 5 * mean distance is ok, means we ned
  let S_max = S / 5.0
  # so need a `γ` such that `s` is stretched to `S_max`
  let γ = S_max / s
  echo γ
  # ouch. Something has to be wrong. γ of 61?

  # corresponds to an energy loss of what?
  let Nitrogen = initElement(7.0.UnitLess, 14.006.g•mol⁻¹, 1.2506.g•dm⁻³.to(g•cm⁻³))
  echo "================================================================================"
  echo "Energy left: ", intBethe(Nitrogen, S.to(cm), 6.GeV.to(eV), dx = 1.m.to(μm)).to(GeV)
  proc print(E: GeV, θ: Radian) =
    let left = intBetheAtmosphere(E, θ = θ).to(GeV)
    echo "E = ", E, ", θ = ", θ, ", Bethe = ", E - left
  print(6.GeV, 0.Radian)
  #print(200.GeV, 0.Radian)  
  #print(200.GeV, 88.°.to(Radian))
  #print(200.GeV, 75.°.to(Radian))

  let E_loss75 = 100.GeV - intBetheAtmosphere(100.GeV, 75.°.to(Radian)).to(GeV)
  plotE_vs_flux(75.°.to(Radian),
                E_loss75, #23.78.GeV, #25.GeV, #E_loss75,
                1.0.GeV,
                90.m⁻²•s⁻¹•sr⁻¹, #65.2.m⁻²•s⁻¹•sr⁻¹,
                2000.GeV, # 854.GeV,
                "_at_75deg")

  
  echo "S@75° = ", h * distanceAtmosphere(75.0.degToRad.rad, d = 15.0.km)
  echo "================================================================================"  
echo E_to_γ(4.GeV)
echo E_to_γ(0.GeV)

proc plotE_vs_flux_and_angles(E_c: GeV, I₀: m⁻²•s⁻¹•sr⁻¹, ε: GeV,
                              suffix = "") =
  ## Generates a plot of the muon flux vs energy for a fixed set of different
  ## angles.
  ##
  ## The energy loss is computed using a fixed 
  let energies = logspace(log10(E_c.float), 2.float, 1000)
  let angles = linspace(0.0, 80.0, 9)
  block CalcLossEachMuon:
    var df = newDataFrame()
    for angle in angles:
      let E = energies.mapIt(it.GeV)
      let θ = angle.°.to(Radian)
      var flux = newSeq[float]()
      var E_initials = newSeq[float]()
      var E_lefts = newSeq[float]()
      var lastDropped = 0.GeV
      for e in E:
        let E_left = intBetheAtmosphere(e, θ).to(GeV)
        if E_left <= 0.0.GeV:
          echo "Skipping energy : ", e, " as muon was lost in atmosphere"
          continue
        elif E_left <= E_c:
          echo "Skipping energy : ", e, " as muon has less than E_c = ", E_c, " energy left"
          lastDropped = e
          continue
        let E₀ = e - E_left
        flux.add muonFlux(e, θ, E₀, E_c, I₀, ε).float
        E_initials.add e.float        
        E_lefts.add E_left.float
      let dfLoc = toDf({E_initials, E_lefts, flux, "angle [°]" : angle})
      #  .filter(f{`E_initials` >= lastDropped.float})
      df.add dfLoc
    ggplot(df, aes("E_initials", "flux", color = factor("angle [°]"))) +
      geom_line() +
      xlab(r"Initial energy [\si{GeV}]") + ylab(r"Flux [\si{m^{-2}.s^{-1}.sr^{-1}}]") +
      scale_x_log10() + scale_y_log10() +
      ggtitle(&"Differential muon flux dependency at different angles{suffix}") +
      ggsave(&"/home/basti/phd/Figs/muons/initial_energy_vs_flux_and_angle_cosmic_muons{suffix}.pdf",
             useTeX = true, standalone = true, width = 600, height = 450)
  
    ggplot(df, aes("E_lefts", "flux", color = factor("angle [°]"))) +
      geom_line() +
      xlab(r"Energy at surface [\si{GeV}]") + ylab(r"Flux [\si{m^{-2}.s^{-1}.sr^{-1}}]") +
      scale_x_log10() + scale_y_log10() +
      ggtitle(&"Differential muon flux dependency at different angles{suffix}") +
      ggsave(&"/home/basti/phd/Figs/muons/final_energy_vs_flux_and_angle_cosmic_muons{suffix}.pdf",
             useTeX = true, standalone = true, width = 600, height = 450)              
  block StaticLoss:
    var df = newDataFrame()
    for angle in angles:
      let E = energies.mapIt(it.GeV)
      let θ = angle.°.to(Radian)
      let E₀ = 100.GeV - intBetheAtmosphere(100.GeV, 0.0.Radian).to(GeV)    
      let flux = E.mapIt(muonFlux(it, θ, E₀, E_c, I₀, ε).float)
      let dfLoc = toDf({energies, flux, "angle [°]" : angle})
      df.add dfLoc
    ggplot(df, aes("energies", "flux", color = factor("angle [°]"))) +
      geom_line() +
      xlab("Energy [GeV]") + ylab("Flux [m⁻²•s⁻¹•sr⁻¹]") +
      scale_x_log10() + scale_y_log10() +
      ggtitle(&"Differential muon flux dependency at different angles{suffix}") +
      ggsave(&"/home/basti/phd/Figs/muons/energy_vs_flux_and_angle_cosmic_muons{suffix}.pdf")


#proc plotE_vs_flux_and_angles(E_c: GeV, I₀: m⁻²•s⁻¹•sr⁻¹, ε: GeV,
#                              suffix = "") =
#  ## Generates a plot of the integrated muon flux vs angles for a fixed set of different
#  ## energies.
#  let angles = linspace(0.0, 90.0, 100)
#  var df = newDataFrame()
#  let energies = linspace(E_c.float, 100.0, 1000)
#  let E = energies.mapIt(it.GeV)
#  for angle in angles:
#    let θ = angle.°.to(Radian)
#    let E₀ = 100.GeV - intBetheAtmosphere(100.GeV, θ).to(GeV)
#    let flux = E.mapIt(muonFlux(it, θ, E₀, E_c, I₀, ε).float) 
#    let dfLoc = toDf({energies, flux, "angle [°]" : angle})
#    df.add dfLoc
#  ggplot(df, aes("energies", "flux", color = factor("angle [°]"))) +
#    geom_line() +
#    xlab("Energy [GeV]") + ylab("Flux [m⁻²•s⁻¹•sr⁻¹]") +
#    scale_x_log10() + scale_y_log10() +
#    ggtitle(&"Differential muon flux dependency at different angles{suffix}") +
#    ggsave(&"/home/basti/phd/Figs/muons/energy_vs_flux_and_angle_cosmic_muons{suffix}.pdf")

# different angles!      
block MuonBehavior:
  plotE_vs_flux_and_angles(0.3.GeV, 90.m⁻²•s⁻¹•sr⁻¹, 854.GeV)

proc unbinnedCdf(x: seq[float]): (seq[float], seq[float]) =
  ## Computes the CDF of unbinned data
  var cdf = newSeq[float](x.len)
  for i in 0 ..< x.len:
    cdf[i] = i.float / x.len.float
  result = (x.sorted, cdf)

import random, algorithm
proc sampleFlux(samples = 1_000_000): DataFrame =
  randomize(1337)
  let energies = linspace(0.1, 100.0, 100_000)
  #let energies = logspace(0, 2, 1000)
  let E = energies.mapIt(it.GeV)
  let flux = E.mapIt(muonFlux(it, 88.0.degToRad.Radian, E₀, E_c, I₀, ε).float)
  # given flux compute CDF
  let fluxCS = flux.cumSum()
  let fluxCS_sorted = flux.sorted.cumSum()
  let fluxCDF = fluxCS.mapIt(it / fluxCS[^1])
  let fluxCDF_sorted = fluxCS_sorted.mapIt(it / fluxCS_sorted[^1])

  let (data, cdf) = unbinnedCdf(flux)

  let dfX = toDf(energies, fluxCS, fluxCS_sorted, fluxCDF, fluxCDF_sorted)
  ggplot(dfX, aes("energies", "fluxCS")) +
    geom_line() +
    ggsave("/t/cumsum_test.pdf")
  ggplot(dfX, aes("energies", "fluxCDF")) +
    geom_line() +
    ggsave("/t/cdf_test.pdf")    
  ggplot(dfX, aes("energies", "fluxCS_sorted")) +
    geom_line() +
    ggsave("/t/cumsum_sorted_test.pdf")    
  ggplot(dfX, aes("energies", "fluxCDF_sorted")) +
    geom_line() +
    ggsave("/t/cdf_sorted_test.pdf")

  ggplot(toDf(data, cdf), aes("data", "cdf")) +
    geom_line() +
    ggsave("/t/unbinned_cdf.pdf")
  
  #if true: quit()
  var lossesBB = newSeq[float]()
  var lossesMP = newSeq[float]()
  var energySamples = newSeq[float]()

  let dedxmin = 1.519.MeV•cm²•g⁻¹
  echo "Loss = ", (dedxmin * Argon.ρ * 3.cm).to(keV)
  
  for i in 0 ..< samples:
    # given the fluxCDF sample different energies, which correspond to the
    # distribution expected at CAST
    let idx = fluxCdf.lowerBound(rand(1.0))
    let E_element = E[idx]
    # given this energy `E` compute the loss
    let lossBB = (E_element.to(eV) - intBethe(Argon, 3.cm, E_element.to(eV), dx = 50.μm)).to(keV).float
    lossesBB.add lossBB
    let lossMP = mostProbableLoss(-1, Argon.Z, Argon.M, E_Element.E_to_γ(), Argon.ρ * 3.cm)
    lossesMP.add lossMP.float
    #echo "Index ", i, " yields energy ", E_element, " and loss ", loss
    energySamples.add E_element.float
  let df = toDf(energySamples, lossesBB, lossesMP)
    .gather(["lossesBB", "lossesMP"], "Type", "Value")
  ggplot(df, aes("Value", fill = "Type")) +
    geom_histogram(bins = 300, hdKind = hdOutline, alpha = 0.5, position = "identity") +
    margin(top = 2) +
    xlim(5, 15) +
    ggtitle(&"Energy loss of muon flux at CAST based on MC sampling with {samples} samples") +
    ggsave("/home/basti/phd/Figs/muons/sampled_energy_loss.pdf")

  ggplot(df, aes("energySamples")) +
    geom_histogram(bins = 300) +
    margin(top = 2) +
    ggtitle(&"Sampled energies for energy loss of muon flux at CAST") +
    ggsave("/home/basti/phd/Figs/muons/sampled_energy_for_energy_loss.pdf")
  let (samples, bins) = histogram(energySamples, bins = 300)
  let dfH = toDf({"bins" : bins[0 ..< ^1], samples})
    .filter(f{`bins` > 0.0 and `samples`.float > 0.0})
  ggplot(dfH, aes("bins", "samples")) +
    geom_line() +
    scale_x_log10() + 
    margin(top = 2) +
    ggtitle(&"Sampled energies for energy loss of muon flux at CAST") +
    ggsave("/home/basti/phd/Figs/muons/sampled_energy_for_energy_loss_manual.pdf")

  ggplot(toDf(energies, flux), aes("energies", "flux")) +
    geom_line() +
    scale_x_log10() +
    ggsave("/tmp/starting_data_e_flux.pdf")

  ggplot(toDf(energies, flux), aes("energies", "flux")) +
    geom_line() +
    ggsave("/tmp/linear_starting_data_e_flux.pdf")
    

discard sampleFlux(samples = 1_000_000)
#+end_src


** Gaseous detector fundamentals

Gaseous detectors consist of a volume filled with some kind of gas,
usually a noble gas with a small amount of molecular gas. Some kind of
entrance window allows the particles to be detected to enter. An
electric field is applied over the gas volume, strong enough to cause
electron-ion pairs created by ionization of the incoming particles to
drift to opposite ends of the volume (magnetic fields may be utilized
in addition). At least on the side of the anode (where the electrons
arrive), a readout of some form is installed to measure the time of
arrival, amount of collected charge and / or the position of the
electrons. Depending on the details, this in principle allows for a 3D
reconstruction of the initial event in the gas volume. The details of
choice of detector gas, applied electric fields, gas volume dimensions
and types of readout have a very large impact on the applications a
detector is useful for. In the following we will focus on the physics
concepts required for gaseous detectors with few \si{cm} long drift
volumes and high spatial resolution readouts.

Note: this section covers the basic fundamentals that will be later
referenced in the thesis. For a much more in-depth understanding of
these concepts, see references cite:sauli2014gaseous and
cite:kolanoski2020particle and to some extent the PDG
cite:Zyla:2020zbs (in particular chapters on particle detectors and
passage of particles through matter; chapter number varies by year).

*** TODOs for this section                                       :noexport:

Write a few general things about gaseous detectors here. I.e. contain
usually mainly a noble gas, with some quencher for rotational and
vibrational modes. These 'cool' the electrons down so that they are in
the Townsend minimum and effectively increases the drift velocity.

Electric fields should be strong enough to let electrons and ions
drift in opposite directions and have a fast enough drift velocity,
but low enough to not cause further ionization.


*** Gas mixtures and Dalton's law [/]
:PROPERTIES:
:CUSTOM_ID: sec:theory:daltons_law
:END:

Of common interest when dealing with gas mixtures is the notion of
partial pressures. In ideal gas theory, a mixture of gases at a
pressure $P$ can be considered to be the sum of the 'partial
pressures' of each gas species

\[
P = \sum_i p_i,
\]

initially noted by John Dalton in 1802 cite:dalton1802essay.

The contribution of each gas only depends on the species' mole
fraction. Typically when considering gas mixtures the fractions of
each gas species is given as a percentage. The percentage already
refers to the mole fraction of each species. As such, the partial
pressure of a single species can be expressed as:

\[
p_i = n_i P
\]

where $n_i$ is the mole fraction of species $i$.

This is an extremely valuable property when computing different
interactions of particles with gas mixtures. For example when
computing the absorption of X-rays after propagating through a certain
distance of a gas mixture, as one can compute absorption for each
partial pressure independently and combine the contributions after
(whether they be added, multiplied or something else of course depends
on the considered process).

**** TODOs for this section                                     :noexport:

- [X] *FIND GOOD REFERENCE FOR DALTON'S LAW*
    -> Reference to original work of John Dalton

*** Ionization energy and average energy per electron-ion pair

In order to understand the signals detected by a gaseous detector the
average number of electrons created by an incident particle should be
known. Denoted by the $W\text{-value}$, it is defined by

\[
W = \frac{I}{\langle N \rangle} 
\]

where $I$ is the mean ionization energy of the gas and $\langle N
\rangle$ the average number of electron-ion pairs generated per
interaction. This number is usually smaller than one, \numrange{0.6}{0.7}
for noble gases and even below \num{0.5} for molecular gases. Not all
energy of the incoming particle is always deposited in the form of
generation of, for example, a photoelectron. Other forms of energy
loss are possible. In molecular gases vibrational and rotational modes
offer even more possibilities, resulting in the smaller values.

The mean excitation energy $I$ of an element is the weighted
combination of the most likely energy levels the element is ionized
from. The exact values are dependent on the specific element and
tabulated values like from NIST cite:hubbell1996nist exist. Above some
$Z$ a rough approximation of $I = \SI{10}{eV} · Z$ can be substituted,
developed by Bloch cite:bloch1933bremsvermogen.

The precise number for $W$ strongly depends on the used gas mixture
and typically requires experimental measurements to determine. Monte
Carlo based simulations can be used as a rough guide, but care must be
taken interpreting the results as significant uncertainty can
remain. Tools for such Monte Carlo simulations include GEANT4
cite:GEANT4:2002zbu [fn:geant4] and MCNelectron
cite:doi:10.1080/00223131.2014.974710 [fn:mcn_electron]. These are
based on atomic excitation cross sections, which are well tabulated in
projects like ENDF cite:brown2018endf (citation for the latest data
release) [fn:endf_website] and LXCat
cite:pancheshnyi2012lxcat,pitchford2017lxcat,carbone2021data [fn:lxcat_website].

# Note: there is also MCNP6 https://mcnp.lanl.gov/ which I will
# conveniently *not* mention, as it is under ridiculous US export
# regulations and therefore the source code is not visible for non US
# citizens. Screw that.

[fn:geant4] [[https://geant4.cern.web.ch]]

[fn:mcn_electron] http://web.vu.lt/ff/a.poskus/mcnelectron/

[fn:endf_website] https://www.nndc.bnl.gov/endf-b8.0/index.html

[fn:lxcat_website] [[https://lxcat.net]]

[fn:w_value_fano_factor] Interestingly, there is an empirical somewhat
linear relationship between the W-value as shown here and the Fano
factor cite:bronic1992relation.
*NOTE*: Given how Fano discovered / described Fano noise, it's maybe
not quite surprising that this is related! 

**** TODOs for this section                                     :noexport:


*FIND GOOD REFERENCES* beyond these two. Better would be a primary
like reference. cite:bronic1992relation,doi:10.1080/00223131.2014.974710 

Why expect ~26 eV for argon

Talk about ionization energy vs. the actual mean energy loss for a
single ionization. Argon ionization energy is only like 15 eV or so,
but effective one is 26.

Leads to our 226 or so primary electrons for our \cefe spectra.


*** Mean free path

While we have already discussed the mean free path for X-rays in
sec. [[#sec:theory:xray_matter_gas]], we should revisit it quickly for
electrons in a gas. It is a necessary concept when trying to
understand other gaseous detector physics concepts, in particular for the gas
amplification. [fn:treatment]

The mean free path of an electron in a gas can be described by

#+NAME: eq:theory:mean_free_path_e_def
\begin{equation}
λ = \frac{1}{nσ}
\end{equation}

where $n = N/V$ is the number density (atoms or molecules per unit
volume) of the gas particles and $σ$ the cross section of electrons
interacting with them. Such cross sections are tabulated for different
elements, for example again see LXCat
cite:pancheshnyi2012lxcat,pitchford2017lxcat,carbone2021data.

Based on the ideal gas law, we can express it as a function of
pressure $p$ and temperature $T$ to

\[
p V = N k_B T ⇔ p = n k_B T,
\]

with the Boltzmann constant $k_B$.  Inserting this via $n$ into
eq. [[eq:theory:mean_free_path_e_def]] yields

#+NAME: eq:theory:mean_free_path_e
\begin{equation}
λ = \frac{k_B T}{p σ}.
\end{equation}

[fn:treatment] Its usefulness depends on the angle from which
different aspects are discussed of course. We mainly use it to
understand the gas amplification later.

**** TODOs for this section                                     :noexport:

The old text:



*NOTE*: This is already explained for X-rays in the previous
section. Do we need this concept in some more detail? Otherwise we can
also remove it later. 

See photons above.

Of major importance for the detection of particles in a gaseous
detector is the mean free path. This is the mean length a particle
traverses in a medium before interactions. For charged particles it
yields the mean distance between individual interaction points,
whereas for a photon it gives the mean length the photon traverses
through the detector before interaction.

Especially for photons this value is of crucial importance, as it
tells us at what distance photons will likely convert depending on
their energy. This is described by the absorption length as mentioned
in section [[#sec:theory:xray_matter_gas]]. This is important as it
directly affects the possible drift distance and thus diffusion
available to the generated electrons.  *TOO MUCH DETAIL, AS WE HAVE
NOT INTRODUCED DETECTORS YET!!*


*** Diffusion
:PROPERTIES:
:CUSTOM_ID: sec:theory:gas_diffusion
:END:

Diffusion in the context of gaseous detectors is the process of the
random walk of electrons either in longitudinal or transverse (to the
electric field) direction they exhibit, when drifting towards the
readout. Depending on the specific detector technology, some
transverse diffusion may be a desired property. For long drift
distances, magnetic fields can be used to significantly reduce the
transverse diffusion.

For an overview of the mathematics of diffusion as a random walk
process, see cite:berg1993random. In the context of gaseous detectors
see for example any of cite:sauli2014gaseous,kolanoski2020particle,Hilke2020

In general for precise numbers measurements must be taken. Monte Carlo
simulations using tools like Magboltz cite:biagi1995magboltz or
PyBoltz cite:pyboltz can be used as a general reference if no
measurements are available.

The mean squared distance $σ_t$ from a starting point after a certain amount
of time $t$ is given by

\[
σ_t = \sqrt{ 2 N D t }
\]

for a random walk in $N$ dimensions, where $D$ is a diffusion
coefficient specifying the movement in distance squared per time.  For
example for a point like source of multiple electrons, after diffusion
of some distance a 2 dimensional gaussian distribution will be
expected with a standard deviation of $σ_t$. By relating the time to
the drift velocity $v$ along an axis and introducing the diffusion
constant $D_t$

\[
D_t = \sqrt{ \frac{2 N D}{v} }
\]

we can further express $σ_t$ as

#+NAME: eq:gas_physics:diffusion_after_drift
\begin{equation}
σ_t = D_t · \sqrt{x},
\end{equation}

which is often practically useful to estimate the expected diffusion
after a certain drift length. 

Note that the terminology of "diffusion constant", "diffusion
coefficient" and similar is often used ambiguously as to whether they
refer to $D$ or $D_t$. Nor is the considered dimensionality $N$ always
clearly indicated. Keeping $N = 1$ and handling multiple dimensions as
independent random walks is a practical approach to take (as long as
it is valid in the application). [fn:diffusion_and_simulation]

[fn:diffusion_and_simulation] Later in chapter
[[#sec:background:mlp:event_generation]] we will discuss Monte Carlo event
generation, which use the diffusion coefficient as an input to
generate clusters after drift. Distance $x$ and $y$ are each sampled
individually according to eq. [[eq:gas_physics:diffusion_after_drift]] and
combined to a radial distance from the cluster center.

**** TODOs for this section                                     :noexport:

- [X] *CITE MAGBOLTZ, PYBOLTZ*
- [X] *HILKE2020*

*EXPLAIN WHERE COMES FROM, COMES DOWN TO $D_t$ AND PARAMETER FROM
MAGBOLTZ*
-> ?

**** Additional thoughts on diffusion                           :noexport:

*this is important as it relates to the 1.5 σ_transverse cut we do for
data cleaning!*

- [ ] *ADD HOW LONGITUDINAL AND TRANSVERSE DIFFUSION RELATE*
  -> This is important in the context of using the FADC as a veto. We
  can measure the transverse diffusion by looking at the event sizes,
  but we cannot do the same for the longitudinal diff (except by
  looking at the FADC events, but the idea being to define
  conservative cuts on FADC using theory)

- [ ] Explain better distinction between diffusion coefficient and its
  standard deviation!
  -> Normal distribution describes position!
  See also:
  [[file:~/org/Papers/gas_physics/randomwalkBerg_diffusion.pdf]]
  <x²> = 2 D t    ( 1 dimension )
  <x²> = 4 D t    ( 2 dimensions )
  <x²> = 6 D t    ( 3 dimensions )
  Also look into Sauli book again (p. 82 eq. (4.5) and eq. (4.6)).
  Also: 
  [[file:~/org/Papers/Hilke-Riegler2020_Chapter_GaseousDetectors.pdf]]
  page 15 sec. 4.2.2.2
  The latter mentions on page 15 that there is a distinction between:
  D = diffusion coefficient
  for which
  σ = √(2 D t) (1 dim)
  is valid
  and
  D* = diffusion constant
  for which:
  σ = D* √(z)
  is valid!

  From PyBoltz source code in ~Boltz.pyx~
  #+begin_src python
            self.TransverseDiffusion1 = sqrt(2.0 * self.TransverseDiffusion / self.VelocityZ) * 10000.0
  #+end_src
  which proves the distinction in the paper:
  √(2 D t) = D* √x
  ⇔ D* = √(2 D t) / √x = √(2 D t / x) = √(2 D / v) (with x = v t)

Describe diffusion based on gas. Needed to get expected photon size
based on conversion at specific height.

What effects affect diffusion?

Random walk + a force acting on particles.

- [X] *COMPUTE USING PYBOLTZ: https://github.com/UTA-REST/PyBoltz*
  -> DONE


*** Drift velocity

The drift velocity is the average speed at which electrons move
towards the anode in a gaseous detector. It is required to understand
how recorded time information relates to longitudinal shape
information of recorded data. Based on the so called 'friction force
model' an analytical expression for the drift velocity in an
electromagnetic field can be written to:

\[
\vec{v} = \frac{e}{m_e} \frac{τ}{1 + ω²τ²}\left( \vec{E} + \frac{ωτ}{B} (\vec{E}
× \vec{B}) + \frac{ω²τ²}{B²}(\vec{E} · \vec{B}) \vec{B} \right)
\]

with the electron charge $e$ and mass $m_e$, in an electric field
$\vec{E}$ and magnetic field $\vec{B}$, given the Lamor frequency $ω =
eB / m_e$ and the mean collision time $τ$. cite:Zyla:2020zbs

For detectors without a magnetic field $B = 0, ω = 0$ and a constant,
homogeneous electric field $E$, this reduces to the Townsend
expression:

\[
v = \frac{e E τ}{m_e}.
\]

If measurements are not available, these can also be computed by Magboltz
cite:biagi1995magboltz or PyBoltz cite:pyboltz, which solve the
underlying transport equation, the Boltzmann equation.

**** TODOs for this section                                     :noexport:

Boltzmann equation:

\begin{align*}
  \frac{∂f}{∂t} + \vec{v} \frac{∂}{∂\vec{r}}f + \frac{∂}{∂\vec{v}}\vec{g} &= Q(t) \\
  \vec{g} &= \left(\frac{e\vec{E}}{m} + \vec{ω} × \vec{v}\right) f
\end{align*}


*MENTION MOLECULAR GASES HAVE HIGHER DRIFT VELOCITY*

Talk about drift velocity of electrons for a given electric
field. Required to know time scales associated with e.g. muons + FADC,
time it takes for X-rays to drift (for random coincidences in long
frames etc.)

- [ ] *USE FORMULA PDG*

The detector used in this thesis does not make use of magnetic
fields. Thus, all terms but the first are zero. Further, the electric
field is constant, leading to the following simplification:

*FIX THIS*
-> ?

*BOLTZMANN EQUATION: https://en.wikipedia.org/wiki/Boltzmann_equation*

- [X] *COMPUTE USING PYBOLTZ: https://github.com/UTA-REST/PyBoltz*
  -> DONE



*** Gas amplification
:PROPERTIES:
:CUSTOM_ID: sec:theory:gas_gain_polya
:END:

In order to turn the individual electrons into a measurable signal,
gaseous detectors use some kind of gas amplification stage. Details
vary, but it is usually a region in the gas volume close to the
readout with a very strong electric field (multiple $\si{kV.cm^{-1}}$)
such that each electron causes many secondary ionizations, leading to
an avalanche effect. In case of the detectors described in this
thesis, amplification factors between $\numrange{2000}{4500}$ are
desired.

An electron in a strong electric field $\vec{E}$ will gain enough
energy to ionize an atom of the gas after a distance

\[
l \geq \frac{I}{|\vec{E}|}
\]

with the ionization potential of the gas, $I$. This needs to be put
into relation with the mean free path,
eq. [[eq:theory:mean_free_path_e]]. If $l \ll λ$ no secondary ionization
takes place and if $l \gg λ$ every interaction leads to ionization,
likely resulting in a breakdown causing an arc (see also Paschen's
law, not covered here). In the intermediate range some interactions
cause secondary ionization, some does not.

We can make the statistical argument that

\[
\mathcal{N} = e^{-l/λ}
\]

is the relative number of collisions with $l > λ$. This
allows to define the probability of finding a number of ionizations
per unit length to be

\[
P(l) = \frac{1}{λ} e^{-l / λ} = α,
\]

where we introduce the definition of the 'first Townsend coefficient',
$α$.  We can insert the definition of the mean free path,
eq. [[eq:theory:mean_free_path_e]], and $l$ into this equation to obtain

#+NAME: eq:theory:townsend_coefficient
\begin{equation}
α(T) = \frac{pσ}{kT} \exp\left( - \frac{I}{|\vec{E}|}\frac{pσ}{kT}\right).
\end{equation}

With this we have an expression for the temperature and pressure
dependency of the first Townsend coefficient. [fn:usefulness] This
derivation followed [[cite:&lucianMsc]] based on [[cite:&engel65_gases]], but
see [[cite:&aoyama85_gas_gain]] for a more general treatment. Similarly to
diffusion and drift parameters, the first Townsend coefficient can be
computed numerically using tools like Magboltz.

Note that [[cite:&sauli2014gaseous]] introduces the Townsend coefficient
as $α = \frac{1}{λ}$, introducing a specific $λ$ and $σ$ referring to
the /ionization/ mean free path and /ionizing/ cross sections. This
can be misleading as it makes it seem as if the Townsend coefficient
is inversely proportional to the temperature. This is of course only
the case in the regime where each interaction actually causes another
ionization.

The first Townsend coefficient can be used to express the
multiplication factor -- or gas amplification -- used in a detector,

\[
G = e^{α x}
\]

as the number increases by $α$ after each step $\mathrm{d}x$.

The statistical distribution describing the number of electrons after
gas amplification is the Polya distribution

\[
p(x) = \frac{N}{G} \frac{(1 + θ)^{1 + θ}}{Γ(1 + θ)}
\left(\frac{x}{G}\right)^θ \exp\left[- \frac{(1 + θ) x}{G}\right]
\]

where $N$ is a normalization constant, $θ$ is another parameter
performing scaling of the distribution and $G$ is the effective gas
gain. $Γ$ refers to the gamma function. It is to note that the term
"polya distribution" in this context is different from other
mathematical definitions, in which polya distribution usually means a
negative binomial distribution. The above definition goes back to
Alkhazov cite:alkhazov1970statistics and in slight variations is
commonly used. Due to the complexity of this definition, care needs to
be taken when performing numerical fits to data with this function
(using bounded parameters and preferring more general non-linear
optimization routines instead of a normal Levenberg-Marquardt
non-linear least squares approach).


- [ ] *REVISIT THE BELOW TWO PARAGRAPHS AFTER INSERTING MORE DETAILS ABOVE!*

The largest impacts on the expected gas amplification have the
electric field, the choice of gas and the temperature of the
gas. While the former two parameters are comparatively easy to
control, the temperature in the amplification region may vary and is
hard to measure. As such depending on the detector details and
application, gas gain variations are expected and corrections based on
a running gas gain value may be necessary.

As the large number of interactions in the amplification region can
excite many atoms of the (typically) noble gas, UV photons can be
produced. Their mean free path is comparatively long relative to the
size of the amplification region. They can start further avalanches,
potentially away from the location of the initial avalanche start,
lowering spatial resolution and increasing apparent primary electron
counts. Molecular gases are added in small fractions to the gas
mixture to provide non UV photon producing ways to absorb energy. In
this context the molecular additions are called 'quencher gases'.

[fn:usefulness] This will be a useful reference later when discussing
possible temperature effects of the detector operated at CAST.

**** TODOs for this section                                     :noexport:


- [ ] *FIX TYPING OF POLYA DISTRIBUTION*
  -> ?

- [X] If it fits here, Polya distribution to describe avalanche effect.

- [X] What gas properties affect the gas gain? Temperature, density etc.

- [X] Gas gain.

- [X] *MENTION UV PHOTONS AND HENCE MOLECULAR GASES CALLED QUENCHER
  GAS*
- [ ] *ADD PLOT OF FUNCTION?*
  -> No, would be too much. Will be seen shortly anyway for real detector.

*** TODO Mobility in a gas and mean free path in amplification region :noexport:

- [ ] Delete this subsection?

- [X] This is now covered in the gas gain chapter!




- [ ] Not sure if I want to include this. For now I guess I
  don't. Will depend on when I reread the detector behavior part.


*NOTE*: The first Townsend coefficient is α = 1/λ, where λ is the mean
free path!


- [ ] This is related to the GridPix variability in the gas gain &
  peak position.
  From PDG it reads "mobility" in gas is inversely proportional to
  density!

- [ ] Find references (Bichsel or other gaseous detector books surely has
some!).
- [ ] maybe mini table of some mobilities
- [ ] mobility vs mean free path?
- [ ] expression how that relates to temperature?    
  

*** Energy resolution [/]

Because of the statistical processes associated with the full
detection method used in gaseous detectors, even a perfect delta like
signal in energy, will lead to a natural spread of the measured
signal. The convolution of different ionization yields, potential
losses and gas gain variation all contribute to such a spread.

As such a typical measure of interest for gaseous detectors is the
energy resolution, which is commonly defined by

\[
ΔE = \frac{σ}{E}
\]

where $σ$ is the standard deviation of the normal distribution
associated with the resolved peak in the detectors data, assuming a --
for practical purposes -- delta peak as the input spectrum. Sometimes
definitions using the full width at half maximum (FWHM) are also used
in place of $σ$. Typical values for the energy resolution defined like
this are smaller than $\SI{15}{\%}$.

If the absolute magnitude of the $σ$ at a given energy is constant,
which at least is partially reasonable as the width is not fully due
to energy dependent effects, the energy resolution is proportional to
$1/E$.

The Fano factor cite:fano63, defined by the variance over the mean of
a distribution $F = σ² / μ$ (typically within some time window),
lowers the ideal energy resolution. It arises in the associated
statistical processes, because there are a finite number of possible
interaction states. For X-rays an additional aspect is due to the
maximum energy transferable into the gas due to the X-rays energy. In
practice though the energy resolution of gaseous detectors is usually
limited by other effects.

**** TODOs for this section                                     :noexport:

- [ ]  *CHECK IF SENTENCE ABOUT FANO FACTOR CORRECT!*
  -> I just checked the Fano paper. It's super long and I'm not sure
  if I understand what the Fano factor in there really is!
  
About Fano factor from cite:bronic1992relation:
#+begin_quote
The Fano factor F represents the ratio of the observed variance of the distribution
of the number of ion pairs to the variance of the Poisson distribution. At high incident
energies the value of the Fano factor is constant, around 0.17 in noble gases, and
between 0.2 and 0.4 in molecular gases. The value of F increases toward unity as the
initial energy of an incident particle decreases toward the ionization potential of a gas
because at low electron energies the non-ionizing collisions become more numerous.
#+end_quote
  

- [X] *THINK ABOUT REPHRASING THIS / GIVING A VALUE FOR IDEAL RESOLUTION?*
  (See Alkhazov paper maybe?)
  -> Rephrased the part, but did not consider Alkhazov
  
- [X] *REWRITE TO USE σ/μ AS OUR 15 PERCENT MATCHES FOR THAT NOT FOR
  FWHM. ONLY BEST IN CLASS REACH BELOW 15 IN THE LATTER CASE*

- [X] What is energy resolution, definition.

Why important for our detector.

- [X] *WRITE SOMETHING FANO?* See Sauli about it. Section 7.5 on energy
resolution and section 3.6 about photo ionization of X-rays.

Fano factors are related to this! Theory and observation
disagree. Fano factor fixes this by doing a scaling. Related to the
fact that theory assumes a perfectly statistical process, but reality
has fixed number of possible interactions, hence not really perfect
statistics.


  

*** Escape photons and peaks | 55Fe as a typical calibration source
:PROPERTIES:
:CUSTOM_ID: sec:theory:escape_peaks_55fe
:END:

Finally, gaseous detectors need to be calibrated, monitored and
tested. This is commonly done with a \cefe source. \cefe is a
radioactive isotope of iron, which decays under inverse beta decay to
$\ce{^{55}Mn}$. Due to the required restructuring of the electronic
shells, the manganese is in an excited state. While the emission of an
Auger electron with $\SI{5.19}{keV}$ dominates with a probability of
\SI{60}{\percent}, as an X-ray source the $Kα₁$ and $Kα₂$ lines with
combined energies of about $\SI{5.9}{keV}$ are of note.

When using such a \cefe source as a calibration source for an argon
filled gaseous detector, the \SI{5.9}{keV} photons will produce a
photo-electron in argon. If this electron fully releases its energy
via further ionizations, the 'photopeak' will be observed at around
$\SI{5.9}{keV}$. If however another photon is produced with an energy
below the $K 1s$ energy of argon ($\SI{3.2}{keV}$) -- for example a
photon produced via $Kα₁$ or $Kα₂$ fluorescence of argon, both at
about $\SI{2.95}{keV}$ -- such a photon has a very long absorption
length in the gas volume, about $l_{\text{abs}} = \SI{3.5}{cm}$
(cf. fig. [[fig:theory:transmission_examples]]). This can cause it to
easily escape the active detection region, especially if the sensitive
region of the detector is comparatively small. The result is a
measured signal of $E_i - E_k = \SI{5.9}{keV} - \SI{2.95}{keV} \approx
\SI{2.9}{keV}$, called the 'escape
peak'. cite:sauli2014gaseous,kolanoski2020particle,hubbell1996nist

Such an additional escape peak is useful as a calibration tool, as it
gives two distinct peaks in a \cefe calibration spectrum, which can be
utilized for an energy calibration.

**** TODOs for this section                                     :noexport:

- [ ] *LUCIAN MENTIONS* in this thesis cite:lucianMsc that the Kα only
  produces 5.75 keV. This is what we use for the *pixel* spectrum, but
  not the charge spectrum.


- [X] *ADD CITATIONS*. For Sauli / Wermes & NIST for numbers.

Explain escape photons, escape peaks, how that gives us an escape peak
in the 55Fe spectra as well as a line at 3 keV in our background data.

Explain Fe ↦ Mn excited ↦ Mn + γ and what spectrum looks like

*INSERT EXAMPLE FIGURE OF SPECTRUM*

* Septemboard detector                                             :Detector:
:PROPERTIES:
:CUSTOM_ID: sec:septemboard
:END:
#+LATEX: \minitoc
The detector in use in the 2014 / 2015 data taking campaign, presented
in section [[#sec:detector:detector_2014_15]] had a few significant
drawbacks for more sensitive searches, in particular for searches at
low energies $\lesssim\SI{2}{\keV}$ and searches requiring low
backgrounds over larger areas on the chip (for example the chameleon
search done in cite:krieger2018search). For this reason a new detector
was built to improve each downside of the detector.

We start with a basic overview of the new detector, the 'septemboard'
detector in section [[#sec:detector:detector_overview]]. From there we
continue looking at each of the new detector features motivating their
addition. All detector upgrades were done to alleviate one or more of
these drawbacks. We will now go through each of the new detector
features and highlight the aspects it is intended to improve on.

Section [[#sec:detector:scintillators]] introduces two new scintillators as
vetoes. These require the addition of an external shutter for the
Timepix, which is realized by usage of a flash ADC (FADC), see section
[[#sec:detector:fadc]]. Further, an independent but extremely important addition is
the replacement of the Mylar window by a silicon nitride window,
section [[#sec:detector:sin_window]]. Another aspect is the addition of another 6
GridPixes around the central GridPix, the 'Septemboard' introduced in
section [[#sec:detector:septemboard]]. 

** TODOs for this section                                         :noexport:

- [ ] *REWRITE PARAGRAPHS*

- [ ] *HAVE TO MENTION FPGA AND TOF BRIEFLY FOR SCINTILLATORS AND FADC!*

Why do we build such a 'complicated' detector?

Background increases to edges, esp. corners.

Background rate has known peaks. 3 keV for the Argon escape
peak. Can't do anything about that in current iteration.

Peak at 8-9 keV. A mix of a copper peak and orthogonal muons, which
are expected to emit about 8 keV through 3 cm. More on this later in
[[Background rate]].

Window doesn't transmit at < 2 keV


The first section introduces the reasoning behind building a more
complicated detector. Existing detector has multiple downsides, seen
in background rate & detector efficiency.

In the further sections we discuss each additional detector feature in
detail and explain why it was added / what drawback of the previous
detector should be improved on.

After we have introduced the detector as a whole we talk about the
calibrations that are necessary to perform sensible measurements.

*IN EACH SUBSECTION START WITH WHAT IT'S SUPPOSED TO HELP WITH?*



** Micromegas working principle

\textbf{Micro} \textbf{Me}sh \textbf{Ga}seous \textbf{S}tructures
(Micromegas) are a kind of \textbf{M}icro\textbf{p}attern
\textbf{G}aseous \textbf{D}etectors (MPGDs) first introduced in 1996
cite:GIOMATARIS199629,GIOMATARIS1998239. The modern Micromegas is
the Microbulk Micromegas cite:Andriamonje_2010.

Interestingly, the name Micromegas is based on the novella Micromégas
by Voltaire published in 1752 cite:voltaire1752micromegas, an early
example of a science fiction story. cite:GIOMATARIS199629

These detectors are -- as the name implies -- gaseous detectors
containing a 'micro mesh'. In the most basic form they are a made of a
closed detector volume that is filled with a suitable gas (often Argon
based gas mixtures are used; Xenon based detectors are in development
for certain applications) allowing ionization. The volume is split
into two different sections, a large drift volume typically
$\mathcal{O}(\text{few }\si{cm})$ and an amplification region, sized
$\mathcal{O}(\SIrange{50}{100}{μm})$. At the top of the volume is a
cathode to apply an electric field. Below the mesh is the readout area
at the bottom of the volume. In standard Micromegas detectors strips
or pads are used as a readout. The electric field in the drift region
is strong enough to avoid recombination of the created electron-ion
pairs and to provide reasonably fast drift velocities
$\mathcal{O}(\si{cm.μs⁻¹})$. The amplification gap on the other hand
is precisely used to multiply the primary electrons using an avalanche
effect. Thus, the electric field reaches values of
$\mathcal{O}(\SI{50}{kV.cm⁻¹})$. These drift and amplification volumes
are achieved by an electric field between a cathode and the mesh as
well as the mesh and the readout area.

Fig. [[micromegas_schematic]] shows a schematic for the general working
principle of such detectors

#+begin_center
#+CAPTION: Working principle of a general Micromegas detector. Specific distances and gas mixture
#+CAPTION: are exemplary. An ionizing photon enters through the
#+CAPTION: detector window into the gas-filled detector body. After a certain distance it produces
#+CAPTION: a photo electron, which ionizes further gas molecules for a specific number of primary
#+CAPTION: electrons (depending on the incoming photon's energy) and gas mixture. The primary electrons
#+CAPTION: drift towards the micromesh due to the drift voltage, thereby experiencing diffusion. 
#+CAPTION: In the much higher voltage in the amplification gap an avalanche of electrons is produced, 
#+CAPTION: enough to trigger the readout electronics (strips or pads).
#+NAME: micromegas_schematic
[[~/org/Figs/thesis/detectors/micromegas_schematic.pdf]]
#+end_center


** Timepix ASIC

The Timepix ASIC (Application Specific Integrated Circuit) is a $256 ×
256$ pixel ASIC of $\num{55}·\SI{55}{μm^2}$, based on the Medipix ASIC
developed for medical imaging applications by the Medipix
Collaboration cite:medipix. The pixels are distributed over an active
area of $\num{1.41}\times\SI{1.41}{\cm^2}$. Each pixel contains a
charge sensitive amplifier, a single threshold discriminator and a
$\SI{14}{bit}$ pseudo random counting logic. It requires use of an
external clock, in the range of \SIrange{10}{150}{MHz}, with
\SI{40}{MHz} being typical clock frequency for the use cases described
in this
thesis. cite:LlopartCudie_1056683,LLOPART2007485_timepix,timepix_manual
A good overview of the Timepix is also given in
cite:lupberger2016pixel.

A picture of a Timepix ASIC is shown in fig. sref:fig:detector:timepix_asic.

The Timepix uses a shutter based readout, either with a fixed shutter
time or using an external trigger to close a frame. After the shutter
is closed in the Timepix, the readout is performed during which the
detector is insensitive. Each pixel further can work in four
different modes:

- hit counting mode / single hit mode :: simply counts the number of
  times the threshold of a pixel was crossed (or whether a pixel was
  activated once in single hit mode).
- \textbf{T}ime \textbf{o}ver \textbf{T}hreshold (ToT) :: In the ToT
  mode the counter of a pixel will count the number of clock cycles
  that the charge on the pixel exceeds the set threshold, which is set
  by an $\SI{8}{bit}$ \textbf{D}igital to \textbf{A}nalog
  \textbf{C}onverter (DAC) while the shutter is open. ToT is
  equivalent to the collected charge of each pixel.
- \textbf{T}ime \textbf{o}f \textbf{A}rrival (ToA) :: The ToA mode
  records the number of clock cycles from the first time the pixel's
  threshold is exceeded to the end of the shutter window. Thus, it
  allows to calculate the time at which the pixel first crossed the
  threshold.

In the context of this thesis only the ToT mode was used.  

*** TODOs for this section                                       :noexport:

- [ ] *CITE LUPBERGER AS REFERENCE FOR THOROUGH TIMEPIX OVERVIEW*

- [ ] *TIMEPIX MANUAL [[file:~/org/Papers/detectors/Timepix_Manual_v1.0.pdf]]*
  states the size to!!!
  cite:timepix_manual
  #+begin_quote
The chip dimensions are 16120x14111 μm2; it has a matrix formed by 256x256 pixels of
55x55 μm2 with an active area of 1.982 cm2
  #+end_quote

- [ ] *THINK ABOUT* setting timepix asic image side by side with
  either micromegas schematic or InGrid. Better InGrid, because then
  the timepix is visible in it!

Check Lucian's master thesis for information about Timepix & gaseous
detector physics. :)

*TALK HERE ABOUT DIFFERENT TOS CALIBRATIONS?*

*ADD ALL USED DETECTOR CALIBRATIONS IN FULL TO APPENDIX. INCLUDE THE
PLOTS FOR TOT. TABLE OF TOT FIT PARAMETERS ETC*

- [X] *INSERT TIMEPIX FIGURE?*


- [X] *CITE TIMEPIX MANUAL FOR SIZES ETC*  


*DOES TOA IN TPX1 START FROM SHUTTER OPEN OR START FROM PIXEL HIT?*

- [X] *LUPBERGER THESIS EXPLAINS IT*, page 30:
  #+begin_quote
  Time of arrival (TOA) mode: In the Medipix chip family, this mode is unique for the Timepix
chip. It was particularly requested by the EUDET collaboration to measure the arrival time of
charge in order to reconstruct the drift time of charge in a TPC. The FCLOCK cycles are counted
from the first rising edge of the discriminator signal until the end of the shutter window. This
way, the arrival time can be calculated, when the timing of the closing shutter is know. If there
are several rising edges of the discriminator signal within one shutter window, only the first one
will have an effect.
  #+end_quote
- [ ] *PIXELS COUNT TO MAX 11810 AS WELL OF COURSE. THAT'S THE TIME
  LIMIT. AT 40 MHZ THIS IS ABOUT 295 μs TIME*

*** Timepix3

The Timepix3 is the successor of the
Timepix. cite:Poikela_2014_timepix3,timepix3_manual It is generally
similar to the Timepix, but would provide 3 important advantages if
used in a gaseous detector for the applications in this thesis:
- clock rates of up to \SI{300}{MHz} for higher possible data rates
  (less interesting for data taking in an axion helioscope)
- a stream based data readout. This means no fixed shutter times and
  no dead time during readout. Instead data is sent out when it is
  recorded in parallel.
- each pixel can record ToT *and* ToA at information at the same
  time. This allows to record the charge recorded by a pixel as well
  as the time it was activated, yielding 3D event reconstruction with
  precise charge information.

An open source readout was developed by the University of Bonn and is
available under cite:tpx3-daq [fn:detector_tpx3_daq]. A gaseous
detector based on this is currently in the prototyping phase, see the
upcoming cite:schiffer_phd.

[fn:detector_tpx3_daq] [[https://github.com/SiLab-Bonn/tpx3-daq]] 

**** TODOs for this section                                     :noexport:

Introduce as something for which readout etc. is currently in
development. Mention improvements so that we can refer back to them in
our conclusion that having time information would be great.

Further out, the Timepix4 is also already finalized. No work on a
readout for these applications has been started yet. 

** GridPix

First experiments of combining a Micromegas with a Timepix readout
were done in 2005 cite:campbell2005detection by using classical
approaches to place a micromesh on top of the Timepix, at the time
still called /TimePixGrid/. While this worked in principle, it showed
a Moiré pattern, due to slight misalignment between the holes of the
micromesh and the pixels of the Timepix. Shortly after, an approach
based on photolithographic post-processing was developed to perfectly
align the Timepix pixels each with a hole of a micromesh
cite:CHEFDEVILLE2006490, called the /InGrid/ (integrated grid). The
commonly used name for a gaseous detector using an InGrid is
GridPix. For an overview of the process to produce InGrids nowadays
to, see cite:lucianMsc. 

The InGrid consists of a \SI{1}{μm} thick aluminum grid, resting on
small pillars \SI{50}{μm} above the Timepix. A silicon-nitride
\ce{Si_x N_y} layer protects the Timepix from direct exposure to the
amplification processes. The main advantage over previous Micromegas
technologies of the GridPix is its ability to detect single electrons.
As long as the diffusion distance is long enough to avoid multiple
electrons entering a single hole of the InGrid, each primary electron
produced during the initial ionization event is recorded.

Fig. [[sref:fig:detector:ingrid_explanation]] shows an image of such an InGrid.

#+begin_src subfigure
(figure ()
        (subfigure (linewidth 0.44) (caption "Timepix ASIC") (label "fig:detector:timepix_asic")
                   (includegraphics (list (cons 'width (linewidth 0.95)))
                                    "~/phd/Figs/timepix_gold.png"))
        (subfigure (linewidth 0.56) (caption "InGrid") (label "fig:detector:ingrid_explanation")
                   (includegraphics (list (cons 'width (linewidth 0.95)))
                                    "~/phd/Figs/ingrid_principle.pdf"))
        (caption
         (subref "fig:detector:timepix_asic") " Picture of a bare Timepix ASIC."
         (subref "fig:detector:ingrid_explanation")
         "Image of an InGrid, which was partially cut for inspection under an
          electron microscope. The pillars seen support the micromesh and
          have a height of " ($ (SI 50 "μm")) ". Each hole is perfectly aligned with
          a pixel of the Timepix below. Typical voltages applied between
          the grid and the Timepix are shown")
        (label "fig:detector:timepix_and_ingrid"))
#+end_src

*** TODOs for this section                                       :noexport:

- [ ] Talk about potential caveats?
  Caveats
  E.g. things like charge up effects etc. discussed in other theses.

- [X] Production nowadays in Berlin IZM. Show sketch of production process?
  Imo should be enough to refer to Lucian's thesis for production
  process. Is there a paper about IZM process? Ask Yevgen & Lucian.
  -> Just referencing to Lucian MSc

\SI{50}{\micro\meter} pillars (amplification gap). Typical gas gains
of 2000-5000. 

Polya plot.

Important: single electron detection efficiency. 

** 2014 / 2015 GridPix detector for CAST [/]
:PROPERTIES:
:CUSTOM_ID: sec:detector:detector_2014_15
:END:

In the course of cite:krieger2018search a first GridPix based
detector for usage at an axion helioscope, CAST, was developed. While
the main result was on the coupling constant of the chameleon
particle, an axion-electron coupling result was computed in
cite:SchmidtMaster.

The detector consists of a single GridPix in a \SI{78}{mm} diameter
gas volume and a drift distance of \SI{3}{cm}. The detector has a
\SI{2}{μm} thick Mylar entrance window for X-rays. This detector
serves as the foundation the detector used in the course of this
thesis was built on. See fig. [[fig:detector:exploded_schematic]] for an
exploded schematic of the detector. Further,
fig. [[fig:detector:background_rate_2014]] shows the achieved background
rate of this detector in the center $\num{5} \times \SI{5}{mm^2}$
region of the detector. The background rate shows the copper $Kα$ line
near $\SI{8}{keV}$, possibly overlaid with a muon contribution as well
as the expected argon $Kα$ lines at $\SI{3}{keV}$. Below $\SI{2}{keV}$
the background rises the lower the energy becomes, likely due
to background- and signal-like events being less geometrically
different at low energies (fewer pixels). The average background rate
in the range from $\SIrange{0}{8}{keV}$ is $\SI{2.8793e-05}{keV^{-1}.cm^{-2}.s^{-1}}$.

#+CAPTION: Exploded view of the GridPix detector used during the 2014/15 data taking campaign
#+CAPTION: at CAST. Consists of a \SI{3}{cm} drift volume with a \SI{78}{mm} inner diameter
#+CAPTION: and a single GridPix at the center.
#+NAME: fig:detector:exploded_schematic
[[~/org/Figs/ingrid_detector_exploded_krieger_thesis.png]]

#+CAPTION: Background rate in the center $\num{5} \times \SI{5}{mm^2}$ for the GridPix used in
#+CAPTION: 2014/15 at CAST. It corresponds to a background rate of $\SI{2.8793e-05}{keV^{-1}.cm^{-2}.s^{-1}}$
#+CAPTION: in the range from $\SIrange{0}{8}{keV}$. *INSERT INTERACTIVE VEGA-LITE VERSION*
#+NAME: fig:detector:background_rate_2014
[[~/phd/Figs/background_rate_2014_gold.pdf]]

**** TODOs for this section                                     :noexport:

- [X] *WHICH COPPER LINE? AND WHICH ARGON?*
- [X] *ADD AVERAGE BACKGROUND RATE IN TEXT*.  
As an example and a reference shown here. The foundation of what is
done in this thesis.

Not sure if this section is the right place. But: Could add background
rate achieved by that detector here?
*YES*
- background rate
- background over chip (latter comes later??)

*** Create background rate plot for 2014 data                    :noexport:

We simply generate the code with our background rate plotting script,
as the 2014/15 dataset background rate is stored in our resources of
the TPA repository. It also outputs the integrated background rates.
#+begin_src sh :dir ~/CastData/ExternCode/TimepixAnalysis/Plotting/plotBackgroundRate :results drawer
plotBackgroundRate --show2014 --energyMax 10.0 \
  --title "Background rate in center 5·5 mm² for GridPix 2014/15 CAST data" \
  --useTeX \
  --outpath ~/phd/Figs/ \
  --outfile background_rate_2014_gold.pdf
#+end_src

#+RESULTS:
:results:
[INFO]:Dataset: 2014/15
[INFO]:	 Integrated background rate in range: 0.0 .. 12.0: 3.0372e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 12.0: 2.5310e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: 2014/15
[INFO]:	 Integrated background rate in range: 0.5 .. 2.5: 8.4056e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.5 .. 2.5: 4.2028e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: 2014/15
[INFO]:	 Integrated background rate in range: 0.5 .. 5.0: 1.4269e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.5 .. 5.0: 3.1708e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: 2014/15
[INFO]:	 Integrated background rate in range: 0.0 .. 2.5: 1.2016e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 2.5: 4.8065e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: 2014/15
[INFO]:	 Integrated background rate in range: 4.0 .. 8.0: 5.7818e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 4.0 .. 8.0: 1.4454e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: 2014/15
[INFO]:	 Integrated background rate in range: 0.0 .. 8.0: 2.3034e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 8.0: 2.8793e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: 2014/15
[INFO]:	 Integrated background rate in range: 2.0 .. 8.0: 1.1610e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 2.0 .. 8.0: 1.9350e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:DataFrame with 5 columns and 50 rows:
     Idx             Rate             yMax             yMin           Energy          Dataset
  dtype:            float            float            float            float         constant
       0          11.8422         13.12774         10.67764              0.2          2014/15
       1           7.5465          8.59779         6.617637              0.4          2014/15
       2           9.1719          10.3181          8.14745              0.6          2014/15
       3           7.7787          8.84413         6.835589              0.8          2014/15
       4           8.3592          9.45909         7.381376                1          2014/15
       5           3.2508         3.984673         2.643112              1.2          2014/15
       6            4.644         5.495964         3.916433              1.4          2014/15
       7           4.7601         5.621061         4.023424              1.6          2014/15
       8           4.4118         5.245432         3.702803              1.8          2014/15
       9           2.5542         3.219597         2.016388                2          2014/15
      10           1.0449         1.519657         0.704645              2.2          2014/15
      11           1.2771         1.787224         0.899653              2.4          2014/15
      12           2.7864         3.475531         2.224329              2.6          2014/15
      13           6.1533         7.114793         5.315002              2.8          2014/15
      14           4.7601         5.621061         4.023424                3          2014/15
      15           5.1084         5.995723         4.345048              3.2          2014/15
      16           3.7152         4.490788         3.065089              3.4          2014/15
      17           1.2771         1.787224         0.899653              3.6          2014/15
      18            1.161         1.653855         0.801667              3.8          2014/15
      19            1.161         1.653855         0.801667                4          2014/15
      20           0.1161         0.381798        0.0202424              4.2          2014/15
      21           1.5093         2.051874         1.098013              4.4          2014/15
      22           0.3483         0.685427         0.159398              4.6          2014/15
      23            1.161         1.653855         0.801667              4.8          2014/15
      24           1.5093         2.051874         1.098013                5          2014/15
      25           0.9288         1.384499          0.60876              5.2          2014/15
      26            1.161         1.653855         0.801667              5.4          2014/15
      27           2.0898         2.704354         1.604131              5.6          2014/15
      28            1.161         1.653855         0.801667              5.8          2014/15
      29           1.0449         1.519657         0.704645                6          2014/15
      30           1.6254         2.183307         1.198198              6.2          2014/15
      31           1.2771         1.787224         0.899653              6.4          2014/15
      32           0.8127          1.24821         0.514242              6.6          2014/15
      33           0.9288         1.384499          0.60876              6.8          2014/15
      34           0.6966         1.110561         0.421413                7          2014/15
      35           0.8127          1.24821         0.514242              7.2          2014/15
      36           2.4381         3.091237         1.912838              7.4          2014/15
      37           2.6703         3.347689         2.120225              7.6          2014/15
      38           3.2508         3.984673         2.643112              7.8          2014/15
      39           5.5728         6.493931         4.775268                8          2014/15
      40           6.2694         7.238737         5.423184              8.2          2014/15
      41           6.3855         7.362609          5.53144              8.4          2014/15
      42           6.3855         7.362609          5.53144              8.6          2014/15
      43           5.4567         6.369513         4.667574              8.8          2014/15
      44            3.483         4.238071         2.853743                9          2014/15
      45           2.5542         3.219597         2.016388              9.2          2014/15
      46           1.5093         2.051874         1.098013              9.4          2014/15
      47           1.0449         1.519657         0.704645              9.6          2014/15
      48           0.8127          1.24821         0.514242              9.8          2014/15
      49                0                0                0               10          2014/15

[INFO]:INFO: storing plot in /home/basti/phd/Figs/background_rate_2014_gold.pdf
[INFO] TeXDaemon ready for input.
shellCmd: command -v xelatex
shellCmd: xelatex -output-directory /home/basti/phd/Figs /home/basti/phd/Figs/background_rate_2014_gold.tex
Generated: /home/basti/phd/Figs/background_rate_2014_gold.pdf
:end:

** Septemboard detector overview 
:PROPERTIES:
:CUSTOM_ID: sec:detector:detector_overview
:END:


Generally, the detector follows the same design as the old detector
shown in sec. [[#sec:detector:detector_2014_15]], mainly so that mounting
it inside of the lead shielding and to the vacuum pipes at CAST is
possible without significant changes. An exploded view of the full
detector can be seen in fig. [[fig:detector:full_septemboard_exploded]].

At the center of the new detector is the 'septemboard', 7 GridPixes
replace the single GridPix on the carrier board,
sec. [[#sec:detector:septemboard]]. Analogue signals induced by the
amplified charges on the center GridPix are now read out using a flash
ADC (FADC), sec. [[#sec:detector:fadc]]. The housing with an inner
diameter of $\SI{78}{mm}$ is again made of acrylic glass, same as in
the old detector. The detector entrance window is replaced by a
$\SI{300}{nm}$ $\ce{Si_3 N_4}$ window (sec. [[#sec:detector:sin_window]]),
which also acts as part of the detector cathode. The copper anode
slots in right above the septemboard. The carrier board sits on the
intermediate board. Below the intermediate board is a bespoke water
cooling made of oxygen-free copper to cool the heat emitted by the
additional 6 GridPixes, sec. [[#sec:detector:water_cooling]]. On the
underside of the intermediate board is a new small silicone
photomultiplier (SiPM), sec. [[#sec:detector:scintillators]]. Finally, a
large veto scintillator is installed above the detector setup at CAST,
also sec. [[#sec:detector:scintillators]].

During developments multiple septemboards were built and tested. The
septemboard used in the final detector is septemboard 'H'. The GridPixes of
the final board are listed in tab. [[tab:detector:septem_h_chips]].

#+CAPTION: Overview of the different chips on septemboard H. The first part of the name corresponds
#+CAPTION: to the position on the wafer and =W69= is Timepix wafer number \num{69}.
#+NAME: tab:detector:septem_h_chips
#+ATTR_LATEX: :booktabs t
|---------+--------|
| Chip    | Number |
|---------+--------|
| E 6 W69 |      0 |
| K 6 W69 |      1 |
| H 9 W69 |      2 |
| H10 W69 |      3 |
| G10 W69 |      4 |
| D 9 W69 |      5 |
| L 8 W69 |      6 |
|---------+--------|


#+CAPTION: Exploded view of the main GridPix septemboard detector. The FADC and
#+CAPTION: large veto scintillator paddle are not shown for obvious reasons. At the
#+CAPTION: center of the detector is the 'septemboard', 7 GridPix on a carrier board.
#+CAPTION: The housing is made of acrylic glass, same as in the old detector. The
#+CAPTION: top shows the \SI{300}{nm} \ce{Si_3 N_4} window. Below the intermediate board
#+CAPTION: is the water cooling made of pure copper. At the bottom, the SiPM veto
#+CAPTION: scintillator can be seen.
#+NAME: fig:detector:full_septemboard_exploded
#+ATTR_LATEX: :height 0.5\textheight
[[~/phd/Figs/detector/detector-mk4c-assembly-exploded-whitebg-no-cables-cropped.jpg]]

*** TODOs for this section [/]                                   :noexport:

- [ ] *TALK ABOUT SIZES OF FULL DETECTOR*

- [ ] *SEPTEMBOARD CAN BE NAMED HERE INSTEAD OF ABOVE POSSIBLY*
- [ ] *REARRANGE THE TABLE TO ALIGN LEFT COLUMN*  

*AS LAST SECTION?*

- [ ] *REPLACE IMAGE BY A PROPER RAYTRACED RENDER*
  
** Detector readout system (Virtex, ...) [0/1]

The detector is operated by a Xilinx Virtex-6 \textbf{F}ield
\textbf{P}rogrammable \textbf{G}ate \textbf{A}rray (FPGA) in the form
of a Virtex-6 ML605 evaluation board. [fn:ml605_weblink] It is
connected to the intermediate board via two
\textbf{H}igh-\textbf{D}efinition \textbf{M}ultimedia
\textbf{I}nterface (HDMI) cables. The Virtex-6 contains the firmware
controlling the Timepix ASICs and correlating the scintillator and
FADC signals (see sec. [[#sec:daq:tof]]), the Timepix Operating Firmware
(TOF). The high voltage (HV) supply both for the septemboard as well
as for the scintillators sit inside a VME crate, which also houses the
FADC. A USB connection is used to read out and control the FADC and HV
supply via the computer running the data acquisition and control
software (see sec. [[#sec:daq:tos]]), the Timepix Operating Software
(TOS). A schematic of this setup is shown in
fig. [[fig:detector:flowchart_setup]], which leaves out the SiPM and
temperature readout.

#+CAPTION: Flowchart of the whole detector and readout system
#+NAME: fig:detector:flowchart_setup
[[file:~/org/Doc/Detector/figs/2016_detector_setup_schematic.pdf]]

[fn:ml605_weblink]
https://www.xilinx.com/products/boards-and-kits/ek-v6-ml605-g.html
(visited 2022/10/17)

*** TODOs for this section                                       :noexport:

Note however, that in this plot the SiPM is not illustrated, as it is
connected to the bottom of the intermediate board and only provides an
offline flag to be used in the analysis.

- [ ] *CLARIFY SIPM. PROBABLY ADD TO PLOT*


** Scintillator vetoes [0/0]
:PROPERTIES:
:CUSTOM_ID: sec:detector:scintillators
:END:

The first general improvement is the addition of two scintillators for
veto purposes. While both have slightly different goals, each is there
to help with the removal of muon signals or muon induced events (for
example X-ray fluorescence) in the detector. Given that cosmic muons
(ref. section [[#sec:theory:cosmic_radiation]]) dominate the background by
flux, statistically there is a high chance of muons creating X-ray
like signatures in the detector (more on that below *LIKELY REMOVE*). By tagging muons
before they interact near the detector, these can be correlated with
events seen on the GridPix and thus possibly be vetoed if precise time
information is available.

The first scintillator is a large 'paddle' installed above the
detector installation, aiming to tag a large fraction of cosmic muons
traversing in the area around the detector. It has a Canberra 2007
base and the photomultiplier tube (PMT) is a Bicron
Corp. 31.49x15.74M2BC408/2-X (first two numbers: dimensions in
inches). The full outer dimensions of the scintillator paddle are
closer to $\SI{42}{cm} \times \SI{82}{cm}$. It is the same
scintillator paddle used during the Micromegas data taking behind the
LLNL telescope prior and after the data taking campaign with the
detector described in this thesis.

For this scintillator, muons which traverse through this scintillator
and the gaseous detector volume are not the main use case. They can be
easily identified by the geometric properties of the induced tracks
(their zenith angles are relatively small, resulting in track like
signatures as the GridPix readout is orthogonal to the zenith
angle). There is a small chance however that a muon can ionize an atom
of the detector material, which may emit an X-ray upon
recombination. One particular source of background can be attributed
to the presence of copper whose $Kα$ lines are at
$\sim\SI{8.04}{\keV}$ as well as fluorescence of the argon gas with
its $Kα$ lines at $\sim\SI{2.95}{keV}$ (see. table
[[tab:theory:xray_fluorescence]] in sec. [[#sec:theory:xray_fluorescence]]).

Fig. sref:fig:detector:fadc_veto_paddle_expl shows a schematic of a
side view of the detector chamber with the scintillator paddle on top.
When a muon traverses the scintillator a counter $t_{\text{Veto}}$
starts on the FPGA. Two different cases are shown. In the extreme, a
muon may traverse close to the cathode or close to the anode / readout
plane. This changes the time of the total drift time and therefore the
time difference between the trigger time $t_{\text{Veto}}$ and the
readout time, which in the readout is precisely the value of the
counter on the FPGA. The drift velocity of $\sim\SI{2}{cm.μs⁻¹}$ and
height of the detector chamber ($\SI{3}{cm}$) therefore allow to set
an upper limit on the maximum time between a veto paddle trigger and
the GridPix readout of about $\SI{1.5}{μs}$. At a clock speed of
$\SI{40}{MHz}$ this corresponds to $\SI{60}{clock;cycles}$, a number
we will later try to see in the data. As the location at which the
muon traverses through the detector is random and homogeneous
throughout the detection volume, we expect to see a flat distribution
up to the maximum possible time and then a sharp drop (equivalent to
muons at the cathode).

The second scintillator is a small silicon photomultiplier (SiPM)
installed on the underside of the PCB on which the septemboard is
installed. This scintillator was calibrated and set up as part of
cite:JannesBSc. We are interested in tagging precisely those muons,
which enter the detector orthogonally to the readout plane. This
implies zenith angles of almost $\SI{90}{°}$ such that the elongation
in the transverse direction of the muon track is small enough to
result in a small eccentricity. From the Bethe equation the mean
energy loss of muons in the used gas mixture is about $\SI{8}{\keV}$
along the $\SI{3}{\cm}$ of drift volume in the detector (see
fig. [[fig:theory:muon_argon_3cm_bethe_loss]] for the energy loss). This
coincides with the copper Kα lines and should lead to another source
of background in this energy range. Although the muon background will
have a much wider distribution than the copper lines which is
dominated by the energy resolution of the detector. In a similar
manner to the veto paddle we can make an estimate on the typical time
scales associated from the time of the scintillator trigger to the
GridPix detection. A muon that traverses orthogonally trough the
detector can be taken to leave an instant ionization track and trigger
the SiPM at the same time ($\mathcal{O}(\SI{100}{ps}) \ll \SI{25}{ns}$
for one clock cycle). As such, the relevant time scale is the drift
time until enough charge has drifted towards the grid as to pass the
activation threshold.

Fig. sref:fig:detector:fadc_sipm_expl ionization of a muon is a
statistical process. Depending on the density of the charge cloud for
muons orthogonal to readout plane, time to accumulate enough charge to
trigger FADC differs. With an average energy deposition of a muon in
argon gas of $\sim\SI{2.67}{keV.cm^{-1}}$ and drift velocity again of
$\SI{2}{cm.μs⁻¹}$ the accumulation time can be estimated. For example
assuming an FADC activation threshold of $\SI{1.5}{keV}$ the necessary
charge is accumulated on $\SI{0.56}{cm}$, which takes about
$\SI{280}{ns} \approx \SI{11}{clock.cycles}$ to accumulate. As the
ionization is a statistical process, different tracks will have
deposited different amounts of energy. Therefore we expect a peak at
relatively low clock cycles with a tail up to the same
$\SI{60}{clock.cycles}$ (in case the full $\SI{3}{cm}$ track needs to
be accumulated to activate the FADC).

#+begin_src subfigure
(figure ()
        (subfigure (linewidth 0.5) (caption "Veto paddle") (label "fig:detector:fadc_veto_paddle_expl")
                   (includegraphics (list (cons 'width (linewidth 0.95)))
                                    "~/phd/Figs/detector/fadc_scintillators/FADC_veto_explanation.pdf"))
        (subfigure (linewidth 0.5) (caption "SiPM") (label "fig:detector:fadc_sipm_expl")
                   (includegraphics (list (cons 'width (linewidth 0.95)))
                                    "~/phd/Figs/detector/fadc_scintillators/FADC_sipm_explanation.pdf"))
        (caption
         (subref "fig:detector:fadc_veto_paddle_expl")
         "Schematic of expected signals for different muons from the zenith passing through scintillator
          paddle and detector. " ($ "t_{\text{Veto}}") " marks beginning of a counter. Where the muon
          traverses changes drift time and thus time difference between two times. "
          (subref "fig:detector:fadc_sipm_expl")
          "Ionization of a muon is a statistical process. Depending on the density of the charge cloud
           for muons orthogonal to readout plane, time to accumulate enough charge to trigger FADC differs.")
        (label "fig:detector:fadc_scintillators_explanation"))
#+end_src


*** TODOs for this section [9/14]                                :noexport:

- [ ] *MOVE EXACT NUMBERS TO ANALYSIS PART OF BACKGROUND W SCINTIS?*
  -> Refers to the numbers of the drift velocity, 
  -> Maybe. Decide that once reviewed those sections!

- [ ] *REFERENCE SECTION ON MUON LOSSES*
  -> Partially done. Referencing the bethe bloch & most probable loss plots.

- [X] *ADD NOEXPORT OF CALCULATIONS OF MUON ANGLES POSSIBLE*

- [X] *EXPLAIN FIG* for schematic

- [X] *PROVIDE EXPECTED TIMES FOR EACH OF THESE TWO CASES*
  -> Done.

- [X] *REPHRASE*
  -> Positive HV is very specific here!

- [X] *REFERENCE JANNES BSC THESIS FOR SIPM*

- [ ] *GIVE NAME OR WHATEVER OF SIPM*
- [X] *GIVE NAME OR WHATEVER OF BIG PADDLE*  

- [X] *CONSIDER REPHRASING SENTENCE ABOUT BETHE EQ GIVING US 8 KEV*

- [X] *SCHEMATIC OF MUON IONIZATION*

- [X] *NOTE: MAYBE INSTEAD START WITH SEPTEMBOARD? THEN IN OTHER FEATURES
CAN MENTION THAT THINGS ARE ONLY FOR CENTER CHIP EG*
  -> We should maybe start with the exploded view of the Septemboard,
  because that allows us to introduce the name 'septemboard' and makes
  explaining things much easier. 

- [X] *SHOW THE EXPLANATION PLOTS FOR WHAT HAPPENS WITH MUONS HERE?*

*** Discussion of orthogonal muons on the FADC                   :extended:

While writing the thesis at some point I had the following thoughs:
#+begin_quote
- [ ] *QUESTION*: This is *VERY IMPORTANT* for our interpretation.
  Given the 'slow' drift velocity of about 2cm/μs, this means
  orthogonal muons of course take about 1.5μs to traverse the
  detector. The FADC trigger window is 2560 ns = 2.56 μs.
  So: Does the FADC even *TRIGGER* if the charge accumulates that
  slowly??? *UHHHHHH*
  I mean from this we expect to have signals that are extremely long,
  no? 1.5/2.5 = 0.58 of the whole trigger window! In *RISING EDGE* Or
  rather a sort of flat thing, as the charge is removed 'quickly' on
  those time scales. Very confusing thought....
  I mean there is a chance the FADC *DID NOT* trigger in those events,
  which could explain why the FADC + SIPM aren't that effective!
  -> We *could* study this a bit, if we look into the FADC dataset in
  which we placed the detector towards the zenith. Question: what does
  the data look like, in which the *FADC DID NOT TRIGGER*? If there is
  significant contribution of spherical events of ~8 keV then DUHHH.
#+end_quote

To investigate this we can use ~plotData~ from ~TimepixAnalysis~ to
make a bunch of plots comparing properties like the energy of clusters
with and without FADC. Both for the raw data as well as for the
results of the ~likelihood~ application (i.e. after all cuts).

We will use the entire Run-3 dataset for both cases, because the
scintillators were fully working in those.

First the plots of the raw data without FADC:
#+begin_src sh :dir ~/org/Figs/statusAndProgress/FADC/orthogonalMuonsOnFadc/ :results drawer
plotData \
    --h5file ~/CastData/data/DataRuns2018_Reco.h5 \
    --runType=rtBackground \
    --config ~/CastData/ExternCode/TimepixAnalysis/Plotting/karaPlot/config.toml \
    --ingrid --fadc \
    --cuts '("../fadcReadout", -0.1, 0.5)' \
    --applyAllCuts \
    --chips 3 \
    --region crGold
#+end_src

#+RESULTS:
:results:
figs/DataRuns2018_Reco_2023-10-03_22-41-58
:end:

where we only plot chip 3 in the center 5·5 cm² and apply the cut to
the ~fadcReadout~ flag (such that we avoid any floating point
issues). ~fadcReadout == 0~ means no FADC and ~1~ means FADC.

Let's wrap them all up in one PDF:
#+begin_src sh :dir ~/org/Figs/statusAndProgress/FADC/orthogonalMuonsOnFadc/ :results drawer
dir=figs/DataRuns2018_Reco_2023-10-03_22-41-58
pdfunite $dir/*.pdf run3_no_fadc_histograms.pdf
#+end_src

#+RESULTS:
:results:
:end:

[[~/org/Figs/statusAndProgress/FADC/orthogonalMuonsOnFadc/run3_no_fadc_histograms.pdf]]

Now the same _with_ the FADC. Note for this plot change the rise time
in ~config.toml~ to 2500 upper and 500 bins!
#+begin_src sh :dir ~/org/Figs/statusAndProgress/FADC/orthogonalMuonsOnFadc/ :results drawer 
plotData \
    --h5file ~/CastData/data/DataRuns2018_Reco.h5 \
    --runType=rtBackground \
    --config ~/CastData/ExternCode/TimepixAnalysis/Plotting/karaPlot/config.toml \
    --ingrid --fadc \
    --cuts '("../fadcReadout", 0.5, 1.1)' \
    --applyAllCuts \
    --chips 3 \
    --region crGold \
    --quiet
#+end_src

#+RESULTS:
:results:
figs/DataRuns2018_Reco_2023-10-03_23-45-37
:end:

Let's wrap them all up in one PDF:
#+begin_src sh :dir ~/org/Figs/statusAndProgress/FADC/orthogonalMuonsOnFadc/ :results drawer
dir=figs/DataRuns2018_Reco_2023-10-03_23-45-37
pdfunite $dir/*.pdf run3_with_fadc_histograms.pdf
#+end_src

#+RESULTS:
:results:
:end:

[[~/org/Figs/statusAndProgress/FADC/orthogonalMuonsOnFadc/run3_with_fadc_histograms.pdf]]


And now for the result of the data with all cuts applied:
#+begin_src sh :dir ~/org/Figs/statusAndProgress/FADC/orthogonalMuonsOnFadc/ :results drawer
plotData \
    --h5file ~/org/resources/lhood_lnL_04_07_23/lhood_c18_R3_crAll_sEff_0.8_lnL_scinti_fadc_septem_line_vQ_0.99_default_cluster.h5 \
    --runType=rtBackground \
    --config ~/CastData/ExternCode/TimepixAnalysis/Plotting/karaPlot/config.toml \
    --ingrid --fadc \
    --cuts '("../fadcReadout", -0.1, 0.5)' \
    --applyAllCuts \
    --chips 3 \
    --region crGold
#+end_src

#+RESULTS:
:results:
figs/lhood_c18_R3_crAll_sEff_0.8_lnL_scinti_fadc_septem_line_vQ_0.99_default_cluster_2023-10-03_22-46-10
:end:
#+begin_src sh :dir ~/org/Figs/statusAndProgress/FADC/orthogonalMuonsOnFadc/ :results drawer
dir=figs/lhood_c18_R3_crAll_sEff_0.8_lnL_scinti_fadc_septem_line_vQ_0.99_default_cluster_2023-10-03_22-46-10
pdfunite $dir/*.pdf run3_lhood_no_fadc_histograms.pdf
#+end_src

#+RESULTS:
:results:
:end:

[[~/org/Figs/statusAndProgress/FADC/orthogonalMuonsOnFadc/run3_lhood_no_fadc_histograms.pdf]]

#+begin_src sh :dir ~/org/Figs/statusAndProgress/FADC/orthogonalMuonsOnFadc/ :results drawer
plotData \
    --h5file ~/org/resources/lhood_lnL_04_07_23/lhood_c18_R3_crAll_sEff_0.8_lnL_scinti_fadc_septem_line_vQ_0.99_default_cluster.h5 \
    --runType=rtBackground \
    --config ~/CastData/ExternCode/TimepixAnalysis/Plotting/karaPlot/config.toml \
    --ingrid --fadc \
    --cuts '("../fadcReadout", 0.5, 1.1)' \
    --applyAllCuts \
    --chips 3 \
    --region crGold
#+end_src

#+RESULTS:
:results:
figs/lhood_c18_R3_crAll_sEff_0.8_lnL_scinti_fadc_septem_line_vQ_0.99_default_cluster_2023-10-03_22-46-44
:end:

#+begin_src sh :dir ~/org/Figs/statusAndProgress/FADC/orthogonalMuonsOnFadc/ :results drawer
dir=figs/lhood_c18_R3_crAll_sEff_0.8_lnL_scinti_fadc_septem_line_vQ_0.99_default_cluster_2023-10-03_22-46-44
pdfunite $dir/*.pdf run3_lhood_with_fadc_histograms.pdf
#+end_src

#+RESULTS:
:results:
:end:

[[~/org/Figs/statusAndProgress/FADC/orthogonalMuonsOnFadc/run3_lhood_with_fadc_histograms.pdf]]


To summarize the results:
- [[~/org/Figs/statusAndProgress/FADC/orthogonalMuonsOnFadc/run3_no_fadc_histograms.pdf]]
  -> This file shows that there are barely any events near the
  $\SI{8}{keV}$ range for events without an FADC trigger. This already
  seems to indicate that orthogonal muons _should_ be triggering the
  FADC.
- [[~/org/Figs/statusAndProgress/FADC/orthogonalMuonsOnFadc/run3_with_fadc_histograms.pdf]]
  -> Shows much higher statistics in the same range. Rise time shows a
  very long tail, but no visible peaks at higher values.
- [[~/org/Figs/statusAndProgress/FADC/orthogonalMuonsOnFadc/run3_lhood_no_fadc_histograms.pdf]]
  -> There are literally *no* events in the $\SI{8}{keV}$ range after
  the likelihood cuts without an FADC trigger!
- [[~/org/Figs/statusAndProgress/FADC/orthogonalMuonsOnFadc/run3_lhood_with_fadc_histograms.pdf]]
  -> There are plenty of events in the $\SI{8}{keV}$ range after
  the likelihood cuts *with* the FADC. And all their rise times are
  between 45 and 70 clock cycles! This means either there are no
  orthogonal muons in this dataset or for some reason their rise time
  is also exceptionally short, which seems surprising.

At this point we could investigate further and see what the
correlation of rise times actually looks like more generally, but
well. At least for _all_ data the rise time looks unsuspicious. It's
just a long exponential like tail. 

*** Maximum allowed angle before being vetoed                    :extended:

The section below explains the reasoning behind why an angle of
$\SI{88}{°}$ was chosen when computing the muon flux at CAST under
shallow angles in sec. [[#sec:theory:calc_muon_angular_flux]] and why
this is the smallest angle at which a muon is likely going to pass the
normal likelihood cut and thus requires the SiPM.

The reason ϑ = 88° was chosen is due to the restriction on the maximum
allowed eccentricity for a cluster to still end up as a possible
cluster in our 8-10 keV hump. See the =eccentricity= subplot in
fig. [[8_10_keV_properties]].

#+begin_center
#+CAPTION: See the eccentricity subplot for an upper limit on the allowed eccentricity
#+CAPTION: for events in the 8-10 keV hump. Values should not be above ε = 1.3.
#+NAME: 8_10_keV_properties
[[~/org/Figs/statusAndProgress/muonStudies/lhood_facet_remaining_8_10_keV.pdf]]
#+end_center

From this we can deduce the eccentricity should be smaller than ε =
1.3. What does this imply for the largest possible angles allowed in
our detector? And how does the opening of the "lead pipe window"
correspond to this?

Let's compute by modeling a muon track as a cylinder. Reading off the
mean =width= from the above fig. to w = 5 mm and taking into account
the detector height of 3 cm we can compute the relation between
different angles and corresponding eccentricities.

In addition we will compute the largest possible angle a muon (from
the front of the detector of course) can enter, under which it does
not see the lead shielding.

#+begin_src nim :flags -d:QuietTikZ=true :results drawer :tangle ~/phd/code/max_muon_angle_est.nim
import unchained, ggplotnim, strformat, sequtils
let w = 5.mm # mean width of a track in 8-10keV hump
let h = 3.cm # detector height
proc computeLength(α: UnitLess): mm =  ## todo: add degrees?
  ## α: float # Incidence angle
  var w_prime = w / cos(α)       # projected width taking incidence
                                 # angle into account
  let L_prime = tan(α) * h       # projected `'length'` of track
                                 # from center to center
  let L_full = L_prime + w_prime # full `'length'` is bottom to top, thus
                                 # + w_prime
  result = L_full.to(mm)
proc computeEccentricity(L_full, w: mm, α: UnitLess): UnitLess =
  let w_prime = w / cos(α)
  result = L_full / w_prime

let αs = linspace(0.0, degToRad(25.0), 1000)
let εs = αs.mapIt(it.computeLength.computeEccentricity(w, it).float)
let αsDeg = αs.mapIt(it.radToDeg)
let df = toDf(αsDeg, εs)

# maximum eccentricity for text annotation
let max_εs = max(εs)
let max_αs = max(αsDeg)

# compute the maximum angle under which `no` lead is seen
let d_open = 28.cm # assume 28 cm from readout to end of lead shielding
let h_open = 5.cm # assume open height is 10 cm, so 5 cm from center
let α_limit = arctan(h_open / d_open).radToDeg

# data for the limit of 8-10 keV eccentricity
let ε_max_hump = 1.3 # 1.2 is more reasonable, but 1.3 is the
                     # absolute upper limit
echo df.head(1)
echo α_limit
echo ε_max_hump
echo max_εs
echo max_αs
ggplot(df, aes("αsDeg", "εs")) +
  geom_line() +
  geom_linerange(aes = aes(x = α_limit, yMin = 1.0, yMax = max_εs),
                 color = color(1.0, 0.0, 1.0)) +
  geom_linerange(aes = aes(y = ε_max_hump, xMin = 0, xMax = max_αs),
                 color = color(0.0, 1.0, 1.0)) +
  geom_text(aes = aes(x = α_limit, y = max_εs + 0.1,
                      text = "Maximum angle no lead traversed")) +
  geom_text(aes = aes(x = 17.5, y = ε_max_hump + 0.1,
                      text = r"Largest $ε$ in $\SIrange{8}{10}{keV}$ hump")) +
  xlab(r"$α$: Incidence angle [°]") +
  ylab(r"$ε$: Eccentricity") +
  ylim(1.0, 4.0) +
  ggtitle(&"Expected eccentricity for tracks of mean width {w}") +
  ggsave("~/phd/Figs/muonStudies/exp_eccentricity_given_incidence_angle.pdf", useTeX = true, standalone = true,
        width = 600, height = 360)
#+end_src

#+RESULTS:
:results:
DataFrame with 2 columns and 1 rows:
     Idx       αsDeg          εs
  dtype:        float        float
       0            0            1

10.12467165539782
1.3
3.535709570444218
25.00000000000023
[INFO] TeXDaemon ready for input.
shellCmd: command -v xelatex
shellCmd: xelatex -output-directory /home/basti/phd/Figs/muonStudies /home/basti/phd/Figs/muonStudies/exp_eccentricity_given_incidence_angle.tex
Generated: /home/basti/phd/Figs/muonStudies/exp_eccentricity_given_incidence_angle.pdf
:end:


Resulting in fig. [[exp_eccentricity_given_incidence_angle]].

#+begin_center
#+CAPTION: Relationship between incidence angle of muons of a width of 5 mm and
#+CAPTION: their expected mean eccentricity. Drawn as well are the maximum angle
#+CAPTION: under which no lead is seen (from the front) as well as the larges ε
#+CAPTION: seen in the data.
#+NAME: exp_eccentricity_given_incidence_angle
[[~/phd/Figs/muonStudies/exp_eccentricity_given_incidence_angle.pdf]]
#+end_center

This leads to an upper bound of ~3° from the horizontal. Hence the
(somewhat arbitrary choice) of 88° for the ϑ angle above.



** FADC [0/4]
:PROPERTIES:
:CUSTOM_ID: sec:detector:fadc
:END:

As the Timepix is read out in a shutter based fashion and typical
shutter lengths for low rate experiments are long compared to the rate
of cosmic muons, the scintillators introduced in previous section
require an external trigger to close the Timepix shutter early if a
signal is measured on the Timepix. This is one the main purposes of
the \text{f}lash \textbf{a}nalog to \textbf{d}igital
\textbf{c}onverter (FADC) that is part of the detector. This is done
by decoupling the induced analogue signals from the grid of the center
GridPix.

The specific FADC used for the detector is an Caen V1792a. It runs at
an internal $\SI{50}{MHz}$ or $\SI{100}{MHz}$ clock and utilizes virtual
frequency multiplication to achieve sampling rates of $\SI{1}{GHz}$ or
$\SI{2}{GHz}$, respectively. It has 4 channels, each with a cyclic
register of $\num{2560}$ channels. At an operating clock frequency of
$\SI{1}{GHz}$ that means each channel covers the last
$\sim\SI{2.5}{\micro\second}$ at any time. cite:fadc_manual

The raw signal decoupled from the grid is first fed into an Ortec 142
B pre-amplifier and then feeds into an Ortec 474 shaping amplifier,
which integrates and shapes the signal as well as amplifies it. For a
detailed introduction to this FADC system, see the thesis of
A. Deisting cite:Deisting and cite:SchmidtMaster for further work
integrating it into this detector. In addition see the FADC manual
cite:fadc_manual [fn:fadc_manual] for a deep explanation of the
working principle of this FADC.

#+CAPTION: Schematic of the setup to decouple signals induced on the grid of the
#+CAPTION: InGrid. The signal is decoupled in the sense that the capacitor essentially
#+CAPTION: acts as a low pass filter, thus removing the constant HV. Only the
#+CAPTION: high frequency components of the induced signals on top of the HV pass
#+CAPTION: into the branch leading to the FADC. In the detector of this thesis, 
#+CAPTION: a capacitance of \SI{10}{nF} was used instead. The decoupling is implemented 
#+CAPTION: on the intermediate board. Schematic taken from cite:Deisting. 
#+NAME: fig:detector:fadc_circuit
[[~/phd/Figs/decouple_fadc.pdf]]

The analogue signal of the center grid is decoupled via a small
$C_{\text{dec}} = \SI{10}{nF}$ capacitor in parallel to the high
voltage line. For a schematic of the circuit see
fig. [[fig:detector:fadc_circuit]]. When a primary electron traverses
through a hole in the grid and is amplified, the back flowing ions
induce a small voltage spike on top of the constant high voltage
applied to the grid. The parallel capacitor filters out the constant
high voltage and only transmits the time varying induced signals. Such
signals - the envelope of possibly many primary electrons - are
measured by the FADC.

This signal can be used for two distinct things:
1. it may be used as a trigger to close the shutter of the ongoing
   event. Ideally, we want to only measure a single physical event
   within one shutter window. A long shutter time can statistically
   result in multiple events happening, which the FADC trigger helps
   to alleviate.
2. By nature of the signal production & drift properties of the
   primary electrons before they reach the grid, the signal shape can
   theoretically be used to determine a rough longitudinal shape of
   the event. The length of the FADC event should be proportional to
   the size of the primary electron cloud distribution along the
   'vertical' detector axis. 

The former allows us to reduce the number of events with multiple
physical events and acts as a trigger for the scintillators. This in
turn means possible muon induced X-ray fluorescence can be vetoed. The
latter potentially allows to differentiate between a muon traversing
orthogonally through the readout plane and an X-ray due to their
longitudinal shape difference.

The working principle of how the FADC and the scintillators can be
used together to remove certain kinds of background by correlating
events in the scintillators, the FADC and the GridPix is shown in
fig. [[fig:detector:scintillator_fadc_shutter_close]].

#+CAPTION: Schematic showing how the FADC and scintillators are used together
#+CAPTION: to tag possible coincidence events and close the shutter early to
#+CAPTION: reduce the likelihood of multi-hit events. If the scintillator
#+CAPTION: triggers when the shutter is open, a clock starts counting up to
#+CAPTION: 4096 clock cycles. On every new trigger this clock is reset. If
#+CAPTION: the FADC triggers, the scintillator clock values are read out and
#+CAPTION: can be used to correlate events in the scintillator with FADC and
#+CAPTION: GridPix information. Further, the FADC trigger is used to close the
#+CAPTION: Timepix shutter $\SI{50}{μs}$ after the trigger.
#+NAME: fig:detector:scintillator_fadc_shutter_close
[[~/phd/Figs/scintillator_fadc_shutter_close.pdf]]

[fn:fadc_manual] A PDF version is available at:
https://archive.org/details/manualzilla-id-5646050/

*** TODOs for this section                                       :noexport:

- [ ] *UPDATE SCHEMATIC TO SAY e.g. 1.5 μs*!


- [X] *HAVE PRE AMPLIFIER BEFORE FADC. ORTEC*

- [ ] *MERGE BOTH SCHEMATICS?*
  -> Hmm, maybe, but the captions are already pretty long of both of
  them.

- [ ] *SHOW IMAGE OF FADC AND SHAPING AMPLIFIER*
- [ ] *PICTURE OF FADC*
  -> A picture would just take more space and it's not particularly
  interesting.
  
- [ ] *COPY OVER SECTION ABOUT MUONS / TOA INFO FROM IAXO TDR TEXT?*
  -> ?

*** Update schematic                                             :noexport:

Need to type out μs because we don't use ~unicode-math~ by default in LaTeXDSL.
#+begin_src nim
import latexdsl
let body = r"If $Δt \lesssim \mathcal{O}(\SI{2}{\micro\second})$ events considered correlated, flag them."
compile("/tmp/text_fadc_scintillator.tex", body)
#+end_src

** SiN window [/]
:PROPERTIES:
:CUSTOM_ID: sec:detector:sin_window
:END:

Next up, a major limitation of the previous detector was its limited
combined efficiency below $\SI{2}{keV}$, due to its $\SI{2}{μm}$ Mylar
window. Therefore, the next improvement for the new detector is an
ultra-thin silicon nitride $\ce{Si_3 N_4}$ window of $\SI{300}{nm}$
thickness and $\SI{14}{mm}$ diameter, developed by Norcada™ [fn:norcada_website]. A
strongback support structure consisting of 4 lines of $\SI{200}{μm}$
thick and $\SI{500}{μm}$ wide $\ce{Si_3 N_4}$, helps to support a pressure
difference of up to $\SI{1.5}{bar}$. On the outer side a $\SI{20}{nm}$
thin layer of aluminum is coated to allow the window to be part of the
detector cathode. The strongback occludes about $\SI{17}{\%}$ of
the full window area. In reality it is slightly more, as the
strongbacks become somewhat wider towards the edges. In the center
most region they are straight and in the center $\num{5} \times
\SI{5}{mm²}$ area, they occlude $\SI{22.2}{\%}$.

Fig. sref:fig:detector:strongback_structure_mc shows the idealized strongback
structure without a widening towards the edges of the
window. Fig. sref:fig:detector:window_image shows an image of one such
window under testing conditions in the laboratory, as it withstands a
pressure difference of $\SI{1.5}{bar}$.

#+begin_src subfigure
(figure ()
        (subfigure (linewidth 0.5) (caption "Window strongback schematic") (label "fig:detector:strongback_structure_mc")
                   (includegraphics (list (cons 'width (linewidth 0.95)))
                                    "~/phd/Figs/SiN_window_occlusion.png"))
        (subfigure (linewidth 0.5) (caption "Image") (label "fig:detector:window_image")
                   (includegraphics (list (cons 'width (linewidth 0.95)))
                                    "~/phd/Figs/300nm_SiN_holds.jpg"))
        (caption
         (subref "fig:detector:strongback_structure_mc")
         "shows an idealized schematic of the window strongback based on a simple MC simulation. "
         ($ (SI 22.2 "\\percent")) " of the area inside the inner " ($ "\\num{5} \\times \\SI{5}{mm^2}")
         "area (black square) are occluded."
         (subref "fig:detector:window_image")
         "shows an image of one such window while testing in the laboratory, if it holds " ($ (SI 1.5 "bar")))
        (label "fig:detector:window_image_and_strongback"))
#+end_src

As the main purpose is the increase of transmission at low energies,
fig. [[fig:detector:window_efficiency_comparison]] shows the transmission
of the mylar window of the old detector and the new $\ce{Si_3 N_4}$
window in the energy range below $\SI{3}{keV}$. The $\ce{Si_3 N_4}$ window
shows a significant increase in transmission below $\SI{2}{keV}$, which
is very important for the sensitivity in solar axion-electron and
chameleon searches, which both peak near $\SI{1}{keV}$ in their solar
flux. The window alone significantly increases the signal to noise
ratio of these physics searches.

#+CAPTION: Comparison of the transmission of a $\SI{2}{μm}$ Mylar window and a
#+CAPTION: $\SI{300}{nm}$ $\ce{Si_3 N_4}$ window. The efficiency gains become
#+CAPTION: more and more pronounced the lower the energy is, aside from the
#+CAPTION: absorption edge of carbon at around $\SI{250}{eV}$ and above about
#+CAPTION: $\SI{1.75}{keV}$. In the interesting range around $\SI{1}{keV}$ significant
#+CAPTION: transmission gains are achieved.
#+NAME: fig:detector:window_efficiency_comparison
[[~/phd/Figs/detector/window_transmisson_comparison.pdf]]

[fn:norcada_website] https://www.norcada.com/

*** TODOs for this section                                       :noexport:

- [X] *FIX REFERENCE TO INLINE LATEX LABELS!*

- [X] *POSSIBLY UPDATE IMAGE OF WINDOW*
  
- [X] *UPDATE IMAGE OF TRANSMISSION*

*** Calculation of strongback window structure plot :noexport:

#+begin_src nim :tangle code/window_strongback.nim
## Super dumb MC sampling over the entrance window using the Johanna's code from `raytracer2018.nim`
## to check the coverage of the strongback of the 2018 window
##
## Of course one could just color areas based on the analytical description of where the
## strongbacks are, but this is more interesting and looks fun. The good thing is it also
## allows us to easily compute the fraction of pixels within and outside the strongbacks.
import ggplotnim, random, chroma
proc colorMe(y: float): bool =
  const
    stripDistWindow = 2.3  #mm
    stripWidthWindow = 0.5 #mm
  if abs(y) > stripDistWindow / 2.0 and
     abs(y) < stripDistWindow / 2.0 + stripWidthWindow or
     abs(y) > 1.5 * stripDistWindow + stripWidthWindow and
     abs(y) < 1.5 * stripDistWindow + 2.0 * stripWidthWindow:
    result = true
  else:
    result = false

proc sample() =
  randomize(423)
  const nmc = 5_000_000
  let black = color(0.0, 0.0, 0.0)
  var dataX = newSeqOfCap[float](nmc)
  var dataY = newSeqOfCap[float](nmc)
  var strongback = newSeqOfCap[bool](nmc)
  for idx in 0 ..< nmc:
    let x = rand(-7.0 .. 7.0)
    let y = rand(-7.0 .. 7.0)
    if x*x + y*y < 7.0 * 7.0:
      dataX.add x
      dataY.add y
      strongback.add colorMe(y)
  let df = toDf(dataX, dataY, strongback)
  echo "A fraction of ", df.filter(f{`strongback` == true}).len / df.len, " is occluded by the strongback"
  let dfGold = df.filter(f{abs(idx(`dataX`, float)) <= 2.25 and
                           abs(idx(`dataY`, float)) <= 2.25})
  echo "Gold region: A fraction of ", dfGold.filter(f{`strongback` == true}).len / dfGold.len, " is occluded by the strongback"
  ggplot(df, aes("dataX", "dataY", fill = "strongback")) +
    geom_point(size = 1.0) +
    # draw the gold region as a black rectangle
    geom_linerange(aes = aes(y = 0, x = 2.25, yMin = -2.25, yMax = 2.25), color = "black") +
    geom_linerange(aes = aes(y = 0, x = -2.25, yMin = -2.25, yMax = 2.25), color = "black") +
    geom_linerange(aes = aes(x = 0, y = 2.25, xMin = -2.25, xMax = 2.25), color = "black") +
    geom_linerange(aes = aes(x = 0, y = -2.25, xMin = -2.25, xMax = 2.25), color = "black") +
    xlab("x [mm]") + ylab("y [mm]") +
    ggtitle("Idealized schematic of the window layout. Strongback in purple.") +
    ggsave("/home/basti/phd/Figs/SiN_window_occlusion.png", 640, 500)#width = 1150, height = 1000)
sample()
#+end_src

#+RESULTS:

*** Calculation of transmission efficiency [0/0]                 :noexport:

Let's calculate the transmission for =Si₃N₄= and Mylar windows using
[[https://github.com/SciNim/xrayAttenuation][=xrayTransmission=]].

#+begin_src nim :tangle /home/basti/phd/code/window_transmission_comparison.nim
import std / strutils
import xrayAttenuation, ggplotnim
# generate a compound of silicon and nitrogen with correct number of atoms
let Si₃N₄ = compound((Si, 3), (N, 4))
#Si₃N₄.plotTransmission(3.44.g•cm⁻³, 300.nm.to(Meter))
# instantiate Mylar
let mylar = compound((C, 10), (H, 8), (O, 4))
# mylar.plotTransmission(1.4.g•cm⁻³, 2.μm.to(Meter), energyMax = 3.0)

echo mylar.name()
echo Si₃N₄.name()
# define energies in which to compute the transmission
# (we don't start at 0, as at 0 energy the parameters are not well defined)
let energies = linspace(1e-2, 3.0, 1000)

proc compTrans[T: AnyCompound](el: T, ρ: g•cm⁻³, length: Meter): DataFrame =
  result = toDf({ "Energy [keV]" : energies })
    .mutate(f{float: "μ" ~ el.attenuationCoefficient(idx("Energy [keV]").keV).float},
            f{float: "Trans" ~ transmission(`μ`.cm²•g⁻¹, ρ, length).float},
            f{"Compound" <- el.name()})
var df = newDataFrame()
# compute transmission for Si₃N₄ (known density and desired length)
df.add Si₃N₄.compTrans(3.44.g•cm⁻³, 300.nm.to(Meter))
# and for 2μm of mylar
df.add mylar.compTrans(1.4.g•cm⁻³, 2.μm.to(Meter))
# create a plot for the transmissions
echo df
let dS = r"$\SI{300}{nm}$" #pretty(300.nm, 3, short = true)
let dM = r"$\SI{2}{\micro\meter}$" #pretty(2.μm, 1, short = true)
let si = r"$\mathrm{Si}₃\mathrm{N}₄$"
ggplot(df, aes("Energy [keV]", "Trans", color = "Compound")) +
  geom_line() +
  xlab(r"Energy [$\si{keV}$]") + ylab("Transmission") +
  xlim(0.0, 3.0) + 
  ggtitle(r"Transmission examples of $# $# and $# Mylar" % [dS, si, dM]) +
  ggsave("/home/basti/phd/Figs/detector/window_transmisson_comparison.pdf",
         width = 600, height = 360,
         useTex = true, standalone = true) 
#+end_src


** Septemboard - 6 GridPixes around a center one [0/7]
:PROPERTIES:
:CUSTOM_ID: sec:detector:septemboard
:END:

The main motivation for extending the readout area from a single chip
to a 7 chip readout is to reduce background towards the outer sides of
the chip, in particular in the corners. Against common intuition
however, it also plays a role for events, which have cluster centers
near the center of the readout. The latter is due to gas ionization
being a statistical process. In particular in lower energy events,
tracks may have gaps in them large enough to avoid being detected as a
single cluster for standard radii in cluster searching
algorithms. This is particularly of interest as different searches
produce an 'image' at different positions and sizes on the
detector. While the center chip is large enough to fully cover the
image for essentially all models, it may not be in the regions of
lowest background. Hence, improvements to larger areas are needed.

The septemboard is implemented in such a way to optimize the loss of
active area due to bonding requirements and general manufacturing
realities. As the Timepix ASIC is a $\SI{16.1}{mm}$ by $\SI{14.1}{mm}$
large chip (the bonding area adding $\SI{2}{mm}$ on one side), the upper
two rows are installed such that they are inverted to another. The
bonding area is above the upper row and below the center row. The
bottom row again has its bonding area below. This way the top two rows
are as close together as realistically possible, with a decent gap on
the order of $\SI{2}{mm}$ between the middle and bottom row. Any gap is
potentially problematic as it implies loss of signal in that area,
complicating the possible reconstruction methods. The layout can be
seen in fig. [[fig:detector:occupancy_sparking_run_241]] in the next section.

All 7 GridPix are connected in a daisy-chained way. This means that in
particular for data readout, each chip is read out one after
another. The dead time for readouts therefore is approximately 7 times
the readout time of a single Timepix. A single Timepix has a readout
time of $\sim\SI{25}{ms}$ at a clock frequency of $\SI{40}{MHz}$ (the
frequency used for this detector). This leads to an expected readout
time of the full septemboard of
$\SI{175}{ms}$. [fn:detector_readout_time] Such a long readout time
leads to a strong restriction of the possible applications for such a
detector. Fortunately, for the use cases in a very low rate experiment
such as CAST, long shutter times are possible, mitigating the effect
on the fractional dead time to a large extent.

Fig. [[fig:detector:cluster_centers_likelihood]] shows a heatmap of all
cluster centers during roughly $\SI{2000}{h}$ of background data after
passing these clusters through a likelihood based cut method aiming to
filter out non X-ray like clusters (details of this follow later in
sec. [[#sec:background:method]]). It is clearly visible that the further
a cluster center is towards the chip edges, and especially the
corners, the more likely it is to be considered an X-ray like
cluster. This has an easy geometric explanation. Consider a perfect
track traversing over the whole chip. In this case it is very
eccentric. Move the same track such that its center is in one of the
corners and rotate it by $\SI{45}{°}$ and suddenly the majority of the
track won't be detected on the chip anymore. Instead something roughly
circular remains visible, 'fooling' the likelihood method. For a
schematic illustrating this, see fig. [[fig:detector:gridpix_ring_veto_idea]].

The septemboard therefore is expected to significantly reduce the
background over the whole center chip, with the biggest effect in the
regions with the most amount of background. 

#+CAPTION: Cluster centers left after 2014/15 like cuts applied to about $\SI{2000}{h}$ of
#+CAPTION: background data. Background increasing dramatically towards edges and
#+CAPTION: corners.
#+NAME: fig:detector:cluster_centers_likelihood
[[~/phd/Figs/backgroundClusters/background_cluster_centers.pdf]]

#+CAPTION: Illustration of the basic idea behind the GridPix veto ring. If a cluster on the center
#+CAPTION: chip is X-ray like and near the corners, checking the outer chips close to the corner
#+CAPTION: for a track containing the center cluster can overrule the X-ray like definition of
#+CAPTION: the center chip only.
#+NAME: fig:detector:gridpix_ring_veto_idea
[[~/org/Figs/InGridSeptemExplanation/septem_explanation_lnL_monokai.pdf]]

[fn:detector_readout_time] The ideal readout time for one chip is $t =
\SI{917504}{bits} · \SI{25}{ns} = \SI{22.9942}{ms}$
cite:&lupberger2016pixel, but this does not take into account overhead
from the FPGA, sending data to the computer and processing in TOS. We
will later see that the practical readout time of the final detector
is closer to almost $\SI{500}{ms}$ under high rate conditions
(e.g. \cefe calibration runs) and $\sim\SI{200}{ms}$ for low rate
background conditions.

*** TODOs for this section                                       :noexport:

- [ ] *First paragraph and later paragraph talk about the same thing!!!*

*REFERENCE PAPER ABOUT TRACKS IN TPCS. IONIZATION STATISTICAL AND SO ON*

*THE LATTER NEEDS MORE WORDING ELSEWHERE / CLUSTERING ALGORITHM EXPL /
SEPTEM VETO*

- [X] *REFERENCE* that schematic of how everything is connected is explained
in the detector @ CAST? Or explain it here, then refer back?
-> Already done further up. 

- [X] *USE EXACT MEASURES OF THE TIMEPIX BASED ON TIMEPIX MANUAL*
  16.1 times 14.1 seems fine.

- [X] *TODO: NEED CAPTION AND LABEL FOR BACKGROUND CLUSTERS*

- [X] *SIDE BY SIDE INCLUDING A SEPTEM EVENT SHOWING TRACK CUT LEADS
  TO CIRCLE*
  -> But that makes it exceptionally small!
  -> Added an additional illustration based on what we used back in
  ~2016. 

- [X] *MENTION SEPTEMBOARDS ARE NAMED BY LETTERS, WHICH ONE USED IN
  DETECTOR*
  -> Not here! Already done in septemboard introduction

- [X] *SHOW SCHEMATIC OF LAYOUT*
  -> Indirectly in the temperature part!

- [X] *FIND OUT WHERE 25 MS READOUT FOR SINGLE TIMEPIX COMES FROM*
  -> Luppis thesis as mentioned in the footnote now.
- [X] *READOUT TIME:*
  #+begin_quote
  The main driver for the readout speed is the time to readout the
  complete matrix (one frame) from the chip and this value is fixed for
  a given FCLOCK frequency. A frame consists of 917504 bits, which have
  to be packed to the data stream. As the data is sampled with FCLOCK,
  the same amount of clock cycles is needed, what defines the readout
  time
  #+end_quote

    page 83 of Lupberger thesis. 917504 * 25ns (one clock at 40MHz) = 22.9442 ms per frame under
  ideal conditions

*** GridPix veto ring illustration                               :noexport:

The event used in the illustration is event 23707 of run 242.

The full event display:
[[~/org/Figs/statusAndProgress/exampleEvents/example_track_corner_gridpix_ring_run242_event23707.pdf]]

and the SVG of the illustration:
[[~/org/Figs/InGridSeptemExplanation/septem_explanation_lnL_monokai.svg]]

*** Compute the cluster backgrounds                              :noexport:

To compute these cluster backgrounds, we need the following
ingredients:
- the fully reconstructed data files =DataRuns201*_Reco.h5= 
- the prepared CDL data from the 2019 dataset
  =calibration-cdl_2019.h5= and the X-ray reference datasets that
  define the X-ray like properties.
- apply the likelihood method to all background events in one of the
  files (gives enough statistics) to get a resulting file containing
  only passed clusters *over the whole chip*.

With the resulting file we can then use
[[file:~/CastData/ExternCode/TimepixAnalysis/Plotting/plotBackgroundClusters/plotBackgroundClusters.nim]]
to plot these cluster centers.

Assuming the reconstructed data files are found in *...* and the CDL
data files in *...*, let's generate the data after likelihood method:
(this takes ~2 minutes or so, so better run it in a terminal instead of
via C-c C-c)
#+begin_src sh 
likelihood \
    -f ~/CastData/data/DataRuns2017_Reco.h5 \
    --h5out /tmp/lhood_2017_full_chip.h5 \
    --cdlYear=2018 \
    --region=crAll \
    --cdlFile=/home/basti/CastData/data/CDL_2019/calibration-cdl-2018.h5 \
    --lnL
#+end_src

- [ ] *WHY DOES THIS* produce background suppression numbers below 1
  towards the corners?


Compile the plotting tool if not done:
#+begin_src sh :dir ~/CastData/ExternCode/TimepixAnalysis/Plotting/plotBackgroundClusters
nim c -d:danger --threads:on plotBackgroundClusters.nim
#+end_src

Now we can create the plot:
#+begin_src sh
plotBackgroundClusters \
    -f /t/lhood_2017_full_chip.h5 \
    --title "2000 h background data, cuts applied" \
    --outpath ~/phd/Figs/backgroundClusters/ \
    --energyMin 0.2 \
    --energyMax 12.0 \
    --zMax 15.0 
#+end_src

#+RESULTS:
|                        reading: | /t/lhood_2017_full_chip.h5 |         |         |                                                                         |       |       |               |            |    |    |             |    |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |              |        |
| @["/t/lhood_2017_full_chip.h5"] |                            |         |         |                                                                         |       |       |               |            |    |    |             |    |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |              |        |
|                       DataFrame |                       with |       3 | columns | and                                                                     | 20202 | rows: |               |            |    |    |             |    |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |              |        |
|                             Idx |                          x |       y |   count |                                                                         |       |       |               |            |    |    |             |    |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |              |        |
|                          dtype: |                        int |     int |     int |                                                                         |       |       |               |            |    |    |             |    |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |              |        |
|                               0 |                          2 |     247 |       1 |                                                                         |       |       |               |            |    |    |             |    |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |              |        |
|                               1 |                          3 |     247 |       1 |                                                                         |       |       |               |            |    |    |             |    |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |              |        |
|                               2 |                          6 |     166 |       1 |                                                                         |       |       |               |            |    |    |             |    |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |              |        |
|                               3 |                          9 |      33 |       1 |                                                                         |       |       |               |            |    |    |             |    |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |              |        |
|                               4 |                          9 |     122 |       1 |                                                                         |       |       |               |            |    |    |             |    |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |              |        |
|                               5 |                          9 |     128 |       1 |                                                                         |       |       |               |            |    |    |             |    |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |              |        |
|                               6 |                          9 |     224 |       1 |                                                                         |       |       |               |            |    |    |             |    |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |              |        |
|                               7 |                         10 |       7 |       1 |                                                                         |       |       |               |            |    |    |             |    |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |              |        |
|                               8 |                         10 |      57 |       1 |                                                                         |       |       |               |            |    |    |             |    |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |              |        |
|                               9 |                         10 |     106 |       1 |                                                                         |       |       |               |            |    |    |             |    |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |              |        |
|                              10 |                         10 |     107 |       1 |                                                                         |       |       |               |            |    |    |             |    |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |              |        |
|                              11 |                         10 |     146 |       1 |                                                                         |       |       |               |            |    |    |             |    |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |              |        |
|                              12 |                         10 |     147 |       1 |                                                                         |       |       |               |            |    |    |             |    |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |              |        |
|                              13 |                         10 |     165 |       2 |                                                                         |       |       |               |            |    |    |             |    |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |              |        |
|                              14 |                         10 |     166 |       1 |                                                                         |       |       |               |            |    |    |             |    |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |              |        |
|                              15 |                         10 |     185 |       1 |                                                                         |       |       |               |            |    |    |             |    |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |              |        |
|                              16 |                         10 |     188 |       1 |                                                                         |       |       |               |            |    |    |             |    |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |              |        |
|                              17 |                         10 |     202 |       1 |                                                                         |       |       |               |            |    |    |             |    |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |              |        |
|                              18 |                         10 |     224 |       1 |                                                                         |       |       |               |            |    |    |             |    |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |              |        |
|                              19 |                         10 |     231 |       1 |                                                                         |       |       |               |            |    |    |             |    |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |              |        |
|                                 |                            |         |         |                                                                         |       |       |               |            |    |    |             |    |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |              |        |
|                         [INFO]: |                     Saving |    plot |      to | /home/basti/phd/Figs/backgroundClusters//background_cluster_centers.pdf |       |       |               |            |    |    |             |    |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |              |        |
|                           INFO: |                        The | integer |  column | `x`                                                                     | has   | been  | automatically | determined | to | be | continuous. | To | overwrite | this | behavior | add | a | `+ | scale_x/y_discrete()` | call | to | the | plotting | chain. | Choose | `x` | or | `y` | depending | on | which | axis | this | column | refers | to. | Or | apply | a | `factor` | to | the | column | name | in | the | `aes` | call, | i.e. | `aes(..., | factor("x"), | ...)`. |
|                           INFO: |                        The | integer |  column | `y`                                                                     | has   | been  | automatically | determined | to | be | continuous. | To | overwrite | this | behavior | add | a | `+ | scale_x/y_discrete()` | call | to | the | plotting | chain. | Choose | `x` | or | `y` | depending | on | which | axis | this | column | refers | to. | Or | apply | a | `factor` | to | the | column | name | in | the | `aes` | call, | i.e. | `aes(..., | factor("y"), | ...)`. |




** Water cooling and temperature readout for the septemboard [0/0]
:PROPERTIES:
:CUSTOM_ID: sec:detector:water_cooling
:END:

During development of the septemboard one particular set of problems
manifested. While testing a prototype board with 5 active GridPix in a
gaseous detector, the readout was plagued by excessive noise
problems. The detector exhibited a large number of frames with more
than $\num{4096}$ active pixels (the limit for a zero suppressed
readout) and common pixel values of $\num{11810}$ indicating overrun ToT
counters. On an occupancy (sum of all active pixels) of the individual
chips, it is quite visible the data is clearly not due to cosmic
background. Fig. [[fig:detector:occupancy_sparking_run_241]] shows such an
occupancy with the color scale topping out at the $80^{\text{th}}$
percentile of the counts for each chip individually. The chip in the
bottom left shows a large number of sparks (overlapping half ellipses
pointing downwards) at the top end. Especially the center chip in the
top row shows highly structured activity, which is in contrast to the
expectation of a homogeneous occupancy for a normal background
run. In addition on all chips some level of general noise on certain
pixels is visible (some being clearly more active than others
resulting in a scatter of 'points').

#+CAPTION: Occupancy of a testing background run with $\mathcal{O}(\SI{1}{s})$ long frames
#+CAPTION: using septemboard F during development without any kind of cooling. This also shows
#+CAPTION: the layout of the full septemboard with realistic spacing.
#+NAME: fig:detector:occupancy_sparking_run_241
[[~/phd/Figs/detector/sparking/sparking_occupancy_80_quantile_run_241.pdf]]

The intermediate board and carrier board used during these tests were
the first boards equipped with two PT1000 temperature sensors. One on
the bottom side of the carrier board and another on the intermediate
board. Each is read out using a =MAX31685= micro controllers. Both of
which are communicated with via a =MCP2210= USB-to-SPI micro
controllers over a single USB port on the intermediate board. The
single =MCP2210= communicates with both temperature sensors via the
Serial Peripheral Interface (SPI) (see
sec. [[#sec:daq:temperature_readout]] for more information about the
temperature logging and readout). In the run shown in
fig. [[fig:detector:occupancy_sparking_run_241]] the temperature sensors
were not functional yet, as the readout software was not written. The
required logic was added to the Timepix Operating System (TOS), the
readout software of the detector, motivated by this noise activity to
monitor the temperature before and during a data taking period. The
temperature on the carrier board indicated temperatures of
$\sim\SI{75}{\celsius}$ in background runs similar to the one of
fig. [[fig:detector:occupancy_sparking_run_241]]. One way to get a measure
for the noise like activity seen on the detector is to look at the
rate of active pixels over time. With values well above numbers
expected due to background, excess temperature seemed a possible cause
for the issues. As no proper cooling mechanism was available, a
regular desk fan was placed pointing at the detector when it was run
without any kind of shielding. This saw the temperature under the
carrier board drop from $\SI{76}{\celsius}$ down to
$\SI{68}{\celsius}$. As a result the majority of noise disappeared as
can be seen in fig. sref:fig:detector:sparking_run_with_fan_mean_hits
with the temperature curve during the full run in
fig. sref:fig:detector:sparking_run_with_fan_temps. [fn:detector_sparking_run_268] [fn:detector_troubleshooting]

The features visible in the occupancy plots are thus likely multiple
different artifacts due to too high temperatures. A mixture of real
sparks (bottom left chip in
fig. [[fig:detector:occupancy_sparking_run_241]]) and possible
instabilities that possibly affect voltages for the pixels (and thus
change the thresholds of each pixel). As the temperature is measured
on the bottom side of the carrier board, temperatures in the
amplification region are likely significantly higher. 

#+begin_src subfigure
(figure ()
        (subfigure (linewidth 0.5) (caption "Temperature") (label "fig:detector:sparking_run_with_fan_temps")
                   (includegraphics (list (cons 'width (linewidth 0.95)))
                                    "~/phd/Figs/detector/sparking/temperature_sparking_run_268.pdf"))
        (subfigure (linewidth 0.5) (caption "Mean hit rate") (label "fig:detector:sparking_run_with_fan_mean_hits")
                   (includegraphics (list (cons 'width (linewidth 0.95)))
                                    "~/phd/Figs/detector/sparking/mean_hit_rate_sparking_run_268.pdf"))
        (caption
         (subref "fig:detector:sparking_run_with_fan_temps") " shows the temperature on the bottom side of the
          carrier board ('septem') and intermediate board ('IMB') during the background run. The point at which
          the desk fan is placed next to the detector is clearly visible by the " ($ (SI 8 "\\celsius")) " drop in
          temperature from about " ($ (SI 76 "\\celsius")) " to " ($ (SI 68 "\\celsius")) "."
         (subref "fig:detector:sparking_run_with_fan_mean_hits") " shows the mean hit rate of each of the 5 chips
          installed on the carrier board at the time during the same run. The placement of the desk fan is easily
          visible as a reduction in mean rate on all chips."
         )
         (label "fig:detector:sparking_run_with_fan"))
#+end_src

Following this a bespoke water cooling was designed by T. Schiffer
made from oxygen-free copper with $\SI{3}{mm}$ channels for water to
circulate through the copper body. cite:schiffer_phd The body has the
same diameter as the intermediate board and is installed right
below. The water circulation is handled by an off-the-shelf pump and
radiator from Alphacool [fn:detector_alphacool] intended for water
cooling setups for desktop computers. The pump manages a water flow
rate of about $\SI{0.3}{\liter\per\minute}$ through the $\SI{3}{mm}$
channels in the copper. In common operation the temperatures on the
carrier board are between $\SIrange{45}{50}{\celsius}$ and noise free
operation is possible.

[fn:detector_troubleshooting] The realization that the issues are
purely due to temperature effects was only after several months of
eliminating many other options, both on the software as well as the
hardware side. In particular power supply instabilities were long
considered to be a source of problems. While they possibly also had an
impact, better power supplies were built with larger capacitors to
better deal with large variations in required power. 

[fn:detector_alphacool] https://www.alphacool.com/

[fn:detector_sparking_run_268] See the full thesis version for the
occupancy of the run with temperature readout in the subsection
after this if interested.

*** TODOs for this section [3/5]                                 :noexport:

- [ ] *TAKE OUT DETAILS* about the MCP2210 etc. logic to read out
  sensors etc. That is going to be explained later on anyway.

- [X] *REF TOBI THESIS & UPCOMING PAPER*
  - [ ] *UPCOMING PAPER CITE*
- [X] See
  ~/org/Papers/tobias_schiffer_septemboard_cooling_chapter.pdf~
  for his chapter about the cooling device! Good for the size of holes
  etc and check if our bla bla is correct!
  -> 3 mm channels!
- [X] *REFERENCE CODE TO TEMP READOUT?*
  -> Will be referenced in section about readout itself

- [ ] *REWRITE SENTENCE*:
  Further, the gas gain is proportional to the temperature, it is possible slight height
  differences of the InGrid cause local amplification events, similar to
  a photomultiplier tube.
  -> This was after the "As the temperature is measured" ... sentence.


*** Sparking behavior :noexport:

See the mails containing "Septem F" (among other things) for the
information about sparking behavior. From that we can also deduce the
run numbers of the noisy runs (run 241 is one of them); just keep in
mind that the run numbers are overlapping with some CAST run numbers,
as for CAST we started again at 0.

Specific run path of noisy run used in occupancy plot above:
=Run_241_170216-13-49=
So run from February 2017.


Let's plot the temperature during the sparking run in which we
installed the fan.

This is essentially a reproducible version of the following plot:
[[file:~/org/Figs/temps_plot_septemF_76_68deg_1s.pdf]]
#+begin_src nim :tangle /home/basti/phd/code/sparking_temperature.nim
import ggplotnim, times

# Laptop:
#const path = "/mnt/1TB/CAST/2017/development/Run_268_170418-05-43/temp_log.txt"
# Desktop:
const path = "~/CastData/data/2017/development/Run_268_170418-05-43/temp_log.txt"

proc p(x: string): DateTime =
  result = x.parse("YYYY-MM-dd'.'HH:mm:ss", local())
let df = readCsv(path, sep = '\t', skipLines = 2, colNames = @["IMB", "Septem", "DateTime"])
  .filter(f{string -> bool: p(`DateTime`) < initDateTime(19, mApr, 2017, 0, 0, 0, 0, local())})
  .gather(@["IMB", "Septem"], "Type", "Temperature")
  .mutate(f{"Timestamp" ~ p(`DateTime`).toTime().toUnix()})

## XXX: fix plotting of string columns as date scales, due to discrete / continuous mismatch and lacking
## `dataScale` field
ggplot(df, aes("Timestamp", "Temperature", color = "Type")) +
  geom_line() +
  # scale_x_continuous() +
  ggtitle("Temperature during run on 2017/04/18 with fan next to detector") + 
  xlab("Time of day") + ylab("Temperature [°C]") +
  #margin(top = 2.0) + 
  scale_x_date(isTimestamp = true,
               formatString = "HH:mm:ss",
               dateSpacing = initDuration(hours = 2),
               dateAlgo = dtaAddDuration,
               timeZone = local()) +
  ggsave("/home/basti/phd/Figs/detector/sparking/temperature_sparking_run_268.pdf",
        width = 600, height = 360, useTeX = true, standalone = true)
df.writeCsv("/home/basti/phd/resources/temperature_sparking_run_268.csv")  
#+end_src

#+RESULTS:
| INFO:      | The                                                                     | integer           | column                                 | `Timestamp`                                                             | has | been | automatically | determined | to | be | continuous. | To | overwrite | this | behavior | add | a | `+ | scale_x/y_discrete()` | call | to | the | plotting | chain. | Choose | `x` | or | `y` | depending | on | which | axis | this | column | refers | to. | Or | apply | a | `factor` | to | the | column | name | in | the | `aes` | call, | i.e. | `aes(..., | factor("Timestamp"), | ...)`. |
| [INFO]     | TeXDaemon                                                               | ready             | for                                    | input.                                                                  |     |      |               |            |    |    |             |    |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |                      |        |
| shellCmd:  | command                                                                 | -v                | xelatex                                |                                                                         |     |      |               |            |    |    |             |    |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |                      |        |
| shellCmd:  | xelatex                                                                 | -output-directory | /home/basti/phd/Figs/detector/sparking | /home/basti/phd/Figs/detector/sparking/temperature_sparking_run_268.tex |     |      |               |            |    |    |             |    |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |                      |        |
| Generated: | /home/basti/phd/Figs/detector/sparking/temperature_sparking_run_268.pdf |                   |                                        |                                                                         |     |      |               |            |    |    |             |    |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |                      |        |

Next up we need to compute the mean hit rate of the four most active
chips and plot it against time.

How will we go about doing that? Read and reconstruct the run, then
manually extract hits per time, bin by time and that's it?

#+begin_src sh
# cd /mnt/1TB/CAST/2017/development/
cd ~/CastData/data/2017/development/
raw_data_manipulation -p Run_268_170418-05-43 --runType background --out raw_268_sparking.h5
reconstruction -i raw_268_sparking.h5 --out reco_268_sparking.h5
#+end_src

#+RESULTS:

With the resulting file, we can now generate the plot of the hits over
time.

This is a reproducible version of the following plot:
[[file:~/org/Figs/hitrate_per_time_septemF_76_68deg_1s.pdf]]
#+begin_src nim :tangle /home/basti/phd/code/sparking_hit_rate_over_time.nim
import std / [options, sequtils, times]
import ggplotnim, nimhdf5, unchained
defUnit(Second⁻¹)
import ingrid / tos_helpers
# Laptop
# const path = "/mnt/1TB/CAST/2017/development/reco_268_sparking.h5"
# Desktop
const path = "~/CastData/data/2017/development/reco_268_sparking.h5"
let h5f = H5open(path, "r")

var df = newDataFrame()
var dfR = newDataFrame()
for chip in 0 ..< 5:
  let dsets = @["hits"]
  let dfC = h5f.readRunDsets(
    268,
    chipDsets = some((chip: chip, dsets: dsets)),
    commonDsets = @["timestamp"]
  )
    .mutate(f{"chip" <- chip})
    .arrange("timestamp")
  df.add dfC

  # and directly compute the hit frequency
  let hits = dfC["hits", int]
  let time = dfC["timestamp", int]
  let ts = time.map_inline((x - time[0]).s)
  const Interval = 30.min
  var i = 0
  var rate = newSeq[Second⁻¹]()
  var rateTime = newSeq[float]()
  while i < time.len:
    var h = 0
    var Δt = 0.s
    let t0 = time[i]
    echo "Starting at t0 = ", t0
    while Δt < Interval and i < time.len:
      h += hits[i]
      if i > 0:
        Δt += ts[i] - ts[i-1]
      inc i
    rate.add (h.float / Δt)
    echo "To ", time[i-1]
    rateTime.add((time[i-1] + t0) / 2.0)
    h = 0
  dfR.add toDf({"rate" : rate.mapIt(it.float), rateTime, "chip" : chip})
echo df
echo dfR

dfR = dfR.filter(f{int -> bool: fromUnix(`rateTime`).inZone(local()) < initDateTime(19, mApr, 2017, 0, 0, 0, 0, local())})
ggplot(dfR, aes("rateTime", "rate", color = factor("chip"))) +
  geom_point() +
  scale_y_log10() + 
  scale_x_date(isTimestamp = true,
               formatString = "HH:mm:ss",
               dateSpacing = initDuration(hours = 2),
               dateAlgo = dtaAddDuration,
               timeZone = local()) +
  xlab("Time of day") + ylab(r"Rate [$\si{pixel.s^{-1}}$]") + 
  ggsave("/home/basti/phd/Figs/detector/sparking/mean_hit_rate_sparking_run_268.pdf",
        width = 600, height = 360, useTex = true, standalone = true)

dfR.writeCsv("/home/basti/phd/resources/mean_hit_rate_sparking_run_268.csv", precision = 10)
#+end_src

Finally, combine both and plot together:
#+begin_src nim :tangle /home/basti/phd/code/temperature_and_sparking.nim
import std / times
import ggplotnim
const path = "/home/basti/phd/resources/"
let df = readCsv(path & "temperature_sparking_run_268.csv")
let dfR = readCsv(path & "mean_hit_rate_sparking_run_268.csv")
  .group_by("chip")
  .mutate(f{"rateNorm" ~ `rate` / max(`rate`) * 80.0})
  .rename(f{"Timestamp" <- "rateTime"})

let sa = secAxis(name = "Hit rate [a.u.]",
                 trans = f{1.0 / 80.0})
                 #invTransFn = f{`rateNorm` * 80.0})

ggplot(df, aes("Timestamp", "Temperature", color = "Type")) +
  geom_line() +
  geom_point(data = dfR, aes = aes("Timestamp", "rateNorm", color = factor("chip"))) + 
  # ggtitle("Temperature during run on 2017/04/18 in which fan was placed next to detector") + 
  xlab("Time of day") + ylab("Temperature [°C]") +
  margin(top = 2.0) +
  scale_y_continuous(secAxis = sa) + 
  scale_x_date(isTimestamp = true,
               formatString = "HH:mm:ss",
               dateSpacing = initDuration(hours = 2),
               dateAlgo = dtaAddDuration,
               timeZone = local()) + 
  legendPosition(0.835, 0.1) +
  yMargin(0.05) + 
  ggsave("/home/basti/phd/Figs/detector/sparking/temperature_and_sparking_run_268.pdf")

#+end_src

#+RESULTS:


And finally, let's also recreate the occupancy plot
[[file:~/phd/Figs/detector/sparking/occupancy_sparking_septem5chips_300V.pdf]] of run 241 during
development to showcase the sparking behavior.

In order to do that, we first need to reconstruct the run containing
the data:
#+begin_src sh
cd /mnt/1TB/CAST/2017/development/
raw_data_manipulation -p Run_241_170216-13-49 --runType background --out raw_241_sparking.h5
reconstruction raw_241_sparking.h5 --out reco_241_sparking.h5
#+end_src

With the reconstructed data file at hand, we can first of all generate
a large number of plots for each chip:
#+begin_src sh
plotData --h5file reco_241_sparking.h5 \
         --runType rtBackground \
         --config ~/CastData/ExternCode/TimepixAnalysis/Plotting/karaPlot/config.toml \
         --ingrid --occupancy
#+end_src
which can be adjusted according to the user's preference of course.

(For this plot in particular it's really important not use the =ToT=
cutting feature in =raw_data_manipulation= via the =rmToTLow= and
=rmToTHigh= in the =config.toml= file)

With the file in place, let's now create the plot of the occupancies
for each chip, embedded in the layout of the septemboard (at least for
the 5 chips that were on this septemboard F).


- [ ] *REPLACE BELOW PLACEMENT BY ~geometry.nim~ IMPLEMENTATION*
#+begin_src nim :tangle /home/basti/phd/code/occupancy_sparking_septem_layout.nim
import std / os except FileInfo
import std / strutils
import ingrid / [tos_helpers, ingrid_types]
import nimhdf5, ggplotnim, ginger

## The Septemboard layout code is a port of the code used in the python based event 
## display for TOS.

const
  Width = 14.1
  Height = 14.1
  BondHeight = 2.0
  FullHeight = Height + BondHeight
  NumChips = 7

  # If this is set to `true` the final plot will only contain the actual raster image. No legend or axes
  OnlyRaster = true

  Run = 241

type
  SeptemRow = object
    left: float
    right: float
    wspace: float
    top: float
    bottom: float

proc initSeptemRow(nChips: int, x_size, y_size, x_dist, x_offset, y_t_offset, y_b_offset, dist_to_row_below: float): SeptemRow =
  # this class implements a single row of chips of the septem board
  # nChips: number of chips in row
  # x_dist: distance in x direction between each chip
  # x_offset: offset of left edge of first chip in row from
  #           left side of center row
  # calculate width and height of row, based on chips and dist
  let width = nChips.float * Width + (nChips - 1).float * x_dist
  let height_active = Height
  let height_full = FullHeight + dist_to_row_below
  # using calc gridspec one calculates the coordinates of the row on
  # the figure in relative canvas coordinates
  # include padding by adding or subtracting from left, right, top, bottom
  result.left      = x_offset / x_size
  result.right     = result.left + width / x_size
  result.wspace    = x_dist / x_size
  result.top       = 1.0 - y_t_offset / y_size
  result.bottom    = result.top - height_active / y_size

proc initSeptemBoard(padding, fig_x_size, fig_y_size, scaling_factor: float): seq[SeptemRow] =
  # implements the septem board, being built from 3 septem rows

  proc initRows(y_size, scaled_x_size, scaled_y_size, y_row1_row2, y_row2_row3, row2_x_dist: float): seq[SeptemRow] =
    # this function creates the row objects for the septem class
    # calculation of row 1 top and bottom (in abs. coords.):
    let
      # (top need to add padding to top of row 1)
      row1_y_top    = y_size - BondHeight - Height
      # bottom in abs. coords.
      row1_y_bottom = 2 * FullHeight + y_row1_row2 + y_row2_row3 - Height
      # offset of left side from septem in abs. coords.
      row1_x_offset = 6.95
    # now create the first row with all absolute coordinates
    result.add initSeptemRow(2, scaled_x_size, scaled_y_size, 0.85, row1_x_offset, row1_y_top, row1_y_bottom, y_row1_row2)
    # calculation of row 2 top and bottom (top & bottom of row2 not affected by padding):
    let
      row2_y_top    = y_size - FullHeight - y_row1_row2 - Height
      row2_y_bottom = FullHeight + y_row2_row3 + BondHeight - Height
      # no offset for row2, defines our left most position in abs. coords.
      row2_x_offset = 0.0 #padding * x_size
    result.add initSeptemRow(3, scaled_x_size, scaled_y_size, row2_x_dist, row2_x_offset, row2_y_top, row2_y_bottom, y_row2_row3)
    # calculation of row 3 top and bottom (add padding to bottom):
    let
      row3_y_top    = y_size - 2 * FullHeight - y_row1_row2 - y_row2_row3 - Height
      row3_y_bottom = BondHeight - Height
      row3_x_offset = 7.22
    result.add initSeptemRow(2, scaled_x_size, scaled_y_size, 0.35, row3_x_offset, row3_y_top, row3_y_bottom, 0)

  # include a padding all around the septem event display of 'padding'
  # use size of figure to scale septem accordingly to have it always properly
  # scaled for the given figure
  # take the inverse of the scaling factor (want 1/2 as input to scale to half size)
  let scaling_factor = 1.0 / scaling_factor
  # first calculate the ratio of the figure
  let fig_ratio = float(fig_x_size) / float(fig_y_size)
  # distances between different rows in absolute coordinates
  let
    y_row1_row2 = 0.38
    y_row2_row3 = 3.1
    # size in y direction of whole septem board in absolute coordinates
    y_size      = 3 * FullHeight + y_row1_row2 + y_row2_row3
    # already define row2_x_dist here (in absolute coordinates) to calculate x_size
    row2_x_dist = 0.35
    # 3 chips * width + 2 * distance between chips (in absolute coordinates)
    x_size      = 3 * Width + (3 - 1) * row2_x_dist
  # calculate the ratio of the septem board
  var ratio = float(x_size) / float(y_size)
  # now calculate the needed ratio to get the correct scaling of the septem on any
  # figure scale. fig_ratio / own ratio
  ratio = fig_ratio / ratio
  let
    # scaled x and y sizes
    scaled_x_size = x_size * ratio * scaling_factor
    scaled_y_size = y_size * scaling_factor
  # and now create the row objects
  result = initRows(y_size, scaled_x_size, scaled_y_size, y_row1_row2, y_row2_row3, row2_x_dist)

proc readVlen(h5f: H5File,
              fileInfo: FileInfo,
              runNumber: int,
              dsetName: string,
              chipNumber = 0,
              dtype: typedesc = float): seq[seq[dtype]] =
  ## reads variable length data `dsetName` and returns it
  ## In contrast to `read` this proc does *not* convert the data.
  let vlenDtype = special_type(dtype)
  let dset = h5f[(fileInfo.dataPath(runNumber, chipNumber).string / dsetName).dset_str]
  result = dset[vlenDType, dtype]

proc calcOccupancy[T](x, y: seq[seq[T]], z: seq[seq[uint16]] = @[]): Tensor[float] =
  ## calculates the occupancy of the given x and y datasets
  ## Either for a `seq[seq[T: SomeInteger]]` in which case we're calculating
  ## the occupancy of a raw clusters or `seq[T: SomeFloat]` in which case
  ## we're dealing with center positions of clusters
  result = newTensor[float]([NPix, NPix])
  # iterate over events
  for i in 0 .. x.high:
    let
      xEv = x[i]
      yEv = y[i]
    var zEv: seq[uint16]
    if z.len > 0:
      zEv = z[i]
    ## continue if full event.
    ## TODO: replace by solution that also works for clusters!!
    #if xEv.len >= 4095: continue
    for j in 0 .. xEv.high:
      if zEv.len > 0:
        result[xEv[j].int, yEv[j].int] += zEv[j].float
      else:
        result[xEv[j].int, yEv[j].int] += 1.0

proc occForChip(h5f: H5File, chip: int, fileInfo: FileInfo): (Tensor[int], Tensor[int], Tensor[float]) =        
  const NPix = 256
  let
    xD = h5f.readVlen(fileInfo, Run, "x", chip, dtype = uint8)
    yD = h5f.readVlen(fileInfo, Run, "y", chip, dtype = uint8)
    zD = h5f.readVlen(fileInfo, Run, "ToT", chip, dtype = uint16)
  let occ = calcOccupancy(xD, yD) # , zD)
  var
    x = newTensorUninit[int](NPix * NPix)
    y = newTensorUninit[int](NPix * NPix)
    z = newTensorUninit[float](NPix * NPix)
  var i = 0
  for idx, val in occ:
    x[i] = idx[0]
    y[i] = idx[1]
    z[i] = val
    inc i
  result = (x, y, z)
      
proc handleOccupancy(h5f: H5File,
                     chip: int,
                     fileInfo: FileInfo,
                     quant: float = 0.0): PlotView =
  # get x and y datasets, stack and get occupancies
  let (x, y, z) = h5f.occForChip(chip, fileInfo)
  let df = toDf(x, y, z)
  var quant = quant
  if quant == 0.0:
    quant = percentile(z, 80)
  result = ggcreate(
    block:
      var plt = 
        ggplot(df, aes("x", "y", fill = "z"), backend = bkCairo) +
          geom_raster() +
          scale_fill_continuous(scale = (low: 0.0, high: quant)) + #high: 1600.0)) +
          xlim(0, NPix) + ylim(0, NPix)
      if OnlyRaster:
        plt = plt + theme_void() + hideLegend()
      plt
  )
  #ggplot(df, aes("x", "y", fill = "z"), backend = bkCairo) +
  #    geom_raster() +
  #    scale_fill_continuous(scale = (low: 0.0, high: 1600.0)) +
  #    xlim(0, NPix) + ylim(0, NPix) +
  #    ggsave("/t/test_occ_0_.pdf")

proc drawBounds(v: Viewport) =
  v.drawBoundary(writeName = true)
  for ch in mitems(v.children):
    ch.drawBounds()

proc calcQuantileChip3(h5f: H5File, fileInfo: FileInfo): float =
  let (x, y, z) = h5f.occForChip(3, fileInfo)
  result = percentile(z, 80)  

proc addRow(view: Viewport, h5f: H5File, septem: seq[SeptemRow], fileInfo: FileInfo, i, num, chipStart: int, showEmpty = false) =
  let width = septem[i].right - septem[i].left
  let height = septem[i].top - septem[i].bottom

  var row = view.addViewport(left = septem[i].left, bottom = septem[i].bottom,
                             width = width, height = height)
  row.layout(num, 1) #, margin = quant(septem[i].wspace, ukRelative))

  #let quant = calcQuantileChip3()
  
  for j in 0 ..< num:
    if not showEmpty:
      let plt = handleOccupancy(h5f, chipStart + j, fileInfo) #, quant)
      let v = if OnlyRaster: plt.view[4] else: plt.view
      var pltView = v.relativeTo(row[j]) 
      row.embedAt(j, pltView)
    row[j].drawBoundary()

  view.children.add row

## Read the data from the reconstructed H5 file of run 241
const path = "/mnt/1TB/CAST/2017/development/reco_$#_sparking.h5" % $Run
let h5f = H5open(path, "r")
let fileInfo = getFileInfo(h5f)

let
  fig_x_size = 10.0
  fig_y_size = 12.04186
  ratio = fig_x_size / fig_y_size

let septem = initSeptemBoard(0.0, fig_x_size, fig_y_size, 1.0)
let ImageSize = fig_x_size * DPI
let view = initViewport(c(0.0, 0.0),
                        quant(fig_x_size, ukInch), quant(fig_y_size, ukInch), backend = bkCairo,
                        wImg = ImageSize, hImg = ImageSize / ratio)

view.addRow(h5f, septem, fileInfo, 0, 2, 5, showEmpty = true)
view.addRow(h5f, septem, fileInfo, 1, 3, 2)
view.addRow(h5f, septem, fileInfo, 2, 2, 0)

view.draw("/home/basti/phd/Figs/sparking_occupancy_80_quantile_run_$#.pdf" % $Run)
#+end_src

Running the above code for run 268 (the run we installed the fan and
had temperature readout) yields fig. [[fig:detector:occupancy_sparking_run_268]].

#+CAPTION: Occupancy of a testing background run with $\mathcal{O}(\SI{1}{s})$ long frames
#+CAPTION: using septemboard F during development without any kind of cooling and temperature
#+CAPTION: logging. Temperatures on the underside of the carrier board reached \SI{76}{\celsius}
#+CAPTION: before the fan was placed next to it.
#+NAME: fig:detector:occupancy_sparking_run_268
[[~/phd/Figs/detector/sparking/sparking_occupancy_80_quantile_run_268.pdf]]

** Detector efficiency [0/0]

For the applications at CAST, the detector is filled with $\ce{Ar}$ / $\ce{iC_4 H_{10}}$ :
\SI{97.7}{\%} / \SI{2.3}{\%} gas. Combined with its \SI{300}{nm}
\ce{Si_3 N_4} window, the combined detection efficiency can be
computed, if the \SI{20}{nm} \ce{Al} coating for the detector cathode
is included by computing the product of the different
efficiencies. The efficiency of the window and coating are the
transmissions of X-rays at different energies for each material $t_i$. For
the gas, the absorption probability of the gas $a_i$ is needed. As such

\[
ε_{\text{tot}} = t_{\ce{Si_3 N_4}} · t_{\ce{Al}} · a_{\ce{Ar} / \ce{iC_4H_{10}}}
\]

describes the full detector efficiency assuming the parts of the
detector, which are not obstructed by the window strongbacks. For a
statistical measure of detection efficiency the occlusion of the
window needs to be taken into account. Because it is position
(and thus area) dependent, the need to include it is decided on a case
by case basis. For the absorption of a gas mixture, we can use
Dalton's law and compute the absorption of the individual gases according
to their mole fractions (their percentage as indicated by the gas
mixture) and then compute it for each partial pressure

\[
a_i = \text{Absorption}(P_{\text{total}} · f_i)
\]

where $P_{\text{total}}$ is the total pressure of the gas mixture (in
this case \SI{1050}{mbar} and $f_i$ is the fraction of the gas
$i$. 'Absorption' simply refers to the generic function computing the
absorption for a gas at a given pressure (see
sec. [[#sec:theory:daltons_law]] and sec. [[#sec:theory:xray_matter_gas]]).

The full combined efficiency as presented here is shown in
fig. [[fig:detector:combined_efficiency]]. Different aspects dominate the
combined efficiency (purple line) in different energy ranges. At
energies above $\SI{5}{keV}$ the probability of X-rays to not generate a
photoelectron within the $\SI{3}{cm}$ of drift distance becomes the
major factor for a loss in efficiency. This means the combined
efficiency at $\SI{10}{keV}$ is slightly below $\SI{30}{\%}$. The
best combined efficiency of about $\SI{95}{\%}$ is reached at
about $\SI{3.75}{keV}$ where both the absorption is likely and the
energy is high enough to transmit well through the window. The argon
$K 1s$ absorption edge is clearly visible at around $\SI{3.2}{keV}$. At
energies below the mean free path of X-rays is significantly longer as
the $K 1s$ absorption is a significant factor in the possible
generation of a photoelectron. The window leads to a similar, but inverse, effect
namely due to the $K 1s$ line of $\ce{Si}$ at around
$\SI{1.84}{keV}$. Because transmission is desired through the window
material, the efficiency /increases/ once we go below that
energy. Finally, the nitrogen $K 1s$ line also contributes to an
increase in efficiency once we cross below about $\SI{400}{eV}$. The
average efficiencies in the energy ranges between $\SIrange{0}{3}{keV}$ and
$\SIrange{0}{10}{keV}$ are $\SI{73.42}{\%}$ and $\SI{67.84}{\%}$, respectively.

The improvement in efficiency at energies below $\SI{3}{keV}$ in
comparison to the mylar window used in the 2014/15 detector (see
sec. [[#sec:detector:sin_window]]) leads to a significant
improvement in possible signal detection at those energies, which is
especially important for searches with peak fluxes around
$\SIrange{1}{2}{keV}$ as is the case for the axion-electron coupling or
a possible chameleon coupling.

#+CAPTION: Combined detection efficiency for the full detector, taking into account
#+CAPTION: the gas filling of $\SI{1050}{mbar}$ $\ce{Ar}$ / $\ce{iC_4 H_{10}}$, the $\SI{300}{nm}$
#+CAPTION: $\ce{Si_3 N_4}$ window and its $\SI{20}{nm}$ $\ce{Al}$ coating. 
#+NAME: fig:detector:combined_efficiency
[[~/phd/Figs/detector/detector_efficiency.pdf]]

*** TODOs for this section [0/2]                                 :noexport:

Outside of that, the general background rate expected from the
detector should match and exceed the previous detector, due to the
additional detector features. 

- [ ] *ADD NUMBERS FOR AVERAGE EFFICIENCY IN RANGES, WHERE X AND Y*
- [ ] *REPLACE BY NATIVE TIKZ PLOT + VEGA*  

*** Calculation of full detection efficiency   :noexport:

#+begin_src nim :tangle /home/basti/phd/code/detector_efficiency.nim
import std / strutils
import xrayAttenuation, ggplotnim
# generate a compound of silicon and nitrogen with correct number of atoms
let Si₃N₄ = compound((Si, 3), (N, 4))
let al = Aluminium.init()

# define energies in which to compute the transmission
# (we don't start at 0, as at 0 energy the parameters are not well defined)
let energies = linspace(0.03, 10.0, 1000)

# instantiate an Argon instance
let ar = Argon.init()
# and isobutane
let iso = compound((C, 4), (H, 10))

proc compTrans[T: AnyCompound](el: T, ρ: g•cm⁻³, length: Meter): Column =
  let df = toDf({ "Energy [keV]" : energies })
    .mutate(f{float: "μ" ~ el.attenuationCoefficient(idx("Energy [keV]").keV).float},
            f{float: "Trans" ~ transmission(`μ`.cm²•g⁻¹, ρ, length).float},
            f{"Compound" <- el.name()})
  result = df["Trans"]
    
var df = toDf({ "Energy [keV]" : energies })
# compute transmission for Si₃N₄ (known density and desired length)
df[Si₃N₄.name()] = Si₃N₄.compTrans(3.44.g•cm⁻³, 300.nm.to(Meter))
# and aluminum coating
df[al.name()] = al.compTrans(2.7.g•cm⁻³, 20.nm.to(Meter))

# and now for the gas mixture.
# first compute partial pressures
const fracAr = 0.977
const fracIso = 0.023
# using it we can compute the density of each by partial pressure theorem (Dalton's law)
let ρ_Ar = density(1050.mbar.to(Pascal) * fracAr, 293.K, ar.molarMass)
let ρ_Iso = density(1050.mbar.to(Pascal) * fracIso, 293.K, iso.molarWeight)

# now add transmission of argon and iso
df[ar.name()] = ar.compTrans(ρ_Ar, 3.cm.to(Meter))
df[iso.name()] = iso.compTrans(ρ_Iso, 3.cm.to(Meter))

let nSiN = r"$\SI{300}{nm}$ $\ce{Si_3 N_4}$"
let nAl = r"$\SI{20}{nm}$ $\ce{Al}$"
let nAr = r"$\SI{3}{cm}$ $\ce{Ar}$ Absorption"
let nIso = r"$\SI{3}{cm}$ $\ce{iC_4 H_{10}}$ Absorption"
let nArIso = r"$\SI{3}{cm}$ $\SI{97.7}{\percent} \ce{Ar} / \SI{2.3}{\percent} \ce{iC_4 H_{10}}$"

# finally just need to combine all of them in useful ways
# - argon + iso
df = df.mutate(f{"Trans_ArIso" ~ `Argon` * `C4H10`},
               f{"Abs ArIso" ~ 1.0 - `Trans_ArIso`},
               f{"Abs Ar" ~ 1.0 - `Argon`},
               f{"Abs Iso" ~ 1.0 - `C4H10`},
               f{"Efficiency" ~ idx("Abs ArIso") * `Si3N4` * `Aluminium`})
  .rename(f{nSiN <- "Si3N4"},
          f{nAl <- "Aluminium"},
          f{nAr <- "Abs Ar"},
          f{nIso <- "Abs Iso"},
          f{nArIso <- "Abs ArIso"}) # ,                    
  .gather([nSiN, nAl, nAr, nIso, nArIso, "Efficiency"], "Material", "Efficiency")

echo "Mean efficiency 0-3  keV = ", df.filter(f{idx("Energy [keV]") < 3.0})["Efficiency", float].mean  
echo "Mean efficiency 0-5  keV = ", df.filter(f{idx("Energy [keV]") < 5.0})["Efficiency", float].mean
echo "Mean efficiency 0-10 keV = ", df.filter(f{idx("Energy [keV]") < 10.0})["Efficiency", float].mean

ggplot(df, aes("Energy [keV]", "Efficiency", color = "Material")) +
  geom_line() +
  xlab("Energy [keV]") + ylab("Efficiency") +
  xlim(0.0, 10.0) + 
  ggtitle(r"Transmission (absorption for gases) of relevant detector materials and combined \\" &
    "detection efficiency of the Septemboard detector",
    titleFont = font(12.0)) +
  margin(top = 1.5, right = 2.0) +
  titlePosition(0.0, 0.8) + 
  legendPosition(0.42, 0.15) + 
  ggsave("/home/basti/phd/Figs/detector/detector_efficiency.pdf",
         width = 600, height = 400,
         #width = 800, height = 600,
         useTex = true, standalone = true) 
#+end_src
#+RESULTS:
Mean efficiency 0-3  keV = 0.7342084765204602
Mean efficiency 0-5  keV = 0.7544999372201439
Mean efficiency 0-10 keV = 0.6783959312693081


* Data acquisition and detector monitoring          :Detector:
:PROPERTIES:
:CUSTOM_ID: sec:daq
:END:
#+LATEX: \minitoc
Having introduced the detector used for the data taking in this
thesis, we will now introduce the data acquisition (DAQ) software for the
detector (sec. [[#sec:daq:tof]] and [[#sec:daq:tos]]), discuss the data
formats used for readout as well as the logging facilities. Further,
we will introduce the calibrations performed for the Timepix to
achieve a correct operation with a GridPix detector as well as the
calibrations for the scintillators and FADC. Finally, we will present
the monitoring tools to monitor the detector operation (different
event displays) which are used for different data taking purposes.

- introduce TOS
- introduce data format
- logging data (temperature)
- different timepix calibrations (here?) maybe just introduce
  different calibrations and then in data taking part talk about what
  they actually look like?

- [X] *INTRODUCE DAQ ACRONYM*  

** Timepix Operating Firmware - TOF [0/2]
:PROPERTIES:
:CUSTOM_ID: sec:daq:tof
:END:

Starting with the firmware of the detector, the \textbf{T}imepix
\textbf{O}perating \textbf{F}irmware (TOF), which runs on the Virtex-6
FPGA, specifically a Xilinx Virtex-6 (V6) ML605 evaluation board. TOF
controls the Timepix ASICs of the Septemboard (both the slow control
aspects and data taking) as well as coordinating the scintillator
signals and FADC trigger. It is a VHDL project, intended to run at a
clock frequency of $\SI{40}{MHz}$. Communication with the GridPixes is
done via two \textbf{H}igh-\textbf{D}efinition \textbf{M}ultimedia
\textbf{I}nterface (HDMI) cables, while communication with the readout
software on the DAQ computer is handled via Ethernet.

For a detailed introduction to TOF, see cite:lupberger2016pixel as
well as cite:schiffer_phd.

The firmware versions used for each data taking period can be found in
appendix [[#sec:appendix:configuration:tof_versions]].

*** TODOs for this section                                       :noexport:

- [X] *CHECK IF VHDL*
  -> Confirmed by Tobi on discord.
- [X] *LUPBERGER THESIS*

** Timepix Operating Software - TOS [0/2]
:PROPERTIES:
:CUSTOM_ID: sec:daq:tos
:END:

The \textbf{T}imepix \textbf{O}perating \textbf{S}ystem (TOS) is the
computer-side data acquisition software to read out Timepix based
detectors. It is an object oriented C++ project, available at cite:TOS_github. [fn:TOS_versions] 
The project needs to be used in conjunction with the \textbf{T}imepix
\textbf{O}perating \textbf{F}irmware (TOF), which communicates with
TOS via Ethernet. The TOS project started as far back as 2009 by
people at the University of Mainz. Next is a short overview over the
basic blocks that make up the main logic of the software. 

The fully object oriented nature of the project means that there are
different classes for the different software pieces:
- =Console= :: A class representing the user facing REPL
  (Read-Evaluate-Print Loop, an 'interpreter') to control the
  software
- =PC= :: A class representing the network layer and communication side
  of the software, sitting between the console and lower layers.
- =FPGA= :: A class representing the functionality required to control
  the FPGA on the Virtex-6 evaluation board.
- =Chip= :: A class representing each Timepix ASIC and its
  functionality.
- =HFManager= :: A class unifying the FADC & Wiener HV control unit as
  they are both controlled via USB, installed in a VME crate. This
  class contains individual attributes that contain explicit classes
  for these two devices. The name is shortened for 'High Voltage and
  FADC Manager'.
  - =V1729= :: A class representing the Ortec Flash ADC.
  - =HV*= :: Multiple classes representing HV channels, groups and more.
- =MCP2210= :: A class representing the PT1000 temperature sensors
  installed on the detector via a =MCP2210= micro controller,
  optionally connected via USB. The actual micro-controllers with
  attached PT1000s are =MAX31685= models.
- Misc :: there are a few further classes of minor interest to the general
  functionality of TOS (tab command completion and history, classes
  to set masks on the chips, etc.)

In general TOS is a fully command line driven software package, with
its own REPL (Read-Evaluate-Print Loop; the name for an
interactive terminal process, which takes commands that are evaluated
and returns to the terminal). It brings all the expected features one
might wish from a REPL, including auto completion, history lookup,
emacs style keybindings (based on GNU Readline [fn:gnu_readline]) and more.

The aforementioned =HFManager= and the temperature sensors are
optional pieces that are not required for basic Timepix
operation. Their functionality has to be activated via a manual
command, =ActivateHFM=. This triggers the USB connection to the VME
crate and tries to find the Wiener HV module as well as the Ortec FADC
in the crate. Additionally, the temperature sensors are attempted to
be found (via a secondary, optional USB connection). If the latter are
found a continuous temperature logging begins (see
sec. [[#sec:daq:temperature_readout]]). 

The HV controls are specific to Wiener HV power supplies. In principle
the implemented functionality is a fully featured HV controller that
supports all Wiener functionality like grouping different channels to
ramp up together, kill channels on a trip and more. Most importantly
it implements a custom slow HV ramping logic, which keeps the
relative potentials constant between channels in a group to avoid
tripping a channel.

An example of a typical startup procedure is shown in listing
[[TOS_startup_commands]], in this case to start a background run. Note
that most essential commands in TOS also have shortened names via
numbers, due to historic reasons (TOS originally did not have
autocompletion or allowed moving the cursor in text input, making
typing complex names cumbersome and error prone), which is why many of
the inputs are simple numbers.

#+CAPTION: An example of the typical startup routine of TOS for a background data taking measurement at CAST
#+CAPTION: for the Septemboard based GridPix detector. The indented lines refer to commands given to the 
#+CAPTION: previous command at top level.
#+LABEL: TOS_startup_commands
#+begin_src sh
user@ingrid-DAQ~/ ./TOS
  > 7 # number of chips
  > 4 # preload
> SetChipIDOffset
  > 190
> lf # load FSR values for the chips
  > # return 7 times enter to load default paths
> uma # create a uniform matrix for all chips
  > 1 # Matrix settings
  > 0
  > 1
  > 1
  > 0
> LoadThreshold # load threshold equalisation files
  > 4 # write matrix
  > 3 # read out
  > 3 # 2nd readout to make sure pixels are 'empty'
> ActivateHFM # startup HV & FADC controls
> SetFadcSettings # load the FADC settings
> Run # start a data taking run
  > 1 # run time via # frames
  > 0
  > 0
  > 0
  > 2 # shutter range select
  > 30 # shutter time select (2 + 30 yields ~2.2 s frames)
  > 0 # zero suppression
  > 1 # FADC usage
  > 0 # accept FADC settings
#+end_src

[fn:TOS_versions] There are unfortunately 2 different versions of TOS,
as development diverged for different readout systems. One version is
for the Xilinx Virtex-6 (V6) ML605 evaluation board and the other for
the \textbf{S}calable \textbf{R}eadout \textbf{S}ystem (SRS). The V6
version can read out only a single detector (with up to 8 Timepix
ASICs), but supports readout of an Ortec FADC and controlling a Wiener
HV module via VME. The SRS version instead supports neither of these
additional features, but supports multiple detectors at the same
time. The detector used in this thesis is read out using the Virtex-6
board.

[fn:gnu_readline] https://tiswww.case.edu/php/chet/readline/rltop.html


*** TODOs for this section [/]                                   :noexport:

*ACCORDING TO WIENER VME MANUAL ALL FILES
ARE OPEN SOURCE*
https://wikihost.nscl.msu.edu/S800Doc/lib/exe/fetch.php?media=wiki:manual_vm-usb_9_01_1.pdf
[[file:~/org/Papers/manual_vm-usb_9_01_1.pdf]]
-> page 13

- [ ] *PUSH TOS TO GITHUB OR SIMILAR AND REFERENCE*
  -> Pushed, but not public yet!
- [ ] *CHECK AGAIN WIENER VME SOURCES OPEN SOURCE*
  -> See above.
- [X] *ADD FULL NAME OF V6 BOARD*

Link to repositories (maybe we can make the Virtex TOS public?) 

- [ ] *Link to TOF firmware.*
  -> ??? I guess they are on our office computer?

- [X] *Septem event display example.* Section further down

- [X] *INSERT TOS CONFIG FILES SOMEWHERE*
  -> Appendix, but there's a configuration section. Referred there.
  
*** TOS output data format
:PROPERTIES:
:CUSTOM_ID: sec:daq:tos_output_format
:END:

When starting a data taking run with TOS, a new directory for the run
is created in the ~data/runs~ subdirectory. The name will be of the
form ~Run_<run number>_YYMMdd-HH-mm~ where the run number is an
increasing number based on the last run present in the directory and
the suffix is the date and time of day when starting the run. This
directory contains the configuration of all DACs for each chip,
~fsr<chip number>.txt~, the written configuration matrix for all
pixels, ~matrix<chip number>.txt~ and finally the data files
~data<event number>.txt~. If an FADC is used for the readout additional
~data<event number>.txt-fadc~ files are created, one for each file the
FADC triggered (sec. [[#sec:daq:fadc_data_files]]). 

The Timepix data files are stored -- for historic reasons -- in raw ASCII
format. Two different readout modes (with different output
formats) are supported. For the following explanation it is assumed
the Timepix is used in the ToT (Time-over-Threshold) mode.
- full matrix readout :: reads out the whole Timepix ASIC(s) and writes
   a single 256x256 pixel matrix as an ASCII file (for each chip). 256
   lines, each containing space separated ToT values for each pixel.
- zero suppressed readout :: reads out only those pixels that have ToT
   values larger than 0, up to \num{4096} pixels. Stores the data in
   TSV files (tab separated values) '=X Y ToT=' with an additional
   header. The header contains a global "run" and "event" header,
   which contains information about the run the event is taken from
   and a "chip" header, which contains information about the specific
   Timepix ASIC(s) being read out (up to 8 can be read out at the same
   time using TOS).
   
As for our purposes most events are extremely sparse (< 500 pixels
active) the zero suppressed readout is the only relevant readout
mode. The data files can be split into 3 distinct parts. A global run
header, see listing [[code:daq:zero_suppressed_readout_run_header]], which
contains information about the run the event is part of including
important settings used as well as the timestamp of the event. Next is
an event specific header, which contains specific information about
the event in relation to the FADC and the scintillators, see listing
[[code:daq:zero_suppressed_readout_event_header]]. The final part of the
zero suppressed data files is the chip header and tab separated value
part of the '~X Y ToT~' pairs of the active pixels for each chip of
the detector in that event, see listing
[[code:daq:zero_suppressed_readout_chips]].

#+CAPTION: TOS generated data files start with a general header, which mainly contains 
#+CAPTION: information about the run the data file is part of. The only exception is the
#+CAPTION: =dateTime= field, which represents the timestamp of the event. [fn:daq_datetime_header]
#+NAME: code:daq:zero_suppressed_readout_run_header
#+begin_src toml
## [General]
## runNumber:        339
## runTime:          7200
## runTimeFrames:    0
## pathName:         data/runs/Run_339_190218-10-36
## dateTime:         2019-02-18.10:36:34
## numChips:         7
## shutterTime:      2
## shutterMode:      verylong
## runMode:          0
## fastClock:        0
## externalTrigger:  0
#+end_src

#+CAPTION: After the general header follows the event header in similar fashion. It records
#+CAPTION: the event number and information about the FADC and scintillators. If the FADC triggered
#+CAPTION: ~fadcReadout~ is 1. Scintillator triggers may then be values in $[0, 4096)$. The ~fadcTriggerClock~
#+CAPTION: is the clock cycle of the Timepix frame in which the FADC trigger was received.
#+NAME: code:daq:zero_suppressed_readout_event_header
#+begin_src sh
## [Event]
## eventNumber:      2
## useHvFadc:        1
## fadcReadout:      1
## szint1ClockInt:   0
## szint2ClockInt:   0
## fadcTriggerClock: 647246
#+end_src

#+CAPTION: The event header is followed by the beginning of the actual GridPix data. Each chip
#+CAPTION: appears with a 3 line chip header containing number and name as well as the number
#+CAPTION: of hits seen by that chip in the event. ~numHits~ lines follow with '~X Y TOT~' values
#+CAPTION: in tab seperated fashion. This snippet would be followed by the remaining chips, 
#+CAPTION: as many as written in the run header [[code:daq:zero_suppressed_readout_run_header]]
#+CAPTION: as ~numChips~.
#+NAME: code:daq:zero_suppressed_readout_chips
#+begin_src sh
# chipNumber: 0
# chipName:   E 6 W69
# numHits:    0
# chipNumber: 1
# chipName:   K 6 W69
# numHits:    0
# chipNumber: 2
# chipName:   H 9 W69
# numHits:    2
106     160     75
211     142     2
#+end_src

[fn:daq_datetime_header] It is an oversight that the =dateTime= field
is part of the =[General]= header instead of the =[Event]= header.

**** TODOs for this section                                     :noexport:

- [ ] *MAYBE* move this section _after_ the configuration file? That
  way the FADC explanation makes a bit more sense? For the HV of
  course it is pretty much the same.

- [ ] The below was still mentioned in the section above, but I don't
  think this is needed here anymore, given that we explain this
  before:
  #+begin_quote
  The Timepix is only capable of shutter based readouts. Typically, a
  fixed shutter is used. The readout is complicated for the case of
  using an FADC, in which case an FADC signal can be used as an external
  trigger to close the shutter early. This will be further explained in
  the FADC section, [[FADC]]
  *REWRITE THIS PART, REFER TO SCHEMATIC ABOUT TIMEPIX*
  #+end_quote

- [X] TOS needs to talk about data format that was used in V6 TOS. Stupid
  ASCII files. Mention that in hindsight the time should have been
  invested to either use a really simple binary format (like NIO) or HDF5
  (even if painful from C++).

- [X] *EXPLAIN RUN DATA LOCATION. DATA STORED IN =data/runs= AND
  DIRECTORY NAME SCHEMA*
- [ ] *HV AND WIENER VME EXPLANATION*  
- [X] *POSSIBLY SPLIT ZERO SUPPRESSED OUTPUT INTO CHUNKS? !! GENERAL
  HEADER, EVENT HEADER, CHIP HEADER, DATA*
- [ ] *ADD SECTION (EVEN IF POSSIBLY NOEXPORT) EXPLAINING THE FIELIDS
  OF THE DATA FILE*
- [ ] *ADD EXPLANATION OF HOW TO CALCULATE SHUTTER LENGTH. POSSIBLY
  SOMEWHERE HERE? OR IN TIMEPIX INTRO? NEED IT LATER TO EXPLAIN LENGTH
  OF EVENTS*
  -> This is explained later where we talk about extracting
  information from the data files in sec. [[#sec:reco:event_duration]].

**** FADC data files [/]
:PROPERTIES:
:CUSTOM_ID: sec:daq:fadc_data_files
:END:

If the FADC triggered during an event, as indicated by the
~fadcReadout~ field in the event header seen in listing
[[code:daq:zero_suppressed_readout_event_header]], an additional data file
is written with the same name as the event file, but a ~.txt-fadc~
extension. It contains a memory dump of the channels of the circular
memory of the FADC plus a basic header about the FADC settings and the
information about when the trigger happened.

The different fields in the header, see listing
[[code:daq:fadc_data_header]], are as follows:
- ~nb of channels~: decimal value of a 4-bit field that decides the
  number of active channels. ~0~ corresponds to using all channels as
  separate. We only use a single channel. [fn:fadc_chosen_settings]
- ~channel mask~: decimal value of a 4-bit field to (de-)activate
  channels. ~15~ corresponds to all 4 channels active.
- ~posttrig~: how many clock cycles in the $\SI{50}{MHz}$ [fn:base_clock] base clock of
  the FADC it continues taking data before commencing the readout
  (useful to record the rest of the signal and center it in the
  readout window)
- ~pretrig~: the minimum acquisition time before a trigger is allowed
  to happen, in units of the $\SI{50}{MHz}$ [fn:base_clock] base clock.
- ~triggerrecord~: together with ~posttrig~ allows to reconstruct the
  time of the trigger in the acquisition window
- ~frequency~: decimal representation of a 6-bit field to select the
  operating frequency. ~2 = 0b000010~ corresponds to $\SI{1}{GHz}$
  operation.
- ~sampling mode~: decimal representation of a 3-bit field changing
  the operation mode (manual or automatic trigger) and register
  working mode (12 or 14-bit sensitivity of each register). We run in
  manual trigger and 12-bit mode. [fn:fadc_chosen_settings]
- ~pedestal run~: a 1-bit flag indicating whether this file is a
  pedestal run.

#+CAPTION: The file starts with a header indicated by ~#~. Some of the values are
#+CAPTION: decimal representation of bit fields, hence the weird values like
#+CAPTION: "0 channels". It mixes both the configuration used as well as the
#+CAPTION: time the trigger occurred (~triggerrecord~). 
#+NAME: code:daq:fadc_data_header
#+begin_src sh
# nb of channels: 0  
# channel mask: 15
# posttrig: 80
# pretrig: 15000
# triggerrecord: 56
# frequency: 2
# sampling mode: 0
# pedestal run: 0
#+end_src

The data portion starts with another semi-header of 12 data points,
see listing [[code:daq:fadc_data_header_2]]. It contains fields that are
not explained in the FADC manual, but instead refer to "reserved for
expert usage" cite:fadc_manual. One exception is data point 2, which
is the so called Vernier, which could be used to determine the trigger
time within two registers to get up to $\sim\SI{50}{ps}$ RMS accurate
time information. For our purposes though $\SI{1}{ns}$ time resolution
is more than enough, given the signal undergoes integration and
differentiation of a multiple of that in the shaping amplifier, anyway.

#+CAPTION: After the header starts the data portion with some auxiliary information.
#+CAPTION: The lines are neither of significant interest to us, nor are they properly
#+CAPTION: explained in the manual. The second number corresponds to the Vernier, which
#+CAPTION: can be used to determine the trigger more precisely than between individual
#+CAPTION: register values, which is also not important for our purposes, as $\SI{1}{ns}$
#+CAPTION: resolution is plenty.
#+NAME: code:daq:fadc_data_header_2
#+begin_src sh
# Data:
# 3928
# 8022
# 3957
# 8076
# 3928
# 8023
# 3957
# 8077
# 2048
# 6138
# 2031
# 6151
#+end_src

The final portion of the file contains the actual data ($\num{10240}$
lines) and 3 fields at the very end to reconstruct the trigger within
the acquisition window [fn:fadc_data_last_3_lines]. The data
represents a pure memory dump of the cyclic register. See listing
[[code:daq:fadc_data_raw]] for a shortened example.

#+CAPTION: Actual data portion of the FADC data. The first $\num{10240}$ lines represent
#+CAPTIOn: a memory dump of the cyclic registers at trigger time (that is in their natural
#+CAPTION: order instead of starting from the register in which the trigger was recorded).
#+CAPTION: It starts at register 0 for channel 0, followed by register 0 of channel 1, and so on.
#+CAPTION: As such each $4^{\text{th}}$ line corresponds to one channel. This is why the
#+CAPTION: values jump so much from line to line.
#+CAPTION: The last 3 lines are information to recover the trigger point in the acquisition
#+CAPTION: window [fn:fadc_data_last_3_lines].
#+NAME: code:daq:fadc_data_raw
#+begin_src sh
2028 # register 0, channel 0
6119 # register 0, channel 1
1999 # register 0, channel 2
6100 # register 0, channel 3
2021 # register 1, channel 0
6108 # ...
... # 10240 lines of data in total
# 56
# 4096
# 0
#+end_src

[fn:base_clock] If running in $\SI{1}{GHz}$ mode. Else it corresponds to
$\SI{100}{MHz}$ clocks.

[fn:fadc_chosen_settings] As of writing this thesis, I don't remember
why the choice was made to only use a single channel instead of using
all 4 channels to extend the time interval (development of these
things happened between 2015-2017). It's possible there were issues
trying to combine all 4 channels. But it's also just as likely it was
an oversight due to lack of time combined with the fact that a
$\SI{2.5}{μs}$ window is long enough for all intents and
purposes. However, combining all 4 channels would even yield a long
enough acquisition window when running in the $\SI{2}{GHz}$ sampling
mode. Similarly, the choice of the 12-bit readout mode may represent
plenty resolution in ADC values, but it seems prudent to not use the
14-bit mode given availability. All in all it leaves me head
scratching (and thinking the likely reason will have been lack of time
and being happy things working in the first place at the time).

[fn:fadc_data_last_3_lines] The last 3 lines of the data portion
contain the trigger record, which is already printed by us in the
header part and the ~Valp_cp~ and ~Vali_cp~ registers, which are only
important if the FADC is used at a sampling frequency of $<
\SI{1}{GHz}$, which is why we ignore it here.

***** TODOs for this section [/]                               :noexport:

- [ ] Maybe make this a full section and not subsection of TOS output format?

- [X] *Introduce FADC data files.*
- [X] *VERIFY THE EXPLANATION OF THE HEADER FIELDS*
- [X] *ISN'T THE POSTTRIG VALUE SOMETHING ONE SHOULD TAKE INTO ACCOUNT
  WHEN COMPUTING THE TIME OF THE ACTUAL TRIGGER IN RELATION TO OTHER
  DETECTOR FEATURES?*
  -> No, because our explanation was wrong. It has nothing to do with
  when the trigger is sent! It is sent, whenever it appears. It just
  changes the time *after the trigger* that the FADC continues
  recording data into the registers to record full signal shapes and
  center the signal!

***** Explanation of beginning of data portion                 :noexport:

Explanation of the data header [[code:daq:fadc_data_header_2]] here:
- =hvFadcManager.cpp= =writeFadcData=
- FADC manual page 27 lists the data sent by the FADC.
  
However, neither explains what the "first sample" and the "rest
baseline" is. The manual calls these "expert features" and doesn't
explain them... That's how you keep it as an expert feature! The
second line is the Vernier, that I honestly don't really understand
either. I think it allows to more precisely find the time of trigger
between two register entries? Ah yes, see page 14 of the manual about
the Vernier. Allows for 50 ps RMS time information between two bins.

***** Explanation of FADC settings                             :noexport:

As mentioned in the footnote in the previous section, I really don't
understand the choice of FADC settings we used for the actual data
taking. It's quite possible I'm nowadays just not aware of something
important, but well. Either way, unfortunately I only started being
serious about note taking about my work around the beginning of the
data taking period in October 2017. So retracing my thoughts during my
master thesis (2015-2016) and beginning of my PhD is unfortunately pretty much
impossible.

However, as mentioned the settings are good enough for what we are
doing with the data. The much bigger issues are related to the noise
we observed at times etc., which will be mentioned later.

*** TOS configuration file
:PROPERTIES:
:CUSTOM_ID: sec:daq:tos_config_file
:END:

Everything related to the =HFManager= in TOS is controlled by a
configuration file, normally located in
=TOS/config/HFM_settings.ini=. The TOS configuration files used during
CAST data taking are found in appendix
[[#sec:appendix:configuration:tos_config]]. We will go through the
sections of it one by one and explain them.

Starting with the =[General]= section, listing
[[code:daq:general_config]]. This section defines the VME related
settings. The VME address of the HV module installed in the crate is
used as the base address. The FADC address in the same VME crate is
calculated from an offset in units of the VME address spacing of
=0x0400=. [fn:base_address_hv]

#+CAPTION: General section of the TOS configuration file. It sets the base address
#+CAPTION: of the HV module installed in the VME crate. The FADC address is given
#+CAPTION: as an offset from the base address.
#+NAME: code:daq:general_config
#+begin_src toml
[General]
sAddress_fadc = 1
baseAddress_hv = 0x4000
#+end_src 

The next section =[HvModule]=, listing [[code:daq:hv_module_config]], are
general settings about the used HV module. The settings are related to
the =KillEnable= feature of Wiener HV power supplies, the ramping
speed of the HV channels and the time in seconds between sanity checks
of the HV during data taking. [fn:check_hv_interval_setting]

#+CAPTION: The =[HvModule]= section contains settings related to the HV module as a whole.
#+CAPTION: Whether a single channel tripping causes all channels to ramp down (=KillEnable=)
#+CAPTION: the ramp speed and interval in which the HV module sanity status is checked.
#+NAME: code:daq:hv_module_config
#+begin_src toml
[HvModule]
setKillEnable                   = true
# Voltage and Current RampSped currently set to arbitrary value
# in percent / second
moduleVoltageRampSpeed          = 0.1
moduleCurrentRampSpeed          = 50
# checkModuleTimeInterval       = 60, checks the status of the
# module every 60 seconds during a Run, between two events
checkModuleTimeInterval         = 60
#+end_src

Next up, the =[HvGroups]= section in listing [[code:daq:hv_groups_config]]
defines the different groups that combine multiple channels. There are
multiple different kinds of groups in Wiener HV power supplies. The
important groups are ramping groups and trip groups. Essentially, if
one channel in a group starts ramping / trips all others also start
ramping / shut off the HV, respectively. The section in the config
file mainly exposes the already predefined sets of groups that are
relevant for the Septemboard detector in TOS. 

#+CAPTION: =[HvGroups]= defines multiple groups of different HV channels together. The
#+CAPTION: config file does not expose arbitrary groupings, but only sets flags whether
#+CAPTION: groups are active and what their numbers are.
#+NAME: code:daq:hv_groups_config
#+begin_src toml
# if this flag is set to true, anode and grid
# will be coupled to one group
[HvGroups]
anodeGridGroupFlag              = true
# grid is master channel of set on group
anodeGridGroupMasterChannel     = 4
anodeGridGroupNumber            = 0
monitorTripGroupFlag            = true
monitorTripGroupNumber          = 1
rampingGroupFlag                = true
rampingGroupNumber              = 2			     
gridChannelNumber               = 4
anodeChannelNumber              = 5
cathodeChannelNumber            = 8
#+end_src

After the =[HvGroups]= section is the definition of the individual HV
channels in listing [[code:daq:hv_channels_config]]. Here the physical
channels on the device are mapped to the desired voltages and current
bounds as well as to a human readable name. The fields repeat with
increasing prefix numbers.

#+CAPTION: This is an excerpt of the full =[HvChannels]= section for a single HV channel.
#+CAPTION: It maps the physical HV connectors to their voltages, current bounds and
#+CAPTION: a human readable name. In this case the grid of the GridPixes of the Septemboard
#+CAPTION: all receive a voltage of $\SI{300}{V}$. The naming scheme of the fields is 
#+CAPTION: hardcoded for practical reasons and simply repeats with increasing numbers.
#+NAME: code:daq:hv_channels_config
#+begin_src toml
[HvChannels]
# all currents given in A (vmecontrol shows mA)
0_Name                          = grid
0_Number                        = 5
0_VoltageSet                    = 300
0_VoltageNominal                = 500
0_VoltageBound                  = 2.5
0_CurrentSet                    = 0.000050
0_CurrentNominal                = 0.000500 
0_CurrentBound                  = 0
#+end_src

Second to last is the =[FADC]= section in listing
[[code:daq:fadc_config]]. As the name implies it configures all parameters
of the FADC. The main parameter to change is the
=fadcTriggerThresholdRegisterAll= parameter, which defines the trigger
threshold in effectively $\si{mV}$. Depending on the amount of noise in
the system, adjustments to the threshold may be necessary. 

#+CAPTION: The =[FADC]= section configures the FADC. The most important setting is the trigger
#+CAPTION: threshold as it defines the voltage required to trigger the FADC. 
#+NAME: code:daq:fadc_config
#+begin_src toml
[Fadc] # FADC Settings
fadcTriggerType                 = 3 
fadcFrequency                   = 2
fadcPosttrig                    = 80
fadcPretrig                     = 15000
# was 2033 before, 1966 corresponds to -40 mV
fadcTriggerThresholdRegisterAll = 1966 
# run time of a single pedestal run for the FADC in ms
fadcPedestalRunTime             = 100
# number of acquisition runs done for each pedestal calibration
fadcPedestalNumRuns             = 10
# using channel 0 on FADC as trigger source, thus bit 0 = 1!
fadcChannelSource               = 1
# set FADC mode register (mainly to enable 14-bit readout)
fadcModeRegister                = 0b000
#+end_src

The last section of the configuration file is the =[Temperature]=
section, which deals with the safety ranges of the temperature of the
detector. If the temperature leaves the safe range, the detector is to
be shut down. [fn:temperature_safety_config_settings]

#+CAPTION: =[Temperature]= defines the safe operating ranges of the detector. If the
#+CAPTION: range is left, the detector is to be shut down. 
#+NAME: code:daq:temperature_config
#+begin_src toml
[Temperature] # temperature related parameters, all temps in °C
safeUpperTempIMB                = 61
safeUpperTempSeptem             = 61
safeLowerTempIMB                = 0
safeLowerTempSeptem             = 0
#+end_src

[fn:base_address_hv] If in doubt about what the base address of the HV
supply in the VME crate is, start one of the Wiener HV programs (for
example =isegControl=), as it auto detects the module and prints the
address.

[fn:check_hv_interval_setting] The =checkModuleTimeInterval= setting
to check the HV status during the data taking was disabled at CAST, as
it caused issues due to false alarms of the HV status. Given that the
=KillEnable= flag was used, it was deemed unimportant. Attempting to
fix it would have caused possible data loss as it would have been
tested on the live detector.

[fn:temperature_safety_config_settings] The temperature safety range
is coupled to the =checkModuleTimeInterval= setting in the previous
footnote. It was disabled together with the above during actual data
taking.

**** TODOs about this section [/]                               :noexport:

- [ ] *MAYBE* move the FADC parts to the explanation of the FADC data
  files? I'm not sure the order of things makes so much sense as it is
  now. At no point are we actually explaining what the different
  settings really are!
  -> Maybe the config file could actually be introduced before the
  data files are explained?

**** Understanding FADC settings                                :extended:

- [ ] *This should at least* be an :extended: section with an
  explanation of all the fields that one can actually set.

Given the time since last working with this, I need to look up the
values of the TOS config file in the FADC manual.

For reference our settings:
#+begin_src toml
[Fadc] # FADC Settings
fadcTriggerType                 = 3 
fadcFrequency                   = 2
fadcPosttrig                    = 80
fadcPretrig                     = 15000
# was 2033 before, 1966 corresponds to -40 mV
fadcTriggerThresholdRegisterAll = 1966 
# run time of a single pedestal run for the FADC in ms
fadcPedestalRunTime             = 100
# number of acquisition runs done for each pedestal calibration
fadcPedestalNumRuns             = 10
# using channel 0 on FADC as trigger source, thus bit 0  1!
fadcChannelSource               = 1
# set FADC mode register (mainly to enable 14-bit readout)
fadcModeRegister                = 0b000
#+end_src

=FP_FREQUENCY= is the name for the address =0x01= for the. It needs 6
bits of data:
#+begin_src 
Bits 0-5 Function
Val = 1 => Fsample = 2GHz.
Val = 2 => Fsample = 1GHz.
Val = 4 => Fsample = 500MHz.
Val = 5 => Fsample = 400MHz.
Val = 10 => Fsample = 200MHz.
Val = 20 => Fsample = 100MHz.
Val = 40 => Fsample = 50MHz.
#+end_src
As such our used value of ~fadcFrequency = 2~ corresponds to
$\SI{1}{GHz}$ as I remembered.

=MODE_REGISTER= is for value =0x03= and its 3 bits of data.
[[/home/basti/phd/Figs/fadc_settings_mode_register.png]]
Our value of =0b000= for the data means no interruption tagging, 12
bits data output and normal acquisition mode.

The ~fadcTriggerThresholdRegisterAll~ controls the trigger threshold
of the FADC. See [[cite:&fadc_manual]] page 31-32:
#+begin_quote
TRIGGER THRESHOLD DAC : common pre-loading register of the DACs. This
12-bit register covers the range from –1V (000) to +1V (FFF). By USB
or GPIB, one has access to the MSBs and LSBs via 2 distinct
sub-addresses. The access is necessarily made in the order MSB (0B)
then LSB (0A). By VME, the access is made via a single sub-address
(0A). After loading of this register, one must transfer the value in
the analog converter via the LOAD_TRIGGER THRESHOLD DAC (09) command.
#+end_quote

This implies the trigger is calculated by:
#+begin_src nim
import unchained
const U_range = 2.V
const DAC_range = 4096
proc threshold(x: float): MilliVolt =
  result = (U_range / DAC_range * x - 1.V).to(mV)
echo threshold(1966.0) 
#+end_src

#+RESULTS:
: -40.0391 mV

which precisely reproduces our $\SI{-40}{mV}$ number.



*** HV control via TOS
:PROPERTIES:
:CUSTOM_ID: sec:daq:tos_hv_control
:END:

As is being alluded to in the previous section 
[[#sec:daq:tos_config_file]], the HV control built into TOS can also
handle ramping the channels of the detector. This is particularly
convenient as it offers a very smooth ramping mode, which keeps the
voltage potentials between all channels under a constant
ratio. This allows for automatic ramping even for highly sensitive
channels (like the GridPix grid).

In order to use the HV control and ramp the channels via TOS,
=ActivateHFM= must be followed by =InitHV=, which attempts to connect
to the HV power supply using the configuration of the config file. If
the channels are not ramped up, a call to =RampChannels= will start
the smooth ramping process (see listing [[code:daq:ramp_hv_channels]]).

#+CAPTION: The required commands to ramp the HV channels using the configuration
#+CAPTION: from the config file using a smooth ramping mode.
#+NAME: code:daq:ramp_hv_channels
#+begin_src sh
> ActivateHFM
> InitHV
> RampChannels
#+end_src

If the HV is to be ramped down, a call to =ShutdownHFM= will ask
whether the channels should be ramped down.

There are a multitude of further commands available to communicate
with the module, check the voltages, print status information etc.

Note that TOS can be started without ramping the HV channels and
stopped without ramping the channels down. It is capable of connecting
to a running HV power supply or leave it running after shut down. 

*** Temperature monitoring
:PROPERTIES:
:CUSTOM_ID: sec:daq:temperature_readout
:END:

The two =MAX31685= micro controllers read out =PT1000= sensors, a
group of \textbf{R}esistance \textbf{T}emperature \textbf{S}ensors
(RTDs), which measure temperatures by its effect on the electrical
resistance. They are platinum based and have a resistance of
$\SI{1000}{Ω}$ at $\SI{0}{\celsius}$. As the expected change in
resistance is well understood, the temperature can be precisely
measured.

The micro controller communicates with another micro controller, an
=MCP2210= via the \textbf{S}erial \textbf{P}eripheral
\textbf{I}nterface (SPI). SPI allows to address both =MAX31685= from
the single =MCP2210=. The =MCP2210= is a USB-to-SPI micro
controller. The USB connection from the intermediate board with the
computer is separate from the rest of the detector
communication. =TOS= communicates with it via the standard
\textbf{H}uman \textbf{I}nteface \textbf{D}evice (HID) driver and
utilizes an existing open source library for the =MCP2210=
cite:wong_mcp2210, which is slightly adapted.

The =ActivateHFM= command mentioned in the previous section also
attempts to find the USB device of the =MCP2210= (the two are
intertwined mainly, as the Septemboard detector is the only detector
with either of the two features). If it is found, temperature logging
starts immediately and the log files are placed in the default =log=
directory of the =TOS= repository. Once a data taking run starts,
the logging location is moved over to the data storage directory of
the run. In either case the log file is named =temp_log.txt= and
contains one temperature value for the intermediate board sensor
(=Temp_IMB=) and one for the carrier board sensor (=Temp_Septem=)
computed -- based on an average over $\SI{5}{s}$ -- and a timestamp
(=DateTime=). A short snippet of the temperature log is shown in listing
[[code:daq:temperature_readout]]. [fn:temp_logs_lost]

#+CAPTION: Snippet of a temperature log file as recorded for a run during the
#+CAPTION: CAST detector lab measurements. Tabs were replaced by spaces for
#+CAPTION: better visual alignment here.
#+NAME: code:daq:temperature_readout
#+begin_src sh
# Temperature log file
# Temp_IMB  Temp_Septem   DateTime
26.5186     42.1472       2019-02-16.16:11:45
26.5217     42.2798       2019-02-16.16:11:51
26.5202     42.4371       2019-02-16.16:11:57
26.5309     42.5944       2019-02-16.16:12:03
26.5324     42.7347       2019-02-16.16:12:09
26.5355     42.8627       2019-02-16.16:12:15
26.5416     42.9707       2019-02-16.16:12:21
26.5432     43.0771       2019-02-16.16:12:27
26.5616     43.1804       2019-02-16.16:12:33
...
#+end_src

[fn:temp_logs_lost] This default temperature logging location was also
used as an unintended fallback mechanism during data taking, if the HV
of the detector was considered out of certain bounds. Unfortunately,
the bounds checking was entangled with the HV module sanity checks. As
both features were very only implemented shortly before data taking,
they triggered data taking aborts. For that reason the feature was
disabled manually in code for the data taking at CAST. This however
triggered a secondary code path for the temperature logging, storing
it in the default location outside the specific run directories. As an
effect the majority of CAST temperature logging data has been lost, as
most of it was overwritten several times. About daily manual
temperature measurements are still left and show the detector
operating in a good temperature range. Precise correlations with
certain detector behaviors are unfortunately impossible. The two
different code paths for the temperature logging are essentially a bug
in the code that was never intended, stemming from the fact that
temperature logging must be done to a 'global' location outside of
data taking (as no data taking specific directory exists). Due to how
the temperature logging and HV & FADC controls were added to TOS,
these things were more entangled than necessary. A more thorough
testing period of the detector and software package should have been
performed, but was not in scope.

**** TODOs for this section [/]                                 :noexport:


- [ ] *SHOULD EXPLANATION OF THE READOUT _HARDWARE_ NOT BE IN THE
  DETECTOR CHAPTER?*
  -> Probably yes. In particular once / if we move the data readout
  parts of the detailed descriptions to the appendix, it becomes more
  important to put the actual hardware description into the detector
  chapter where we already mention it a bit.

- [X] *INSERT TEMPERATURE LOGGING EXAMPLE SNIPPET*
- [ ] *THINK ABOUT PUTTING FOOT NOTE INTO ACTUAL TEXT*
- [X] *ADD CITATION* https://github.com/kerrydwong/MCP2210-Library  


*** TOS development                                              :extended:

As mentioned in one of the footnotes in the previous section, there
are nowadays 2 independent versions of TOS.

The detector used for CAST in 2014/15 (and thus its successor used in
this thesis) was based on a readout using the Virtex 6 FPGA. This
system was, at the point I started on my master thesis in 2015,
already quite diverged from the SRS based system, which was mainly
developed for multi chip detectors that were initially planned for a
large GridPix based TPC to be used for the ILD (the detector planned
for the ILC, the International Linear Collider to be built in Japan).

In addition there was recent a master thesis (by Alexander Deisting
[[cite:&Deisting]]), which included work on using an FADC to read out the
induced charges on the grid of the InGrid by decoupling the signal
using a capacitor. The software library to interact with the used FADC
had partially been implemented into the Virtex 6 TOS.

As the FADC was an integral part in the new detector design, it was
natural to start with the Virtex 6 version. 

At the same time the SRS TOS version at the time was even more
ugly than the same code paths in the Virtex 6 TOS, due to its
hardcoded extra loops for each FEC of the TOS. At a later time the SRS
TOS was required for other detectors and so development effort was
spent on both systems unfortunately. 


** schematic of whole readout chain [0/1]  :noexport:

*ELSEWHERE AS WELL?*

Create a full flow chart of how everything is connected.

We have our notes about where each cable goes etc.

We have a schematic in the master thesis. That can be modified a bit
for the PhD thesis.

- [X] *THIS IS IN DETECTOR OVERVIEW NOW*

** Septemboard event display [0/0]

In order to monitor the data taking process while the detector is
running, an online event display tool was developed during the first
CAST data taking period in March 2017. It is a Python [fn:daq_python]
based project making heavy use of =matplotlib= cite:Hunter:2007 for
an interactive view of both the Septemboard events as they are
recorded, the FADC readout as well as a general information header
about the current data taking run.

The backend consists of a multiprocessing architecture with multiple
worker processes. One process watches the current run directory for
changes and reads the raw data files, another performs basic event
reconstruction and a last one updates the current event to be
displayed. The main process renders the =matplotlib= based graphical
user interface (GUI) [fn:daq_backend]. 

Fig. [[fig:daq:septemboard_event_display_example]] shows the graphical
user interface of the septemboard event display during a background
run. General information about the current run and event is shown in
the box at the top center. The top left box shows hit specific
information for the current event.  The current septemboard event is
always shown in the left pane in a realistic layout of the
septemboard. By default the Viridis [fn:viridis] color scale is used
in the display of the septemboard events, each shown as an image of
$(256, 256)$ pixels. If a chip did not record any activity during an
event, its plot remains white for easier identification of few hits
compared to no hits. The color scale can be adjusted when starting the
program and the images can be downsampled by any factor of 2 for
better visibility, as is done in
fig. [[fig:daq:septemboard_event_display_example]]. The right pane of the
event display shows the last recorded event of the FADC. It does not
automatically update the plot every time a new septemboard event is
recorded, as there can be multiple events without FADC activity and it
is useful to be able to glance at the last large event on the
FADC. The filename is printed as the title of the plot to show which
septemboard event it corresponds to.

#+CAPTION: Screenshot of the Septemboard event display showing a background
#+CAPTION: event from a CAST data taking run in 2017. The pixel density in the
#+CAPTION: septemboard on the left has been downsampled by a factor of 2 from
#+CAPTION: $(256, 256)$ for each chip to $(128, 128)$ for better visibilty of
#+CAPTION: the activity. The event display shows general information like run and
#+CAPTION: event number in the box at the top, hit specific information for the 
#+CAPTION: current event in the left top box and the current septemboard event
#+CAPTION: in the left pane. The right pane shows the last FADC event (if no
#+CAPTION: new FADC event is recorded, it stays).
#+NAME: fig:daq:septemboard_event_display_example
[[~/phd/Figs/daq/example_event_display.pdf]]

The event display provides multiple forms of interactivity, such as an
"auto follow" mode (the default in which new events are shown as they
are recorded), a "playback" mode (walks through all events with a
certain delay and general back and forth optionality. Further, a
shortcut to save images directly exists, as well as simple computation
of aggregate statistics of the current data taking period (different
occupancy maps and simple histograms showing the number of hits per
each event and chip).

All in all it provides a simple, but powerful way to monitor the
detector activity online as it takes data. [fn:daq_downsides]

[fn:daq_python] https://python.org

[fn:daq_backend] =matplotlib= provides a multitude of different GUI
backends. The explicit choice depends on the specific machine
(available backends may differ) and preference. Common choices are GTK
and TkAg. https://matplotlib.org/stable/users/explain/figure/backends.html

[fn:viridis] See here for the introduction of Viridis and its siblings: https://bids.github.io/colormap/

[fn:daq_downsides] The main drawbacks are related to it being a
Python based project that utilizes =matplotlib= possibly too
heavily. The combination means the tool is not useful for fast data
taking, as it is too slow to show events in real time if data taking
exceeds one frame per second significantly.

*** TODOs for this section [1/1]                                 :noexport:

- [X] *CITE SOMETHING FOR VIRIDIS COLORSCALE*

* Detector calibration for operation                               :Detector:
:PROPERTIES:
:CUSTOM_ID: sec:operation_calibration
:END:
#+LATEX: \minitoc

This chapter introduces the calibrations required to get the
Septemboard detector into a usable state for data taking at an
experiment. This includes calibrating the Timepix ASICs of the
Septemboard, sec. [[#sec:operation_calibration:timepix]], to work noise
free at the lowest possible thresholds, defining the correct working
parameters for the FADC, sec. [[#sec:operation_calibration:fadc]] and
setting the correct thresholds for the scintillators,
sec. [[#sec:operation_calibration:scintillators]].

There will be a later chapter about the type of calibrations that are
done based on real data to -- for example -- calibrate the charge or
energy, chapter [[#sec:calibration]].

** Timepix calibrations
:PROPERTIES:
:CUSTOM_ID: sec:operation_calibration:timepix
:END:

Before a Timepix based detector can be used for data taking, different
calibrations have to be performed. We will discuss those calibrations,
which are performed before any data taking here. Starting with a =THS=
optimization and threshold equalization
(sec. [[#sec:operation_calibration:ths_opt_equalization]]). Next the ToT
calibration is introduced, section
[[#sec:operation_calibration:tot_calibration]]. The first two are
calibrations that are used to set different DACs on the Timepix to
good working points. The ToT calibration on the other hand is to
interpret the ToT values in amount of charge of recorded
electrons. For an introduction to S-Curve scans see the extended
version of the thesis.

In principle there are many other calibrations one could perform, as
the Timepix has 13 different DACs. Most are used with
default values that are seen in tab. [[tab:daq:common_dac_values]].

Important references for the Timepix in general and for the
calibration procedures explained below are
cite:LLOPART2007485_timepix,LlopartCudie_1056683,timepix_manual,lupberger2016pixel.

*** TODOs for this section [/]                                   :noexport:

What kind of calibrations exist. How do they work, what do they do
etc.

From a purely detector standpoint.

Polya distribution goes here somewhere. Related to gaseous detector
physics & our detector in particular.



- [ ] *LEAVE THE IMPORTANT REFERENCE PART IN? ALREADY MENTIONED WHEN
  TIMEPIX INTRODUCED*

- [X] *ELSEWHERE AS WELL*
  -> Its own chapter for these things.
- [ ] *REPHRASE, SCURVE NOT TO SET A DAC. SCURVE USED TO ESTIMATE THE THRESHOLD IN ELECTRONS*
  
These calibrations are mainly explained to give context for the used
detector calibrations at CAST. All calibrations can be found in
appendix *APPEND CALIBRATIONS*.


*** =THS= optimization and threshold equalization
:PROPERTIES:
:CUSTOM_ID: sec:operation_calibration:ths_opt_equalization
:END:

For an optimal operation of a Timepix based detector, each pixel
should ideally have the same threshold. While all pixels are
theoretically identical, imperfections in the production process will
always lead to slight differences, either locally (transistor
threshold voltage or current mismatches
cite:LLOPART2007485_timepix,pelgrom1989matching ) or global effects
like small supply voltage instabilities. Therefore, each pixel has 4
independently selectable current sources to minimize the spread of
threshold values
cite:timepix_manual,LLOPART2007485_timepix. Together all 4 sources
act as an effective 4-bit DAC to slightly adjust the threshold. The
absolute current for the 4 sources is dependent on the global =THS=
DAC, allowing currents in the range of $\SIrange{0}{40}{nA}$.

To achieve a good calibration for a homogeneous threshold, first the
=THS= DAC has to be set correctly. This is referred to as the =THS=
optimization. Once the correct value is found, the 4-bit DAC on each
pixel can be adjusted to minimize the spread of threshold values of
all pixels together.

If the =THS= DAC is set too high, the 4-bit DAC on each pixel will be
too coarse for a fine adjustment (as the 'current steps' will be too
large). If it is too low, not enough range will be available to adjust
each pixel to an equal noise / sensitivity level (not enough current
available via the 4 current sources). The goal of the =THS=
optimization is therefore to find just the right value, as to provide
a range of values such that all values can be shifted to the same
threshold of the threshold DAC =THL=.

The algorithm scans a range of =THL= values through a subset of
$\num{4096}$ pixels using different 4-bit DAC values. First 0 for all
pixels and then the maximum value of 15. At each =THL= and 4-bit
value the number of hits due to pure noise is recorded for each
pixel. The weighted mean of the =THL= values, using the number of hits
as weight, is the value of interest for each pixel and each 4-bit DAC
value: the effective =THL= noise value for that pixel. For each of the
two cases (4-bit value 0 and 15) we can then compute a histogram of the
number of pixels at each =THL= value. The resulting histogram will be
a normal distribution around a specific =THL= value. The stopping
criterion, which defines the final =THS= value, is such that these two
distributions overlap at the 3 RMS level. This is performed by
comparing the means of the 0 and 15 value distribution at a starting
=THS= value and again at half of that =THS= value. Using linear
regression of the two differences, the optimal =THS= value is
computed.

With a suitable =THS= value set, the actual threshold equalization can
start. The algorithm used is fundamentally very similar to the logic
of the =THS= optimization. Each pixel of the chip is scanned for a
range of =THL= values and the weighted =THL= noise mean is computed
both at a 4-bit DAC value of 0 and at 15. The normalized deviation of
each pixel's =THL= value to the mean =THL= value of all pixels is
computed. Using a linear regression the optimal required shift (in
units of the 4-bit DAC) yields the final 4-bit DAC value for each
pixel.

An example of the 0 and 15 value distributions as well as the
distribution using the final 4-bit DAC values for each pixel is shown
in fig. [[sref:fig:daq:optimal_ths_distribution]]. [fn:daq_histo_source] Each
of the distributions represent different 4-bit DAC settings of all
pixels of the chip. Orange ("min") represents all pixels using a 4-bit
DAC value of 0, purple ("max") of 15. In green is the same
distribution for the case where every pixel uses its optimal 4-bit DAC
value. The threshold equalization thus yields a very strong reduction
in the =THL= spread of all pixels. Fig. [[sref:fig:daq:4bit_dac_distribution]]
shows how all pixels are spread in the values of the 4-bit DAC. The
narrow equalized line of fig. [[sref:fig:daq:optimal_ths_distribution]] is
achieved by a normal distribution around $\num{8}$ of the 4-bit DAC
values, with only very few at the edges of the DAC ($\num{0}$ and
$\num{15}$). Finally, fig. [[fig:daq:4bit_dac_heatmap]] shows a heatmap of
an entire chip with its 4-bit DAC values after equalization.

#+begin_src subfigure
(figure ()
        (subfigure (linewidth 0.5) (caption "THL distributions") (label "fig:daq:optimal_ths_distribution")
                   (includegraphics (list (cons 'width (linewidth 0.95)))
                                    "~/phd/Figs/detector/calibration/ths_optimization_distributions_example.pdf"))
        (subfigure (linewidth 0.5) (caption "4-bit distributions") (label "fig:daq:4bit_dac_distribution")
                   (includegraphics (list (cons 'width (linewidth 0.95)))
                                    "~/phd/Figs/detector/calibration/optimized_equalization_bits_example.pdf"))
        (caption
         (subref "fig:daq:optimal_ths_distribution")
          "Distributions of different 4-bit DAC settings of all
          pixels of the chip. Orange (\"min\"): all pixels using a 4-bit
          DAC value of 0, purple (\"max\"): 15. Green: every pixel uses the
          optimal 4-bit DAC value after equalization. The result is a significant
          in =THL= value spread of all pixels."
         (subref "fig:daq:4bit_dac_distribution")
          "Distribution of all 4-bit DAC values for the pixels after the
          threshold equalization. A normal distribution around a middle
          value is expected to largest likelihood of achieving a flat
          threshold around the whole chip. Very few pixels are either in 
          value \\num{0} or \\num{15}, implying few pixels likely outside their
          range to adjust to the required threshold. In this example
          represented is the center chip of the Septemboard with its calibration
          from July 2018.")
        (label "fig:daq:thl_and_4bit_distr"))
#+end_src

#+CAPTION: Heatmap of the distribution of the 4-bit DAC values of all chips
#+CAPTION: as they are spread over the full Timepix. In this example
#+CAPTION: represented is the center chip of the Septemboard with its calibration
#+CAPTION: from July 2018.
#+NAME: fig:daq:4bit_dac_heatmap
[[~/phd/Figs/detector/calibration/heatmap_threshold_equalization_example.pdf]]

[fn:daq_histo_source] The plot is generated from the
=thresholdMeans.txt= file created as part of the equalization
procedure in TOS.

[fn:daq_tos_code_quality] Having to check out the TOS code to verify
the logic of the THS optimization and equalization procedures reminded
me of the abhorrent code quality of that code base. Holy moly...

**** TODOs for this section [/]                                 :noexport:

- [ ] Maybe move heatmap of equalization to extended version?

- [X] *I THINK THS OPT DOES NOT!! USE TEST PULSES* Yes, pretty sure
  now. The THS optimization (and obviously threshold equalization)
  does not use test pulses

- [ ] *INTRODUCE THE NAME 'PIXEL DAC' INSTEAD 4-BIT DAC?*  

- [ ] *MAYBE REPHRASE THE FOLLOWING. BUT THIS IS WHAT'S HAPPENING. THE TOS CODE IS PRETTY SIMPLE, JUST BLOATED AND UGLY*

- [X] *FIRST EXPLAIN THS OPT, THEN EQ, BOTH USING THE PLOT*  

- [X] *REWRITE BELOW FOLLOWING THE ABOVE NOW*
- [X] *MENTION THIS IS TO MAKE EQUALIZATION OF EACH PIXEL DAC NICE*

- [ ] *PSEUDO CODE ALGORITHM? I THINK IT WOULD CLARIFY THE EXPLANATION QUITE A BIT. ESPECIALLY BECAUSE IT'S NOT DIFFICULT. JUST TOS IS
  UGLY*

- [X] *PLOT OF THESE TWO HISTOGRAMS. SHOW MAYBE IDEALIZED EXAMPLE OF BAD THS
  AND GOOD THS VALUE*

- [X] *LOOK AT TOS CODE AGAIN*
  
- [X] *LOOK AT TOS CODE FOR EQUALIZATION AGAIN*

- [X] *CROSS CHECK THE NAMES ETC*

**** Generate the plot for the THS optimization result :noexport:

#+begin_src nim :tangle code/ths_optimization.nim
import ggplotnim

proc main(fname: string) =
  var df = readCsv(fname, sep = '\t', colNames = @["x", "y", "min", "max", "bit", "opt"])
  let breaks = linspace(-0.5, 15.5, 17).toSeq1D
  echo breaks
  ggplot(df, aes("bit")) +
    geom_histogram(breaks = breaks, hdKind = hdOutline) +
    scale_x_continuous() +
    xlim(-0.5, 16.5) +
    xlab("4-bit DAC") + 
    ggtitle("Distribution of all equalization bits after optimization") + 
    ggsave("/home/basti/phd/Figs/detector/calibration/optimized_equalization_bits_example.pdf",
           useTeX = true, standalone = true)
  df = df.gather(["min", "max", "opt"], "type", "THL")
  ggplot(df.filter(f{`THL` > 330.0 and `THL` < 460.0}), aes("THL", fill = "type")) +
    geom_histogram(binWidth = 1.0, position = "identity", hdKind = hdOutline, alpha = 0.7) +
    ggtitle("THL distributions for all equalization bits at 0, 15 and optimized") +
    #xlim(330, 460) + 
    ggsave("/home/basti/phd/Figs/detector/calibration/ths_optimization_distributions_example.pdf",
           useTeX = true, standalone = true)
when isMainModule:
  import cligen
  dispatch main
#+end_src

Laptop:
#+begin_src sh
./code/ths_optimization -f ~/septemH_calibration/SeptemH_FullCalib_2018_2/chip3/thresholdMeans3.txt
#+end_src
Desktop / Laptop:
#+begin_src sh
./code/ths_optimization -f ~/CastData/ExternCode/TimepixAnalysis/resources/ChipCalibrations/Run3/chip3/thresholdMeans3.txt
#+end_src

#+RESULTS:
| @[-0.5,    | 0.5,                                                                                 | 1.5,              | 2.5,                                      | 3.5,                                                                                 | 4.5, | 5.5, | 6.5, | 7.5, | 8.5, | 9.5, | 10.5, | 11.5, | 12.5, | 13.5, | 14.5, | 15.5] |
| [INFO]     | TeXDaemon                                                                            | ready             | for                                       | input.                                                                               |      |      |      |      |      |      |       |       |       |       |       |       |
| shellCmd:  | command                                                                              | -v                | xelatex                                   |                                                                                      |      |      |      |      |      |      |       |       |       |       |       |       |
| shellCmd:  | xelatex                                                                              | -output-directory | /home/basti/phd/Figs/detector/calibration | /home/basti/phd/Figs/detector/calibration/optimized_equalization_bits_example.tex    |      |      |      |      |      |      |       |       |       |       |       |       |
| Generated: | /home/basti/phd/Figs/detector/calibration/optimized_equalization_bits_example.pdf    |                   |                                           |                                                                                      |      |      |      |      |      |      |       |       |       |       |       |       |
| [INFO]     | TeXDaemon                                                                            | ready             | for                                       | input.                                                                               |      |      |      |      |      |      |       |       |       |       |       |       |
| shellCmd:  | command                                                                              | -v                | xelatex                                   |                                                                                      |      |      |      |      |      |      |       |       |       |       |       |       |
| shellCmd:  | xelatex                                                                              | -output-directory | /home/basti/phd/Figs/detector/calibration | /home/basti/phd/Figs/detector/calibration/ths_optimization_distributions_example.tex |      |      |      |      |      |      |       |       |       |       |       |       |
| Generated: | /home/basti/phd/Figs/detector/calibration/ths_optimization_distributions_example.pdf |                   |                                           |                                                                                      |      |      |      |      |      |      |       |       |       |       |       |       |

#+begin_src nim :tangle code/threshold_equalization_heatmap.nim
import ggplotnim
import std / [sequtils, strutils]

proc main(fname: string) =
  let aranged = toSeq(0 .. 255).mapIt($it)
  var df = readCsv(fname, sep = '\t', colNames = aranged)
  df["y"] = toSeq(0 .. 255)
  df = df.gather(aranged, "x", "4-bit DAC")
    .mutate(f{"x" ~ `x`.parseInt})
  echo df
  
  ggplot(df, aes("x", "y", fill = "4-bit DAC")) +
    geom_raster() + 
    #scale_x_continuous() +
    #xlim(-0.5, 16.5) +
    xlim(0, 255) + ylim(0, 255) + 
    ggtitle("Heatmap of all equalization bits after optimization") + 
    ggsave("/home/basti/phd/Figs/detector/calibration/heatmap_threshold_equalization_example.pdf",
           useTeX = true, standalone = true)

when isMainModule:
  import cligen
  dispatch main
#+end_src

#+begin_src sh
./code/threshold_equalization_heatmap -f ~/septemH_calibration/SeptemH_FullCalib_2018_2/chip3/threshold3.txt
#+end_src

**** Relevant code for calculation of mean values from TOS :noexport:

Filling of =sum= in =THscan=. =array_pos= is effectively the =THL=
value currently being scanned. =pix_tempdata= is the response matrix
of each pixel (contains hit counter for each pixel). Also fills
=hit_counter=, which is simply the counts.
#+begin_src c++
    fpga->DataFPGAPC(pix_tempdata2,chp); //!!!only one chip!!!
    for(short y=step;y<256;y+=(256/pix_per_row)){
	for(short x=0;x<256;x++){
	    if(pix_tempdata2[y][x]>=20 and pix_tempdata2[y][x]!=11810){
		//if (pix_tempdata2[y][x]>=200) {std::cout << "hits for thl " << thl <<" :" << pix_tempdata2[y][x] << std::endl;}
		p3DArray[y][x][array_pos] = pix_tempdata2[y][x];
		//if(LFSR_LookUpTable[(*VecData)[chp][y][x]]>=20 and LFSR_LookUpTable[(*VecData)[chp][y][x]]!=11810){
		//p3DArray[y][x][array_pos] = LFSR_LookUpTable[(*VecData)[chp][y][x]];
		sum[y][x]+=p3DArray[y][x][array_pos]*(array_pos);
		hit_counter[y][x]+=p3DArray[y][x][array_pos];
	    }
	    else{
		p3DArray[y][x][array_pos] = 0;
		sum[y][x]+=0;
		hit_counter[y][x]+=0;
	    }
	}
    }
#+end_src
And in the =THSopt= the code to compute the mean:

#+begin_src c++
	for(y=0;y<256;y++){
	    for(x=0;x<256;x++){
		if (hit_counter0[y][x]!=0){
		    mean0[y][x] = sum0[y][x]/hit_counter0[y][x];
		    mean0entries += 1;
		    summean0 += mean0[y][x];
		}
		if (hit_counter15[y][x]!=0){
		    mean15[y][x] = sum15[y][x]/hit_counter15[y][x];
		    mean15entries += 1;
		    summean15 += mean15[y][x];
		}
	    }
	}
#+end_src

Length of shutter used is
#+begin_src c++
    // calling CountingTime with second argument == 1
    // corresponds to n = 1, power of 256
    fpga->CountingTime(10, 1);
#+end_src
(could compute the length, but not important right now)

Given that the =THscan= is run for each THL value (and thus summing up
all contributions of all THL values for =sum0=), the algorithm
effectively computes:
#+begin_src
mean = Σ_i #hits_i * THL_i / Σ_i #hits_i
#+end_src
which is simply *the weighted mean of the =THL= value, weighted by the
number of hits.* Essentially we compute the THL value with the most
dominant noise? In a sense it makes sense as changing the 4-bit DAC
will move around the position of that noise effectively. The 

The point of interest then here is the fact that the number of hits
depends on the THL value strongly. We only see the number of injected
test pulses, if we're above the noise. Ideally we don't want to see
any noise due to too low =THL= range. Therefore let's check what is
used in TOS.

We will verify this by computing the same value for an S-curve
calibration file:
#+begin_src nim
import ggplotnim

const path = "/home/basti/septemH_calibration/SCurve/chip_3/voltage_100.txt"
let df = readCsv(path, sep = '\t', header = "#", colNames = @["THL", "counts"])
  .filter(f{`THL` > 424})
echo df
let thls = df["THL", float]
let counts = df["counts", float]
var sum = 0.0
var hits = 0.0
for (thl, count) in zip(thls, counts):
  sum += count * thl
  hits += count
echo "Mean value = ", sum / hits
#+end_src

#+RESULTS:
| DataFrame |  with |      2 |           columns | and | 175 | rows: |
|       Idx |   THL | counts |                   |     |     |       |
|    dtype: |   int |    int |                   |     |     |       |
|         0 |   425 |   1000 |                   |     |     |       |
|         1 |   426 |   1000 |                   |     |     |       |
|         2 |   427 |   1000 |                   |     |     |       |
|         3 |   428 |   1000 |                   |     |     |       |
|         4 |   429 |   1000 |                   |     |     |       |
|         5 |   430 |   1000 |                   |     |     |       |
|         6 |   431 |   1000 |                   |     |     |       |
|         7 |   432 |   1000 |                   |     |     |       |
|         8 |   433 |   1000 |                   |     |     |       |
|         9 |   434 |   1000 |                   |     |     |       |
|        10 |   435 |   1000 |                   |     |     |       |
|        11 |   436 |   1000 |                   |     |     |       |
|        12 |   437 |   1000 |                   |     |     |       |
|        13 |   438 |   1000 |                   |     |     |       |
|        14 |   439 |   1000 |                   |     |     |       |
|        15 |   440 |   1000 |                   |     |     |       |
|        16 |   441 |   1000 |                   |     |     |       |
|        17 |   442 |   1000 |                   |     |     |       |
|        18 |   443 |   1000 |                   |     |     |       |
|        19 |   444 |   1000 |                   |     |     |       |
|           |       |        |                   |     |     |       |
|      Mean | value |      = | 463.8075954222876 |     |     |       |

which results in a mean value of 463.8. Given the range of data
that's, surprise, what we would expect from a weighted mean with the
hit counter used.

Of course, in the =THS= optimization the input is purely noise and not
a fixed set of test pulses.

*** Final =THL= (threshold) DAC value selection [0/0]

Once the detector is =THS= optimized and threshold equalized, the
final threshold value of the =THL= DAC can be determined for the data
taking. While measurements like an S-Curve scan (see
sec. [[#sec:operation_calibration:scurve_scan]]) can be used to understand
where the noise level of the chip is in terms of =THL= values, it is
typically not a reliable measure as the real noise depends strongly on
the shutter length. If an experiment -- like a low rate experiment as
CAST -- requires long shutter lengths, the best way to determine the
lowest possible noise-free =THL= value is to perform a simple scan
through all =THL= values using the shutter length in use for the
experiment. 

For a correctly equalized chip a sharp drop off of noisy pixels should
be visible at a certain threshold. In principle the =THL= value at
which no more pixels are noisy is the ideal =THL= value.

TOS first performs a quick scan in a range ~THL~ range given by the
user using short shutter lengths. The determined drop values that
still see some noise to a noise-free range is used as a basis for a
long shutter length scan using a shutter length given by the user. For
safe noise free operation one should choose a ~THL~ value 2 or 3 above
the first noise-free THL value at the target shutter
length. Especially for long shutter lengths it is important to perform
this calibration without any high voltage applied to the detector as
otherwise cosmic background starts to affect the data.

**** TODOs for this section [/]                                 :noexport:

Old paragraph for above:
#+begin_quote
- [ ] *REPHRASE AND NOT TALK ABOUT ENC HERE?*
Each Timepix pixel has an electronic noise charge (ENC) of *CHECK
THIS* electrons. Of course the behavior of the charges on the pixels
are statistically distributed. For the different calibrations
typically very short shutter opening times are used to get fast
calibrations. For practical data takings at experiments like CAST,
very long shutter times on the order of $\mathcal{O}(> \SI{1}{\s})$
are used however. Due to the statistical nature of noise, a =THL=
value that is noise less may not be fully noise free for long
shutters. Therefore, one often uses =THL= values that are slightly
larger (i.e. ~3 values larger of the 10 bit DAC).

In this sense the S-Curve scan is a good cross check for whether the
=THL= value seems sensible, but in practice a =THL= scan with a longer
shutter time is more useful.
#+end_quote



... THL scan at desired shutter length! Semi automatically. Scan a
take no noise + 2

- [ ] *CHECK ENC VALUE OF TIMEPIX AND CITE TIMEPIX PAPER*
  -> ENC is ~100 (timepix paper), but the ENC isn't the relevant
  property here. I think I misunderstood what the ENC really is.
- [ ] *FOR THE SECTION ABOVE I THINK ONLY THE MINIMALLY DETECTABLE
  CHARGE IS RELEVANT?*
- [X] *REWRITE FULL SECTION WITH A CLEAR HEAD*  

*** ToT calibration
:PROPERTIES:
:CUSTOM_ID: sec:operation_calibration:tot_calibration
:END:

The purpose of the ~ToT~ (\textbf{T}ime \textbf{o}ver
\textbf{T}hreshold) calibration is not to perform a calibration for
stable operation of a Timepix based detector, but rather to interpret
the data received. It is needed to interpret the =ToT= values recorded
by each pixel as a charge, i.e. a number of recorded electrons.

This is done by injecting charges onto the individual pixels -- 'test
pulses'. Capacitors are present to inject very precise voltage bursts
onto the pixels. In case of the Timepix 1, each pixel uses a
capacitance of $\SI{8}{fF}$ cite:timepix_manual. Knowing the
capacitance and the voltage induced on them, the number of injected
electrons can be easily calculated from

\[
Q = C U.
\]

By varying the injected charge and recording the resulting ToT values
of the pixels a relation between electrons and ToT values is
determined:

\[
f(p) = a p + b - \frac{c}{p - t}
\]
where $a, b, c$ and $t$ are parameters to be determined via a
numerical fit and $p$ is the test pulse height in $\si{mV}$.

As such, inverting the relation this can be used to compute a charge
for a given =ToT= value: 

\[
  f(\mathtt{ToT}) = \frac{α}{2 a} \left( \mathtt{ToT} - (b - a t) +
    \sqrt{ \left( \mathtt{ToT} - (b - a t) \right)^2 + 4 (a b t + a c -
    a t \mathtt{ToT} ) } \right)
\]
where $\mathtt{ToT}$ is the time over threshold value recorded for a
pixel, the constants $a, b, c, t$ the fit parameters determined above
and $α$ the conversion factor relating the number of electrons from a
pulse height of $\SI{1}{mV}$. 

An example of a ToT calibration of one chip is shown in
fig. [[fig:septem:tot_calibration_example]].

#+NAME: fig:septem:tot_calibration_example
#+CAPTION: Example of a ToT calibration measurement for the chip H10 W69, the center
#+CAPTION: chip of the Septemboard, as it was done for the CAST data taking period 2.
[[~/phd/Figs/detector/calibration/tot_calib_3.pdf]]

**** TODOs for this section [4/4]                               :noexport:

- [X] add =--outpath= argument to =plotCalibration= and use it to
  place it where we read it from
#+begin_src sh :exports none
# To generate fig:septem:tot_calibration_example
plotCalibration --tot --runPeriod=Run2 --useTeX \
                --file ~/CastData/ExternCode/TimepixAnalysis/resources/ChipCalibrations/Run2/chip3/TOTCalib3.txt \
                --outpath ~/phd/Figs/detector/calibration/
#+end_src

#+RESULTS:
| ***        | testlinfit                                                 |            status | =                                         |                                                          1 |
| χ²         | =                                                          |            1.7516 | (10                                       |                                                       DOF) |
| χ²/dof     | =                                                          |           0.17516 |                                           |                                                            |
| NPAR       | =                                                          |                 4 |                                           |                                                            |
| NFREE      | =                                                          |                 4 |                                           |                                                            |
| NPEGGED    | =                                                          |                 0 |                                           |                                                            |
| NITER      | =                                                          |                 8 |                                           |                                                            |
| NFEV       | =                                                          |                37 |                                           |                                                            |
| P[0]       | =                                                          |          0.352852 | +/-                                       |                                                  0.0420413 |
| P[1]       | =                                                          |           50.7553 | +/-                                       |                                                    17.2347 |
| P[2]       | =                                                          |           2386.45 | +/-                                       |                                                    1878.16 |
| P[3]       | =                                                          |          -21.2788 | +/-                                       |                                                    21.0224 |
| [INFO]     | TeXDaemon                                                  |             ready | for                                       |                                                     input. |
| shellCmd:  | command                                                    |                -v | xelatex                                   |                                                            |
| shellCmd:  | xelatex                                                    | -output-directory | /home/basti/phd/Figs/detector/calibration | /home/basti/phd/Figs/detector/calibration//tot_calib_3.tex |
| Generated: | /home/basti/phd/Figs/detector/calibration//tot_calib_3.pdf |                   |                                           |                                                            |

- [X] Required understanding for our charge calibration.

- [X] Show function that is being fitted to it.

- [X] *NOTE: AS FAR AS I CAN TELL, THE TOT CALIBRATION ALREADY
  REQUIRES A CORRECT =THL= DAC VALUE!*

The following is the doc string of the function for the ToT
calibration function used in ~TimepixAnalysis~. 
#+begin_src nim
  ## calculates the charge in electrons from the TOT value, based on the TOT calibration
  ## from MarlinTPC:
  ## measured and fitted is ToT[clock cycles] in dependency of TestPuseHeight [mV]
  ## fit function is:
  ##   ToT[clock cycles] = a*TestPuseHeight [mV]  + b - ( c / (TestPuseHeight [mV] -t) )
  ## isolating TestPuseHeight gives:
  ##   TestPuseHeight [mV] = 1/(2*a) * (ToT[clock cycles] - (b - a*t) +
  ##                         SQRT( (ToT[clock cycles] - (b - a*t))^2 +
  ##                               4*(a*b*t + a*c - a*t*ToT[clock cycles]) ) )
  ## conversion from charge to electrons:
  ##   electrons = 50 * testpulse[mV]
  ## so we have:
  ## Charge[electrons] = 50 / (2*a) * (ToT[clock cycles] - (b - a*t) +
  ##                     SQRT( (ToT[clock cycles] - (b - a*t))^2 +
  ##                           4*(a*b*t + a*c - a*t*ToT[clock cycles]) ) )
#+end_src

*** S-Curve scan [0/0]                                           :extended:
:PROPERTIES:
:CUSTOM_ID: sec:operation_calibration:scurve_scan
:END:

The S-Curve scan is one of 2 different ways to determine the optimal
=THL= value.

The purpose of the S-curve scan is to understand the relationship
between injected charges in electron and the =THL= DAC values by
providing a ${\text{\# } e^-}/\mathtt{THL}\text{ step}$ number (or
without a =ToT= calibration $\mathtt{ToT}/\mathtt{THL}$). 

It works by injecting charges onto each pixel and checking the pixel
response of each pixel at different =THL= values. Below a certain
=THL= value all pixels will respond to the injected charge. At some
point certain pixels will be insensitive to the induced charge and a
90° rotated "S" will form. By fitting an error function to this S an
ideal =THL= value can be deduced.

By calculating =THL= value at which half of all test pulses are
recorded, we can compute the number of electrons corresponding to that
=THL= DAC value, as we know the amplitude of the test pulse and thus
number of injected electrons.

Fig. [[fig:daq:s_curve_examples]] shows an S-Curve scan of chip 0 of the
Septemboard using the calibration from July 2018. The center peak in
the middle is the noise peak of the detector at the shutter length
used for the S-Curve scan. The symmetrical shape is due to specific
implementation details of how the pixels function, the upper side is
the one of interest. The falling edge of each curve can be fit by the
cumulative distribution function of a normal distribution. The center
point (half way between both plateaus) corresponds to the effective
threshold of the detector at that injected charge.

#+NAME: eq:daq:s_curve_fit_function  
\begin{equation}
f(μ, σ, N) = \frac{N}{2} · \text{erfc}((x - μ) / (σ · \sqrt{2}))
\end{equation}

where the parameter $N$ is simply a scaling factor and $μ$ represents
the x value of the half-amplitude point. $σ$ is the spread of the drop
and $\text{erfc}$ is the complementary error function $\text{erfc}(x)
= 1 - \text{erf}(x)$. The error function is of course just the
integral over a normal distribution up to the evaluation point $x$:

\[
\text{erf}(x) = \frac{2}{\sqrt{π}} ∫_0^{x} e^{-t²} dt
\]

Given that the number of injected electrons is known for each test
pulse amplitude (see sec. [[#sec:operation_calibration:tot_calibration]]), we can compute
the relationship of the number of electrons per =THL= value step. This
is called the =THL= calibration and an example corresponding to
fig. [[fig:daq:s_curves_examples]] is shown in
fig. [[fig:daq:thl_calibration_example]], where the =THL= values used
correspond to the $μ$ parameters of
eq. [[eq:daq:s_curve_fit_function]]. The resulting fit is useful, as it
allows to easily convert a given =THL= DAC value into an effective
number of electrons, which then corresponds to the effective threshold
in electrons required to activate a pixel on average. When looking at
the distribution of charges in a dataset, that cutoff in electrons is
of interest (see sec. [[#sec:daq:polya_distribution_threshold]]).

#+CAPTION: S-Curve scan of chip 0 of the Septemboard using the calibration from
#+CAPTION: July 2018. The scan works by injecting \num{1000} test pulses at different
#+CAPTION: amplitudes onto the pixels. Each line represents one such measurement
#+CAPTION: and each point is the mean number of counted hits for all pixels with injected
#+CAPTION: test pulses.
#+CAPTION: The center peak in the middle is the noise peak of the
#+CAPTION: detector. The symmetrical shape is due to specific implementation
#+CAPTION: details of how the pixels function. The upper side is the one of
#+CAPTION: interest. The falling edge of each curve can be fit by the complement of the
#+CAPTION: cumulative distribution function of a normal distribution. The center point
#+CAPTION: (half way between both plateaus) corresponds to the effective threshold 
#+CAPTION: of the detector at that injected charge. The fit parameters for all calibrations
#+CAPTION: are shown in tab. *INSERT*.
#+NAME: fig:daq:s_curves_example
[[~/phd/Figs/detector/calibration/s_curves_0_lX_200.0_lY_3750.0.pdf]]

#+CAPTION: The =THL= calibration can be used to gauge the 'threshold gain' the
#+CAPTION: =THL= DAC has on the number of electrons required to cross the threshold.
#+CAPTION: The =THL= DAC in the Timepix normally adjusts the threshold by about
#+CAPTION: \num{25} electrons per DAC value cite:timepix_manual, which is reproduced
#+CAPTION: well here (parameter $1/m$). The root of the linear fit (written as
#+CAPTION: $f⁻¹(y=0)$ in the annotation) corresponds to the position of the noise peak
#+CAPTION: in fig. [[fig:daq:s_curves_example]]. *THIS IS INVERTED OR THE PLOT IS WRONG!*
#+NAME: fig:daq:thl_calibration_example
[[~/phd/Figs/detector/calibration/thl_calibration_chip_0_lX_422.0_lY_3550.0.pdf]]


**** TODOs for this section [5/18]                              :noexport:

- [ ] *REWRITE THE CAPTION OF THL PLOT*

- [ ] *TURN INTO IF ELSE FUNCTION FOR z* (defined in seqmath)
  #+begin_src nim
  let z = (x - x0) / (sigma * sqrt(2.0))
  if z > 1.0:
    result = 0.5 * erfc(z)
  else:
    result = 0.5 * (1.0 - erf(z))
  #+end_src

- [ ] *MAIN PURPOSE IS NOT!! THL VALUE DETERMINATION*
- [ ] *LIKELY MOVE THIS EITHER TO APPENDIX OR EVEN EXTENDED THESIS*  

- [ ] *ADD FIT FUNCTION FOR SCURVE*

- [ ] *REFER TO THE CODE THAT DOES THE FIT*
  https://github.com/Vindaar/TimepixAnalysis/blob/master/Analysis/ingrid/calibration/calib_fitting.nim#L111

- [X] *FIX UP TYPESETTING OF FIT PARAMETERS IN THL PLOT*

- [ ] *INSERT AND REFERENCE TABLE OF ALL FIT PARAMETERS FOR THE SCURVES*

- [ ] *CHECK WHAT 25e/THL IS CALLED IN TABLE OF TIMEPIX MANUAL*

- [X] *FIX PARAMETERS OF THL CALIBRATION. THEY ARE NONSENSICAL AND NOT ALIGNED*
  
- [X] *ADD THAT AT EVEN LOWER VALUE WE JUST SEE NOISE AND THEN DUE TO
  IMPLEMENTATION DETAILS THE WHOLE THING INVERTS*

- [X] *ADD ITS OWN SHORT SECTION ABOUT THRESHOLD SCANNING WITH BELOW
  REWRITTEN TEXT*

- [ ] *ADD NUMBER OF EXPECTED ELECTRONS AS THRESHOLD AS MENTIONED IN TIMEPIX ORIGINAL PAPER*

- [X] *PLOT OF SCURVE SCANS*

- [ ] *APPENDIX ALL SCURVE SCANS FOR CAST DATA TAKING*
  -> If anything only for the extended thesis. We don't use them for
  literally anything.

- [ ] *UNDERSTAND HOW WE CAN USE SCURVE TO DEDUCE THRESHOLD AND EXPLAIN IT*
- [ ] *S-CURVE CAN BE USED TO DETERMINE #electron / THL DAC VALUE*
  (first gets THL value against pulse height of course)
- [ ] *MAKE THAT FIT AND SHOW EXAMPLE*
  -> That's the fit in [[fig:daq:thl_calibration_example]] no?

- [ ] *ADD EQUATION FOR MINIMUM DETECTABLE CHARGE AND GIVE A NUMBER*  

**** Compute electrons per THL DAC value [0/2]                  :extended:

- [ ] *FIGURE OUT WHY 2017 CALIBRATION HAS ABOUT 50e/THL COMPARED TO
  25e/THL!*

- [ ] *REPLACE BELOW BY =plotCalibration= CALL?* That already handles
  it, even though currently via plotly.

For the S-Curve plot 
#+begin_src sh
plotCalibration --scurve --chip 0 --runPeriod Run3 --useTeX --legendX 200.0 --legendY 3750 --outpath ~/phd/Figs/detector/calibration
#+end_src

and for the THL calibration:
#+begin_src sh
plotCalibration --scurve --chip 0 --runPeriod Run3 --useTeX --legendX 422.0 --legendY 3550 --outpath ~/phd/Figs/detector/calibration
#+end_src

the only difference being the legend placement (as the coordinates are
applied to both plots).

The following is essentially the main code required to plot the
SCurves, determine the middle point and fit THL calib (not the SCurve
fit though):
#+begin_src nim :tangle /home/basti/phd/code/s_curve_electrons_per_thl.nim
import std / [strutils, strscans, os, strformat] 
import ggplotnim
import unchained

proc charge(voltage: mV): UnitLess =
  ## Returns the number of electrons given a voltage pulse of amplitude `voltage`
  ## at the 8.fF capacitor of the Timepix1
  result = 8.fF * voltage / e

#const path = "/home/basti/septemH_calibration/SCurve/chip_3/voltage_*.txt"
#const path = "/home/basti/septemH_calibration/CalibJul2018/SCurves/chip_1/voltage_*.txt"
const path = "/home/basti/septemH_calibration/SeptemH_FullCalib_InGridDatabase/chip3/SCurve/voltage_*.txt"
#const path = "/home/basti/septemH_calibration/SeptemH_FullCalib_2018_2/chip0/SCurve/voltage_*.txt"
var charges = newSeq[float]()
var thls = newSeq[int]()
for file in walkFiles(path):
  let (success, _, voltage) = scanTuple(file.extractFilename, "$*_$i.txt$.")
  if voltage == 0: continue # skip 0
  charges.add charge(voltage.mV)
  ## we'll do the simplest approach to get the correct THL value:
  ## - strip everything before noise peak (to have single THL value)
  ## - compute
  var df = readCsv(file, sep = '\t', header = "#", colNames = @["THL", "counts"])
  let thlAtMax = df.filter(f{int: `counts` == `counts`.max})["THL", int][0] # must be single element
  const TestPulses = 1000
  df = df.filter(f{`THL` > thlAtMax})
    .mutate(f{int: "DiffHalf" ~ abs(`counts` - TestPulses div 2)})
    .filter(f{int: `DiffHalf` == min(col("DiffHalf"))}) # f{int: `DiffHalf` < 200})
  thls.add df["THL", int][0] # must be single element

import polynumeric
let fit = polyFit(thls.toTensor.asType(float),
                  charges.toTensor,
                  polyOrder = 1)
echo fit
proc linear(x, m, b: float): float = m * x + b

let thlFit = linspace(thls.min, thls.max, 10)
let chargesFit = linspace(charges.min, charges.max, 10)
var dfFit = toDf({ "thls" : thlFit,
                   "charges" : thlFit.map_inline(linear(x, fit[1], fit[0])) })
echo dfFit
let df = toDf(thls, charges)

ggplot(df, aes("thls", "charges")) + 
  geom_point() +
  geom_line(data = dfFit, aes = aes("thls", "charges"), color = parseHex("FF00FF")) +
  xlab("THL DAC") + ylab("Injected charge [e⁻]") + 
  ggtitle(&"Fit parameters: m = {fit[1]:.2f} e⁻/THL, b = {fit[0]:.2f} e⁻") +
  ggsave("/home/basti/phd/Figs/charge_per_thl.pdf")
#+end_src

#+RESULTS:

**** Compute minimally detectable charge                        :extended:

Ref:
- cite:LLOPART2007485_timepix Timepix paper
- cite:LlopartCudie_1056683 Llopert PhD thesis

The following quotes explain how to compute the effective threshold:

page 5/10:
#+begin_quote
The electronic noise and effective threshold can be measured using the
s-curve method [8] when the pixel is set in counting mode.
#+end_quote

page 5/10:
#+begin_quote
The effective threshold is at 50% of this s-curve. The charge
difference between the 97.75% and 2.25% of the s- curve is four times
the RMS noise of the front end assuming a gaussian distributed noise.
#+end_quote

#+begin_quote
The measured electronic noise is 99:4 ± 3:8 e⁻ rms for hole
collection and 104:8 ± 6 e⁻ rms for electron collection.  The
measured DAC step gain is 24:7 ± 0:7 e⁻ =step for hole collection
and 25:4 ± 1:2 e⁻ =step for electron collection.
#+end_quote

page 7/10:
#+begin_quote
The threshold variation before equalization is
~240 e⁻ rms and after equalization the achieved noise free
threshold variation is ~35 e⁻ rms for both polarities.
#+end_quote

page 7/10:
#+begin_quote
The minimum detectable charge can be calculated by quadratically
adding the measured electronic noise and the threshold variation
because both measurements are uncorrelated.
#+end_quote

#+begin_quote
Before equalization the minimum detectable charge for the full matrix
is ~1600 e⁻ and after equalization is ~650 e⁻ for both polarities.
#+end_quote

This means:
- [ ] compute the 97.75 ⇔ 2.25% range of the S-curve. Yields 4 times
  RMS noise. Width in THL of that can be converted to #e⁻ using THL
  calibration.
  -> electronic noise $N_e$
- [ ] compute width of optimized threshold variation based on
  histogram. Fit gaussian (?) to optimized threshold variation. σ of
  that gaussian is width in THL. Convert THL to electrons.
  -> threshold variation $N_t$

- [ ] *CHECK IF THIS IS CORRECT*:
  If I'm not mistaken, the noise peak we see in the S-curve scan is
  essentially the same as the optimized distribution from the
  threshold equalization.

Then with those two parameters we compute the effective detectable
charge as:

\[
N_d = √(N_e² + N_t²)
\]

which for the numbers listed above yields

\[
N_d = √(105² + 35²) = 110
\]

which is not close to the expected 650 e⁻!

Checking in the PhD thesis of Llopert, page 115, equation 4.5 is:

\[
\text{MinDetect}Q = 6·\sqrt{ ENC² + σ_{\text{dist}}²}
\]
which then works out nicely (110·6 = 660 is close enough)!

So in theory we can compute this for all our chips and all
calibrations.

- [ ] *CALCULATE FOR ALL CHIPS AND PUT INTO APPENDIX*

*** Pólya distribution & gas gain
:PROPERTIES:
:CUSTOM_ID: sec:daq:polya_distribution_threshold
:END:

In a gaseous detector the gas amplification (see
sec. [[#sec:theory:gas_gain_polya]]) allows to easily exceed the minimum
detectable charge of $\mathcal{O}(\SIrange{500}{1000}{e^-})$. The
typically used =THL= threshold will be quite a bit higher than the
'theoretical limit' however for multiple reasons. One can either
compute the effective threshold in use based on the =THL= calibration
as explained in sec. [[#sec:operation_calibration:scurve_scan]], or use an experimental
approach by utilizing the Pólya distribution as introduced in
sec. [[#sec:theory:gas_gain_polya]]. By taking data over a certain period
of time and computing a histogram of the charge values recorded by
each pixel, a Pólya distribution naturally arises.

This Pólya distribution has two different use cases. First of all it
is the source of the empirical gas gain the detector actually operated
under. One of the parameters of the Pólya parametrization as
introduced in section [[#sec:theory:gas_gain_polya]] is a measure for the
gas gain. Alternatively, the mean of the histogram can also be used as
a measure for the gas gain (which is the main number used in later
chapters).

Secondly, the Pólya can also be used to determine the actual
activation threshold of the detector, by simply checking what the
lowest charge is that sees significant statistics.
  
An example of such a Pólya distribution is seen in
fig. [[fig:daq:polya_example_chip0]], where we see the same chip 0 using
the same calibration from July 2018 as in the figures in the previous
section [[#sec:operation_calibration:scurve_scan]]. The data is a \SI{30}{min} interval of
background data at CAST. The reasoning behind looking at a fixed time
interval for the Pólya will be explained in section [[#sec:calib:gas_gain_time_binning]].
The pink line represents the fit of the Pólya
distribution to the data. The dashed part of the line was not used for
the fit and is only an extension using the final fit parameters. The
cutoff at the lower end due to the chip's threshold is clearly
visible. The fit determines a gas gain of about $\num{2700}$, compared
to the mean of the data yielding about $\num{2430}$. 

Based on the data a threshold value of - very roughly - $\num{1000}$ can
be estimated. Using the =THL= calibration of the chip as shown in
fig. [[fig:daq:thl_calibration_example]] yields a value of

\[
Q(\text{THL} = 419) = 26.8 · 419 - 10300 = 929.2
\]

where we used the fit parameters as printed on the plot ($1/m$ and
$f⁻¹(y=0)$) and the =THL= DAC value of $\num{419}$ as used during the
data taking for this chip. Indeed, the real threshold is in the same
range, but clearly a bit higher than the theoretical limit for this
chip. This matches our expectation.

#+CAPTION: An example of a Pólya distribution of chip 0 using the calibration
#+CAPTION: of July 2018 based on \SI{30}{min} of background data.
#+CAPTION: The lower cutoff is easily visible. The pink line represents the
#+CAPTION: fit of the Pólya distribution to the data. In the dashed region the
#+CAPTION: line was extended using the final fit parameters.
#+NAME: fig:daq:polya_example_chip0
[[~/phd/Figs/gas_gain_run_306_chip_0_placeholder_example.pdf]]

**** TODOs for this section [/]                                 :noexport:

- [ ] *THIS SECTION NEEDS TO BE COMPLETELY REVAMPED I THINK*
  -> Giving information about the minimum charge is useful. But it
  does not require very much text.
  -> We _have_ to introduce somewhere well how the gas gain is
  deduced. We have the gas gain time binning section, but the question
  is if we should only introduce how we do it there?
- [ ] *Probably move this to* [[#sec:calibration]]!
  -> *OR* to reconstruction chapter?

- [ ] *THE REFERENCE TO fig. [[fig:daq:thl_calibration_example]] IS
  CONTAINED IN SCURVE SECTION!*




- [X] *TALK ABOUT GAS GAIN*
- [ ] *MAYBE MOVE EXPLANATION ABOUT HOW TO EXTRACT GAS GAIN FROM POLYA
  TO EXPLANATION IN THEORY?*
- [ ] *IF IT STAYS HERE, INTRODUCE WHAT "MEAN OF HISTOGRAM" EVEN MEANS!*  


- [ ] *USE IT TO DETERMINE THRESHOLD*
- [X] *COMPUTE THRESHOLD IN EXAMPLE FROM THL CALIB AND COMPARE*
- [X] *INTRODUCE IT IS USED AS THE PRACTICAL WAY TO COMPUTE GAS GAIN*
- [ ] *MENTION THAT WE BIN BY TIME OF 90min AND THAT WE WILL EXPLAIN
  IN LATER CHAPTER HOW TIME INTERVAL WAS CHOSEN*
- [ ] *GENERATE A NEW PLOT OF POLYA FOR THESIS*

- [ ] *EXPLAIN HOW WE CHOOSE THE FITTING RANGE FOR THE POLYA*
  -> see the ~statusAndProgress~ section ~Determine polya fit range dynamically~!  

Explain how all charge values combined as a histogram generate a
~Pólya distribution from which we can deduce the gas gain.

**** Generate Polya plot for chip 0, run period 3 [0/1]         :noexport:

The current placeholder polya distribution (although it's the right
chip and calibration) is:
#+begin_src sh
basti at void in /mnt/1TB/CAST/2018_2/out/DataRuns2018_Raw_2020-04-28_16-13-28 λ
  cp gas_gain_run_306_chip_0_5_30_min_1545294386.pdf \
     ~/phd/Figs/gas_gain_run_306_chip_0_placeholder_example.pdf
#+end_src

- [ ] *REPLACE BY BETTER PLOT. USING 90MIN AMONG OTHER THINGS*

*** Final Septemboard calibrations [0/0]
:PROPERTIES:
:CUSTOM_ID: sec:operation_calibration:final_septemboard_calibration
:END:

The detector was calibrated according to the descriptions of the
previous sections prior to both major data taking campaigns at CAST
(see sec. [[#sec:cast:data_taking_campaigns]] for a detailed overview of
both campaigns), once in October 2017 and then again in July 2018.

For an overview of all calibration parameters of interest, see the
appendix [[#sec:appendix:septemboard_calibrations]].

Tables [[tab:daq:thl_ths_calibration_run2]] and
[[tab:daq:thl_ths_calibration_run3]] show the =THL= and =THS= DAC values
used for the Septemboard detector at CAST during the data taking
campaign from October 2017 to March 2018 (run 2) and October 2018 to December
2018 (run 3), respectively. The other DACs were all set to the same values for
all chips in both data taking campaigns with the detector, shown in
tab. [[tab:daq:common_dac_values]].

#+CAPTION: The =THL= and =THS= DAC values for each of the chips of the
#+CAPTION: Septemboard (board H) detector used at CAST for the data taking
#+CAPTION: campaign from October 2017 to March 2018 (run 2).
#+NAME: tab:daq:thl_ths_calibration_run2
#+ATTR_LATEX: :booktabs t
|-----+--------+--------+--------+---------+---------+--------+--------|
| DAC | E6 W69 | K6 W69 | H9 W69 | H10 W69 | G10 W69 | D9 W69 | L8 W69 |
|-----+--------+--------+--------+---------+---------+--------+--------|
| THL |    435 |    435 |    405 |     450 |     450 |    400 |    470 |
| THS |     66 |     69 |     66 |      64 |      66 |     65 |     66 |
|-----+--------+--------+--------+---------+---------+--------+--------|


#+CAPTION: The =THL= and =THS= DAC values for each of the chips of the
#+CAPTION: Septemboard (board H) detector used at CAST for the data taking
#+CAPTION: campaign from October 2018 to December 2018 (run 3).
#+NAME: tab:daq:thl_ths_calibration_run3
#+ATTR_LATEX: :booktabs t
|-----+--------+--------+--------+---------+---------+--------+--------|
| DAC | E6 W69 | K6 W69 | H9 W69 | H10 W69 | G10 W69 | D9 W69 | L8 W69 |
|-----+--------+--------+--------+---------+---------+--------+--------|
| THL |    419 |    386 |    369 |     434 |     439 |    402 |    462 |
| THS |     68 |     66 |     66 |      65 |      69 |     65 |     64 |
|-----+--------+--------+--------+---------+---------+--------+--------|
  
#+CAPTION: DAC values and settings that are common between data taking periods and 
#+CAPTION: all chips of the Septemboard for CAST, from the ~fsr~ configuration file.
#+NAME: tab:daq:common_dac_values
#+ATTR_LATEX: :booktabs t
|-------------+------------|
| DAC         |      Value |
|-------------+------------|
| IKrum       |         20 |
| Hist        |          0 |
| GND         |         80 |
| Coarse      |          7 |
| CTPR        | 4294967295 |
| BiasLVDS    |        128 |
| SenseDAC    |          1 |
| DACCode     |          6 |
| RefLVDS     |        128 |
| Vcas        |        130 |
| ExtDAC      |          0 |
| Disc        |        127 |
| Preamp      |        255 |
| FBK         |        128 |
| BuffAnalogA |        127 |
| BuffAnalogB |        127 |
|-------------+------------|

Further, figure [[fig:daq:thl_optimized_distributinons]] shows the
optimized =THL= distributions of all chips after threshold
equalization for run 2 (left) and 3 (right).

#+CAPTION: Distributions of the =THL= values of all Septemboard (board H) chips
#+CAPTION: at the noise peak with the calibration for for run 2 on the left and
#+CAPTION: run 3 on the right.
#+ATTR_LATEX: :width 1\textwidth
#+NAME: fig:daq:thl_optimized_distributinons
[[~/phd/Figs/detector/calibration/sepemboard_all_thl_optimized.pdf]]

**** TODOs for this section [1/6]                               :noexport:

- [ ] *MAYBE MOVE THIS TO APPENDIX TOO?*

- [ ] *MAKE SURE TO ADD CALIBRATIONS TO APPENDIX!*

- [ ] *TURN PLOT INTO TIKZ*

- [X] *WHAT ELSE*?
- [ ] *DISTRIBUTIONS OF 4-BIT DAC?* Only appendix
- [ ] *THEORETICAL THRESHOLDS COMPUTED AS IN SECTION ABOVE. RESULT
  ADDED TO TABLE?*
  

**** Generate the FSR tables for all chips and each run period  :extended:

Let's generate the tables containing the table for the FSR DAC values
for all chips for each of the different run periods.

We will use the =ChipCalibrations= directory that is part of the
=TimepixAnalysis= repository in the =resources= directory.

Further, we will create a plot of all pixels showing the noise peak in
THL values based on the optimized equalization.

#+begin_src nim :tangle code/generate_fsr_table.nim :results raw
import std / [sequtils, strutils, os, tables, algorithm]

const path = "/home/basti/CastData/ExternCode/TimepixAnalysis/resources/ChipCalibrations/"
const periods = ["Run2", "Run3"]
const chipInfo = "chipInfo.txt"
const thrMean = "thresholdMeans$#.txt"
const chips = toSeq(0 .. 6)

import ggplotnim
proc readThresholdMeans(path: string, chip: int): DataFrame =
  echo path / thrMean
  result = readCsv(path / (thrMean % $chip), sep = '\t', colNames = @["x", "y", "min", "max", "bit", "opt"])
    .select("opt")
    .rename(f{"THL" <- "opt"})
    .mutate(f{"chip" <- chip})

# parse the names of the chips from the run info file
var df = newDataFrame()    
for period in periods:
  var header = @["DAC"]
  var tab = initTable[int, seq[(string, int)]]()
  var dfPeriod = newDataFrame()
  for chip in chips:
    proc toTuple(s: seq[seq[string]]): seq[(string, int)] =
      for x in s:
        doAssert x.len == 2
        result.add (x[0], x[1].parseInt)
    let chipPath = path / period / "chip" & $chip
    let data = readFile(chipPath / "fsr" & $chip & ".txt")
      .splitLines
      .filterIt(it.len > 0)
      .mapIt(it.split)
      .toTuple()

    tab[chip] = data

    # read chip name and add to header
    proc readChipName(chip: int): string =
      result = readFile(chipPath / chipInfo)
        .splitLines()[0] 
      result.removePrefix("chipName: ")
    header.add readChipName(chip)

    dfPeriod.add readThresholdMeans(chipPath, chip)
  dfPeriod["Run"] = period
  df.add dfPeriod
  
  proc invertTable(tab: Table[int, seq[(string, int)]]): Table[string, seq[(int, int)]] =
    result = initTable[string, seq[(int, int)]]()
    for chip, data in pairs(tab):
      for (dac, value) in data:
        if dac notin result:
          result[dac] = newSeq[(int, int)]()
        result[dac].add (chip, value)

  proc wrap(s: string): string = "|" & s & "|\n"
  proc toOrgTable(s: seq[seq[string]], header: seq[string]): string =
    let tabLine = wrap toSeq(0 ..< header.len).mapIt("------").join("|")
    result = tabLine
    result.add wrap(header.join("|"))
    result.add tabLine
    for x in s:
      doAssert x.len == header.len
      result.add wrap(x.join("|"))
    result.add tabLine

  proc toOrgTable(tab: Table[string, seq[(int, int)]],
                  header: seq[string]): string =
    var commonDacs = newSeq[seq[string]]()
    var diffDacs = newSeq[seq[string]]()
    for (dac, row) in pairs(tab):
      var fullRow = @[dac]
      let toAdd = row.sortedByIt(it[0]).mapIt($it[1])
      if toAdd.deduplicate.len > 1:
        fullRow.add toAdd
        diffDacs.add fullRow 
      else:
        commonDacs.add @[dac, toAdd.deduplicate[0]]
    result.add toOrgTable(diffDacs, header)
    result.add "\n\n"
    result.add toOrgTable(commonDacs, @["DAC", "Value"])
    # now add common dacs table
  echo "Run: ", period
  echo tab.invertTable.toOrgTable(header)

ggplot(df.filter(f{`THL` > 100}), aes("THL", fill = factor("chip"))) +
  facet_wrap("Run") + 
  geom_histogram(binWidth = 1.0, position = "identity", alpha = 0.7, hdKind = hdOutline) +
  ggtitle("Optimized THL distribution of the noise peak for each chip") +
  ylab(r"\# pixels") +
  facetHeaderText(font = font(12.0, alignKind = taCenter)) + 
  ggsave("/home/basti/phd/Figs/detector/calibration/sepemboard_all_thl_optimized.pdf",
         useTeX = true, standalone = true,
         width = 1000, height = 600)
#+end_src

#+RESULTS:
Run: Run2
|-----+--------+--------+--------+---------+---------+--------+--------|
| DAC | E6 W69 | K6 W69 | H9 W69 | H10 W69 | G10 W69 | D9 W69 | L8 W69 |
|-----+--------+--------+--------+---------+---------+--------+--------|
| THL |    435 |    435 |    405 |     450 |     450 |    400 |    470 |
| THS |     66 |     69 |     66 |      64 |      66 |     65 |     66 |
|-----+--------+--------+--------+---------+---------+--------+--------|


|-------------+------------|
| DAC         |      Value |
|-------------+------------|
| IKrum       |         20 |
| Hist        |          0 |
| GND         |         80 |
| Coarse      |          7 |
| CTPR        | 4294967295 |
| BiasLVDS    |        128 |
| SenseDAC    |          1 |
| DACCode     |          6 |
| RefLVDS     |        128 |
| Vcas        |        130 |
| ExtDAC      |          0 |
| Disc        |        127 |
| Preamp      |        255 |
| FBK         |        128 |
| BuffAnalogA |        127 |
| BuffAnalogB |        127 |
|-------------+------------|

Run: Run3
|-----+--------+--------+--------+---------+---------+--------+--------|
| DAC | E6 W69 | K6 W69 | H9 W69 | H10 W69 | G10 W69 | D9 W69 | L8 W69 |
|-----+--------+--------+--------+---------+---------+--------+--------|
| THL |    419 |    386 |    369 |     434 |     439 |    402 |    462 |
| THS |     68 |     66 |     66 |      65 |      69 |     65 |     64 |
|-----+--------+--------+--------+---------+---------+--------+--------|


|-------------+------------|
| DAC         |      Value |
|-------------+------------|
| IKrum       |         20 |
| Hist        |          0 |
| GND         |         80 |
| Coarse      |          7 |
| CTPR        | 4294967295 |
| BiasLVDS    |        128 |
| SenseDAC    |          1 |
| DACCode     |          6 |
| RefLVDS     |        128 |
| Vcas        |        130 |
| ExtDAC      |          0 |
| Disc        |        127 |
| Preamp      |        255 |
| FBK         |        128 |
| BuffAnalogA |        127 |
| BuffAnalogB |        127 |
|-------------+------------|

*** High voltage [0/0]
:PROPERTIES:
:CUSTOM_ID: sec:operation_calibration:high_voltage
:END:

The high voltage settings used for the Septemboard detector are shown
in tab. [[tab:operation_calibration:hv_settings]]. The target is a drift
field on the order of $\SI{500}{V.cm⁻¹}$ and an amplification field of
about $\SI{60}{kV.cm⁻¹}$. The main voltages to choose are the grid
voltage (to determine the amplification field) and the cathode voltage (to
determine the drift field). The other voltages are computed based on a
constant field gradient. Entries ring 1 and ring 29 are the voltages
applied to the field shaping ring running around the detector volume
to achieve a more homogeneous field.

#+CAPTION: Table of high voltages in use for the InGrid Mk. IV. 
#+CAPTION: Note that the veto scintillator is not controlled via
#+CAPTION: the iseg module, but by a CAEN N470.
#+NAME: tab:operation_calibration:hv_settings
#+ATTR_LATEX: :booktabs t
|-------------+---------+-------------+------------------|
| Description | Channel | Voltage / V | TripCurrent / mA |
|-------------+---------+-------------+------------------|
| grid        |       0 |        -300 |            0.050 |
| anode       |       1 |        -375 |            0.050 |
| cathode     |       2 |       -1875 |            0.050 |
| ring 1      |       3 |        -415 |            0.100 |
| ring 29     |       4 |       -1830 |            0.100 |
| veto scinti |       5 |       +1200 |                2 |
| SiPM        |       6 |       -65.6 |             0.05 |
|-------------+---------+-------------+------------------|

**** TODOs for this section [3/3]                               :noexport:

- [X] *INTRODUCE RING 1 AND RING 29*!!!

- [X] *MENTION HV SETTINGS USED* Table plus short description.

- [X] *SHOWN IN CAST CHAPTER. MOVE HERE?*
  -> Moved here, but still shown in CAST chapter too.

** FADC [/]
:PROPERTIES:
:CUSTOM_ID: sec:operation_calibration:fadc
:END:

The FADC requires care about multiple aspects. First the settings need
to be chosen that configure both the operating characteristics, data
readout and trigger threshold. Next, the Ortec 474 shaping amplifier
has multiple different settings. Finally, in order to interpret the
signals received from the FADC, a so-called "pedestal run" should be
recorded.

- FADC settings :: The chosen settings, as seen in the config file
  explanation in section [[#sec:daq:tos_config_file]], configure the FADC
  to run at a frequency of $\SI{1}{GHz}$ as to cover a running time
  interval of $\SI{10.24}{μs}$. While it could run at up to
  $\SI{2}{GHz}$, $\SI{1}{ns}$ per time bin is accurate enough given
  the time scales associated with the amplifier (see below) and a
  longer interval is more useful. Further, an external trigger is sent
  out to TOS if the threshold is exceeded. The threshold itself is set
  to $\SI{-40}{mV}$ [fn:trigger_threshold]. The value was chosen based
  on trial and error to avoid no triggers based on baseline
  noise. Given the range of up to $\SI{-1}{V}$, the relative threshold
  is pretty low. Finally, the operation mode and data readout is set,
  the input channel is chosen and the pedestal run parameters are
  configured (see below).
- Amplifier settings :: The 474 Ortec shaping amplifier has 3 settings
  of note. The absolute gain as a multiplier and a signal integration as
  well as differentiation time. The initial settings were set to an
  amplification of =6x=, an integration time of $\SI{50}{ns}$ and a
  differentiation time of $\SI{50}{ns}$. However, these were changed
  during the data taking campaign, see more on this in section
  [[#sec:calibration:fadc_noise]].
- Pedestals :: The $4 · \num{2560}$ registers of the FADC are part of
  4 separate cyclic registers. Due to hardware implementation details,
  the absolute recorded values of each register is arbitrary. In a
  pedestal run multiple measurements ($\mathcal{O}(10)$ of a certain
  length ($\SI{100}{ms}$ in our case) are performed and the pedestal
  values averaged. The resulting values represent a mean value for the
  typical value in each register, hence the name 'pedestal'. To
  interpret a real measured signal, these pedestals are subtracted
  from the recorded signal. Each of the 4 FADC channels may have very
  different pedestal values, but within a single channel they are
  usually within $\lesssim\num{50}$ ADC
  values. Fig. [[fig:daq:fadc_pedestal_run]] shows the pedestals of all 4
  FADC channels. The pedestals drift over time, but the associated
  time scales are long. Alternatively, the pedestals can be computed
  from real data by computing a truncated mean in each register, which
  we'll discuss later in sec. [[#sec:reco:fadc_pedestal_calc]].

More details to each of these will be given later where it is of
importance.

#+CAPTION: The FADC pedestal run used for CAST data split by each of the 4 FADC channels. Each
#+CAPTION: channel's pedestals vary by about $\mathcal{O}(30)$ ADC values. The first few registers
#+CAPTION: in each channel are not shown, as they are outlying by $\sim\num{100}$.
#+NAME: fig:daq:fadc_pedestal_run
#+Attr_LATEX: :width 1\textwidth
[[~/phd/Figs/detector/calibration/fadc_pedestal_split_by_channel.pdf]]

[fn:fadc_manual] https://archive.org/details/manualzilla-id-5646050/

[fn:fadc_pedestals_manual] The FADC pedestals can also be computed
from real data by computing a truncated mean of all FADC files in a
data taking run. This yields a usable pedestal to use for that
run. See the extended version of this document for a section about
this.

[fn:trigger_threshold] The trigger threshold DAC is a 12-bit DAC. Its
values correspond to $\SI{-1}{V}$ at ~000~ and $\SI{1}{V}$ at
~FFF~. Hence $\num{1966}$ is roughly $\SI{-40}{mV}$.

*** TODOs for this section [/]                                   :noexport:

  
- [X] pedestals can be obtained from a pedestal run as explained
  here _or_ computed from real data due to signals making up small
  fraction of data and having lots of statistics in one run. Explain
  biased truncated mean used.

- [X] *MOVE PEDESTAL DISCUSSION TO* [[#sec:calibration]]!
  -> We will just mention here that they are "random" values and we'll
  use real data later to correct for them.
  -> *OR* data reconstruction chapter!!!
  -> Moved to [[#sec:reco:fadc_pedestal_calc]]. Calibration is "higher
  level" than this.
- [X] *MOVE THE PEDESTAL* calibration to the later chapter on our
  general calibration stuff?
  *OR* data reconstruction!!!

- [ ] *EQUATION* To convert FADC value of 1966 to 40 mV!!!  

- [X] *THRESHOLD. 40 mV. TO AVOID NOISE TRIGGERS*
- [X] *AMPLIFIER SETTINGS* (CAST data taking notes)
- [X] *PEDESTAL*
- [X] *REWRITE TAKING INTO ACCOUNT PEDESTAL CALCULATION FROM DATA*
- [ ] *LINK TO PEDESTAL CALCULATIONS FROM DATA CODE*
  
*** Generate plot of pedestals                                   :extended:

#+begin_src nim :tangle code/fadc_pedestals_plot.nim
import std / [strutils, os, sequtils, algorithm]
import ggplotnim

const path = "/home/basti/CastData/ExternCode/TimepixAnalysis/resources/pedestalRuns/"
const file = "pedestalRun000042_1_182143774.txt-fadc"
# read the FADC files using our CSV parser. Everything `#` is header
# aside from the last 3 lines. Skip those using `maxLines`
var df = readCsv(path / file, header = "#", maxLines = 10240)
  .rename(f{"val" <- "nb of channels: 0"})
# generate indices 0, 0, 0, 0, 1, 1, 1, 1, ..., 2559, 2559, 2559, 2559 
df["Register"] = toSeq(0 ..< 2560).repeat(4).concat.sorted
df["Channel"] = @[1, 2, 3, 4].repeat(2560).concat
when false:
  df["Channel"] = @[1, 2, 3, 4].repeat(2560)
  df = df.mutate(f{int -> bool: "even?" ~ `Channel` mod 2 == 0})
  echo df
  echo df.tail(20)
  ggplot(df, aes("Channel", "val", color = "even?")) +
    geom_point(size = 1.0) +
    ggtitle("FADC channel values of pedestal run") +  
    ggsave("/home/basti/phd/Figs/detector/calibration/fadc_pedestal_run.pdf")
  #         useTeX = true, standalone = true)
  ggplot(df.group_by("even?").filter(f{float -> bool: `val` < percentile(col("val"), 95)}),
         aes("Channel", "val", color = "even?")) +
    facet_wrap("even?", scales = "free") +
    facetMargin(0.5) +
    margin(bottom = 1.0, right = 3.0) +
    geom_point(size = 1.0) +
    legendPosition(0.91, 0.0) +
    ggtitle("FADC channel values of pedestal run, split by even and odd channel numbers") +
    ggsave("/home/basti/phd/Figs/detector/calibration/fadc_pedestal_split_even_odd.pdf", width = 1200, height = 600)#
  #         useTeX = true, standalone = true)
else:
  ggplot(df.group_by("Channel").filter(f{float -> bool: `val` < percentile(col("val"), 99)}),
         aes("Register", "val", color = "Channel")) +
    facet_wrap("Channel", scales = "free") +
    geom_point(size = 2.0) +
    facetMargin(0.5) +
    margin(bottom = 1.0, right = 2.0) +
    legendPosition(0.87, 0.0) + 
    ylab("Register") + 
    ggtitle("FADC register pedestal values, split by channels") +    
    xlab("Channel", margin = 0.0) + 
    ggsave("/home/basti/phd/Figs/detector/calibration/fadc_pedestal_split_by_channel.pdf",
          useTeX = true, standalone = true)
#+end_src

*** Calculate pedestals from real FADC data [/]                  :extended:

We will now see what happens if we compute the FADC pedestals from the
raw data by computing a truncated mean of all FADC files in a single
run and comparing to the real pedestal run we normally use as a
reference.

*UPDATE*: <2022-12-26 Mon 00:48> This was a big success. We will use
this in our real data from now on, move this to ~statusAndProgress~
and rewrite the section above with that in mind, i.e. explain how to
calc pedestals.
- [ ] *REWRITE MAIN TEXT*

- [ ] *TEST PEDESTAL CALC FOR BACKGROUND DATA*

- [ ] look at c-blake's ideas using `MovingStat`, a logarithmic histogram etc. to avoid
  the multiple passes over the data as we do using `sort`! See my journal.org notes about
  this!

#+begin_src nim :tangle code/attempt_fadc_pedestals_from_data.nim
import std / [strutils, os, sequtils, sugar, algorithm]
import ggplotnim

# to read from H5 input
import nimhdf5
import ingrid / tos_helpers
import ingrid / ingrid_types

proc readFadc(f: string): DataFrame =
  # read the FADC files using our CSV parser. Everything `#` is header
  # aside from the last 3 lines. Skip those using `maxLines`
  result = readCsv(f, header = "#", maxLines = 10240)
    .rename(f{"val" <- "nb of channels: 0"})
  #result["Channel"] = toSeq(0 ..< result.len)
  result["Register"] = toSeq(0 ..< 2560).repeat(4).concat.sorted
  result["Channel"] = @[1, 2, 3, 4].repeat(2560).concat

const Size = 5000

proc getFadcDset(h5f: H5File, runNumber: int): H5DataSet =
  let fadcGroup = fadcRawPath(runNumber)
  doAssert fadcGroup in h5f
  let group = h5f[fadcGroup.grp_str]
  result = h5f[(group.name / "raw_fadc").dset_str]
  
proc readChannel(h5f: H5File, dset: H5DataSet, start: int): seq[uint16] =
  let toRead = min(Size, dset.shape[0] - start)
  result = read_hyperslab(dset, uint16,
                          offset = @[start, 0],
                          count = @[toRead, dset.shape[1]])
  
import weave
proc percIdx(q: float, len: int): int = (len.float * q).round.int

proc biasedTruncMean*[T](x: Tensor[T], axis: int, qLow, qHigh: float): Tensor[float] =
  ## Computes the *biased* truncated mean of `x` by removing the quantiles `qLow` on the
  ## bottom end and `qHigh` on the upper end.
  ## ends of the data. `q` should be given as a fraction of events to remove on both ends.
  ## E.g. `qLow = 0.05, qHigh = 0.99` removes anything below the 5-th percentile and above the 99-th.
  ##
  ## Note: uses `weave` internally to multithread along the desired axis!
  doAssert x.rank == 2
  result = newTensorUninit[float](x.shape[axis])
  init(Weave)
  let xBuf = x.toUnsafeView()
  let resBuf = result.toUnsafeView()
  let notAxis = if axis == 0: 1 else: 0
  let numH = x.shape[notAxis] # assuming row column major, 0 is # rows, 1 is # cols
  let numW = x.shape[axis]
  parallelFor i in 0 ..< numW:
    captures: {xBuf, resBuf, numH, numW, axis, qLow, qHigh}
    let xT = xBuf.fromBuffer(numH, numW)
    # get a sorted slice for index `i`
    let subSorted = xT.atAxisIndex(axis, i).squeeze.sorted
    let plow = percIdx(qLow, numH) 
    let phih = percIdx(qHigh, numH)

    var resT = resBuf.fromBuffer(numW)
    ## compute the biased truncated mean by slicing sorted data to lower and upper
    ## percentile index
    var red = 0.0 
    for j in max(0, plow) ..< min(numH, phih): # loop manually as data is `uint16` to convert
      red += subSorted[j].float
    resT[i] = red / (phih - plow).float
  syncRoot(Weave)
  exit(Weave)

defColumn(uint16, uint8)  
proc readFadcH5(f: string, runNumber: int): DataFrame = #seq[DataTable[colType(uint16, uint8)]] =
  let h5f = H5open(f, "r")
  let registers = toSeq(0 ..< 2560).repeat(4).concat.sorted
  let channels = @[1, 2, 3, 4].repeat(2560).concat
  let idxs = arange(3, 2560, 4)
  ## XXX: maybe just read the hyperslab that corresponds to a single channel over
  ## the whole run? That's the whole point of those after all.
  ##  -> That is way too slow unfortunately
  ## XXX: better replace logic by going row wise N elements instead of column wise.
  ## Has the advantage that our memory requirements are constant and not dependent
  ## on the number of elements in the run. If we then average over the resulting N
  ## pedestals, it should be fine.
  let dset = getFadcDset(h5f, runNumber)
  var val = newTensor[float](2560 * 4)
  when true:
    var slices = 0
    for i in arange(0, dset.shape[0], Size):
      # read 
      let data = readChannel(h5f, dset, i)
      let toRead = min(Size, dset.shape[0] - i)
      echo "Reading..."
      let dT = data.toTensor.reshape([toRead, data.len div toRead])
      echo "Read ", i, " to read up to : ", toRead, " now processing..."
      inc slices
      val += biasedTruncMean(dT, axis = 1, qLow = 0.2, qHigh = 0.98)
    for i in 0 ..< 2560 * 4:
      val[i] /= slices.float
  result = toDf({"Channel" : channels, val, "Register" : registers})
  #for fadc in h5f.iterFadcFromH5(runNumber):
  #  let datCh3 = fadc.data[idxs] # .mapIt(it.int)
  #  var df = toDf({"val" : dat, "Register" : registers, "Channel" : channels})
  #  result.add df

## Main function to avoid bug of closure capturing old variable  
proc readFadcData(path: string, runNumber: int): DataFrame =
  var df = newDataFrame()
  if path.endsWith(".h5"):
    doAssert runNumber > 0
    df = readFadcH5(path, runNumber)
  else:
    var dfs = newSeq[DataFrame]()
    var i = 0
    for f in walkFiles(path / "*.txt-fadc"):
      echo "Parsing ", f
      dfs.add readFadc(f)
      inc i
      if i > 5000: break
    let dfP = assignStack(dfs)
    var reg = newSeq[int]()
    var val = newSeq[float]()
    var chs = newSeq[int]()
    for (tup, subDf) in groups(dfP.group_by(["Channel", "Register"])):
      echo "At ", tup
      let p20 = percentile(subDf["val", float], 20)
      let p80 = percentile(subDf["val", float], 80)
      reg.add tup[1][1].toInt
      chs.add tup[0][1].toInt
      let dfF = if p80 - p20 > 0: subDf.filter(dfFn(subDf, f{float: `val` >= p20 and `val` <= p80}))
                else: subDf
      val.add dfF["val", float].mean
    df = toDf({"Channel" : chs, val, "Register" : reg})
  df.writeCsv("/t/pedestal.csv")
  echo df.pretty(-1)
  result = df

proc main(path: string, outfile: string = "/t/empirical_fadc_pedestal_diff.pdf",
          plotVoltage = false,
          runNumber = -1,
          onlyCsv = false
         ) =
  let pData = readFadcData(path, runNumber)
  if onlyCsv: return
  const path = "/home/basti/CastData/ExternCode/TimepixAnalysis/resources/pedestalRuns/"
  const file = "pedestalRun000042_1_182143774.txt-fadc"
  let pReal = readFadc(path / file)
  echo "DATA= ", pData
  echo "REAL= ", pReal
  var df = bind_rows([("Data", pData), ("Real", pReal)], "ID")
    .spread("ID", "val")
    .mutate(f{"Diff" ~ abs(`Data` - `Real`)})
    # alternatively compute the voltage corresponding to the FADC register values,
    # assuming 12 bit working mode (sampling_mode == 0)
    .mutate(f{"DiffVolt" ~ `Diff` / 2048.0})
  var plt: GgPlot
  if plotVoltage:
    plt = ggplot(df, aes("Register", "DiffVolt", color = "Channel")) +
      ylim(0, 100.0 / 2048.0)    
  else:
    plt = ggplot(df, aes("Register", "Diff", color = "Channel")) +
      ylim(0, 100)
  plt +
    geom_point(size = 1.5, alpha = 0.75) +
    ylab("Difference between mean and actual pedestal [ADC]") + 
    ggtitle("Attempt at computing pedestal values based on truncated mean of data") +
    margin(top = 2) +
    xlim(0, 2560) + 
    ggsave(outfile)
  
when isMainModule:
  import cligen
  dispatch main
#+end_src

We can call it on different runs with FADC data:
#+begin_src sh
code/attempt_fadc_pedestals_from_data \
#    -p /mnt/1TB/CAST/2017/DataRuns/Run_77_171102-05-24 \
    -p ~/CastData/data/2017/Run_96_171123-10-42 \
#    --outfile ~/phd/Figs/detector/calibration/pedestal_from_data_compare_real_run77.pdf
    --outfile ~/phd/Figs/detector/calibration/pedestal_from_data_compare_real_run96.pdf    
#+end_src

#+RESULTS:

for an early 2017 run
#+begin_src sh
code/attempt_fadc_pedestals_from_data \
    -p /mnt/1TB/CAST/2018_2/DataRuns/Run_303_181217-16-52 \
    --outfile ~/phd/Figs/detector/calibration/pedestal_from_data_compare_real_run303.pdf
#+End_src

#+RESULTS:

for a late 2018 run.

These yield fig. \subref{fig:daq:fadc_pedestals_from_data_compare_real}

#+begin_src subfigure
(figure () 
  (subfigure (linewidth 0.5) (caption "Run 77") (label "fig:daq:fadc_pedestals_from_data_compare_real_run77")
             (includegraphics (list (cons 'width (linewidth 0.95)))
                              "~/phd/Figs/detector/calibration/pedestal_from_data_compare_real_run77.pdf"))
  (subfigure (linewidth 0.5) (caption "Run 303") (label "fig:reco:fadc_pedestals_from_data_compare_real_run303")
             (includegraphics (list (cons 'width (linewidth 0.95)))
                              "~/phd/Figs/detector/calibration/pedestal_from_data_compare_real_run303.pdf"))

  (caption
   "Calculation of the FADC pedestals from data by averaging all channels
    over all FADC data files of a single run using a truncated mean from the 20-th
    to 80-th percentile of the distribution. Both data runs show a comparatively small
    variance. Arguably it may make sense to \\textit{always} compute it based on each
    run instead of relying on a pedestal run though.")
  (label "fig:daq:fadc_pedestals_from_data_compare_real")
)
#+end_src

Surprisingly, the deviation for the end 2018 run is lower than for the
2017 run, despite the 2017 run being closer in time to the real
pedestal run.

Keep in mind that in our data taking we used the 12 bit readout
mode. This means the register values divided by \num{2048} correspond
to the voltages recorded by the register. As such the absolute values
of the deviations are quite a bit smaller than $\lesssim \SI{48}{mV}$
(which is small given the absolute range of $±\SI{1}{V}$ of the FADC.

#+begin_src nim :tangle code/fadc_apply_data_based_pedestal.nim
import nimhdf5, ggplotnim
import std / [strutils, os, sequtils]
import ingrid / [tos_helpers, fadc_helpers, ingrid_types, fadc_analysis]

proc stripPrefix(s, p: string): string =
  result = s
  result.removePrefix(p)

proc plotIdx(df: DataFrame, fadcData: Tensor[float], idx: int) =
  let xmin = df["argMinval", int][idx]
  let xminY = df["minvals", float][idx]
  let xminlineX = @[xmin, xmin] # one point for x of min, max
  let fData = fadcData[idx, _].squeeze
  let xminlineY = linspace(fData.min, fData.max, 2)

  let riseStart = df["riseStart", int][idx]
  let fallStop = df["fallStop", int][idx]
  let riseStartX = @[riseStart, riseStart]
  let fallStopX = @[fallStop, fallStop]
  let baseline = df["baseline", float][idx]  
  let baselineY = @[baseline, baseline]
  
  let df = toDf({ "x"         : toSeq(0 ..< 2560),
                  "baseline"  : baseline,
                  "data"      : fData,
                  "xminX"     : xminlineX, 
                  "xminY"     : xminlineY,
                  "riseStart" : riseStartX,
                  "fallStop"  : fallStopX })
                  # Comparison has to be done by hand unfortunately
  let path = "/t/fadc_spectrum_baseline.pdf"
  ggplot(df, aes("x", "data")) +
    geom_line() +
    geom_point(color = color(0.1, 0.1, 0.1, 0.1)) +
    geom_line(data = df.head(2), aes = aes("x", "baseline"),
                     color = "blue") +
    geom_line(data = df.head(2), aes = aes("xminX", "xminY"),
                     color = "red") +
    geom_line(data = df.head(2), aes = aes("riseStart", "xminY"),
                     color = "green") +
    geom_line(data = df.head(2), aes = aes("fallStop", "xminY"),
                     color = "pink") +
    ggsave(path)

proc plotCompare(data, real: Tensor[float]) =
  let path = "/t/fadc_spectrum_compare.pdf"
  for idx in 0 ..< data.shape[0]:
    let df = toDf({ "x" : toSeq(0 ..< 2560),
                    "data" : data[idx, _].squeeze,
                    "real" : real[idx, _].squeeze })
      .gather(["data", "real"], "type", "vals")
    ggplot(df, aes("x", "vals", color = "type")) +
      geom_line() +
      #geom_point(color = color(0.1, 0.1, 0.1, 0.1)) +
      ggsave(path)
    sleep(1000)

proc getFadcData(fadcRun: ProcessedFadcRun, pedestal: seq[uint16]): Tensor[float] =
  let ch0 = getCh0Indices()
  let
    fadc_ch0_indices = getCh0Indices()
    # we demand at least 4 dips, before we can consider an event as noisy
    n_dips = 4
    # the percentile considered for the calculation of the minimum
    min_percentile = 0.95
    numFiles = fadcRun.eventNumber.len
  
  var fData = ReconstructedFadcRun(
    fadc_data: newTensorUninit[float]([numFiles, 2560]),
    eventNumber: fadcRun.eventNumber,
    noisy: newSeq[int](numFiles),
    minVals: newSeq[float](numFiles)
  )

  for i in 0 ..< fadcRun.eventNumber.len:
    let slice = fadcRun.rawFadcData[i, _].squeeze
    let data = slice.fadcFileToFadcData(
      pedestal,
      fadcRun.trigRecs[i], fadcRun.settings.postTrig, fadcRun.settings.bitMode14,
      fadc_ch0_indices).data
    fData.fadc_data[i, _] = data.unsqueeze(axis = 0)
    fData.noisy[i]        = data.isFadcFileNoisy(n_dips)
    fData.minVals[i]      = data.calcMinOfPulse(min_percentile)
  when false:
    let data = fData.fadcData.toSeq2D
    let (baseline, xMin, riseStart, fallStop, riseTime, fallTime) = calcRiseAndFallTime(
      data, 
      false
    )
    let df = toDf({ "argMinval" : xMin.mapIt(it.float),
                    "baseline" : baseline.mapIt(it.float),
                    "riseStart" : riseStart.mapIt(it.float),
                    "fallStop" : fallStop.mapIt(it.float),
                    "riseTime" : riseTime.mapIt(it.float),
                    "fallTime" : fallTime.mapIt(it.float),
                    "minvals" : fData.minvals })
    for idx in 0 ..< df.len:
      plotIdx(df, fData.fadc_data, idx)
      sleep(1000)

  result = fData.fadcData

proc readFadc(f: string): DataFrame =
  # read the FADC files using our CSV parser. Everything `#` is header
  # aside from the last 3 lines. Skip those using `maxLines`
  result = readCsv(f, header = "#", maxLines = 10240)
    .rename(f{"val" <- "nb of channels: 0"})
  #result["Channel"] = toSeq(0 ..< result.len)
  result["Register"] = toSeq(0 ..< 2560).repeat(4).concat.sorted
  result["Channel"] = @[1, 2, 3, 4].repeat(2560).concat

proc main(fname: string, runNumber: int) =
  var h5f = H5open(fname, "r")
  let pedestal = readCsv("/t/pedestal.csv") # created from above
    .arrange(["Register", "Channel"])
  echo pedestal
  const path = "/home/basti/CastData/ExternCode/TimepixAnalysis/resources/pedestalRuns/"
  const file = "pedestalRun000042_1_182143774.txt-fadc"
  let pReal = readFadc(path / file)
  
  let fileInfo = h5f.getFileInfo()
  for run in fileInfo.runs:
    if run == runNumber:
      let fadcRun = h5f.readFadcFromH5(run)
      let fromData = fadcRun.getFadcData(pedestal["val", uint16].toSeq1D)
      let fromReal = fadcRun.getFadcData(pReal["val", uint16].toSeq1D)

      plotCompare(fromData, fromReal)
      
when isMainModule:
  import cligen
  dispatch main
#+end_src

- [ ] *SHOW PLOT OF THE ABOVE (~plotCompare~) AS EXAMPLE OF BAD
  PEDESTALS VS GOOD PEDESTALS?*
  -> Would be nice for extended thesis!

** Scintillator calibration
:PROPERTIES:
:CUSTOM_ID: sec:operation_calibration:scintillators
:END:

The final piece of the detector requiring calibration, are the two
scintillators. As both of them are only used as digital triggers and
no analogue signal information is recorded, a suitable discriminator
threshold voltage has to be set.

*** TODOs for this section                                       :noexport:

- [ ] *MENTION THAT WE PREFER TOO HIGH THRESHOLD OVER TOO LOW? BETTER
  FEWER SIGNALS THAN LOTS OF NOISE?*

*** Large scintillator paddle

The large veto scintillator paddle was calibrated at the RD51
laboratory at CERN prior to the data taking campaign in
March 2017. Using two smaller, calibrated scintillators to create a
coincidence setup of the three scintillators, measurements were taken
at different thresholds. Each measurement was $\SI{10}{\minute}$ long.
An amplifier was installed after the PMT to increase the signal.

The Canberra 2007 base and Bicron Corp. 31.49x15.74M2BC408/2-X PMT
require a positive high voltage. It was supplied with
$\SI{1200}{V}$. Table [[tab-daq-scintillator_coincidence_measurements]]
shows the recorded measurements. Based on these a threshold of
$\SI{-110}{mV}$ was chosen for the CAST data
taking. Fig. [[fig:daq:veto_scintillator_coincidence]] also shows the data
from table. While the coincidence counts at $\SI{-110}{mV}$ are below
the visible plateau above $\SI{-100}{mV}$, the threshold was chosen as
the raw counts were still considered too high compared to expectation
based on cosmic muon rate and the size of the
scintillator. [fn:threshold_choice]

#+CAPTION: Measurements for the calibration of the large veto scintillator taken
#+CAPTION: at RD51 at CERN with two smaller, calibrated scintillators in a coincidence.
#+CAPTION: Each measurement was \SI{10}{min}. The thresholds set on the discriminator for
#+CAPTION: the veto scintillator were originally measured
#+CAPTION: with a 10x scaling and have been rescaled here to their correct values.
#+NAME: tab-daq-scintillator_coincidence_measurements
#+ATTR_LATEX: :booktabs t
|----------------+---------------+--------------------|
| Threshold / mV | Counts Szinti | Counts Coincidence |
|----------------+---------------+--------------------|
|          -59.8 |         31221 |                634 |
|          -70.0 |         30132 |                674 |
|          -80.4 |         28893 |                635 |
|          -90.3 |         28076 |                644 |
|         -100.5 |         27012 |                684 |
|         -110.3 |         25259 |                566 |
|         -120.0 |         22483 |                495 |
|         -130.3 |         19314 |                437 |
|         -140.3 |         16392 |                356 |
|         -150.5 |         13677 |                312 |
|         -160.0 |         11866 |                267 |
|         -170.1 |         10008 |                243 |
|----------------+---------------+--------------------|

#+CAPTION: Calibration measurements for the veto scintillator printed in table
#+CAPTION: [[tab-daq-scintillator_coincidence_measurements]]. The line is simply an
#+CAPTION: interconnection of all data points. The errors simply represent Poisson-like
#+CAPTION: $\sqrt{N}$ uncertainties.
#+NAME: fig:daq:veto_scintillator_coincidence
[[~/phd/Figs/detector/calibration/veto_scintillator_calibration_coinc_rd51.pdf]]

See the appendix [[#sec:appendix:scintillator_calibration_notes]] for a
reproduction of the notes taken during the calibration of the veto
paddle scintillator.

[fn:threshold_choice] While it is unclear to me now given it's been 5
years, I believe at the time of the calibration we wrongly assumed a
muon rate of $\SI{100}{Hz.m⁻².sr⁻¹}$ instead of about
$\SI{1}{Hz.min⁻¹}$. The former number only works out if one integrates
it over the $\cos^2(θ)$ dependence, _but only along $θ$_ and not $φ$!
Either way, the number seems problematic. However, it did misguide us
in likely choosing a too low threshold, as using the former number
yields an expected number of counts of $\sim\num{32000}$ compared to
only $\sim\num{20000}$ in our naive approach.

**** TODOs for this section [/]                                 :noexport:

- [ ] *MUON RATE MENTIONED IN FOOTNOTE. Hz/min ? WHAT IS
  THAT. MISSING cm²?*
  -> *CHECK THIS!!!*

- [X] Part that now lives in appendix.
  -> ??? I think referring to notes of scintillator calibration
  -> Referenced the appendix.

**** Generate the plots of the scintillator calibration data    :extended:

#+begin_src nim :var tbl=tab-daq-scintillator_coincidence_measurements :results none :exports none
import ggplotnim, sequtils
let df = toDf({ "Thr" : tbl["Threshold / mV"],
                "Szinti" : tbl["Counts Szinti"],
                "Coinc" : tbl["Counts Coincidence"] })
  .mutate(f{"SzintiErr" ~ sqrt(`Szinti`)},
          f{"CoincErr" ~ sqrt(`Coinc`)})
## XXX: `ebLinesT` is broken?!  
ggplot(df, aes("Thr", "Szinti")) +
  geom_point() + geom_line() +
  geom_errorbar(aes = aes(yMin = f{`Szinti` - `SzintiErr`},
                          yMax = f{`Szinti` + `SzintiErr`}),
                errorBarKind = ebLines,
                color = parseHex("FF00FF")) + 
  xlab(r"Threshold [\si{mV}]") + ylab(r"Counts [\#]") +
  ggtitle(r"Calibration measurements of \SI{10}{min} each") + 
  ggsave("/home/basti/phd/Figs/detector/calibration/veto_scintillator_calibration_rd51.pdf",
         useTeX = true, standalone = true)

ggplot(df, aes("Thr", "Coinc")) +
  geom_point() + geom_line() +
  geom_errorbar(aes = aes(yMin = f{`Coinc` - `CoincErr`},
                          yMax = f{`Coinc` + `CoincErr`}),
                errorBarKind = ebLines) +   
  xlab(r"Threshold [\si{mV}]") + ylab(r"Counts [\#]") +
  ggtitle(r"Calibration measurements of \SI{10}{min} each in 3-way coincidence") + 
  ggsave("/home/basti/phd/Figs/detector/calibration/veto_scintillator_calibration_coinc_rd51.pdf",
         useTeX = true, standalone = true)
#+end_src
  

**** Raw scintillator data  :extended:

#+CAPTION: Calibration measurements for the veto scintillator printed in table
#+CAPTION: [[tab-daq-scintillator_coincidence_measurements]]. In this case the
#+CAPTION: raw data is shown instead of the coincidence. The line is simply an
#+CAPTION: interconnection of all data points. The errors are colored to be seen at all.
#+NAME: fig:daq:veto_scintillator_raw_counts
[[~/phd/Figs/detector/calibration/veto_scintillator_calibration_rd51.pdf]]

In particular looking at the raw counts, in hindsight I would now
probably choose a threshold closer to \SI{-90}{mV} or
\SI{-95}{mV}. But well.

**** Calculate expected rate  :extended:

Let's compute the expected rate based on a mean cosmic muon rate at
the surface and the area of the scintillator.

- [X] *NOTE: <2023-03-14 Tue 11:49>* The talk about the veto system of
  the SDD detector at the IAXO collaboration meeting March 2023 had
  the following number for the sea level muon rate:
  0.017 Hz•cm⁻²
  -> Ah, this is close to ~1 cm⁻²•min⁻¹!
  #+begin_src nim
import unchained
let rate = 0.017.Hz•cm⁻²
echo "Rate in ", rate.toDef(min⁻¹•cm⁻²)
  #+end_src

  #+RESULTS:
  : Rate in 1.02 cm⁻²•min⁻¹

The scintillator has a size of 31.49x15.74 inches and we roughly have
a mean cosmic muon rate of 1 per cm⁻² min⁻¹. Measurement time was 600 s.
#+begin_src nim
import unchained
let rate = 1.cm⁻²•min⁻¹
let area = 31.49.inch * 15.74.inch
let time = 600.s
echo "Expected rate: ", time * rate * area
#+end_src

#+RESULTS:
: Expected rate: 32617.1 UnitLess

So about 32,000 counts in the 10 min time frame. It's a bit
frustrating that for some reason during that calibration we assumed a
muon rate of 100 Hz m⁻² sr⁻¹, so that we only got an expected number
of counts of about 20,000.

If we assume the 100 Hz m⁻² sr⁻¹ number and integrate only over $θ$
(not $φ$ as we should also!) using the $\cos² θ$ dependence we get a
more comparable number:

#+begin_src nim
import unchained
let integral = 1.5708.sr # ∫_{-π/2}^{π/2} cos²(θ) dθ = 1.5708
let rate = 100.Hz•m⁻²•sr⁻¹
let angleInt = 2*π
let time = 600.s
let area = 31.49.inch * 15.74.inch
echo rate * time * area * integral
#+end_src

#+RESULTS:
: 30138.2 UnitLess

In any case, it seems like our assumption of 20000 as seen in appendix
[[#sec:appendix:scintillator_calibration_notes]] is clearly flawed and
lead to a possibly too large threshold for the discriminator.

In addition: why the hell did I not take note of the size of the
scintillators that Theodoros gave us? That would be a much better
cross check for the expected rate. It is certainly possible that our
choice of $\SI{-110}{mV}$ was actually due to an expected coincidence
rate matching at that threshold given the sizes of the calibrated
scintillators, but I can't verify that anymore.

*** SiPM [0/0]

The SiPM was calibrated during the bachelor thesis of Jannes Schmitz
in 2016 cite:JannesBSc based on a coincidence measurement with
calibrated scintillators.

**** TODOs for this section [/]                                 :noexport:

The exact threshold used *FIND A VALUE OR REPHRASE*.
- [ ] *FIND VALUE*

- [ ] *WELL* If I understand Jannes correctly there's not much in
  terms of a "value" to be had. Pretty much lost. One could try to
  measure that nowadays of course, but there's not really a point in
  doing so.  

* Data reconstruction [0/1]                                  :reconstruction:
:PROPERTIES:
:CUSTOM_ID: sec:reconstruction
:END:

#+LATEX: \minitoc

As discussed in the previous chapter, in particular section
[[#sec:daq:tos_output_format]], the output data format of the Septemboard data
acquisition software is ASCII based. We will now go through the
general data reconstruction based on recorded data for each of the
detector components. Motivations for the type of reconstructions
performed, will be given where necessary.

Starting with the GridPix data in section [[#sec:reco:gridpix_data]] as
it's the main component and its properties and shortcomings define the
need for the other detector components. The FADC data reconstruction
follows in section [[#sec:reco:fadc_data]]. Finally the scintillators are
mentioned in section [[#sec:reco:scintillators_data]].

There is an additional long section in the appendix
[[#sec:appendix:software]] that goes through the software used for the
data reconstruction intended for people using these tools in the
future. And appendix [[#sec:appendix:full_data_reconstruction]] shows how
the full data reconstruction for all CAST data presented in this
thesis is performed.

** TODOs for this section [/]                                     :noexport:


- [X] *WRITE CHAPTER SUMMARY*

- explain the whole data reconstruction pipeline? What we do with
  ingrid data, clustering, reconstruction
- everything up to energy calibration
- [ ] wherever we finally show the real calibrations of the detector
  for each run, show:
  - [X] the full FSR (for one chip, then differences for each,
    particularly THS & THL)
    -> Done in operation_calibration
  - [X] show plot of THL calibration from SCurves, with the THL value as a
    "rectangle" line (from y axis to fit, down to x axis)
    -> Done in operation_calibration    
  - [ ] in Polya plot show the theoretical threshold in electrons as a
    vertical line (computed from the used THL DAC value and THL
    calibration)
    -> Partially done in operation_calibration, but needs rework!!!

- [X] *SHORT INTRO TO TIMEPIX ANALYSIS AND NIM HERE. LIKE 2
  PARAGRAPHS.* 
  -> Below.

** ~TimepixAnalysis~ and Nim

The data reconstruction software handling the processes mentioned in
the remainder of this chapter is the ~TimepixAnalysis~ cite:TPA [fn:tpa_github]
framework. It is only a "framework" in a loose sense, as it is a set
of programs to parse data from different Timepix (usually GridPix) DAQ
software packages, process and analyze it. In addition it contains
a large number of tools to visualize that data as well as analyze
auxiliary data like lists of data taking runs, CAST log files and
more.

The entire code base is written in the Nim programming language
cite:nim [fn:nim_lang]. Nim is a statically typed, compiled language
with a Python-like whitespace sensitive syntax, taking inspirations
from Pascal, Modula and in particular ideas regarding type safety from
Ada. Further, it has a strong metaprogramming focus with full access
to its abstract syntax tree (AST) at compile time, offering Lisp-like
macro functionality. This allows to construct powerful and concise
domain specific languages (DSLs). Nim compiles its code by default
first to C code, which can then utilize the decades of compiler
optimization techniques available via GCC cite:gcc or Clang
cite:LLVM:CGO04, while allowing to target every single platform
supported by C (which effectively means almost all). As such it also
achieves performance on-par with C, while providing high-level
features normally associated with languages like Python.

Nim was chosen as the language of choice for ~TimepixAnalysis~, due to
its combination of concise and clean syntax, high-level features for
productivity, high performance due to native code generation, easy
interfacing with existing C and C++ code bases and its strong
metaprogramming features, which allow to reduce boilerplate code to a
minimum.

Appendix [[#sec:appendix:timepix_analysis]] contains a detailed overview of the
important tools part of ~TimepixAnalysis~ and how they are used for
the context of the data analysis presented in this thesis. Read it if
you wish to understand how to recreate the results presented in this thesis.

[fn:tpa_github] https://github.com/Vindaar/TimepixAnalysis

[fn:nim_lang] [[https://nim-lang.org]]

** TOS data parsing [0/0]
:PROPERTIES:
:CUSTOM_ID: sec:reco:tos_data_parsing
:END:

The first part of the GridPix data reconstruction is the parsing of
the raw ASCII data files, presented in
[[#sec:daq:tos_output_format]]. This is implemented in the
~raw_data_manipulation~ program [fn:reco_raw_data_manip], part of
~TimepixAnalysis~ cite:TPA. Its main purpose is the
conversion of the inefficient ASCII data format to the more
appropriate and easier to work with HDF5 [fn:hdf5] cite:hdf5 format,
a binary file format intended for scientific datasets. While this data
conversion is the main purpose, pixels with ~ToT~ or ~ToA~ values
outside a user defined range can be filtered out at this
stage [fn:toa_tot_filter_in_raw]. Each data taking run is processed
separately and will be represented by one group (similar to a
directory on a file system) in the output HDF5 file.

As the data is being processed anyway, at this time we already compute
an occupancy map for each chip in the run. This allows for a quick
glance at the (otherwise unprocessed) data.

[fn:reco_raw_data_manip]
https://github.com/Vindaar/TimepixAnalysis/blob/master/Analysis/ingrid/raw_data_manipulation.nim

[fn:hdf5] HDF5 cite:hdf5 is the Hierarchical Data Format of
version 5. It is a binary data format intended for scientific
datasets, which uses an in-file layout similar to a virtual file
system. Datasets (equivalent to files) are stored in groups
(equivalent to directories). Metadata can be attached to either and
linking between datasets, even across files is supported. It supports
a large number of compression filters to reduce the file size of the
stored data.

[fn:toa_tot_filter_in_raw] These type of cuts are applied at this
stage of the processing, because for certain use cases or certain
detectors specific ~ToT~ or ~ToA~ ranges are of no interest / contain
junk data (because of a faulty chip for example). In this case it is
useful to remove such data in this preprocessing stage to lighten the
workload for anything after.

*** Example of running ~raw_data_manipulation~ [0/1]             :extended:

- [ ] GIVE EXAMPLE
  -> Need an actual run to work with. Use ~TPAResources~?

#+begin_src sh
raw_data_manipulation -p <path> --runType rtBackground --h5out /tmp/run_foo.h5
#+end_src


*** HDF5 data layout generated by ~raw_data_manipulation~        :optional:

The listing [[code:reco:abstract_hdf5_layout]] shows the layout of the
data stored in the HDF5 files after the ~raw_data_manipulation~
program has processed the TOS run folders. The data is structured
in groups based on each run, chip and the FADC (if
available). Generally each "property" is stored in its own dataset
for performance reasons to allow faster access to individual subsets
of the data (read only the hits, only $x/y$ data, etc.). While HDF5
supports even heterogeneous compound datasets (that is different data
types in different "columns" of a 2D like dataset), these are only
used sparingly and not at all in the ~raw_data_manipulation~ output,
as reading individual columns from these is inefficient. 

#+CAPTION: An abstract overview of the general layout of the generated HDF5 files.
#+CAPTION: Each entry shown that does not have any children is an HDF5 group. Every
#+CAPTION: leaf node is an HDF5 dataset. The data is ordered by availability. Each
#+CAPTION: run is a separate group. Within all chips have their own groups with data
#+CAPTION: associated to that chip. The common datasets are those that contain data
#+CAPTION: from the TOS event header. FADC data is that, which is contained in the
#+CAPTION: FADC files (for which fewer than regular TOS data files exist).
#+NAME: code:reco:abstract_hdf5_layout
#+begin_src toml
- runs
  - run_<number>
    - chip_0 # one for each chip in the event
      - Hits # number of hits in each event
      - Occupancy # a 2D occupancy map of this run
      - ToT # all ToT values of this run
      - raw_ch # the ToT/ToA values recorded for each event (ragged data)
      - raw_x # the x coordinates recorded for each event (ragged data)
      - raw_y # the y coordinates recorded for each event (ragged data)
    - chip_i # all other chips
      - ...
    - fadc # if available
      - eventNumber # event number of each entry
                    # (not all events have FADC data)
      - raw_fadc # raw FADC data (uncorrected, all 10240 registers)
      - trigger_record # temporal correction factor for each event
    - fadcReadout # flag if FADC was readout in each event
    - fadcTriggerClock # clock cycle FADC triggered
    - szintillator trigger clocks # datasets for each scintillator
    - timestamp # timestamp of each event
  - run_i # all other runs
    - ...
#+end_src

**** Notes on data layout                                       :extended:

Of course we generate the layout from real data files. Either using
~h5ls~ or using ~nimhdf5~, most conveniently using its iterators
and/or the JSON interface of it.

** Expectation of event shapes [0/0]

Based on the theoretical aspects of a gaseous detector as explained in
chapter [[#sec:theory_detector]] and the expected kinds of signal sources
at an experiment like CAST (c.f. [[#sec:cast]]), we can have a good
expectation of the kinds of signals that a GridPix detector records
for different types of events.

The signal source we are interested in for an axion helioscope are
soft energy X-rays, below $\SI{10}{keV}$. The main goal in later
determining a background rate and computing a physics result from data
is to filter out these X-rays from the rest of the data the detector
records. The dominant source of background in any gaseous detector at
surface level is due to cosmic muons.

Fortunately, muons and X-rays behave very different in the
detector. X-rays generally produce a single photoelectron, which
creates further primary electrons in a local region. These drift under
transverse diffusion to the readout plane, which effectively gives
them a roughly circular shape. Muons on the other hand produce
electrons (which each produce further local primaries) on a track
along their entire path through the gas volume. Under most angles this
implies their shape is very eccentric, i.e. 'track-like'.

Two example events, one of a $\sim\SI{5.9}{keV}$ \cefe X-ray
and the other of a typical muon is shown in
fig. [[fig:reco:example_signal_background_event]].

#+CAPTION: Two example events one might see in the detector, left a common background event
#+CAPTION: of a (likely) muon track, which enters the readout plane (hence the slightly
#+CAPTION: triangular shape) and right a classical $\SI{5.9}{keV}$ X-ray from a \cefe
#+CAPTION: calibration source.
#+NAME: fig:reco:example_signal_background_event
[[~/phd/Figs/reco/gridpix_example_events.pdf]]

Given the distinct geometric properties of these different types of
events and the fact that a GridPix provides extremely high spatial
resolution and single electron efficiency, the data reconstruction
fully embraces this. Most of the computed properties, which we will
introduce in the next sections, are directly related to geometric
properties of the events.

*** TODOs for this section [0/2]                                 :noexport:

- [X] introduce what signal and background events actually look like
  -> motivates the kind of steps we then perform. So an "intro"
  section, then description of the processes.
- [X] data parsing?
- [X] clustering (our custom and DBSCAN cite:ester1996density)
- [X] in particular the geometric reconstruction. Split up our explanation
  schematic made in inkscape and somehow present that?
  -> Used later.

- [X] *THIS WAS: * ~***~ Expectation of event shapes [0/0]
  -> And is again.

- [ ] *SHOULD WE MENTION ALPHAS, NEUTRONS ETC HERE?*
- [ ] *MOTIVATED BY PHYSICAL STUFF SHOWN IN THEORY, DESCRIBE
  EXPECTATION OF WHAT EVENTS SHOULD LOOK LIKE*

*** Generate example events for known events                     :extended:  

While we have two pretty nice events to plot as examples, in
principle, they are generated by =karaPlot= (an extensive plotting
tool part of TPA) and thus not quite suited to a thesis.

As we know the run number and event number, we can just generate them
quickly here. The two events (and plots) are:
- [[~/org/Figs/statusAndProgress/exampleEvents/background_event_run267_chip3_event1456_region_crAll_hits_200.0_250.0_centerX_4.5_9.5_centerY_4.5_9.5_applyAll_true_numIdxs_100.pdf]]
- [[~/org/Figs/statusAndProgress/exampleEvents/calibration_event_run266_chip3_event5791_region_crAll_hits_200.0_250.0_centerX_4.5_9.5_centerY_4.5_9.5_applyAll_true_numIdxs_100.pdf]]
so run 267 and 266, events 1456 and 5791.

Ah, I had forgotten that these are not the event numbers, but their
indices of the clusters. Therefore, we'll just search for pretty
events in the same runs.
  
#+begin_src nim :tangle code/generate_example_gridpix_event.nim
# Laptop
#const calib = "/mnt/1TB/CAST/2018_2/CalibrationRuns/Run_266_181107-22-14"
#const back  = "/mnt/1TB/CAST/2018_2/DataRuns/Run_267_181108-02-05"
# Desktop
# All raw files found in `/mnt/4TB/CAST`. The two runs needed here copied to
from std/os import expandTilde
const calib = "~/CastData/data/2018_2/Run_266_181107-22-14" # calibration
const back  = "~/CastData/data/2018_2/Run_267_181108-02-05" # data

const cEv = 5898
const bEv = 1829 # this event is nice
import ingrid / tos_helpers
import std / [strformat, os, strutils, sequtils]
import ggplotnim

proc toFile(i: int, path: string): string =
  let z = align($i, 6, '0')
  path / &"data{z}.txt"

proc drawPlot() =   
  let protoFiles = readMemFilesIntoBuffer(@[toFile(cEv, calib.expandTilde), toFile(bEv, back.expandTilde)])
  var df = newDataFrame()
  var names = @["X-ray", "Background"]
  for (pf, name) in zip(protoFiles, names):
    let ev = processEventWithScanf(pf)
    let pix = ev.chips[3].pixels
    if pix.len == 0: return
    let dfL = toDf({ "x" : pix.mapIt(it.x.int), "y" : pix.mapIt(it.y.int),
                     "ToT" : pix.mapIt(it.ch.int), "type" : name })
    df.add dfL
  echo df
  ggplot(df, aes("x", "y", color = "ToT")) +
    facet_wrap("type") +
    geom_point() +
    xlim(0, 256) + ylim(0, 256) +
    #theme_font_scale(2.0) +
    #margin(left = 3, bottom = 3, right = 5) +
    margin(right = 3) + 
    facetHeaderText(font = font(12.0, alignKind = taCenter)) + 
    ggsave("/home/basti/phd/Figs/reco/gridpix_example_events.pdf", width = 800, height = 400, useTeX = true, standalone = true)

drawPlot()
#+end_src

** Data reconstruction
:PROPERTIES:
:CUSTOM_ID: sec:reco:data_reconstruction
:END:

With the data stored in an HDF5 file after processing the raw data
with ~raw_data_manipulation~, the actual data and event reconstruction
can begin. This is handled by the
~reconstruction~ [fn:reco_reconstruction] program. It continues from
the HDF5 file created before and proceeds to reconstruct all runs in
the given input file.

For each run, each GridPix chip is processed sequentially, while all
events for that chip are then processed in parallel using
multithreading. For each event, the data processing is essentially a
two step process:
1. perform cluster finding, see section [[#sec:reco:cluster_finding]].
2. compute geometric properties for each found cluster, see section
   [[#sec:reco:cluster_geometry]].

[fn:reco_reconstruction]
https://github.com/Vindaar/TimepixAnalysis/blob/master/Analysis/ingrid/reconstruction.nim

*** TODOs for this section [/]                                   :noexport:

- [ ] *REFER TO ~reconstructSingleChip~ HERE*

Basic reconstruction:
- [X] per run, per chip
- [X] clustering (2 cluster methods)
- [X] geometric properties
  - [X] find rotated coordinate system
  - [X] compute parameters for long & short axis
  - [X] resulting parameters
- [X] table of parameters (we have that somewhere already)
- [X] use a description to describe all parameters like
  - X :: description    

*** Cluster finding 
:PROPERTIES:
:CUSTOM_ID: sec:reco:cluster_finding
:END:

The cluster finding algorithm splits a single event into possibly
multiple clusters. Clusters are defined based on a certain notion of
distance (the details depend on the clustering algorithm used). The
multiple clusters from a single event are then treated fully equally
for the rest of the analysis. The fact that they originate from the
same event has no further relevance (with a slight exception for one
veto technique, which utilizes clustering over multiple chips, more on
that in section [[#sec:background:septem_veto]].

There are two different cluster finding algorithms implemented for use
in ~TimepixAnalysis~. By default one is strictly used for the general
cluster finding as part of the reconstruction, the other is intended
to be used for one of the vetoes (again, sec. [[#sec:background:septem_veto]]). The
choice is user configurable however. [fn:clustering] 

- Default :: The default one is the same clustering algorithm
  introduced for the data reconstruction of the 2014/15 GridPix
  detector in cite:krieger2018search. It defines a cluster by all
  pixels within the squares of side length $N$ centered around each
  pixel. It is best thought of as a recursive square neighbor search
  around each pixel. For each neighbor in the search square, start
  another search square. Once no neighbor finds any neighbors not
  already part of the cluster, it is finished.
- DBSCAN :: The secondary clustering algorithm is the
  \textbf{D}ensity-\textbf{b}ased \textbf{s}patial \textbf{c}lustering
  of \textbf{a}pplications with \textbf{n}oise (DBSCAN)
  cite:ester1996density algorithm. In contrast to the default
  algorithm it is - as the name implies - a density based
  algorithm. This means it distinguishes points which have more
  neighbors (high density) from those with few neighbors (low
  density). The algorithm has a parameter ~minSamples~, which defines
  the density threshold. If a point has at least ~minSamples~ neighbors within a
  (euclidean) distance of $ε$ (the second parameter) it is considered
  a "core point". All core points build a cluster with all other
  points in their reach. Those points in reach of a core point,
  but do itself not have ~minSamples~ neighbors are still part of the
  cluster. Any point _not_ in reach of a core point is a "noise
  point". The main advantage of this algorithm over many other more
  classical algorithms is the ability to separate clusters close to one
  another, which are not separateable by a linear cut. This results in
  a more humanly "intuitive" clustering.
  DBSCAN is one of the most widely used clustering algorithm in many
  scientific fields and even in 2017 was still considered highly
  relevant cite:10.1145/3068335.

Another clustering algorithm (currently not implemented) is CLASSIX
cite:&CLASSIX, which promises fast clustering based on sorting along
the first principal component axis. Based on its properties as
presented in its paper it could be an extremely useful algorithm for
our application and should be investigated in the future.

[fn:clustering] The clustering logic of TPA is found here: https://github.com/Vindaar/TimepixAnalysis/blob/master/Analysis/ingrid/private/clustering.nim

[fn:default_algo_core] The heart of the algorithm is the following
pixel search:
https://github.com/Vindaar/TimepixAnalysis/blob/master/Analysis/ingrid/private/clustering.nim#L120-L148

**** TODOs for this section [/]                                 :noexport:

- [ ] *THINK ABOUT EXAMPLE OF CLUSTERING THAT DBSCAN FINDS AND DEFAULT
  WOULDN'T* NON LINEAR CLUSTERABLE

**** CLASSIX clustering algorithm [/]                           :extended:

- [ ] *OR SHOULD THIS GO INTO MAIN SECTION BEFORE?* OR AS :optional:?

There is one further clustering algorithm, which is extremely exciting
and seems like a great candidate for a clustering algorithm for
~TimepixAnalysis~. That is the CLASSIX algorithm, introduced as "a
fast and explainable clustering method" cite:CLASSIX.

See the GitHub page with many examples here:
https://github.com/nla-group/classix

It is an algorithm, which first sorts the data based on the first
principal component in the data. 

**** Clustering bug in MarlinTPC for 2014/15 data               :extended:

One of the initial goals of ~TimepixAnalysis~ was the reproduction of
the background rate computed for the 2014/15 data with the MarlinTPC
framework. While that whole ordeal wasted a lot of time trying to
achieve the exact same results from both frameworks to satisfy other
people, among other things it lead to the discovery of a clustering
bug in MarlinTPC (which was finally the point that let me drop this
pursuit).

- [ ] *INSERT DISCUSSION FROM STATUSANDPROGRESS ABOUT MARLIN CLUSTER
  BUG* See section ~sec:marlin_vs_tpa_output~ in TPA for the Marlin
  clustering bug.

*** Calculation of geometric properties
:PROPERTIES:
:CUSTOM_ID: sec:reco:cluster_geometry
:END:

For each individual cluster the geometric event reconstruction is up
next. As the basic differentiator between X-rays and common background
events is their circularity, most properties are in some sense related
to how eccentric clusters are. Therefore, the first thing to be
computed for each cluster, is the rotation angle [fn:rotation_angle].

The rotation angle is found via a non linear optimization of

\begin{align*}
  x'_i &= \cos(θ) \left( x_i - \bar{x} \right) · P - \sin(θ) \left( y_i - \bar{y} \right) · P \\
  y'_i &= \sin(θ) \left( x_i - \bar{x} \right) · P + \cos(θ) \left( y_i - \bar{y} \right) · P 
\end{align*}

where $θ$ is the rotation angle (in the context of the optimization
the parameter to be fitted), $x_i, y_i$ the coordinates of the $i\text{-th}$
pixel in the cluster, and $\bar{x}, \bar{y}$ the center coordinates of
the cluster. $P = \SI{55}{μm}$ is the pixel pitch of a Timepix. The
resulting variables $x'_i, y'_i$ define a new rotated coordinate
system. From these coordinates, the RMS [fn:rms_terminology] of each of these new axes is
computed via

\begin{align*}
  x_{\text{RMS}} &= \sqrt{ \frac{1}{N} \left( \sum_i x^{\prime2}_i \right) - \frac{1}{N²} \left( \sum_i x'_i \right)² }\\
  y_{\text{RMS}} &= \sqrt{ \frac{1}{N} \left( \sum_i y^{\prime2}_i \right) - \frac{1}{N²} \left( \sum_i y'_i \right)² }.
\end{align*}


Based on these we then simply redefine

\begin{align*}
σ_{\text{transverse}} &= \text{min}(x_{\text{RMS}}, y_{\text{RMS}}) \\
σ_{\text{longitudinal}} &= \text{max}(x_{\text{RMS}}, y_{\text{RMS}}),
\end{align*}

which then define the eccentricity $ε$ to (see also fig. [[sref:fig:reco:prop_expl_ecc]])

\[
ε = \frac{σ_{\text{longitudinal}}}{σ_{\text{transverse}}},
\]

guaranteeing $ε \leq 1$.

During the non linear optimization, the algorithm attempts to maximize
the eccentricity. In a track like cluster, the maximum eccentricity is
found under the rotation angle $θ$, which points along the longest
axis of the cluster. The resulting rotated coordinate system after the
fit has converged, is illustrated in fig. sref:fig:reco:prop_expl_axes.

Once the rotation angle and therefore the rotated coordinate system of
a cluster is defined, most other properties follow in a straight
forward fashion. In the rotated coordinate system the axis along the
long axis of the cluster is called "longitudinal" and the short axis
"transverse" in the following. The higher moments skewness and
kurtosis for each axis are computed as well as the length and width of
the cluster based on the biggest spread of pixels along each axis.  In
addition to the geometric properties a few other properties like the
number of pixels are also computed. Three of the most important
variables are illustrated in
fig. sref:fig:reco:property_explanations. These enter the likelihood
cut definition as we will see in sec. [[#sec:background:likelihood_method]].

#+begin_src subfigure
(figure ()
        (subfigure (linewidth 0.5) (caption "Rotated axes") (label "fig:reco:prop_expl_axes")
                   (includegraphics (list (cons 'width (linewidth 0.95)))
                                    "~/org/Figs/InGridPropExplanation/long_short_axis.pdf"))
        (subfigure (linewidth 0.5) (caption "Eccentricity") (label "fig:reco:prop_expl_ecc")
                   (includegraphics (list (cons 'width (linewidth 0.95)))
                                    "~/org/Figs/InGridPropExplanation/eccentricity.pdf")
                   (spacing "")) ;; avoid spacing to break line
        (subfigure (linewidth 0.5) (caption "Fraction in transverse RMS") (label "fig:reco:prop_expl_fracRms")
                   (includegraphics (list (cons 'width (linewidth 0.95)))
                                    "~/org/Figs/InGridPropExplanation/frac_in_trans_rms.pdf"))
        (subfigure (linewidth 0.5) (caption "Length divided by transverse RMS") (label "fig:reco:prop_expl_ldiv")
                   (includegraphics (list (cons 'width (linewidth 0.95)))
                                    "~/org/Figs/InGridPropExplanation/length_div_rms_trans.pdf"))        
  (caption
   "Schematic explanation of the basic cluster reconstruction and the three most 
    important geometric properties."
   (subref "fig:reco:prop_expl_axes")
    "defines the rotated coordinate system found by non-linear optimization of the long and
     short cluster axis. "
    "Along the long and short axes, "
    (subref "fig:reco:prop_expl_ecc")
    ", the transverse standard deviation " ($ "σ_{\\text{transverse}}") " is computed, which then defines the eccentricity 
     by this ratio."
    (subref "fig:reco:prop_expl_fracRms")
    "shows the definition of a less obvious variable: the fraction of pixels within a 
     circle of one " ($ "σ_{\\text{transverse}}") " radius around the cluster center.
     Similarly "
    (subref "fig:reco:prop_expl_ldiv")
    " shows the full cluster length defined by the furthest active pixels in the cluster
     divided by " ($ "σ_{\\text{transverse}}") " as another variable. These three variables enter
     the likelihood cut used for background suppression."
   )
  (label "fig:reco:property_explanations")
)
#+end_src

#+begin_comment
Probably won't end up using this table, instead use the descriptions
below.

#+CAPTION: Table of all the (mostly) geometric properties of a single cluster computed during the
#+CAPTION: =reconstruction= tool. All but the likelihood, charge and energy properties are computed
#+CAPTION: during the first pass of the tool.
#+NAME: tab:geometric_properties
#+ATTR_LATEX: :booktabs t
| Property                  | Meaning                                                          |
|---------------------------+------------------------------------------------------------------|
| igCenterX                 | =x= position of cluster center                                   |
| igCenterY                 | =y= position of cluster center                                   |
| igHits                    | number of pixels in cluster                                      |
| igEventNumber             | event number cluster is from                                     |
| igEccentricity            | eccentricity of the cluster                                      |
| igSkewnessLongitudinal    | skewness along long axis                                         |
| igSkewnessTransverse      | skewness along short axis                                        |
| igKurtosisLongitudinal    | kurtosis along long axis                                         |
| igKurtosisTransverse      | kurtosis along short axis                                        |
| igLength                  | size along long axis                                             |
| igWidth                   | size along short axis                                            |
| igRmsLongitudinal         | RMS along long axis                                              |
| igRmsTransverse           | RMS along short axis                                             |
| igLengthDivRmsTrans       | length divided by transverse RMS                                 |
| igRotationAngle           | rotation angle of long axis over chip coordinate system          |
| igEnergyFromCharge        | energy of cluster computed from its charge                       |
| igLikelihood              | likelihood value for cluster                                     |
| igFractionInTransverseRms | fraction of pixels within radius of transverse RMS around center |
| igTotalCharge             | integrated charge of total cluster in electrons                  |
| igNumClusters             |                                                                  |
| igFractionInHalfRadius    | fraction of pixels in half radius around center                  |
| igRadiusDivRmsTrans       | radius divided by transverse RMS                                 |
| igRadius                  | radius of cluster                                                |
| igLengthDivRadius         | length divided by radius                                         |
#+end_comment

The following is a list of all properties of a single cluster computed
by the ~reconstruction~ tool. All but the likelihood, charge and energy properties are computed
during the first pass of the tool, namely in the context discussed
above. [fn:geometry_calc]

- igEventNumber :: The event number the cluster is part of (multiple
  clusters may share the same event number).
- igHits :: The number of pixels in the cluster.
- igCenterX / igCenterY :: The center position of the cluster along
  the ~x~ / ~y~ axis of the detector.
- igRotationAngle :: The rotation angle of the long axis of the
  cluster over the chip coordinate system.
- igLength :: The length of the cluster along the long axis in the 
  rotated coordinate system, defined by the furthest pixel at each
  end in that direction.
- igWidth :: The equivalent of *igLength* for the short axis.
# - igRadius :: The radius of the cluster, defined by *IM NOT DEFINED
#   YET, DEFINE ME* <-- Does not exist in TPA
- igRmsLongitudinal :: The root mean square (RMS) along the long axis.
- igRmsTransverse :: The RMS along the short axis.
- igSkewnessLongitudinal / igKurtosisLongitudinal :: The skewness / kurtosis along the long axis.
- igSkewnessTransverse / igKurtosisTransverse :: The skewness / kurtosis along the short axis.
- igEccentricity :: The eccentricity of the cluster, defined by the
  ratio of the longitudinal RMS over the transverse RMS.
- igLengthDivRmsTrans :: The length of the cluster divided by the
  transverse RMS (see fig. sref:fig:reco:prop_expl_ldiv).
- igFractionInTransverseRms :: The fraction of all pixels within a
  radius of the transverse RMS around the center (see fig. sref:fig:reco:prop_expl_fracRms).
# - igRadiusDivRmsTrans :: The radius over the transverse RMS.
# - igFractionInHalfRadius :: Equivalent to *igFractionInTransveresRms*
#   but for a radius of half the cluster radius.
# - igLengthDivRadius :: The length divided by the cluster radius.
- igTotalCharge :: The sum of the charge of the ~ToT~ calibrated
  charges of all pixels in the cluster.
- igEnergyFromCharge :: The calibrated energy of the cluster in $\si{keV}$.
- igLikelihood :: The likelihood value of the cluster for the
  likelihood cut method, explained in detail in section
  [[#sec:background:likelihood_method]].

After the calculation of all geometric properties for all events and
chips, the data is written to an output HDF5 file (similar in format to
the output of ~raw_data_manipulation~) for each run. This concludes
the first pass of ~reconstruction~ over the data.

[fn:rms_terminology] The term 'root mean square' is used although we
actually refer to the standard deviation of the sample. We follow
[[cite:&krieger2018search]], but this ambiguity is often encountered unfortunately.

[fn:rotation_angle] Note that the absolute value of the rotation angle
is of secondary importance. For X-rays the rotation angle is going to
be random, as the definition of a long and short axis in a
(theoretically perfect) circle depends on the statistical distribution
of the pixels. However, for pure muons it allows to map the rotation
angle to the incidence angle.

[fn:calc_eccentricity] https://github.com/Vindaar/TimepixAnalysis/blob/master/Analysis/ingrid/private/geometry.nim#L296-L329

[fn:geometry_calc] In particular all these properties are computed
here: https://github.com/Vindaar/TimepixAnalysis/blob/master/Analysis/ingrid/private/geometry.nim#L331-L391

**** TODOs for this section [/]                                 :noexport:

- [ ] *THINK ABOUT WHETHER BETTER TO CLARIFY WHAT X


- [ ] *REWRITE THIS SECTION*

- [ ] *PROPERLY LINK THE CODE LATER*
  -> Still need to decide how we will do this, but for sure needs a
  specific git tag!

  The properties are computed here:
  https://github.com/Vindaar/TimepixAnalysis/blob/master/Analysis/ingrid/private/geometry.nim#L308-L366
  and here:
  https://github.com/Vindaar/TimepixAnalysis/blob/master/Analysis/ingrid/private/geometry.nim#L517-L569
  

- [X] *EXPLANATION OF THE 3 MAIN PROPERTIES USED FOR LIKELIHOOD*
- [X] *LOOK INTO \subref RENDERING. SHOULDN'T JUST SHOW a) HERE*
  -> Adjusted in our LaTeX defaults now. Option to ~subcaption~ package.

- [ ] *PLACE THE LINKS AFTER FIX UP SOMEWHERE THEY BELONG*
- [ ] *CONSIDER MOVING LIST OF ALL PROPERTIES TO FURTHER DOWN ONCE WE
  HAVE DISCUSSED CHARGE CALIB?*  

**** HDF5 data layout generated by ~reconstruction~             :optional:

The HDF5 file generated by ~reconstruction~ follows closely the one
from ~raw_data_manipulation~. The main difference is that within each
chip group now each chip has a different number of entries in the
datasets, as each entry now corresponds to a single cluster, not an
event from the detector. On some events multiple clusters on a single
chip may be reconstructed, while other events may be fully empty. This
means an additional ~eventNumber~ dataset is required for each chip,
which maps back each cluster to a corresponding event.

Aside from that the other major difference is simply that each chip
has a larger number of datasets, as each computed cluster property is
a single variable. Also additional new datasets will be created during
the data calibration (charge calibration, computation of the gas gain,
etc.).

Listing [[code:reco:abstract_reco_hdf5_layout]] shows the layout in a
similar fashion to the equivalent for ~raw_data_manipulation~ before.

#+CAPTION: Abstract overview of the data layout of the ~reconstruction~ HDF5
#+CAPTION: output. It is essentially the same layout as the ~raw_data_manipulation~
#+CAPTION: HDF5 files, but contains more datasets, due to larger number of
#+CAPTION: properties.
#+NAME: code:reco:abstract_reco_hdf5_layout
#+begin_src toml
- reconstruction
  - run_<number>
    - chip_0 # one for each chip in the event
      - datasets for each property
      - optional datasets for calibrations
    - chip_i # all other chips
      - ...
    - fadc # if available
      - datasets for each FADC property
    - common datasets # copied from `raw_data_manipulation` input 
  - run_i # all other runs
    - ...
#+end_src

*** Example of data reconstruction [/]                           :extended:

- [ ] GIVE EXAMPLE

#+begin_src sh
reconstruction -i <h5file> --out /tmp/reco_foo.h5
#+end_src

*** Data calibration

The next step of the reconstruction is the data calibration. This is a
separate pass over the data as it is optional on the one hand and
requires further inputs about each used GridPix than just the raw data
(different calibration files) on the other hand.

There are different calibrations to be performed:

1. the charge calibration via the application of the ~ToT~ calibration
   as introduced in section [[#sec:operation_calibration:tot_calibration]].
2. the calculation of the gas gain, introduced previously in section
   [[#sec:daq:polya_distribution_threshold]].
3. the energy calibration (see sec. [[#sec:calibration:energy]] and
   [[#sec:calib:final_energy_calibration]]).

Here we will only discuss the ~ToT~ calibration, as the other two are
more involved and require deeper explanations.

The ~ToT~ calibration is in principle performed simply by converting
each ~ToT~ value to an equivalent charge in electrons using the
calibration as presented in section [[#sec:operation_calibration:tot_calibration]]. For each
GridPix used in a detector, a ~ToT~ calibration must be available.

~TimepixAnalysis~ comes with a library and helper program, which
manages a simple database about different GridPixes, their
calibrations and their validity (in time & runs they apply to). The
user needs to add the chips for which they wish to perform a ~ToT~
calibration to the database before it can be performed. See appendix
[[#sec:appendix:software:ingrid_database]] for a detailed overview.

For any chip part of the database, the ~ToT~ calibration is a single
pass over the ~ToT~ values of all runs. This generates a calibrated
charge for every pixel of every cluster and a combined property, the
~totalCharge~ of the full charge of each cluster.

Gas gain values are computed in $\SI{90}{min}$ time intervals for each
chip. This strikes a good balance between enough statistics and
reduced sensitivity to variation in gas gain due to external
effects. As this deserves its own discussion more on this in
sec. [[#sec:calib:gas_gain_time_binning]]. 

Finally, while the energy calibration is also handled by ~reconstruction~, we
will cover it in section [[#sec:calibration]], due to its more
complex nature.

**** TODOs for this section                                     :noexport:

- [ ] *GIVE THIS SECTION A BETTER NAME!*
  -> We currently also call the chapter after CAST 'data
  calibration'...
  -> Well, but we _do_ talk about related concepts here after all.
  -> [[#sec:calibration]] really is the extension of this chapter for the
  more complicated aspects, i.e. energy and gas gain.

And this is where the ugly mess of needing X or Y starts :(
at least for energy calibration, we will refer to section later

- [ ] *REPHRASE PART OF "ONLY MENTION TOT" BUT THEN MENTION GAS GAIN &
  ENERGY LATER*

- [X] *MENTION GAS GAIN PROPERLY*
  -> Well, properly is not correct, but we mention it now.
  -> Not ideal though.
  -> We reference back to the initial gas gain part. Maybe properly
  explain more details in that original section, then here just
  mention it needs to be done and in [[#sec:calibration]] talk about the
  tricky aspects of the binning etc? Seems sensible.

- [ ] *SHOW TOT CALIBRATION AGAIN?*
  -> Not really needed. Just a dumb function.

- [X] *MENTION INGRID DATABASE STORING TOT CALIB DATA*

**** ~InGridDatabase~ [0/1]                                     :optional:
(or :noexport: ?)
- [ ] introduce database including the data structure needed to add a
  detector to the database?

- [ ] *OR SHOULD THIS BE PART OF THE APPENDIX ABOUT SOFTWARE AND IN
  SECTION ABOVE WE MENTOIN AND REFER TO THAT?*

**** Example of data calibration [/]                            :extended:

The following three commands perform the three calibration steps
mentioned above:
- ToT calibration :: ~--only_charge~
- Gas gain calc :: ~--only_gas_gain~
- Energy calibration :: ~--only_energy_from_e~    
#+begin_src sh
reconstruction -i <h5file> --only_charge
reconstruction -i <h5file> --only_gas_gain
reconstruction -i <h5file> --only_energy_from_e
#+end_src

*** Event duration [0/1]
:PROPERTIES:
:CUSTOM_ID: sec:reco:event_duration
:END:

During the reconstruction of the data, another important parameter is
computed, namely the event duration of each individual event. In
principle each event has a fixed length, because the Timepix uses a
shutter based readout, with the shutter length predefined. However, as
the FADC is used as an external trigger to close the shutter early, if
it recorded a signal, all events with an FADC trigger have a shorter
duration.

For the fixed length duration events their length is computed by the
shutter length as indicated in TOS. In section
[[#sec:daq:tos_output_format]], listing
[[code:daq:zero_suppressed_readout_run_header]] the ~shutterTime~ and
~shutterMode~ fields were listed. These define the absolute length of
the shutter opening in (effectively) number of clock cycles. The
~shutterMode~ acts as a modifier to the number of clock cycles:

\[
t_{\text{clocks}}(\mathtt{mode}, t) = 256^{\mathtt{mode}} · t
\]

where $t$ is the ~shutterTime~ and $\mathtt{mode}$ corresponds to the
~shutterMode~. The available modes are:
- ~short~: \num{0}
- ~long~: \num{1}
- ~verylong~: \num{2}

In case of the FADC triggering, the clock cycles after shutter opening
that were recorded up to the trigger is also reported in the data
files, see sec. [[#sec:daq:tos_output_format]], listing
[[code:daq:zero_suppressed_readout_event_header]].

With the number of clock cycles the shutter was open, the total event
duration can then be computed in either case via:

\[
d(t_{\text{clocks}}) = \frac{t_{\text{clocks}} · 46}{40 · \num{1000000}}.
\]

[fn:event_duration_calc] https://github.com/Vindaar/TimepixAnalysis/blob/master/Analysis/ingrid/private/pure.nim#L266-L297

**** TODOs for this section [/]                                 :noexport:

- [ ] *LINK CORRECT CODE*

- [X] How to compute length of events, given
  - shutter mode & length
  - FADC trigger
- [ ] *HERE OR IN CAST SECTION: WHAT SHUTTER LENGTHS WERE ACTUALLY USED?*    

- [ ] *MOVE THIS TO EARLIER?*

*** InGrid database                                              :optional:

Section about InGrid database (or maybe :noexport: instead
of :optional: ?), which explains the idea of storing the chip
calibration data in a single HDF5 file, which can then be easily
accessed from the reconstruction / calibration

**** TODO                                                       :noexport:

- [ ] *THIS* is the second such section already!

** FADC [/]
:PROPERTIES:
:CUSTOM_ID: sec:reco:fadc_data
:END:

The data files created from the FADC data sent upon a trigger are
essentially memory snapshots of the circular register of the FADC. We
will go through the necessary steps to convert that raw data into
usable signals, given the FADC settings we use and the data TOS
generates from it. For a detailed overview of the process see the FADC
manual cite:fadc_manual [fn:reco_fadc_manual].

In ~TimepixAnalysis~ FADC data is automatically parsed from the ASCII
data files into HDF5 files as part of ~raw_data_manipulation~ if FADC
files are present. The spectrum reconstruction is done automatically
as part of the ~reconstruction~ program, but calculation of the
baseline, rise and fall time is an optional step.

*** TODOs for this section [/]                                   :noexport:

- [ ] *ADD SOME KIND OF MOTIVATON FOR WHAT WE MIGHT WANT TO DO WITH THE SIGNALS?*

- [X] *ADD EXPLANATION HOW IT FITS INTO ~raw_data_manipulation~ AND
  ~reconstruction~*
  -> Question whether this needs to be part of this section here. Or
  just part of an :extended: section?

- [ ] *ADD MENTION OF SAVITZKY GOLAY!*

*** FADC pedestal calculation
:PROPERTIES:
:CUSTOM_ID: sec:reco:fadc_pedestal_calc
:END:

As alluded to in sec. [[#sec:operation_calibration:fadc]] the pedestal
values cannot only be taken from a pedestal run recorded before data
taking, but can also be extracted from real data, under the condition
that a decent fraction of FADC registers in a single FADC event
is normally distributed between events. 

The idea is to remove all recorded events in a register, in which it
was involved in a real signal. At least in typical signals recorded
with a GridPix the signal lengths are $\mathcal{O}(\SI{10}{\percent})$
of the window length, leaving plenty of registers free to recover
pedestal information. Regular noise affects things, but is partially
taken care by the truncation and partially cancels out as real noise
is normal distributed around the actual pedestal. This latter approach
is the one used in the data analysis by calculating:

\[
p_i(r) = \text{mean}_{20^{\text{th}}}^{98^{\text{th}}}(\{r_i\})
\]

where $p_i(r)$ is the pedestal in register $r_i$ and the mean is taken
over all data $\{r_i\}$ in that register within the 20-th and
98-th percentile. All data refers to a full data run of
$\sim\SI{1}{day}$. The highly biased nature is due to the real
signals being negative. Removing the smallest $\SI{20}{\percent}$ of
data guarantees in the vast majority of events the full physical
signal is excluded given the typical signal lengths involved. A
small upper percentile is used to exclude possible significant
outliers to the top. While such a biased estimator will not result in
the real mean (and in case of signal and noise free input data thus
the real pedestals), a slight bias is irrelevant, as the baseline is
still calculated for the reconstructed signal which is used to
correct any global offset. 


*** FADC spectrum reconstruction [0/0]

The first step to reconstruct the FADC data is to perform the pedestal
correction. This is simply done by subtracting the pedestals register
by register from the data file

\[
N_{i, \text{corr}} = N_{i, \text{raw}} - N_{i, \text{pedestal}}
\]

with the raw data $N_{i, \text{raw}}$ and the pedestals
$N_{i, \text{pedestal}}$ in register $i$ (as computed based on a
truncated mean of the real data).

With the pedestals removed, the temporal correction is next to unfold
the data into the correct order. This needs to be performed on each of
the \num{2560} registers for each channel separately. The temporal
rotation is performed by shifting all registers by

\[
n_\text{rot} = (\mathtt{TRIG\_REC} - \mathtt{POSTTRIG}) · 20
\]

places to the left. The constants $\mathtt{TRIG\_REC}$ and
$\mathtt{POSTTRIG}$ are those from section [[#sec:daq:fadc_data_files]],
written in each data file in the header.

The final step is to convert the ADC values of each register into
voltages in \si{V}. Given that the ADC covers the range of
\SIrange{-1}{1}{V} as the ADC values 0 to 4096 (16384) with 12 (14)
bit, this means the conversion from ADC to ticks is simply

\[
U_i = \frac{N_{i, \text{corr}} - 2048}{2048}
\]

when using the 12 bit operating mode for each register.

With these corrections applied, the recorded FADC spectrum is
recovered, centered around the trigger position.

[fn:reco_fadc_manual] A PDF of the FADC manual is available here:
https://archive.org/details/manualzilla-id-5646050/

[fn:fadc_code] Data parsing and the mentioned reconstruction code is
found at *CITE*

**** TODOs for this section [0/2]                               :noexport: 

- [ ] *REWRITE TAKING INTO ACCOUNT THAT PEDESTALS ARE COMPUTED BASED
  ON DATA NOW!* -> Partially done by referencing pedestals from
  truncated mean.
  -> Now that part of the expl is one sec above!

- [ ] *ADD REFERENCE TO CODE* https://github.com/Vindaar/TimepixAnalysis/blob/master/Analysis/ingrid/fadc_helpers.nim

*** Signal baseline, rise time and fall time [0/4]               :analysis:
:PROPERTIES:
:CUSTOM_ID: sec:fadc:definition_baseline_rise_fall_time
:END:


Assuming a singular event is recorded with the FADC, the main
properties of interest of the resulting signal pulse are the signal
baseline and based on that the rise and fall time.

Computing the position of the baseline is generally a non trivial
problem, as a priori the position, width and number of signals in the
spectrum is unknown. A reasonable expectation though is that the
majority of points in a signal should lie close to the baseline, as
the fraction of the FADC window covered by a signal is typically less
than a quarter. As such a somewhat empirical way to compute the
baseline $B$ was chosen using a biased truncated mean 

\[
B = \text{mean}_{30^{\text{th}}}^{95^{\text{th}}}(S)  %\text{median}(S) + 0.1 · \max(S)
\]

between the $30^{\text{th}}$ and $95^{\text{th}}$ percentile of the
data. The bias is intended to remove the impact of the negative signal
amplitude and remove the worst positive outliers. An optimal solution
would perform a rigorous peak finding for a signal pulse, remove those
points and compute the mean of the remainder of the
points. [fn:baseline_ideas]

Once the baseline is defined it can be used to determine both the rise
and the fall time. These are generally computed based on the number of
registers between the minimum of the signal and some threshold
slightly below the baseline in order to reduce the effect of local
noise variations. While configurable, the default value of the
threshold $c_B$ ($B$ for baseline) is

\[
c_B = B - 0.1 · \left| B - \text{min}(S)\right|,
\]

$\SI{10}{\%}$ of the difference between the baseline $B$ and the
minimum value of the spectrum $S$ from the baseline. Similarly for the
end of the rise time / beginning of the fall time a similar offset is
used. In this case the threshold value $c_P$ ($P$ for peak) is defined
as

\[
c_P = \text{min}(S) + 0.025 · \left| B - \text{min}(S)\right|,
\]

so $\SI{2.5}{\%}$ of the amplitude above the minimum of the signal
$S$. The position where either threshold is crossed in registers is
based on where the _simple moving average_ (of window size 5) crosses
$c_B$ and $c_P$. The number of registers between $c_B$ and $c_P$
defines the rise time (left of the peak) and fall time (right of the
peak). [fn:naming_rise_fall] At the used clock frequency of
$\SI{1}{GHz}$ each register corresponds to $\SI{1}{ns}$ in time.
  
A reconstructed FADC spectrum including indications for baseline, rise
and fall time as well as the minimum is shown in
fig. [[fig:reco:fadc_reco_example]].

#+CAPTION: Example of a fully reconstructed InGrid event and FADC spectrum from a \SI{5.9}{keV} X-ray
#+CAPTION: recorded with the Septemboard detector during a calibration run. On the left of each
#+CAPTION: plot are all properties computed for the data. In the FADC plot the blue line indicates
#+CAPTION: the baseline. Green vertical: rise time from full to dashed line. Red vertical: point of
#+CAPTION: spectrum minimum. Light red: fall time from dashed to full line. Rise / fall time stops
#+CAPTION: $\SI{2.5}{\%}$ before baseline is reached.
#+ATTR_LATEX: :width 1\linewidth
#+NAME: fig:reco:fadc_reco_example
[[~/phd/Figs/CalibrationRuns2018_Reco_2023-10-15_22-49-53/septemEvents/septem_fadc_run_239_event_1068_region_crAll.pdf]]

[fn:naming_rise_fall] The naming of the rise and fall time in the
context of a negative pulse is slightly confusing. Rise time refers to
the _negative rise_ towards the minimum of the pulse and the fall time
to the time to return-to-baseline.

[fn:baseline_ideas] Aside from performing peak fitting (which is
difficult and requires understanding of the expected signal shapes)
another approach might be a local linear smoothing (e.g. a
Savitzky-Golay filter with polynomial of order 1) in a suitable window
range. The result would be a much more stable spectrum. This could
then be used to compute the numerical derivative from which all those
intervals with a slope smaller than some epsilon provide the dataset
from which to compute the mean. The tricky aspect would be choice of
window size and the behavior in very noisy events.

*** TODOs for this section [/]                                   :noexport:

- [ ] *REPLACE THE PLOT!*
  -> The plot we use now is quite nice as it allows us to explain
  multiple aspects at the same time. Just need to decide what the plot
  should look like and prettify it (probably don't want the properties
  and InGrid event next to it!)
  -> Yes we do. It is very illustrative.
- [ ] *ADD CITATION FOR* https://github.com/Vindaar/TimepixAnalysis/blob/master/Analysis/ingrid/fadc_analysis.nim  

- [X] *CHANGE FADC CODE TO NOT USE MIN, BUT MEAN OF N POINTS AROUND
  MIN? -> ALREADY USES MEAN OF N-th PERCENTILE AROUND ARGMIN*
  -> but only for the minimum *amplitude* and not the minimum value
  that is used to start the search around it! In the vast majority of
  cases this shouldn't matter, but for some weird spectrum shapes it
  might.
  -> This has since been implemented.

- [X] *REWRITE EXPLANATION BASED ON NOW USING:*
  - instead of median + 0.1 · max: truncated mean of 30-th to 95-th
    percentile
  - instead of times to exact baseline, go to baseline - 2.5%
  - do not compute threshold based on individual value, but on a
    moving average of window size 5
- [ ] Also: use all registers and do not set first two registers to
  0!
  -> Need to check if this is still mentioned somewhere!

*** Generate the FADC baseline plot [/]                          :noexport:

- [ ] *GENERATE THE PLOT CURRENTLY USED IN THE ABOVE BODY HERE*
  -> The current version comes from the TPA test suite!

#+begin_src nim :tangle code/plot_fadc_events.nim
import nimhdf5, ggplotnim
import std / [strutils, os, sequtils]
import ingrid / [tos_helpers, fadc_helpers, ingrid_types, fadc_analysis]

proc stripPrefix(s, p: string): string =
  result = s
  result.removePrefix(p)

proc plotIdx(df: DataFrame, fadcData: Tensor[float], runNumber, idx: int) =
  let xmin = df["xmin", int][idx]
  let xminY = df["minvals", float][idx]
  let xminlineX = @[xmin, xmin] # one point for x of min, max
  let fData = fadcData[idx, _].squeeze
  let xminlineY = linspace(fData.min, fData.max, 2)

  let riseStart = df["riseStart", int][idx]
  let fallStop = df["fallStop", int][idx]
  let riseStartX = @[riseStart, riseStart]
  let fallStopX = @[fallStop, fallStop]
  let baseline = df["baseline", float][idx]  
  let baselineY = @[baseline, baseline]
  
  let dfLoc = toDf({ "x"         : toSeq(0 ..< 2560),
                     "baseline"  : baseline,
                     "data"      : fData,
                     "xminX"     : xminlineX, 
                     "xminY"     : xminlineY,
                     "riseStart" : riseStartX,
                     "fallStop"  : fallStopX })
                     # Comparison has to be done by hand unfortunately
  let path = "/t/fadc_spectrum_baseline_$#.pdf" % $idx
  ggplot(dfLoc, aes("x", "data")) +
    geom_line() +
    geom_point(color = color(0.1, 0.1, 0.1, 0.1)) +
    geom_line(aes = aes("x", "baseline"),
              color = "blue") +
    geom_line(data = dfLoc.head(2), aes = aes("xminX", "xminY"),
                     color = "red") +
    geom_line(data = dfLoc.head(2), aes = aes("riseStart", "xminY"),
                     color = "green") +
    geom_line(data = dfLoc.head(2), aes = aes("fallStop", "xminY"),
                     color = "pink") +
    ggtitle("FADC spectrum of run $# and index $#" % [$runNumber, $idx]) +
    xlab("FADC Register") + ylab("FADC signal voltage U [V]") + 
    ggsave(path)
  copyFile(path, "/t/fadc_spectrum_baseline.pdf")

proc toDf[U: object](x: U): DataFrame =
  result = newDataFrame()
  for field, val in fieldPairs(x):
    type T = typeof(val[0]) 
    when T isnot int and T is SomeInteger:
      result[field] = val.asType(int)
    elif T isnot float and T is SomeFloat:
      result[field] = val.asType(float)
    else:
      result[field] = val

proc plotFadc(h5f: H5File, runNumber, sleep: int) =
  var run = h5f.readRecoFadcRun(runNumber)
  var data = h5f.readRecoFadc(runNumber)  
  var df = data.toDf()
  df["minvals"] = run.minvals
  for idx in 0 ..< df.len:
    plotIdx(df, run.fadc_data, runNumber, idx)
    sleep(sleep)

proc main(fname: string, runNumber: int, sleep = 1000) =
  var h5f = H5open(fname, "r")
  let fileInfo = h5f.getFileInfo()
  for run in fileInfo.runs:
    if run == runNumber:
      plotFadc(h5f, run, sleep)
      
when isMainModule:
  import cligen
  dispatch main
#+end_src

- [ ] *MAKE PLOT PRETTY AND RERUN FOR THIS:*
  run 281 and index 1533

*** Generate plot of InGrid and FADC event                       :extended:

The command to produce the plot as seen in the thesis is:
#+begin_src sh
W1=825 W2=675 G_LEFT=0.65 F_LEFT=0.3 L_MARGIN=10 R_MARGIN=4 USE_TEX=true SCALE=1.3 plotData \
       --h5file ~/CastData/data/CalibrationRuns2018_Reco.h5 \
       --runType=rtCalibration \
       --eventDisplay --septemboard \
       --events 1068 --runs 239
#+end_src
The parameters used here are the default now outside the
~USE_TEX=true~ and ~SCALE=1.3~.

The resulting plot needs to be copied over to the ~phd~ repository
manually, as ~plotData~ currently does not support outputting a plot
to a specific file name (it's intended to produce many plots
automatically and act more as a fast way to visualize data under
different cuts).

*** Noise sensitive [/]                                          :optional:

Because of the small amplitude of the associated signals induced on
the grid, electromagnetic interference is a serious issue with this
FADC setup. Ideally, the detector should be installed in a Faraday
cage and a short, shielded LEMO cable should be used to connect it to
the pre-amplifier and amplifier.

**** TODOs for this section                                     :noexport:

- [ ] *WHAT TO DO WITH THIS? WHERE SHOULD THIS GO? MAYBE MENTION IN
  FADC INTRODUCTION ALREADY? OR HERE BECAUSE AFTER ALL? IT SHOWS UP IN
  ANALYSIS AND WE HAVE SIMPLE NOISE DETECTION IN CODE*
  -> Go to analysis / characterization / whatever part!

- [ ] This section does not really belong here. Yes, it is noise
  sensitive, but we'll mention it in the context of data taking woes
  and then later when looking at the different data sets.

*** FADC amplifier settings                                      :extended:

The FADC does not require a proper calibration in the same sense as
the Timepix needs to be calibrated. However, the amplifier settings
have a large impact on the resulting signals. Better measurements of
the effect of the different integration / differentiation settings of
the amplifier would have been very valuable, but were never performed
for lack of time. The same holds for different amplifications as to
properly understand the data ranges the FADC can record and how the
trigger threshold in equivalent $\si{keV}$ on the center GridPix is
related. This would have made smarter decisions about the settings
possible (e.g. to optimize the lowest possible activation to have the
FADC as a trigger for lower energies available). Only a single set of
measurements exists comparing the FADC integration time of
$\SI{50}{ns}$ and $\SI{100}{ns}$, which is only partially useful (in
particular because the other parameters (differentiation time,
amplification etc.) were not properly recorded for these. The problem
is especially that the TOS data does not record these parameters
anyway, as they are physical rotary knobs on the amplifier.

We will talk about these things in sec. [[#sec:calib:fadc]] again when
discussing the impact of noise on the FADC data taking and changing
parameters that were done to mitigate that to an extent.

**** TODOs for this section [/]                                 :noexport:

- [X] *FINISH*

- [ ] *MENTION TALKED ABOUT FURTHER DOWN IN FADC ANALYSIS AS WELL AS
  CAST SECTION MENTIONING CHANGES DUE TO NOISE*

*** Example to reconstruct FADC data                             :extended:

As mentioned in the beginning of sec. [[#sec:reco:fadc_data]]
reconstruction is done as part of ~raw_data_manipulation~:

#+begin_src sh
raw_data_manipulation -p <run path> --runType rtBackground --h5out /tmp/run_foo.h5
#+end_src

In ~reconstruction~:
#+begin_src sh
reconstruction -i <h5file> --only_fadc
#+end_src

** Scintillator data
:PROPERTIES:
:CUSTOM_ID: sec:reco:scintillator_data
:END:

For the scintillator signals we only record a trigger flag and the
number of clock cycles since the last scintillator trigger from the
moment the FADC triggered. These two pieces of information are part of
the Septemboard data files included in the
header. [fn:reco_scinti_data_bug_1] [fn:reco_scinti_data_bug_2]

[fn:reco_scinti_data_bug_1] Important note for people potentially
investigating the raw data from 2017: There was a small bug in the
readout software during the beginning of the 2017 data taking period,
which wrote the scintillator trigger values clock cycle values into
subsequent output files even if no FADC trigger was received (and thus
no scintillator trigger was actually read out). However, there is a
flag for an FADC trigger. To correctly read the first data runs it is
therefore required to not only look at the scintillator trigger clock
cycles, but also at whether the FADC actually triggered. This is
handled in the analysis framework.

[fn:reco_scinti_data_bug_2] In addition to the above bug, there was
unfortunately a more serious bug, which rendered the scintillator
counts useless in the end of 2017 / beginning of 2018 data taking
period. The polarity of the signals was inverted in the detector
firmware, resulting in useless "trigger" information.

*** TODOs for this section [/]                                   :noexport:

- [ ] *WAS POLARITY REALLY THE ISSUE? I THINK SO, BUT NOT SURE*

- [ ] *SHOULD SCINTI BUG NO TRIGGERS IN RUN 2 BUG BE MENTIONED HERE OR
  IN CAST DATA TAKING CHAPTER? IT THINK LATTER* Investigate the scinti
  bug again that caused data loss in the first place. What was wrong?


* Detector installation at CAST & data taking                         :Part3:
:PROPERTIES:
:CUSTOM_ID: sec:cast
:END:

#+LATEX: \minitoc

In this chapter we will cover the data taking with the Septemboard
detector at the CAST experiment. We will begin with a timeline of the
important events and the different data taking periods to give some
reference and put certain things into perspective,
sec. [[#sec:cast:timeline]]. After that we will provide an overview of the
technical aspects (vacuum system, operations, etc.) in section
[[#sec:cast:operation]]. This is followed by further details on different
aspects mentioned in the timeline, which require further details,
sections [[#sec:cast:X]] *WHAT SECTIONS*. In the second to last section
[[#sec:cast:data_overview]] we will provide an overview of the total data
taken with the Septemboard detector. The final section
[[#sec:cast:problems_lessons]] finishes by discussing different issues
encountered and lessons learned.  

The appendix contains an additional chapter
[[#sec:appendix:cast_operations]], which contains details about the
operating procedures with respect to the gas supply and vacuum
system. As the details of that are not particularly relevant after
shutdown of the experiment, it is not discussed here.

** TODOs for this section [0/4]                                   :noexport:

- [ ] *NOTE ABOUT THIS CHAPTER*:
  In my meeting with Klaus on <2023-10-16 Mon> Klaus mentioned that he
  thinks the CAST chapter can probably be cut down to 2-3 pages.
  We can get there if we really only have a rewritten version of the
  Timeline in the thesis that then goes straight over to the data
  overview that we already have at the end of the chapter!


- [ ] *REWRITE ABOVE WITH FINAL STRUCTURE!!*!!
  
- [ ] *INTRODUCE NAMING OF RUN-2 AND RUN-3! RELATED TO DETECTOR
  CALIBRATIONS AND WORKING FEATURES*

- [ ] *INTRODUCE NAMING FOR AIRPORT, JURA, SUNSET, SUNRISE*
  -> Not really needed imo.

bla bla bla, the setup includes a \cefe source mounted to a
pneumatic manipulator, see sec. [[#sec:cast:55fe_manipulator]], ...

- [X] *HAVE A SORT OF TIMELINE APPROACH?*
  Or instead first start with a factual representation of what the
  setup actually looked like and then a "retrospective" of the
  different times? Including the problems that were encountered?
  -> Yup, written.

- [X] alignment photo of laser alignment
- [X] geometer measurements
- [X] X-ray finger for alignment
- [X] section about the 55Fe source and manipulator, with noexport
  section about the software used to control it.  

- [X] compute real dead time as done in `run_statistics.txt` files
  available for 2016 data taking campaign in December (grepping
  through tpc19, I finally found the code:
  [[file:~/CastData/Code/scripts/PyS_timeOfRunFolder.py]]
  -> Contained

- [ ] *MENTION 2016 COSMIC ALIGNMENT MEASUREMENT? Maybe as a footnote
  "for completeness sake"*
  -> Maybe as footnote?
- [ ] *AT VERY LEAST FIND 2016 DATA AND LINK IT IN EXTENDED VERSION!!!!!!*
  -> Might be on laptop or tpc19 or potentially even tpc00.
  -> Definitely not on voidRipper.
  -> Not on Laptop either!
  
*** X-ray finger [/]

- [ ] include funky reconstruction info / expected rate of X-ray
  finger and what we actually got?

  
** Timeline
:PROPERTIES:
:CUSTOM_ID: sec:cast:timeline
:END:

The Septemboard detector was prepared for data taking at the CAST
experiment in July 2017 for preliminary alignment and fit tests. The
detector was mounted behind the LLNL telescope and aligned with a
laser from the opposite side of the magnet on <2017-07-07> (see
fig. sref:fig:cast:laser_alignment1). Vacuum leak tests were performed and
the detector installed on <2017-07-10> (see
fig. [[sref:fig:cast:detector_installed]]). In addition geometer measurements
were done for final alignment and as a reference measurement the day
after. An Amptek COOL-X X-ray generator [fn:amptek] ('X-ray finger')
was installed on the opposite side of the magnet. A calibration
measurement with the X-ray finger ran from <2017-07-13> over
night. The aim of an X-ray finger run is to roughly verify the focal
spot of the X-ray telescope. After this initial test the detector was
dismounted to make space for the KWISP experiment.

Two months later the detector was remounted between <2017-09-11 Mon>
to <2017-09-14 Thu> with another geometer measurement on the last
day. During an attempt to clean the detector water cooling system on
<2017-09-19>, the window of the detector was destroyed (see section
[[#sec:cast:window_accident]]). This required a detector dismount and
transport to Bonn for repairs as the detector was electronically dead
after the incident.

Near the end of October <2017-10-23> the remount of the detector
started and was finished by <2017-10-26> in time for another geometer
measurement and alignment. The next day the veto paddle scintillator
was calibrated using a 3-way coincidence in the RD51 laboratory (see
sec. [[#sec:operation_calibration:scintillators]]), followed by the installation
of the lead shielding and scintillator installation another day
later. With everything ready, data taking of the first data taking
period with the Septemboard detector started on <2017-10-30>. During
the period until <2017-12-22> few minor issues were encountered, see
sec. [[#sec:cast:data_taking_woes_2017]]. As CERN is typically closed over
Christmas and well into January, data taking was paused until
<2017-02-17> (further time is necessary to prepare the magnet for data
taking again).

The second part of the first data taking then continued on until
<2017-04-17>, with a few more small problems encountered, see
[[#sec:cast:data_taking_woes_2018]]. After data taking commenced,
dismounting of the detector began the next day by removing the veto
scintillator and the lead shielding. On <2018-04-20> another X-ray
finger run was performed to get a sense of the placement of the
detector during its actual mount as it was during the first data
taking period. Afterwards, the detector was fully removed by
<2018-04-26> to bring it back to Bonn to fix a few problems.

Data taking was initially intended to continue by summer of 2018. The
fully repaired detector was installed between <2018-07-16> and
<2018-07-19> with a few minor delays due to a change in mounting of
the lead shielding support to accommodate a parallel data taking with
KWISP. Unfortunately, external delays pushed the begin of the data
taking campaign back into late October. On <2018-10-20> the data
taking finally begins after a power supply issue was fixed the day
before. For alignment another geometer measurement was performed on
<2018-07-23>. The issues encountered during this data taking period,
which lasted until <2018-12-20> are mentioned in sec. [[#sec:cast:data_taking_woes_2018_2]].

With the end of 2018 the data taking campaign of the Septemboard was
at an end. The detector was moved over from CAST to the CAST Detector
Lab (CDL) on <2019-02-14> for a measurement campaign behind an X-ray
tube for calibration purposes. Data was taken until <2019-02-21> with
a variety of targets and filters (see sec. [[#sec:calibration:cdl]] in the
next chapter for a deeper overview). Afterwards the detector was
dismounted and taken back to Bonn.

For the results of the different alignments, further see section
[[#sec:cast:alignment]].

#+begin_src subfigure
(figure ()
        (subfigure (linewidth 0.5) (caption "Laser alignment") (label "fig:cast:laser_alignment1")
                   (includegraphics (list (cons 'width (linewidth 0.95)))
                                     "~/phd/Figs/CAST_Alignment/laser_alignment_IMG_20170707_121738.jpg"))
        (subfigure (linewidth 0.5) (caption "Detector installed after alignment") (label "fig:cast:detector_installed")
                   (includegraphics (list (cons 'width (linewidth 0.95)))
                                     "~/org/Figs/CAST_Alignment/detector_installed_after_laser_alignment_IMG_20170710_185009.jpg"))
        (caption
         (subref "fig:cast:laser_alignment1")
          "Alignment of the telescope side pipes using an acrylic glass
           flange with a centered grid and a laser aligned to the magnet
           bore. The central laser spot is the point on the vertical line
           extending out from the center. The other points towards the lower right
           are further refractions. This was better visible by eye."
         (subref "fig:cast:detector_installed")
           "Detector installed on the beamline behind the LLNL telescope on <2017-07-10>.")
        (label "fig:cast:alignment_detector"))
#+end_src

[fn:amptek]
https://www.amptek.com/internal-products/obsolete-products/cool-x-pyroelectric-x-ray-generator

*** TODOs for this section [/]                                   :noexport:

- [ ] *MENTION 55Fe for outer chips here?*

- [ ] *MERGE TWO FIGURES TO SUBFIG*
  
*** Detailed technical timeline [/]                              :extended:

- Initial installation 2017 ::
  - ref:
    https://espace.cern.ch/cast-share/elog/Lists/Posts/Post.aspx?ID=3420
    and [[file:~/org/Documents/InGrid_calibration_installation_2017_elog.pdf]]
  - June/July detector brought to CERN
  - before <2017-07-07 Fri> alignment of LLNL telescope by Jaime
  - <2017-07-07 Fri> laser alignment (see [[file:~/org/Figs/CAST_Alignment/laser_alignment_IMG_20170707_121738.jpg]])
  - <2017-07-10 Mon> vacuum leak tests & installation of detector
    (see:
    [[file:~/org/Figs/CAST_Alignment/detector_installed_after_laser_alignment_IMG_20170710_185009.jpg]])
  - after <2017-07-10 Mon> installation of lead shielding
  - <2017-07-11 Tue> Geometer measurement of InGrid alignment for
    X-ray finger run
  - <2017-07-13 Thu> - <2017-07-14 Fri>: first X-ray finger run (not
    useful to determine position of detector, due to dismount after)
  - after: dismounted to make space for KWISP

- Installation for data taking start ::    
  - Remount in September 2017 <2017-09-11 Mon> - <2017-09-14 Thu>
  - installation from <2017-09-11 Mon> to <2017-09-15 Fri> 
  - <2017-09-14> Alignment with geometers for data taking, magnet warm and under vacuum.

- Window explosion cleaning accident ::
  - weekend: (ref: [[file:~/org/Talks/CCM_2017_Sep/CCM_2017_Sep.org]])
    - calibration (but all wrong)
    - water cooling stopped working
  - next week: try fix water cooling
  - quick couplings: rubber disintegrating causing cooling flow to go to
    zero
  - attempt to clean via compressed air
  - final cleaning <2017-09-19 Tue>: wrong tube, compressed detector...
  - detector window exploded...
    - show image of window and inside detector
  - detector investigation in CAST CDL <2017-09-19 Tue>
    see
    [[file:~/org/Figs/CAST_detector_exploded/broken_window_close_IMG_20170919_152130.jpg]]
    images & timestamps of images
  - study of contamination & end of Sep CCM
  - detector back to Bonn, fixed

- Reinstallation for data taking start (Run 2) ::
  - detector installation before first data taking
  - reinstall in October for start of data taking in 30th Oct 2017
  - remount start <2017-10-23 Mon>
  - <2017-10-26 Thu> Alignment with Geometers (after removal &
    remounting due to window accident) for data taking. Magnet *cold*
    and under vacuum.
  - <2017-10-27 Fri> calibration of scintillator veto paddle in RD51 lab
  - remount installation finished incl. lead shielding <2017-10-28 Sat>
    (mail "InGrid status update" to Satan Forum on <2017-11-09 Thu>)  
  - <data taking period from <2017-10-30 Mon> to <2017-12-22 Fri> in
    2017>
    - between runs 85 & 86: fix of ~src/waitconditions.cpp~ TOS bug,
      which caused scinti triggers to be written in all files up to next
      FADC trigger
    - run 101 <2017-11-29 Wed 6:40> was the first with FADC noise
      significant enough to make me change settings:
      - Diff: 50 ns -> 20 ns (one to left)
      - Coarse gain: 6x -> 10x (one to right)
    - run 109: <2017-12-04 Mon> crazy amounts of noise on FADC
    - run 111: stopped early. tried to debug noise and blew a fuse in
      gas interlock box by connecting NIM crate to wrong power cable
    - run 112: change FADC settings again due to noise:
      - integration: 50 ns -> 100 ns
        This was done at around <2017-12-07 Thu 8:00>
      - integration: 100 ns -> 50 ns again at around
        <2017-12-08 Fri 17:50>.
    - run 121: Jochen set the FADC main amplifier
      integration time from 50 -> 100 ns again, around
      <2017-12-15 Fri 10:20>
  - <data taking period from <2018-02-17 Sat> to <2018-04-17 Tue>
    beginning 2018>
    - start of 2018 period: temperature sensor broken!
    - <2018-02-15 Thu> to <2018-02-17 Sat> issues with moving THL
      values & weird detector behavior. Changed THL values temporarily
      as an attempted fix, but in the end didn't help, problem got worse.
      <2018-02-17 Sat> (ref: gmail "Update 17/02" and
      [[file:~/org/Mails/cast_power_supply_problem_thlshift/power_supply_problem.org]])
      issue with power supply causing severe drop in gain / increase
      in THL (unclear, #hits in 55Fe dropped massively ; background
      eventually only saw random active pixels).
      Fixed by replugging all power cables and improving the grounding
      situation.
      iirc: this was later identified to be an issue with the
      grounding between the water cooling system and the
      detector.
    - by <2018-02-17 Sat 20:41> everything was fixed and detector was
      running correctly again.
    - 2 runs:
      1. <2018-02-15 Thu 7:01>    <2018-02-15 Thu 8:33>
      2. <2018-02-16 Fri 7:00>    <2018-02-16 Fri 8:31>
      were missed because of this.
  - <2018-04-18 Wed> removal of veto scintillator and lead shielding
  - X-ray finger run 2 on <2018-04-20 Fri>. This run is actually
    useful to determine the position of the detector.
  - <2018-04-24 Tue> Geometer measurement after warming up magnet and
    not under vacuum. Serves as reference for difference between
    vacuum & cold on <2017-10-26 Thu>!
  - <2018-04-26 Thu> detector fully removed and taken back to Bonn

- Reinstallation for data taking in Oct 2018 (Run 3) ::
  - installation started <2018-07-16>. Mounting due to lead shielding
    support was more complicated than intended (see mails "ingrid
    installation" including Damien Bedat)
  - shielding fixed by <2018-07-19> and detector installed the next
    couple of days
  - <2018-07-23 Mon> Alignment with Geometers for data taking. Magnet
    warm and not under vacuum.
  - data taking was supposed to start end of September, but delayed.
  - detector had issue w/ power supply, finally fixed on
    <2018-10-19 Fri>. Issue was a bad soldering joint on the Phoenix
    connector on the intermediate board.
    *Note*: See chain of mails titled "Unser Detektor..." starting on
    <2018-10-03 Wed> for more information. Detector behavior was weird
    from beginning Oct. Weird behavior seen on the voltages of the
    detector. Initial worry: power supply dead or supercaps on
    it. Replaced power supply (Phips brought it a few days after), but no change.    
  - data taking starts <2018-10-20 Sat>
  - run 297, 298 showed lots of noise again, disabled FADC on
    <2018-12-13 Thu 18:40> (went to CERN next day)
  - data taking ends <2018-12-20 Thu>
  - runs that were missed:
    1. <2018-10-19 Fri 6:21>    <2018-10-19 Fri 7:51>
    2. <2018-10-28 Sun 5:32>    <2018-10-28 Sun 7:05>
    3. <2018-11-24 Sat 7:08>    <2018-11-24 Sat 7:30>
    The last one was not a full run.
    - [ ] *CHECK THE ELOG FOR WHAT THE LAST RUN WAS ABOUT*
       
- CAST Detector Lab measurements ::
  - detector mounted in CAST Detector Lab <2019-02-14 Thu>
  - data taking from <2019-02-15 Fri> to <2019-02-21 Thu>.
  - detector dismounted and taken back to Bonn

- Outer chip 55Fe calibrations ::
  - ref: [[file:~/org/outerRingNotes.org]]
  - calibration measurements of outer chips with a 55Fe source using a
    custom anode & window
  - between <2021-05-20 Thu> and <2021-05-31 Mon 09:54> calibrations
    of each outer chip using Run 2 and Run 3 detector calibrations
  - <2021-08-31 Tue> start of a new detector calibration
  - another set of measurements between <2021-10-12 Tue 18:00> to
    <2021-10-16 Sat 19:55> with a new set of calibrations

**** TODOs for this section                                     :noexport:

- [X] *COPY BACK TO STATUS AND PROGRESS WITH UPDATES!*
  -> Done <2023-10-18 Wed 21:46>.
  
** Alignment
:PROPERTIES:
:CUSTOM_ID: sec:cast:alignment
:END:

Detector alignment with the X-ray telescope, the magnet and by
extension the solar core during solar tracking is obviously crucial
for a helioscope for a good physics result. The alignment procedure
used for the Septemboard detector is a three-fold approach:

1. alignment of the piping up to the detector using an acrylic glass
   target with a millimeter spaced cross, as seen in
   fig. sref:fig:cast:laser_alignment1. This target is mounted to the
   vacuum pipes in the same way the detector is mounted. A laser is
   installed on the opposite side of the magnet. With the magnet bores
   fully open the laser is aligned such that it propagates the full
   bore and is reflected by the X-ray telescope into the focal
   spot. This alignment guarantees the focal spot location to be near
   the center of the detector. Uncertainty is introduced due to the
   need to remove the acrylic glass target and install the detector,
   as the mounting screws allow for small movements. In addition the
   vacuum pipes are also not perfectly fixed.
2. alignment of the fully installed detector using an X-ray
   finger. The 'X-ray finger' is a small electric X-ray generator (in
   particular an Amptek COOL-X), which is installed in the magnet bore
   at the opposite end of the magnet. The generated X-rays must
   traverse the magnet and telescope, thereby being focused by the
   telescope into the focal spot. As the X-ray finger does not emit
   parallel light, the resulting distribution of the X-rays on the
   detector is not a perfect focal spot, even if the telescope was
   perfect and the detector placed right in the focus. The close
   distance also implies the focal length is slightly different than
   for an infinite source. The mean position of the taken data can
   anyhow be used to determine the likely focal spot position. See
   below for an example and the resulting position from one of the
   X-ray finger runs.
3. alignment by the geometer group at CERN. A theodolite is installed
   in the CAST hall and the location of many targets on the magnet,
   telescope, vacuum pipes and the detector itself are measured up to
   $\SI{0.5}{mm}$ precision at $1σ$ level. See
   fig. sref:fig:cast:alignment_targets for a picture of such a
   target. The initial geometer measurement from <2017-07-11> mainly
   serves as a baseline reference. As the first two alignment
   procedures provide a good alignment, a measurement of the existing
   position by the geometers can then later be used to re-align the
   detector after it was removed relative to the previous baseline
   position relative to the telescope. This assures the detector can
   be remounted and placed in the right location without the need for
   an additional laser alignment.

The X-ray finger run taken in April 2018 can be used as a reference
for the alignment as used during the first data taking. The center
positions of each cluster can be shown as a heatmap, where the number
of hits each pixel received is colored. Computing the mean position of
all those clusters yields the most likely center position of the focal
spot. See fig. [[sref:fig:cast:xray_finger_centers]] for an example of
this. The position of the center based on the mean of all cluster
centers is about $\SI{0.4}{mm}$ away from the center in both axes.

#+begin_src subfigure
(figure ()
        (subfigure (linewidth 0.5) (caption "X-ray finger clusters") (label "fig:cast:xray_finger_centers")
                   (includegraphics (list (cons 'width (linewidth 0.95)))
                                    "~/phd/Figs/CAST_Alignment/xray_finger_centers_run_189.pdf"))
        (subfigure (linewidth 0.5) (caption "Geometer target") (label "fig:cast:alignment_targets")
                   (includegraphics (list (cons 'width (linewidth 0.95)))
                                    "~/phd/Figs/CAST_Alignment/detector_targets_for_alignment_small.png"))
        (caption
         (subref "fig:cast:xray_finger_centers")
          ": Cluster center positions of the X-ray finger run 189 from April
           2018. The red cross marks the center of all cluster centers,
           which is the most likely position of the focal spot. It is "
          ($ "\\sim\\SI{0.4}{mm}") " away from the chip center in both axes.
           The two parallel lines with less clusters are the window strongback.
           The orthogonal line is a graphite spacer in the center of the LLNL telescope."
         (subref "fig:cast:alignment_targets")
          ": Image showing the targets on the detector. The acrylic glass
           cylinders are the fiducial marks used to hold the actual survey
           target. The survey target is a mirror to reflect the laser of the
           theodolite. Image from " (cite "geometer_1") ".")
        (label "fig:cast:xray_finger_alignment"))
#+end_src

With this setup after each remounting a geometer measurement was
performed to align the detector back to the initial laser
alignment. As the second mounting of the detector in September 2017
was not used for any data taking, the associated geometer measurement
is irrelevant.

Tab. [[tab:cast:geometer_alignments]] summarizes the values of the
geometer alignment results for each of the measurements using the
CenterR and CenterF positions defined based on the initial geometer
measurement in July 2017. In each case the shifts in X, Y and Z
direction is usually significantly less than $\SI{1}{mm}$.

#+CAPTION: Overview of the results of the different geometer alignment measurements. The
#+CAPTION: first measuremnt servese as the baseline to define 2 points (CenterR and CenterF)
#+CAPTION: relative to which alignment later is done. The initial alignment is done both
#+CAPTION: by laser and X-ray finger. The second geometer measurement is not useful, as 
#+CAPTION: no data was taken with it, due to the window rupture accident.
#+NAME: tab:cast:geometer_alignments
#+ATTR_LATEX: :booktabs t
|-------------+---------+----------------+----------------+----------------+--------|
| Measurement | Target  | ΔX [$\si{mm}$] | ΔY [$\si{mm}$] | ΔZ [$\si{mm}$] | Useful |
|-------------+---------+----------------+----------------+----------------+--------|
|  11.07.2017 |         |                |                |                | yes    |
|-------------+---------+----------------+----------------+----------------+--------|
|  14.09.2017 | CenterR |           -0.1 |            0.3 |           -0.8 | no     |
|             | CenterF |           -0.1 |            0.3 |           -0.9 |        |
|-------------+---------+----------------+----------------+----------------+--------|
|  26.10.2017 | CenterR |            0.2 |            0.6 |            0.2 | yes    |
|             | CenterF |            0.1 |            0.6 |           -0.1 |        |
|-------------+---------+----------------+----------------+----------------+--------|
|  24.04.2018 | CenterR |            0.5 |            0.5 |            0.0 | yes    |
|             | CenterF |            0.4 |            0.5 |           -0.3 |        |
|-------------+---------+----------------+----------------+----------------+--------|
|  23.07.2018 | CenterR |            1.1 |            0.5 |            0.6 | yes    |
|             | CenterF |            1.0 |            0.5 |            0.3 |        |
|-------------+---------+----------------+----------------+----------------+--------|

For a detailed overview of the geometer measurements see the public
EDMS links under cite:geometer_1,geometer_2,geometer_3,geometer_4,geometer_5
containing the PDF reports for each measurement.

*** TODOs for this section [/]                                   :noexport:

- [X] *REPHRASE ABOVE W.R.T. CORRECT RUN 189 AND WHAT IS WINDOW ETC*
  Might change significantly. Ideally we can cross reference the X-ray
  finger center position to the geometer measurement, i.e. use it for
  the next data taking campaign where we know the deltas to the
  geometer associated with /this/ X-ray finger run.
- [ ] *REPHRASE ABOVE TO NOT READ LIKE EXTENSION OF POINT NUMBER 2
  ABOVE*
  -> Refers to the X-ray finger section after the 1, 2, 3 part.
- [ ] *ADD NOTE HOW IN PRINCIPLE ONE COULD ARGUE THE POSITION IS
  ARTIFICIALLY MOVED UP DUE TO CHIP CUT OFF AT BOTTOM!*
  -> The data is partially cut off at the bottom potentially meaning
  the mean is slightly biased.
- [X] *ROTATE THE PLOT*
  -> And rotate our background & candidates I fear...
  -> Yep, that still has to be done! :)
  -> But rotation for the X-ray finger does not really need to be done
  here. Or well, better rotate by 90° so it is more evident what the
  telescope graphite spacer is.

- [ ] split these things up into something that is not a list? We can
  still add subsections here.   

- [ ] MAYBE rename to Detector setup & alignment
- [ ] maybe explain X-ray finger measurements here fully instead of a
  separate section?
  -> Do we even have a separate section?
- [X] X-ray finger not performed for 2018 data taking for logistical
  reasons and knowledge that geometer measurements give us precise
  information relative to "known good" alignment in 2017.  

- [X] have own section for alignment? Problem is that alignment takes
  place over the whole period, as the detector was removed multiple
  times etc.
- [X] Alignment section which goes over the seen deviations based on
  geometers & X-ray finger run?

- [X] Geometer measurement 1 not relevant directly. But relevant as it
  was used later to align against!
- [X] Geometer measurement 2 irrelevant, as detector was mounted &
  dismounted without any data taking due to window rupture
- [X] Geometer measurement 3 relevant for data taking Run 2
- [X] Geometer measurement 4 relevant for data taking Run 3

- [ ] image showing the targets from 2017/07/11

- [ ] Note: from Oct 2017 report on there's also numbers relative to
  *magnet* fiducials. We care only about telescope fiducials as that's
  what we align to. Maybe ask Johanna again about this...

*About Geometer table:*
- [ ] *CLARIFY WHAT THE SIGN MEANS. IS MINUS UP OR DOWN, LEFT OR
  RIGHT?*
- [ ] *LETS HOPE WHEN THINKING MORE DEEPLY ABOUT NUMBERS IT DOESN'T
  MAKE 1mm IN ΔX APPEAR TOO MUCH!*
  -> This is about comparing the two X-ray finger runs and the
  associated geometer measurements.
  -> Not sure thinking about it more. On the one hand we need to keep
  in mind that the coordinates X & Y should be flipped. Ok, next the
  center Y position (so real X) in the X-ray finger cluster center
  plot for the 2017 July plot should in theory be further at the
  bottom as the chip cuts of data and therefore artifically moves the
  position slightly up. Now the issue is that this is movement *in
  addition to* the already existing ~1 mm that are offset based on
  the difference calculation of the X-ray finger plots, but the X
  difference is only 0.2mm while we are not insensitive to changes in
  X! It's very confusing. Maybe the real takeaway is just that this is
  not super reliable...
  If we can't figure this out later, the correct thing to do will be:
  - Explain the above in a paragraph, numbers don't fully
    match. Therefore both should be taken with a grain of salt &
    systematic uncertainty is simply forced to 0.5mm as a rough 1σ
    behavior.
  *Well*: One explanation could simply be that the movement is non
  linear. A shift in geometer X/Y might not be the same shift in our
  X-ray finger X/Y! 
- [ ] *SEE TIMELINE WHICH ONES ARE UNDER VACUUM AND MAGNET COLD / WARM
  MAYBE MENTION*  

*** Some extra info about the geometer alignment                 :extended:

The following is the snippet from the <2017-09-14> PDF report about
the definition of the CenterR and CenterF positions.
  #+begin_quote
Goal of the operation has been to align the InGRID detector with
respect to the LLNL telescope as on 11.07.2017 after the alignment of
the setup with respect to the LASER installed.  Coordinates of the
measurement on 11.07.2017 are given below.

In order to compare InGRID position on 11.07 and 14.09, two points
close to the detector axis CenterR and CenterF have been defined on
11.07. Afterwards their coordinates for the measurement on 14.09 have
been calculated.
  #+end_quote

*** Generate X-ray heatmap [/]                                   :extended:

- [X] *FIND XRAY FINGER RUN 2, RUN 189!*
  [[file:~/CastData/data/XrayFingerRuns/]]
- [X] *RECREATE BELOW FOR THE OTHER XRAY FINGER RUN!*
  -> Both are created and listed below.
- [ ] *RECREATE PLOTS AS TIKZ + VEGA*
  -> We create them on the Cairo backend using DejaVu Serif, same as
  in the document of the thesis now. This is because the TikZ produced
  vector graphic ends up much larger than the Cairo one.
  Vega is on hold for now.
- [X] *VERIFY THAT WE (LIKELY) HAVE TO ROTATE THE DATA BY 90 DEGREES AS ONE OF THE VERTICAL LINES IS THE TELESCOPE AXIS WHICH SHOULD BE
  HORIZONTAL TO THE GROUND*
  -> Yes, we do.

First let's reconstruct the X-ray finger run:
#+begin_src nim :tangle code/xray_finger_data_parsing.nim
import shell, strutils

proc main(path: string, run: int) =
  # parse data
  let outfile = "/t/xray_finger_$#.h5" % $run
  let recoOut = "/t/reco_xray_finger_$#.h5" % $run
  
  shell:
    raw_data_manipulation -p ($path) "--runType xray --out " ($outfile)
  shell:
    reconstruction -i ($outfile) "--out " ($recoOut)
  
when isMainModule:
  import cligen
  dispatch main
#+end_src

And now we simply create a heatmap of the cluster centers:
 
#+begin_src nim :tangle code/xray_finger_center_plot.nim
import nimhdf5, ggplotnim, options
import ingrid / tos_helpers
import std / [strutils, tables]

proc main(run: int, switchAxes: bool = false) =
  let file = "/t/reco_xray_finger_$#.h5" % $run
  
  #proc readClusters(h5f: H5File): (seq[float], seq[float]) =
  var h5f = H5open(file, "r")
  
  # compute counts based on number of each pixel hit
  proc toIdx(x: float): int = (x / 14.0 * 256.0).round.int.clamp(0, 255)
  var ctab = initCountTable[(int, int)]() 
  
  var df = readRunDsets(h5f, run = run,
                        chipDsets = some((
                          chip: 3, dsets: @["centerX", "centerY"])))
    .mutate(f{"xidx" ~ toIdx(idx("centerX"))},
            f{"yidx" ~ toIdx(idx("centerY"))})
  let xidx = df["xidx", int]
  let yidx = df["yidx", int]
  forEach x in xidx, y in yidx:
    inc cTab, (x, y)
  df = df.mutate(f{int: "count" ~ cTab[(`xidx`, `yidx`)]})
  let centerX = df["centerX", float].mean
  let centerY = df["centerY", float].mean
  discard h5f.close()
 
  echo "Center position of the cluster is at: (x, y) = (", centerX, ", ", centerY, ")"
  let x = if switchAxes: "centerY" else: "centerX"
  let y = if switchAxes: "centerX" else: "centerY"
  let cX = if switchAxes: centerY else: centerX
  let cY = if switchAxes: centerX else: centerY
  ggplot(df, aes(x, y, color = "count")) +
    geom_point(size = 0.75) +
    geom_point(data = newDataFrame(), aes = aes(x = cX, y = cY),
               color = "red", marker = mkRotCross) + 
    scale_color_continuous() +
    ggtitle("X-ray finger clusters of run $#" % $run) +
    xlab(r"x [mm]") + ylab(r"y [mm]") + 
    xlim(0.0, 14.0) + ylim(0.0, 14.0) +
    theme_scale(1.0, family = "serif") + 
    ggsave("/home/basti/phd/Figs/CAST_Alignment/xray_finger_centers_run_$#.pdf" % $run) 
           #useTeX = true, standalone = true)

when isMainModule:
  import cligen
  dispatch main
#+end_src

#+RESULTS:
# Center position of the cluster is at: (x, y) = (7.210714052855218, 5.669514297250704)

First perform the data reconstruction:
#+begin_src sh
./code/xray_finger_data_parsing -p ~/CastData/data/XrayFingerRuns/Run_21_170713-11-03 --run 21
./code/xray_finger_data_parsing -p ~/CastData/data/XrayFingerRuns/Run_189_180420-09-53 --run 189
#+end_src
And now create the plots:
#+begin_src sh
./code/xray_finger_center_plot -r 21 --switchAxes 
./code/xray_finger_center_plot -r 189 --switchAxes
#+end_src

For run 21:
Center position of the cluster is at: (x, y) = (7.210714052855218,5.669514297250704)
For run 189:
Center position of the cluster is at: (x, y) = (7.428075467697270,6.594113570730057)

First the plot for the (unused) X-ray finger run taken at the first
installation before any data taking (detector removed afterwards):
[[file:Figs/CAST_Alignment/xray_finger_centers_run_21.pdf]]

And second the plot of the 2018 X-ray finger run taken _before_ the
detector was removed in Apr 2018. This is the baseline for our idea
where the focal spot is going to be.
file:Figs/CAST_Alignment/xray_finger_centers_run_189.pdf


*** Systematic uncertainty from graphite spacer rotation [/]     :extended:

- [X] Determine the rotation angle of the graphite spacer from the
  X-ray finger data
  -> do now.
  X-ray finger run:
  [[~/phd/Figs/CAST_Alignment/xray_finger_centers_run_189.pdf]]
  ->
  [[~/org/Figs/statusAndProgress/xray_finger_graphite_spacer_angle_run189.png]]
  -> It comes out to 14.17°!
  But for run 21 (between which detector was dismounted of course):
  [[~/org/Figs/statusAndProgress/xray_finger_graphite_spacer_angle_run21.png]]
  -> Only 11.36°!
  That's a huge uncertainty given the detector was only dismounted!
  3°.

- [ ] rotation of telescope!
- [ ] Effect on systematic uncertainty!  

** Detector setup at CAST [/]

The setup of the full beamline from the magnet end cap to the detector
is shown in a render in fig. [[fig:cast:render_beamline_setup]]. The
piping shows a clear kink introduced using a flexible bellow. This
setup is used to move the detector mount further away from the other
beamline to provide more space for two setups side-by-side. At the
same time it is an artifact of the LLNL telescope only being a
$\SI{30}{°}$ portion of a full telescope resulting in the focal plane
not being centered in front of the telescope. Not shown in the image
is the lead shielding installed around the detector as well as the
veto scintillator, which covers the majority of the beamline area. The
lead shielding is a $\SIrange{5}{15}{cm}$ thick castle of lead around
the detector ($\SI{10}{cm}$ on top and behind, $\SI{15}{cm}$ in front and
$\SI{5}{cm}$ and $\SI{10}{cm}$ on each side). An annotated image of the
real setup is seen in fig. [[fig:cast:annotated_setup]], which shows lead
shielding, veto scintillator, \cefe source manipulator and the LLNL
X-ray telescope. The setup is behind the VT3 gate valve of the CAST magnet.

#+CAPTION: Render of the detector setup up to the magnet end cap as seen
#+CAPTION: from above. The beamline kinks away from the other beamline
#+CAPTION: ("below" in this image) to provide more space for two detectors
#+CAPTION: at the same time.
#+CAPTION: Image courtesy of Tobias Schiffer.
#+NAME: fig:cast:render_beamline_setup
[[~/phd/Figs/llnl_cast_gridpix_render_small_annotated.png]]

#+CAPTION: Annotated setup as installed in October 2017 for the first data taking campaign.
#+CAPTION: The detector is seen in its lead shielding, with the veto scintillator covering
#+CAPTION: a large angular portion above the detector. The \cefe source manipulator
#+CAPTION: is seen head-on here. On the right towards the magnet we see the housing of the
#+CAPTION: LLNL X-ray telescope.
#+NAME: fig:cast:annotated_setup
[[~/phd/Figs/CAST_Nov2017Aufbau_annotated_small.png]]

*** TODOs for this section [/]                                   :noexport:

- [ ] *POSSIBLY CHANGE TO SOMETHING W/O RENDER AND SHOW IMAGES OF FULL
  SETUP WITH VETO SCINTI AS WELL*
  -> Generally the annotated render is already shown earlier in the
  thesis. So I don't think it's really needed here. We can reference
  it though.
  
- [ ] *MENTION ~VT3~ AS ITS IMPORTANT*  

- [X] *CHECK THICKNESS OF LEAD SHIELDING*
- [X] *SHOW REAL (ANNOTATED?) IMAGE OF SETUP*  


*** \cefe source and manipulator
:PROPERTIES:
:CUSTOM_ID: sec:cast:55fe_manipulator
:END:

As seen in the previous section the setup includes a \cefe
source. Its purpose is both monitoring of the detector behavior and it
serves as a way to calibrate the energy of events (as mentioned in
theory section [[#sec:theory:escape_peaks_55fe]]). More details on the
usage and importance for data analysis will be given in chapter
[[#sec:calibration]]. It is installed on a pneumatic
manipulator. Using a compressed air line with about $\SI{6}{bar}$
pressure the manipulator can be moved up and down. Under vacuum
conditions of the setup the manipulator is inserted unless the
compressed air is used to keep it out.

A Raspberry Pi [fn:raspi] is installed close to the manipulator and
connects to the two Festo [fn:festo] control sensors at the top and
bottom end of the manipulator using the general purpose input/output
(GPIO) pins. Two pins are used to read the sensor status from each. 5
more pins connect to a $\SI{24}{V}$ relay, which is used to control
the controllers for the compressed air line. The relay is controlled
via pulse width modulation (PWM). The software controlling the GPIO
pins of the Raspberry Pi is written in Python. A client program is
running on a computer in the CAST control room and communicates with
the Raspberry Pi via a network connection on which a server process is
running. It can receive connections via a socket allowing for remote
and programmatic control of the manipulator via a set of simple string
based messages. Further it provides a REPL (read-evaluate-print loop)
to control it interactively. For more details about the software see
the extended version of this thesis.

[fn:raspi] https://www.raspberrypi.org/about/
[fn:festo] https://www.festo.com/

**** TODOs for this section [/]                                 :noexport:

- [ ] *REFERENCE / LINK TO FESTO*
- [ ] *MAYBE SHORTEN INFO ABOUT PWM?*
- [ ] *LINK TO SOFTWARE HERE*

**** Manipulator software and notes [0/1]                       :extended:

- [ ] *MOVE MANIPULATOR CODE TO TPA TOOLS AND LINK TO IT?*
  -> Code can definitely go to TPA repository. The notes I think are
  enough if they are simply added as an Org file into the repository
  as well. They are too specific in some sense?

The source code of the python script running on the Raspberry Pi to
control the manipulator is the following script:

#+begin_src python
#!/usr/bin/env python3.6

import sys
import pigpio
import readline
import logging
import argparse
import time
import socket
import threading
import json
import asyncio
import functools
import weakref

# the program needs to do the following
# 
# - on RPi 7 pins used (5 controlled via software):
#     - relay:
#         - GOOD  - input, Pin 14
#         - OUT   - input, Pin 15
#         - RC IN - output (via PWM), via Pin 14
#         - VRC   - const voltage, using 5V via PIN 2, not done in software
#         - GND   - ground, pin 6, not done in software
#     - sensors 2 pins:
#         - input, read sensor output
# 
# 
# program always listens to GOOD and OUT
# using PWM we activate the manipulator. done by waiting for 
#  - command line input?
#  - reading some file, s.t. this program runs as a daemon and we use
#    some external tool to write file via usb to Pi
#  - finally be able to execute from TOS. easiest via script to call
# reading sensor inputs done in connection with usage of PWM
# 
# finally compile this program to jar to run it


# in order to control the source via network, the basic usage is something like
# the following:
# s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
# s.connect(('localhost', 42000))
# s.send("insert".encode())
# depening on whether the call is from the local machine or not.
# "insert" and "remove" are supported at the moment
# sends back a byte string containing the bool of the
# insertion / removal

#class client(asyncio.Protocol):



# connect to pi at IP address
p = pigpio.pi('10.42.0.91')

# define dict of pins
d = {"GOOD"    : 14,
     "OUT"     : 15,
     "RC_IN"   : 18,
     "S_OPEN"  : 20,
     "S_CLOSE" : 21}

    

class server(threading.Thread):
    # this is a simple server class, which receives the necessary
    # parameters to control the raspberry pi and a socket, from
    # which it listens to commands
    # inherits from threading.Thread to run in a separate thread

    def __init__(self, socket, p, d, pwm):
        # init the object
        self.socket = socket
        self.p      = p 
        self.d      = d
        self.pwm    = pwm
        self._stop  = False
        # now call the Thread init
        threading.Thread.__init__(self)
        # and set it as a daemon, so that it cannot
        # stop the main program from quitting
        self.setDaemon(True)

    async def process_client(self, reader, writer):
        client = writer.get_extra_info('peername')
        print("New client connected: {}".format(client))
        while self._stop == False:
            #data = socket.recv(1024).decode()
            data = (await reader.readline()).decode()
            if data:
                success = self.parse_message(data, client)
                message = self.create_message(client[0], success)
                writer.write(message)
                await writer.drain()
            else:
                writer.close()
                break

    # hacky get loop...
    def get_loop_and_server(self):
        return (self.loop, self.socketserver)

    def run(self):
        # using run we start the thread

        # need a new event loop, in which asyncio works
        self.loop = asyncio.new_event_loop()
        asyncio.set_event_loop(self.loop)
        # start server on specific port open on all interfaces
        self.server_future = asyncio.start_server(self.process_client, host = "0.0.0.0", port = 42000)
        # the returned future is handed to the event loop
        self.socketserver = self.loop.run_until_complete(
            asyncio.ensure_future(
                self.server_future,
                loop = self.loop))
        
        print(self.socketserver.sockets)
        # run 
        self.loop.run_forever()
                
    def stop_server(self):
        # in case stop_server is called, the stop flag is
        # set, such that the while loop, which waits for
        # data from the socket stops
        self._stop = True

        while self.loop.is_running() == True:
            print("current sockets still connected {}".format(self.socketserver.sockets))
            print("loop is still running: {}".format(self.loop.is_running()))

            #self.socketserver.wait_closed()
            self.loop.stop()
            self.server_future.close()
            self.socketserver.close()
            # next line raises an exception, loop still running....
            # TODO: fix problem that we cannot stop the running event loop :(
            self.loop.close(self.loop.run_until_complete(self.socketserver.wait_closed()))
            time.sleep(0.2)
        
        self.loop.close()


    def parse_message(self, data, address):
        # this function parses the data. If there is a function
        # call in the data, we call the appropriate function
        ip, port = address
        result = False
        if 'insert' in data:
            result = insert_source(self.p, self.d, self.pwm)
            #logging.info("source inserted via network socket {}:{}".format(ip, port))
        elif 'remove' in data:
            result = remove_source(self.p, self.d, self.pwm)
            #logging.info("source removed via network socket {}:{}".format(ip, port))
        elif 'out?' in data:
            result = read_out(self.p, self.d)
            #logging.info("out? status requestet via network socket {}:{}".format(ip, port))
        elif 'good?' in data:
            # read GOOD and print
            result = read_good(self.p, self.d)
            #logging.info("good? status requestet via network socket {}:{}".format(ip, port))
        elif 's_open?' in data:
            # read sensor 1 and print
            result = read_sensor_open(self.p, self.d)
            #logging.info("s_open? status requestet via network socket {}:{}".format(ip, port))
        elif 's_close?' in data:
            # read sensor 1 and print
            result = read_sensor_close(self.p, self.d)
            #logging.info("s_close? status requestet via network socket {}:{}".format(ip, port))
        else:
            result = "Unknown command"

        return result

    def create_message(self, client, data):
        # this function creates a JSON message containing the returned value
        # of the RPi call and a clientname
        # using dictionary, we create a json dump and return the encoded
        # string
        message = {"username" : client, "message" : data}
        # add trailing \r\l to indicate end of data stream
        json_data = json.dumps(message) + '\n'
        return json_data.encode()


# set relay_sleep time (time to wait for activation of relay): 50ms
relay_sleep = 50e-3
# set manipulator_sleep time: 1s
manip_sleep = 1

def print_help():

    help_string = """
    The following commands are available:\n
        insert : insert source into bore
        remove : remove source from bore
        out?   : print current value of relay OUT
        good?  : print current value of relay GOOD
        d?     : parameters used for relay (pin layout etc.)
        pwm?   : parameters used for PWM (frequency, duty cycle, ...)
        help   : prints this help
    """

    print(help_string)
    return

    

def read_good(p, d):
    # simple function which returns the value of the
    # GPIO pin for the GOOD output of the relay
    good = bool(p.read(d["GOOD"]))
    return good

def read_out(p, d):
    # simple function which returns the value of the
    # GPIO pin for the OUT output of the relay
    out = bool(p.read(d["OUT"]))
    return out

def read_sensor_open(p, d):
    # simple function which returns bool corresponding to
    # GPIO pin of sensor for OPEN
    val = bool(p.read(d["S_OPEN"]))
    return val

def read_sensor_close(p, d):
    # simple function which returns bool corresponding to
    # GPIO pin of sensor for CLOSED
    val = bool(p.read(d["S_CLOSE"]))
    return val

def configure_pins(p, d):
    # function to export all pins and set to correct modes
    # relay control / reading
    p.set_mode(d["GOOD"], pigpio.INPUT)
    p.set_mode(d["OUT"], pigpio.INPUT)
    p.set_mode(d["RC_IN"], pigpio.OUTPUT)
    # sensor reading
    p.set_mode(d["S_OPEN"], pigpio.INPUT)
    p.set_mode(d["S_CLOSE"], pigpio.INPUT)
    return

def pwm_control(p, d, freq, duty_cycle):
    # function to control the pwm of the RC IN pin
    p.hardware_PWM(d["RC_IN"], freq, duty_cycle)
    return

def insert_source(p, d, pwm):
    # inserts the source into the bore by activating the relay
    # wrapper for source_control
    success = source_control(p, d, pwm, "on")
    return success

def remove_source(p, d, pwm):
    # removes source from bore by disabling the relay
    # wrapping source control
    success = source_control(p, d, pwm, "off")
    return success

def source_control(p, d, pwm, direction):
    # this function provides a generalized interface to control the source
    # inputs:
    #     p: the Pi object
    #     d: the dict. containing the parameters
    #     pwm: the dict. containing pwm parameters
    #     direction: a string describing the direction to move the source
    #         "on"  : insert source
    #         "off" : remove source

    
    pwm_control(p, d, pwm["f"], pwm[direction])
    # relay was triggered: means relay should now read
    # insert:
    # GOOD == True &
    # OUT  == True
    # remove:
    # GOOD == True &
    # OUT  == False
    time.sleep(relay_sleep)
    good = read_good(p, d)
    out  = read_out(p, d)
    success = False

    # set expected values based on insertion / removal
    if direction == "on":
        good_exp = True
        out_exp  = True
    elif direction == "off":
        good_exp = True
        out_exp  = False
    else:
        raise NotImplementedError("only 'on' and 'off' implemented to control source.")

    s1 = None
    s2 = None
    if good == good_exp and out == out_exp:
        # if good is True and out False, everything fine
        #logging.debug("pwm set to {}, relay reports: (good : {}), (out : {})".format(direction, good ,out))
        # after setting of relay, wait again and check sensors
        print('pwm switched, waiting for manipulator to be moved')
        time.sleep(manip_sleep)
        # check sensors
        s1 = read_sensor_open(p, d)
        s2 = read_sensor_close(p, d)

        #logging.debug('sensors report: s1 = {}, s2 = {}'.format(s1, s2))
        # TODO: implement logic, which deals with sensors of manipulators

        if direction is "on":
            # after insertion the sensors should read:
            # s1 (sensor open) == True
            # s2 (sensor close) == False
            if s1 == True and s2 == False:
                success = True
            else:
                success = False
        else:
            if s1 == False and s2 == True:
                # after removal the sensors should read:
                # s1 (sensor open) == False
                # s2 (sensor close) == True
                success = True
            else:
                success = False
        
        if success == False:
            #logging.warning("""WARNING: direction was {}, but sensors read (open): {} (close): {}. 
            #Relay switched correctly.""".format(direction, s1, s2))
            pass

    elif good == good_exp and out != out_exp:
        # something is wrong, seems like relay did not change, both still repot True
        #logging.warning("pwm set to {}, relay good, but OUT still reports True: {}, {}".format(direction, good, out))
        pass
    elif good == False:
        #logging.warning("relay reports bad signal: {}".format(good))
        pass
    else:
        #logging.warning("should not happen. Contact developer.")
        pass

    if direction == "on":
        print("Insertion returned {}".format(success))
        if success == False:
            print("WARNING: insertion may have failed, but sensors read (open): {} (close): {}".format(direction, s1, s2))
            print("However, relay was activated correctly.")    
    elif direction == "off":
        print("Removal returned {}".format(success))
        if success == False:
            print("WARNING: removal may have failed, but sensors read (open): {} (close): {}".format(direction, s1, s2))
            print("However, relay was activated correctly.")    


    # the following lines are here to make sure there is a new prompt
    # even in case a network call was made before
    sys.stdout.write('> ')
    sys.stdout.flush()

    return success
    
def control_loop(p, d, pwm):
    # this function defines the main control loop of the manipulator
    # control
    print('Starting command prompt')
    print('\t insert : inserts source into bore')
    print('\t remove : removes source out of bore')
    print('\t quit   : stop the program')

    # TODO: still need to implement the checks for
    #       - sensor positions
    #         output warning to console and log file in case sensors
    #         don't report what was commanded
    #       - output warning in case signal not good
    
    while True:
        # the sys calls are used to make sure the line is empty before we
        # write to it via input. Don't want two > > to appear (depening
        # on network calls this might happen)
        sys.stdout.write('\r')
        sys.stdout.flush()
        line = input('> ')
        if 'insert' in line:
            insert_source(p, d, pwm)
            #logging.info("source inserted")
        elif 'remove' in line:
            remove_source(p, d, pwm)
            #logging.info("source removed")
        elif 'out?' in line:
            # read OUT and print
            print(read_out(p, d))
        elif 'good?' in line:
            # read GOOD and print
            print(read_good(p, d))
        elif 's_open?' in line:
            # read sensor 1 and print
            print(read_sensor_open(p, d))
        elif 's_close?' in line:
            # read sensor 1 and print
            print(read_sensor_close(p, d))
        elif 'd?' in line:
            # print dictionary
            print(d)
        elif 'pwm?' in line:
            # print dictionary
            print(pwm)
        elif line in ['help', 'h', 'help?']:
            print_help()
        elif line in ['quit', 'q', 'stop']:
            break
        elif line is not "":
            print('not a valid command.')
        else:
            continue

        # perform some logging of input, exit
        #logging.debug("command: {}".format(line))


    # after loop perform final logging?
    #logging.info('stopping program.')

    return

def create_message(client, data):
    # this function creates a JSON message containing the returned value
    # of the RPi call and a clientname
    # using dictionary, we create a json dump and return the encoded
    # string
    message = {"username" : client, "message" : data}
    # add trailing \r\l to indicate end of data stream
    json_data = json.dumps(message) + '\n'
    return json_data.encode()

def main(args):

    # setup arg parser
    parser = argparse.ArgumentParser(description = 'parse log level')
    parser.add_argument('--log', default="DEBUG", type=str)

    parsed_args = parser.parse_args()
    loglevel = parsed_args.log

    # setup logger
    numeric_level = getattr(logging, loglevel.upper(), None)
    if not isinstance(numeric_level, int):
        raise ValueError('Invalid log level: {}'.format(loglevel))


    # add an additional handler for the asyncio logger so that it also
    # writes the errors and exceptions to console
    console = logging.StreamHandler()
    logging.getLogger("asyncio").addHandler(console)

    LOG_FILENAME = 'log/manipulator.log'
    logging.basicConfig(filename = LOG_FILENAME,
                        #stream = sys.stdout, 
                        format = '%(levelname)s %(asctime)s: %(message)s',
                        datefmt='%d/%m/%Y %H:%M:%S',
                        level = numeric_level)


    # now configure all pins
    configure_pins(p, d)

    # define PWM settings
    pwm = {"f"   : 200,
           "off" : 200000,
           "on"  : 400000}

    # set pwm for RC IN pin
    pwm_control(p, d, pwm["f"], pwm["off"])

    # configure readline
    readline.parse_and_bind('tab: complete')
    readline.set_auto_history(True)

    # create the socket for the server
    # instantiate the server object
    # thr = server(serversocket, p, d, pwm)
    thr = server(None, p, d, pwm)
    # and start
    thr.start()

    # now that everything is configured, start the control loop
    control_loop(p, d, pwm)

    #try:
    #thr.stop_server()
    #except:
        

    # after control loop has finished, shut down the server thread
    loop, socketserver = thr.get_loop_and_server()
    # the following is an ugly hack to close the program without getting any
    # exceptions, thrown because the event loop in the server class is
    # not being shut down. Trying, but doesn't work, so this will have
    # to do for now
    try:
        thr.stop_server()
    except:
        socketserver.close()
    
    #loop.close()
    
    
if __name__=="__main__":
    import sys
    main(sys.argv[1:])
#+end_src

The following are my notes taken during development of the hardware &
software that describe the specific hardware in use.

***** DONE Manipulator [3/3]
****** DONE test for leaks
****** DONE test using compressed air, reading sensors
Regarding sensors, setup and hardware: 
Hardware:
    - sensors: Festo 150 857
      accept between 12 and 30 V DC
      max. output amperage: 500 mA
      switch on time: 0.5 ms
      switch off time: 0.03 ms
    - cable  : Festo NEBU-M8G3-K5-LE3 (541 334)
    - cable (power): Festo NEBV-Z4WA2L-R-E-5-N-LE2-S1
Thus, supply sensors with 24 V DC as well. Build setup such that 
valve and sensors receive same 24 V. 
Sensor outputs need to go on RPi GPIO pins. These max value of 
3.3 V (!). 
Using voltage divider something like the following seems
reasonable
#+BEGIN_LaTeX
$\frac{U_{\text{Pi, in}}}{U_{\text{sensor, out}}} = \frac{R_2}{R_1 + R_2}$
#+END_LaTeX
with 
#+BEGIN_LaTeX
$U_{\text{Pi, in}} < 3.3\,\text{V}$
$U_{\text{sensor, out}} = 24\,\text{V}$
#+END_LaTeX
Thus, we'd get:
#+BEGIN_SRC python
R2 = 1e3
R1 = 8.2e3
U_sensor_out = 24
U_pi_in = U_sensor_out * R2 / (R1 + R2)
return U_pi_in
#+END_SRC

#+RESULTS:
: 2.60869565217

Build simple board using these resistors (first check output
 current of sensor does not exceed 0.5 mA! max of RPi) to feed
the sensor values into the RPi. Should be simple?

Tested basic setup today (<2017-08-29 Di 18:47>). 
- 24V power supply prepared
- RPi connected to relay
- tpc20 used to run PyS_manipController.py
- relay connected as:
  - power supply 24V+: relay COM
  - power supply GND: valve GND
  - valve +: relay NO
is all there is to do. :)
      
****** DONE finalize software
The software to readout and control the manipulator needs to be
finished. The [[file:~/CastData/ManipulatorController/PyS_manipController.py][Python script to control manipulator]] currently
creates a server, which listens for connections from a client
connecting to it. Commands are not final yet (use only "insert"
and "remove" so far). 
Still need to:
1. DONE separate server and client into two actually separate threads
2. DONE try using nim client of chat app as the client. allows me to
   use nim, yay.

Note <2017-09-07 Do>: took me the last two days to figure out, why the server
application was buggy. See mails to Lucian and Fabian for an
explanation titled 'Python asyncio'. 
Having a logger enabled, causes asyncio to redirect all error
output from the asyncio code parts to land in the log file.

CLOSED: <2017-09-09 Sa 01:51>
Python server is finished, allows multiple incoming connections at the
same time, thanks to asyncio (what a PITA...).
Final version is [[file:~/CastData/ManipulatorController/PyS_manipController.py][PyS_manipController.py]].
Nim client works well as a client to control the server. See
[[file:~/CastData/ManipulatorController/nim/client.nim][client.nim]] for the code currently in use.

*** Lead shielding layout [/]                                    :noexport:  

The full lead shielding layout can be found here (created by Christoph Krieger):

[[file:resources/lead_shielding_assembly_ingrid_2017.pdf]]

** Window accident
:PROPERTIES:
:CUSTOM_ID: sec:cast:window_accident
:END:

During the preparations of the detector for data taking, it became
clear that the rubber seals of the quick connectors used for the water
cooling system started to disintegrate. The connectors were replaced
by Swagelok connectors, but the water cooling system still contained
rubber pieces blocking the flow. Due to the small diameter and twisted
layout of the cooling ducts in the copper body, the only way at hand
to clean them was a compressed air line, normally used for operation
of the \cefe manipulator (see
sec. [[#sec:cast:55fe_manipulator]]). This cleaning process worked very
well. Multiple cleaning & water pumping cycles were needed, as after
cleaning the system with compressed air, pumping water the next time
moved some remaining pieces, which blocked again. After multiple
cycles at which no more clogging happened upon water pumping a final
cycle was intended. As the gas supply and the water cooling system
after replacement of the quick connectors now used not only the same
tubing, but also the same connectors, the compressed air line was
mistakenly connected to the gas supply instead of water cooling line
by me. The windows -- tested up to $\SI{1.5}{bar}$ pressure -- could not
withstand the sudden pressure of the compressed air line of about
$\SI{6}{bar}$. A sudden and catastrophic window failure broke the
vacuum and shot window pieces as well as possible contamination into
the vacuum pipes towards the X-ray optics.

Because the LLNL telescope is an experimental optics there was worry
about potential oil contamination coming from dirty air of the
compressed air line. An conservative estimate of this given an upper
bound on contamination of the air, volume of the vacuum pipes and the
telescope area was computed. Assuming a flow of compressed air of
$\SI{5}{s}$, a ISO 8573-1:2010 class 4 compressed air contamination of
$\text{ppmv}_{\text{oil}} = \SI{10}{\milli\gram\per\meter\cubed}$ and
all oil in the air sticking to the telescope shells would lead to a
contamination of $c_{\text{oil}} =
\SI{41.7}{\nano\gram\per\cm\squared}$. More realistic is about
$\SI{1}{\percent}$ of that due to the telescope only being less than
$\frac{1}{10}$ of the full system area and the primary membrane pump
likely removing the majority ($>\SI{90}{\percent}$) of the oil in the
first place. This puts an upper limit of $c_{\text{oil}} =
\SI{0.417}{\nano\gram\per\cm\squared}$, which is well below anything
considered problematic for further data taking.

Further, the \cefe source manipulator likely caught most of the
debris, as it was fully inserted due to the necessary removal of the
compressed air line from it, which is normally needed to keep the
manipulator extruded when the system is under vacuum. For this reason
it is unlikely and window debris could have caused significant
scratches in the telescope layers.

After the incident the detector was dismounted and taken to the CAST
detector lab. Fig. sref:fig:cast:window_accident:broken_window shows the
detector from above with the small remaining pieces of the
window. Fig. sref:fig:cast:window_accident:broken_window_inside shows the
detector inside after opening it. A bulge is visible where the gas
inlet is and the compressed air entered. As the detector was
electronically dead after the incident, the decision was made to move
it back to Bonn for repairs. It turned out that the Septemboard had
become loose from the connector.

#+begin_src subfigure
(figure ()
        (subfigure (linewidth 0.5) (caption "Broken window from the inside") (label "fig:cast:window_accident:broken_window")
                   (includegraphics (list (cons 'width (linewidth 0.95)))
                                    "~/phd/Figs/CAST_detector_exploded/broken_window_close_IMG_20170919_152130.jpg"))
        (subfigure (linewidth 0.5) (caption "View into the detector after accident") (label "fig:cast:window_accident:broken_window_inside")
                   (includegraphics (list (cons 'width (linewidth 0.95)))
                                    "~/phd/Figs/CAST_detector_exploded/detector_broken_window_open_top_IMG_20170919_152130.jpg"))
        (caption
         (subref "fig:cast:window_accident:broken_window")
          "shows the cathode of the detector from the inside
           with the broken window. Essentially the full window directly exposed to vacuum is gone."
         (subref "fig:cast:window_accident:broken_window_inside")
          "is the view into the detector without the cathode.
           A bulge of the field cage is visible where the compressed air entered.")
        (label "fig:cast:window_accident:broken_window_subfig"))
#+end_src

*** TODOs for this section [/]                                   :noexport:

- [X] LIKELY NOT LIST EXACT NUMBERS HERE, BUT IF SO REFERENCE ISO
  CLASS
  -> As this will be moved to the appendix, the numbers can remain.
- [X] contamination calculation
- [X] pictures of broken window & detector
- [ ] *REWRITE CODE TO USE UNCHAINED*

*** Calculations of contamination [0/1]                          :extended:

- [ ] *REWRITE TO USE UNCHAINED!!*

Check the appendix [[#sec:appendix:vacuum_contamination]] for the document
written that contains my thoughts about the calculations below.

Here are the calculations done to estimate the contamination. First a
file containing the tubing sizes of the vacuum system:
#+begin_src nim :tangle code/vacuum_contamination/tubing.nim
import tables

type
  # defines the TubesMap datatype, which is a combined object to
  # store the different parts of the tubing each sequences of tuples
  TubesMap* = object
    static_tubes* : seq[tuple[diameter: float, length: float]]
    flexible_tubes* : seq[tuple[diameter: float, length: float]]
    t_pieces* : seq[tuple[diameter: float, length_long: float, length_short: float]]
    crosses* : seq[tuple[diameter: float, length: float]]

proc getVacuumTubing*(): TubesMap =
  # this function returns the data (originally written in calc_vacuum_volume.org
  # as a set of hash maps as a "TubesMap" datatype
  let st_tubing = @[(63.0, 10.0),
                    (63.0, 51.0),
                    (63.0, 21.5),
                    (25.0, 33.7),
                    (63.0, 20.0),
                    (63.0, 50.0),
                    (40.0, 15.5),
                    (16.0, 13.0),
                    (40.0, 10.0)]

  let fl_tubing = @[(16.0,  25.0),
                    (16.0,  25.0 ),
                    (16.0,  25.0 ),
                    (16.0,  25.0 ),
                    (16.0,  40.0 ),
                    (25.0,  90.0 ),
                    (25.0,  80.0 ),
                    (40.0,  50.0 ),
                    (16.0, 150.0 ),
                    (40.0,  80.0 ),
                    (40.0,  80.0)]

  let t_pieces = @[(40.0, 18.0, 21.0),
                   (16.0, 7.0, 4.5),
                   (40.0, 10.0, 10.0)]

  let crosses = @[(16.0, 10.0),
                  (40.0, 14.0),
                  (40.0, 14.0),
                  (40.0, 14.0)]
                    
  let t = TubesMap(static_tubes: st_tubing, flexible_tubes: fl_tubing, t_pieces: t_pieces, crosses: crosses)
  echo "Vacuum tubing is as follows:"
  echo t
  return t
#+end_src

And the actual code using the tubing to calculate possible
contamination:

#+begin_src nim :tangle code/vacuum_contamination/vacuum_contamination.nim
import math
import tubing
import sequtils, future
import typeinfo

# This script contains a calculation for the total volume of the
# currently in use vacuum system at CAST (behind and including LLNL
# telescope)

proc cylinder_volume(diameter, length: float): float =
  # this proc calculates the volume of a cylinder, given a
  # diameter and a length both in cm
  result = PI * pow(diameter / 2.0, 2) * length
  
proc t_piece_volume(diameter, length_long, length_short: float): float =
  # this proc calculates the volume of a T shaped vacuum piece, using
  # the cylinder volume proc
  # inputs:
  # diameter: diameter of the tubing in cm
  # length_long: length of the long axis of the tubing
  # length_short: length of the short axis of the tubing
  result = cylinder_volume(diameter, length_long) + cylinder_volume(diameter, length_short - diameter)

proc cross_piece_volume(diameter, length: float): float =
  # this proc calculates the volume of a cross shaped vacuum piece, using
  # the cylinder volume proc
  # inputs:
  # diameter: diameter of the tubing in cm
  # length: length of one axis of the tubing
  result = 2 * cylinder_volume(diameter, length) - pow(diameter, 3)

proc calcTotalVacuumVolume(t: TubesMap): float =
  # function which calculates the total vacuum volume, using
  # the rough measurements of the length and diameters of all the
  # piping
  # the TubesMap consists of:
  # static_tubes : seq[tuple[diameter: float, length: float]]
  # flexible_tubes : seq[tuple[diameter: float, length: float]]
  # t_pieces : seq[tuple[diameter: float, length_long: float, length_short: float]]
  # crosses : seq[tuple[diameter: float, length: float]]
  # define variables to store static volume etc

  # calc volume of static tubing
  let static_vol = sum(map(
    t.static_tubes, (b: tuple[diameter, length: float]) ->
    float =>
    cylinder_volume(b.diameter / 10, b.length)))
  let flexible_vol = sum(map(
    t.flexible_tubes, (b: tuple[diameter, length: float]) -> 
    float =>
    cylinder_volume(b.diameter / 10, b.length)))
  let t_vol = sum(map(
    t.t_pieces, (b: tuple[diameter, length_long, length_short: float]) ->
    float =>
    t_piece_volume(b.diameter / 10, b.length_long, b.length_short)))
  let crosses_vol = sum(map(
    t.crosses, (b: tuple[diameter, length: float]) ->
    float =>
    cross_piece_volume(b.diameter / 10, b.length)))

  result = static_vol + flexible_vol + t_vol + crosses_vol

proc calcFlowRate(d, p, mu, x: float): float =
  # this function calculates the flow rate following the Poiseuille Equation
  # for a non-ideal gas under laminar flow.
  # inputs:
  # d: diameter of the tube in m
  # p: pressure difference between both ends of the tube in Pa
  # mu: dynamic viscosity of the medium
  # x: length of the tube
  # note: get viscosity e.g. from https://www.lmnoeng.com/Flow/GasViscosity.php
  # returns the flow rate in m^3 / s
  result = PI * pow(d, 4) * p / (128 * mu * x)

proc calcGasAmount(p, V, T: float): float =
  # this function calculates the amount of gas in moles follinwg
  # the ideal gas equation p V = n R T for a given pressure, volume
  # and temperature
  let R = 8.31446
  result = p * V / (R * T)

proc calcVolumeFromMol(p, n, T: float): float =
  # this function calculates the volume in m^3 follinwg
  # the ideal gas equation p V = n R T for a given pressure, amount in mol
  # and temperature
  let R = 8.31446
  result = n * R * T / p
    
proc main() =

  # TODO: checke whether diameter of 63mm for telescope is a reasonable
  # number!
  let t = getVacuumTubing()
  # first of all we need to calculate the total volume of the vacuum
  let volume = calcTotalVacuumVolume(t)
  echo volume

  # now calcualte flow rate through pipe
  let
    # 3 mm diameter
    d = 3e-3
    # 6 bar pressure diff
    p = 6.0e5
    # viscosity of air
    mu = 1.8369247e-4
    # ~2m of tubing
    x = 2.0
    flow = calcFlowRate(d, p, mu, x)

  echo(flow * 1e3, " l / s")

  # given the flow in liter, calc total gas inserted into the system
  let flow_l = flow * 1e3

  # detector volume in m^3
  let det_vol = cylinder_volume(12.0, 3.0) * 1e-6
  echo("Detector volume is : ", det_vol)
  # initial gas volume inside detector (1 bar is argon!), thus
  # only .5 bar
  let n_initial = calcGasAmount(0.5e5, det_vol, 293.15)
  # gas which came in after window ruptured
  let valve_open = 5.0
  # total volume in m^3
  let flow_vol = flow_l * 1e-3 * valve_open
  
  # since the flown volume is given for normal pressure and temp, calc
  # amount of gas
  let n_flow = calcGasAmount(1.0e5, flow_vol, 293.15)
  echo("Initial gas is : ", n_initial, " mol")
  echo("Gas from flow is : ", n_flow, " mol")
  let n_total = n_initial + n_flow
  echo("Total compressed air, which entered system : ", n_total)

  # calc volume corresponding to normal pressure
  let tot_vol_atm = calcVolumeFromMol(1e5, n_total, 293.15)
  echo("Total volume of air at normal pressure : ", tot_vol_atm * 1e3, " l")
  
when isMainModule:
  main()
#+end_src

** Data taking woes [/]
:PROPERTIES:
:CUSTOM_ID: sec:cast:data_taking_woes
:END:

In this section we will cover the smaller issues encountered during
the data taking that are worth naming due to having an impact on the
quality of the data or specific features in the data someone who
analyzes the data should be aware of. We will cover each of the
effectively three data taking periods one after another.

*** TODOs for this section [/]                                   :noexport:  

- [ ] *LATER HAVE SECTION ON FADC NOISE, NOISE DETECTION AND WHAT
  EVENTS LOOK LIKE IN EACH CASE?*
- [ ] *THIS DOES NOT TALK ABOUT DRIFT OF PEAK IN DATA NOR GAIN
  VARIATION! TOPIC FOR ANOTHER SECTION*

*** 2017 Oct - Dec
:PROPERTIES:
:CUSTOM_ID: sec:cast:data_taking_woes_2017
:END:

The first data taking period from <2017-10-30 Mon> to <2017-12-22 Fri>
initially had a bug in the data acquisition software, which failed to
reset the veto scintillator values from one event to the next, if the
next one did not have an FADC trigger. In that case in principle the
veto scintillators should not have any values other than ~0~. However,
as there is a flag in the data readout for whether the FADC triggered
at all, this is nowadays handled neatly in the software by only
checking the triggers if there was an FADC trigger in the first
place. Unfortunately, it was later found that the scintillator
triggers were nonsensical in this data taking period due to firmware
bug anyway.

Starting from the solar tracking run on <2017-11-29 Wed> the analogue
FADC signals showed significant signs of noise activity. This lead to
an effectively extremely high dead time of the detector, because the
FADC triggered pretty much immediately after the Timepix shutter was
opened. As I was on shift during this tracking, I changed the FADC
settings to a value, which got rid of the noise enough to continue
normal data taking. The following changes were made:

- differentiation time reduced from $\SI{50}{ns}$ to $\SI{20}{ns}$
- coarse gain of the main amplifier increased from ~6x~ to ~10x~

Evidently this has a direct effect on the shape of the FADC signals,
to be discussed in sec. [[#sec:calibration:fadc_noise]].

On <2017-12-05 Tue> while trying to investigate the noise problem
which resurfaced the day before despite the different settings, a fuse
blew in the gas interlock box. This caused a loss of a solar tracking
the next day. The still present FADC noise lead me to change the
amplification settings more drastically on <2017-12-07 Thu 8:00>
during the shift:

- integration time from $\SI{50}{ns}$ to $\SI{100}{ns}$

The same day in the evening the magnet quenched causing the shift to
be missed the next day. In the evening of <2017-12-08 Fri> the
integration time was turned down to $\SI{50}{ns}$ again, as the noise
issue was gone again.

A week later the integration time was finally changed again to
$\SI{100}{ns}$. By this time it was clear that there would be no easy
fix to the problem and that it is strongly correlated to the magnet
activity during a shift. For that reason the setting was kept for the
remaining data taking periods.

*** 2018 Feb - Apr
:PROPERTIES:
:CUSTOM_ID: sec:cast:data_taking_woes_2018
:END:

2 days before the data taking period was supposed to start again in
2018 there were issues with the detector behavior with respect to the
thresholds and the gain of the GridPixes. During one calibration run
with the \cefe source the effective gain dropped further and
further such that instead of $\sim\num{220}$ electrons less than
$\sim\num{100}$ were recorded. This turned out to be a grounding issue
of the detector relative to the water cooling system.

Further, the temperature readout of the detector did not work
anymore. It is unclear what happened exactly, but the female micro
USB connector on the detector had a bad soldering joint as was found
out after the data taking campaign. It is possible that replugging
cables to fix the above mentioned issue caused an already weak
connector to fully break.

The second data taking period finally started on <2018-02-17 Sat> and
ran until <2018-04-17 Tue>.

This data taking campaign still ran without functioning scintillators,
due to lack of time and alternative hardware in Bonn to debug the
underlying issue and develop a solution.

*** 2018 Oct - Dec [/]
:PROPERTIES:
:CUSTOM_ID: sec:cast:data_taking_woes_2018_2
:END:

Between the spring and final data taking campaign the temperature
readout as well as the firmware was fixed to get the scintillator
triggers working correctly, with the installation being done end of
July 2018. By the time of the start of the actual solar tracking data
taking campaign at the end of October however, a powering issue had
appeared. This time the Phoenix connector on the intermediate board
had a bad soldering joint, which was finally fixed
<2018-10-19 Fri>. Data taking started the day after.

Two runs in mid December showed strong noise on the FADC again. This
time no amount of changing amplifier settings had any effect, which is
why 2 runs were done without the FADC. For the last runs it was
activated again and no more noise issues appeared.

*** Concluding thoughts about issues

The FADC noise issue was in many ways the most disrupting active issue
the detector was plagued by. In hindsight the standard LEMO cable used
should have been a properly shielded cable. Someone with more
knowledge about RF interference should have assisted in the
installation. In a later section, [[#sec:calibration:fadc_noise]], the
typical signals recorded by the FADC under noise will be shown as well
as mitigation strategies on the software side. Also how the signals and
the FADC activation threshold changed due to the changed settings will
be presented.

**** TODOs for this section [1/1]                               :noexport:
- [X] *FIX REFERENCE TO FADC NOISE LATER*
  -> The section exists but is not written as of <2023-10-19 Thu 14:09>.

** High voltage supply [/]

The high voltage supply is an iseg HV module, which is located in the
VME crate on the airport side of the magnet. The HV is controlled via
a USB connection to the VME crate, which it shares with the FADC. The
veto scintillator however has its own HV supply, since it needs a
positive HV, instead of a negative one.

The detector uses $\num{7}$ different high voltages. $\num{5}$ of
these are for the detector itself, $\num{1}$ for the SiPM and the last
for the veto scintillator on top. Their voltages are shown in
tab. [[tab:cast:high_voltage]]. 

#+CAPTION: Table of high voltages in use for the Septemboard detector.
#+CAPTION: Note that the veto scintillator is not controlled via
#+CAPTION: the iseg module, but by a CAEN N470.
#+NAME: tab:cast:high_voltage
#+ATTR_LATEX: :booktabs t
|-------------+---------+-------------+------------------|
| Description | Channel | Voltage / V | TripCurrent / mA |
|-------------+---------+-------------+------------------|
| grid        |       0 |        -300 |            0.050 |
| anode       |       1 |        -375 |            0.050 |
| cathode     |       2 |       -1875 |            0.050 |
| ring 1      |       3 |        -415 |            0.100 |
| ring 29     |       4 |       -1830 |            0.100 |
| veto scinti |       5 |       +1200 |                2 |
| SiPM        |       6 |       -65.6 |             0.05 |
|-------------+---------+-------------+------------------|

The voltages actually used are defined in [[file:~/TOS/config/HFM_settings.ini][~/TOS/config/HFM_settings.ini]].

The HV cables in use are red cables with LEMO HV connectors. They run
from the detector to an iseg HV module sitting in a VME crate on the
airport side of the magnet. The cables are marked with zip ties and
the same names as in tab. [[tab:cast:high_voltage]]. 

The interlock system for the high voltage supply is detailed in
section [[#sec:cast:hv_interlock]], together with the other interlock
systems in place.

*** TODOs for this section [/]                                   :noexport:

- [X] *MOVE SOME OF THIS TO THE DAQ SECTION WHERE WE HAVE A HV SECTION*
  -> The table is now also part of the detector chapter
  [[#sec:operation_calibration:high_voltage]].

- [ ] *ONCE FINAL: REFERENCE A RELATIVE LINK TO THE TOS REPOSITORY*
  -> Or to Github.


** Vacuum system [/]
:PROPERTIES:
:CUSTOM_ID: sec:cast:vacuum_system
:END:

This section covers the vacuum system of the detector. It is pumped
via a single primary membrane pump and two turbo pumps. One turbo pump
is used to pump the main vacuum vessel of the beam pipe, while the
second small turbo pump is used to pump the interstage part of our
X-ray source manipulator to reduce leakage during movement of the
source.

Fig. [[fig:cast:vacuum-schematic]] shows a schematic of the whole vacuum
system including all interlock systems and the pressure sensors. The
pressures of the sensors P3 and P-MM are used as an interlock for VT3
and the gas supply, respectively.

#+CAPTION: Schematic of the vacuum system behind the LLNL telescope including
#+CAPTION: interlocks and pressure sensors.
#+NAME: fig:cast:vacuum-schematic
#+ATTR_LATEX: :width 1\textwidth :options angle=90
[[file:~/phd/Figs/detector/vacuum_system2017.png]]

*** TODOs for this section [/]                                   :noexport:

- [X] *EXPLAIN INTERLOCKS HERE OR IN APPENDIX?*
  -> This entire thing will be in the appendix later. :)

** Watercooling system & gas supply

In this section the watercooling system as well as the gas supply is
discussed. In section [[#sec:cast:water_gas_schematic]] a combined
schematic of both systems is shown.

*** Watercooling 

In order to keep the detector cool enough to avoid noise and damage to
the septemboard, a watercooling system is used. This section describes
the relevant information for the system as used at CAST.

To readout the temperature two PT1000 temperature sensors are
installed on the detector. One is located on the bottomside of the
intermediate board (outside of the detector volume), while the other
is located on the bottom side of the Septemboard. This temperature
$T_{\text{Septem}}$ is also included in the schematic
[[fig:detector-schematic]] below, because it was intended to be part of
the HV interlock, as described in [[#sec:cast:hv_interlock]]. In the end
it was not due to lack of time for proper testing. [fn:data_loss]

Fig. sref:fig:cast:water_cooling_system shows the main part of the system including the
pump, reservoir and radiator. The tubing is specifically chosen in
*blue* to clear up potential confusion with other tubes used in the
detector system. [fn:cooling_colors] These tubes use special Festo quick couplings, which
cannot be connected to the connectors of the gas supply, to avoid
potential accidents. The tubes have zipties installed on them, which
label the tubes as well, with the naming convention as it is used in
fig. [[fig:detector-schematic]].

- Maintenance :: At the end of every shift it should be checked,
  whether the water level in the reservoir is still above the red line
  seen in Fig. [[sref:fig:cast:water_cooling_reservoir]]. If not, water
  should be added by the shift coordinator (or trusted shifters).

#+begin_src subfigure
(figure ()
        (subfigure (linewidth 0.36) (caption "Reservior") (label "fig:cast:water_cooling_reservoir")
                   (includegraphics (list (cons 'width (linewidth 0.95)))
                                    "~/phd/Figs/CAST_water_cooling_reservoir.jpg"))
        (subfigure (linewidth 0.64) (caption "Water cooling system") (label "fig:cast:water_cooling_system")
                   (includegraphics (list (cons 'width (linewidth 0.95)))
                                    "~/phd/Figs/CAST_water_cooling_system.jpg"))
        (caption
         (subref "fig:cast:water_cooling_reservoir")
          "shows a closeup of the water cooling reservoir with indicated markings to
           be checked for safe water levels."
         (subref "fig:cast:water_cooling_system")
          "is a look from the ground floor at the water cooling system including radiator, pump
           and reservoir.")
        (label "fig:cast:water_cooling_overview"))
#+end_src

[fn:data_loss] As mentioned in a previous footnote, the decision to
take it out of the HV interlock is the reason for loss of temperature
logging data.

[fn:cooling_colors] This change was made after the 'window accident'.


*** Gas supply

The gas supply uses red tubing (in parts where flexible tubing is
used) to differentiate itself from the watercooling
system. Additionally, the tubes have zipties showing which tube is
which. These are located on both ends of the tubes. The naming
convention is the same as in [[fig:detector-schematic]]. The connectors of
the gas line are standard Swagelok connectors.

As can be seen in the schematic, the gas supply has 4 valves on the
inlet side and 2 on the outlet side. In addition a buffer gas volume
is installed before the detector for better flow control. 

It follows a short explanation of the different valves:

- $V_{\text{in, 1}}$ is the main electrovalve installed right after the gas
  bottle outside the CAST hall. 
- $V_{\text{interlock}}$ is installed is the electrovalve installed
  below the platform where the beamline is located. This valve is part
  of the gas supply interlock, as described in section [[#sec:cast:gas_supply_interlock]].
- $V_{\text{in, 2}}$ is the manual valve located on the second gas
  supply mounting below the beamline. 
- $V_{\text{in, N1}}$ is the first needle valve, which is located on
  the sceond gas supply mounting below the beamline. It is the one
  part of the flow meter placed there.
- $V_{\text{P}}$ is the valve inside the pressure controller, which is
  placed roughly below the telescope (on the platform), while
  $P_{\text{Detector}}$ is the pressure gauge inside this controller.

*** Combined schematic (water & gas)
:PROPERTIES:
:CUSTOM_ID: sec:cast:water_gas_schematic
:END:

Fig. [[fig:detector-schematic]] shows a combined schematic of both the
watercooling system and the gas supply. Additionally, the relevant
interlock systems and their corresponding members are shown.

#+CAPTION: Combined schematic of the detector system, consisting of the
#+CAPTION: water cooling system and the gas supply. The interlock systems
#+CAPTION: are shown with dashed lines. See section [[#sec:cast:interlock_systems]]
#+CAPTION: regarding explanations of when the interlock is activated.
#+NAME: fig:detector-schematic
#+ATTR_LATEX: :width 1\textwidth :options angle=90
[[~/org/Doc/Detector/figs/detector_system2017.png]]

** Interlock systems
:PROPERTIES:
:CUSTOM_ID: sec:cast:interlock_systems
:END:

This section describes the interlock systems, which are related to the
detector. There are 3 interlock systems to speak of. 
- The CAST magnet interlock, which prohibits the gate valve VT3 to be
  opened, if the pressure in the vacuum system is not good enough.
- A gas supply interlock, which makes sure the detector is only
  flushed with gas, if all other parameters are considered nominal
  (mainly a good vacuum in the system).
- A HV interlock, which makes sure the detector is only under HV, if
  the temperature of the detector is still good (otherwise a lot of
  sparks are produced) and the currents on the HV lines are nominal.

*** HV interlock
:PROPERTIES:
:CUSTOM_ID: sec:cast:hv_interlock
:END:

#+begin_quote
This section describes the high voltage interlock as it was
intended. But as mentioned multiple times previously it was
deactivated for final data taking due to several bugs causing it do
trigger under non intended circumstances.
#+end_quote

The HV system is part of an interlock, which tracks the following
properties:
- detector temperature
- currents on HV lines
- TO BE IMPLEMENTED: gas pressure inside the detector [fn:gas_pressure]

The detector temperature is measured at two points by PT1000
sensors. One of these is located on the bottom side of the
intermediate board (and thus is more a measure the the temperature
surrounding the detector), while the second is located on the bottom
side of the septemboard. The second is the best possible measure for
the temperature of the InGrids. However, there is still a PCB
separating the sensor from the actual InGrids. This means there is
probably a temperature difference of a minimum of $\SI{10}{\celsius}$
between the measured value and the actual temperature of the InGrids. 

Whenever the TOS is configured to use the FADC readout and control the
HV (note: the two are intertwined, since both sit in the same VME
crate, which is controlled via a single USB connection), a background
process which monitors the temperature is started. If the temperature
exceeds the following boundaries

\[
\SI{0}{\celsius} \leq T \leq \SI{60}{\celsius}
\]

on the lower side of the septemboard, the HV is shut down
immediately. The lower bound is of less practical value in a physical
sense, but in case of sensor problems negative temperature values may
be reported. As the upper bound a value is taken at which sparks and
general noise seen on the pixels becomes noticeable. 

The interlock currents at which the HV trips are already shown in
tab. [[[[tab:cast:high_voltage]]. During ramp up of the HV, these trip
currents are set higher to avoid trips, while capacitors are being
charged.

The gas pressure within the detector was intended to be included into
the interlock system to shut down the HV if the pressure inside the
detector leaves a certain safe boundary, since this could indicate a
leak in the detector or an empty gas bottle.

[fn:gas_pressure] The gas pressure inside the detector was intended to
be added to the HV interlock as well, but also due to lack of time it
never was.

*** Gas supply interlock
:PROPERTIES:
:CUSTOM_ID: sec:cast:gas_supply_interlock
:END:

The gas supply is also part of an interlock system. In case of a
window rupture the potential amount of gas that might enter the vacuum
system should be limited by electrovalves. Ideally the pressure inside
the detector had been included into the gas supply interlock. This
could have made sure the gas inlet and outlet are closed in case the pressure
inside the detector drops (which might indicate a leak somewhere or an
empty gas bottle) or in case of rising pressure.

The latter is not as important though, because a pressure controller is
already installed behind the detector, which controls the flow such
that the pressure stays at $\SI{1050}{\milli \bar}$. While a failure
of the controller is thinkable, potentially leading to a pressure
increase inside the detector, it is questionable whether this could be
dealt with using this interlock system. That is because the pressure
sensor used is part of the pressure controller.

Three electrovalves are placed on the gas line of the detector. One,
$V_{\text{in}}$, is outside of the building next to the gas bottles
(see fig. [[sref:fig:cast:electrovalve-outside]]). The second valve
$V_{\text{interlock}}$ is located right before the buffer volume, next
to the watercooling system, below the shielding platform,
fig. sref:fig:cast:electrovalve-V-interlock. The final electrovalve
$V_{\text{out}}$ is located after the pressure controller on a blue
beam, which supports the optical table below the telescope (see
fig. [[sref:fig:cast:electrovalve-V-out]]). These valves are normally closed,
i.e. in case of power loss they automatically close. They are open if
a voltage is applied to them.

The valves are connected to the pressure sensor $P_{\text{MM}}$ (see
fig. [[fig:cast:vacuum-schematic]]). The pressures to activate the interlock
system is defined by upper and lower thresholds asymmetrically. They
are as follows:

\begin{align*}
P_{\text{MM, Gas enable}} \leq \SI{9.9e-3}{\milli \bar}
\end{align*}

and 

\begin{align*}
P_{\text{MM, Gas disable}} \leq \SI{2e-2}{\milli \bar}
\end{align*}

#+begin_src subfigure
(figure ()
        (subfigure (linewidth 0.33) (caption "Outside $V_{\\text{in}}$") (label "fig:cast:electrovalve-outside")
                   (includegraphics (list (cons 'width (linewidth 0.95)))
                                    "~/org/Doc/Detector/figs/electrovalve_outside.jpg"))
        (subfigure (linewidth 0.33) (caption "Below $V_{\\text{interlock}}$") (label "fig:cast:electrovalve-V-interlock")
                   (includegraphics (list (cons 'width (linewidth 0.95)))
                                    "~/org/Doc/Detector/figs/electrovalve_V_interlock.jpg"))
        (subfigure (linewidth 0.33) (caption "Output $V_{\\text{out}}$") (label "fig:cast:electrovalve-V-out")
                   (includegraphics (list (cons 'width (linewidth 0.95)))
                                    "~/org/Doc/Detector/figs/electrovalve_V_out.jpg"))
        (caption
         (subref "fig:cast:electrovalve-outside")
          ": Location of the electrovalve $V_{\\text{in}}$ located outside the building."
         (subref "fig:cast:electrovalve-outside")
          ": Location of the electrovalve $V_{\\text{interlock}}$ next to the 
           watercooling system below the shielding table."
         (subref "fig:cast:electrovalve-outside")
          ": Location of the electrovalve $V_{\\text{out}}$ connected to the beam
           supporting the optical table, on which the telescope is mounted.")
        (label "fig:cast:interlock_electrovalves"))
#+end_src


**** TODOs for this section [/]                                 :noexport:

#+BEGIN_COMMENT
*CHECK THESE VALUES, they are still wrong I believe*.
#+END_COMMENT

- [X] *MERGE IMAGES INTO ONE SUBFIG*

**** Note on pressures of interlock                             :extended:

It's possible the exact numbers are slightly wrong. At this point I'm
not certain anymore, but I had a note to cross check them in the
document the above was initially in (a document about the detector &
operation for CERN).

*** CAST magnet interlock
:PROPERTIES:
:CUSTOM_ID: sec:cast:cast_magnet_interlock
:END:

The main CAST magnet interlock, as it is relevant to our detector, is
as follows. The gate valve VT3 separating the magnet volume from the
telescope volume is interlocked. Only if the vacuum in the telescope
volume is good enough, VT3 can be opened while the interlock is
activated. 

For this the pressure of $P_{\text{3}}$ is considered relevant
(cmp. fig. [[fig:cast:vacuum-schematic]]). The upper and lower thresholds which
activate and deactivate the interlock are asymmetric and as follows:

\begin{align*}
P_{\text{3, VT3 enable}} &\leq \SI{1e-5}{\milli\bar}
\end{align*}

while

\begin{align*}
P_{\text{3, VT3 disable}} &\leq \SI{8e-5}{\milli\bar}.
\end{align*}

This is to make sure there can be no rapid toggling between the two
states during pumping or flushing the system. 

** X-ray finger runs [/]                                          :noexport:

- [X] should this go to the alignment part? I suppose we could mention
  this as part of the alignment with a heatmap and the center location
  _before_ (?) geometer alignment and movement? For context
  We can then later refer to this again in the context of determining
  the systematic uncertainty on the position.
  -> Yes, this is described enough in the alignment part. Note though
  that we will also need to reference it again for the systematics
  later, but that's fine.

- [ ] *WHEN WE* rewrite the actual CAST section for the final thesis
  (not the appendix part) we can rethink whether to have an explicit
  X-ray finger subsection with a few words about the uncertainty.

2 X-ray finger runs were made. One near beginning, one near end.

Plot of X-ray finger centers.

Mention how this plays into the analysis side, that it means we need
to adjust the ray tracing.

- [X] *ONE RUN NOW PART OF ALIGNMENT, OTHER IS MISSING ON MY LAPTOP*
  -> Both found in [[file:~/CastData/data/XrayFingerRuns/]]
  
** CAST log files [/]

The CAST experiment is controlled by a central computer running a slow
control system written in LabVIEW [fn:labview]. It receives data from
all sensors installed at CAST (vacuum pressures, temperatures, magnet
current, magnet position and so on). A separate LabVIEW program is
responsible for controlling the magnet position, either by moving the
magnet to a specific coordinate or by following the Sun, if it is in
reach of the magnet.

The slow control software records all sensor data and presents it in a
multitude of graphs. All data is also written to log files. There are
two different kinds of log files of interest in the context of the
septemboard detector at CAST.

1. The general slow control log file is a versioned space separated
   values file (similar to a comma separated value (CSV) or tab
   separated value (TSV) file, except using spaces as delimiters). The
   filename contains the version number, which decides the columns
   contained in the file and their order. A ~Version.idx~ file maps
   version numbers to file columns for easy access. These slow control
   log files normally contain one entry every minute. For the
   Septemboard detector of interest in this file are mainly the magnet
   current, relevant vacuum pressure sensors and the state of the gate
   valve ~VT3~.
2. The second kind of log files are the tracking log files. These are
   also space separated value files with fields describing the
   pointing location of the magnet as well as whether the magnet is
   currently tracking the Sun.

Both of these log files need to be read by the analysis software to
decide whether one or multiple trackings took place in a given
background run and if so extract the exact start and end time for a
precise data splitting and calculation of background / solar tracking
time.

Generally the latter log files are all that is needed to determine the
existence of a solar tracking. However, the slow control log file can
be used as a further sanity check to make sure the gate valve was
actually open and the magnet under current during the solar tracking.

This is implemented in the ~cast_log_reader~ program part of
~TimepixAanlysis~. See sec. [[#sec:appendix:software:cast_log_reader]] for
more details on the program. The program first parses the log files and
determines valid solar trackings. Then, given an HDF5 data file
containing the background data each solar tracking is mapped to a
background run. One background run may have zero or more solar
trackings attached to it. In the final step the solar tracking start
and stop information is added to each run as additional meta
data. During the later stages of processing this meta data is then
used to only consider background (non tracking) or solar tracking data
(starting from the computation of background rates).

[fn:labview] https://www.ni.com/labview

*** TODOs for this section [/]                                   :noexport:

- [ ] in the above we simply state the number of trackings etc. Of
  course realistically that is simply not enough. We need the slow
  control and tracking files to compute when a tracking starts and
  stops.
  I think this implies it needs to go before the summary!
- [X] INSERT FOOTNOTE LINK TO LABVIEW  

- [ ] *LINK TO CODE!*

*** Add tracking log information to background files             :extended:

The code dealing with the log files is [[file:~/CastData/ExternCode/TimepixAnalysis/LogReader/cast_log_reader.nim]]

In case previous log information is available we first have to remove
that information. This is done by just calling the program just with
the H5 file as an input:
#+begin_src sh
cast_log_reader h5file -f ~/CastData/data/DataRuns2017_Reco.h5
cast_log_reader h5file -f ~/CastData/data/DataRuns2018_Reco.h5
#+end_src

With an 'empty' (in terms of tracking information) HDF5 file we can
then go ahead and add all tracking information to them. This of course
requires the tracking log files:

#+begin_src sh
./cast_log_reader tracking -p ../resources/LogFiles/tracking-logs --h5out ~/CastData/data/DataRuns2017_Reco.h5
./cast_log_reader tracking -p ../resources/LogFiles/tracking-logs --h5out ~/CastData/data/DataRuns2018_Reco.h5
#+end_src

One can also run it without any output H5 file ~--h5out~ to simply get
information about the runs. In particular all runs

#+begin_src sh
./cast_log_reader tracking -p ../resources/LogFiles/tracking-logs --startTime 2017/09/01 --endTime 2018/12/30
#+end_src
:RESULTS:
FILTERING TO DATE: 2017-09-01T02:00:00+02:00 and 2018-12-30T01:00:00+01:00
<2017-10-31 Tue 6:37>    <2017-10-31 Tue 8:10>
<2017-11-02 Thu 6:39>    <2017-11-02 Thu 8:14>
<2017-11-03 Fri 6:41>    <2017-11-03 Fri 8:15>
<2017-11-04 Sat 6:42>    <2017-11-04 Sat 8:17>
<2017-11-05 Sun 6:43>    <2017-11-05 Sun 8:18>
<2017-11-07 Tue 6:47>    <2017-11-07 Tue 8:23>
<2017-11-06 Mon 6:45>    <2017-11-06 Mon 8:20>
<2017-11-08 Wed 6:48>    <2017-11-08 Wed 8:24>
<2017-11-09 Thu 6:49>    <2017-11-09 Thu 8:26>
<2017-11-10 Fri 6:51>    <2017-11-10 Fri 8:28>
<2017-11-11 Sat 6:52>    <2017-11-11 Sat 8:30>
<2017-11-12 Sun 6:54>    <2017-11-12 Sun 8:31>
<2017-11-14 Tue 6:56>    <2017-11-14 Tue 8:34>
<2017-11-13 Mon 6:55>    <2017-11-13 Mon 8:33>
<2017-11-15 Wed 6:57>    <2017-11-15 Wed 8:36>
<2017-11-17 Fri 7:00>    <2017-11-17 Fri 8:39>
<2017-11-18 Sat 7:01>    <2017-11-18 Sat 8:41>
<2017-11-19 Sun 7:02>    <2017-11-19 Sun 8:43>
<2017-11-25 Sat 7:10>    <2017-11-25 Sat 8:53>
<2017-11-26 Sun 7:11>    <2017-11-26 Sun 8:54>
<2017-11-27 Mon 7:12>    <2017-11-27 Mon 8:56>
<2017-11-28 Tue 7:14>    <2017-11-28 Tue 8:57>
<2017-11-29 Wed 7:15>    <2017-11-29 Wed 8:59>
<2017-11-30 Thu 7:16>    <2017-11-30 Thu 9:00>
<2017-12-01 Fri 7:17>    <2017-12-01 Fri 9:01>
<2017-12-02 Sat 7:18>    <2017-12-02 Sat 8:59>
<2017-12-03 Sun 7:19>    <2017-12-03 Sun 8:58>
<2017-12-04 Mon 7:20>    <2017-12-04 Mon 9:01>
<2017-12-05 Tue 7:21>    <2017-12-05 Tue 8:59>
<2017-12-07 Thu 7:22>    <2017-12-07 Thu 9:02>
<2017-12-09 Sat 7:25>    <2017-12-09 Sat 9:05>
<2017-12-10 Sun 7:26>    <2017-12-10 Sun 9:01>
<2017-12-11 Mon 7:27>    <2017-12-11 Mon 9:00>
<2017-12-12 Tue 7:28>    <2017-12-12 Tue 9:02>
<2017-12-13 Wed 7:28>    <2017-12-13 Wed 9:00>
<2017-12-14 Thu 7:29>    <2017-12-14 Thu 8:59>
<2017-12-15 Fri 7:29>    <2017-12-15 Fri 9:00>
<2017-12-16 Sat 7:30>    <2017-12-16 Sat 9:01>
<2017-12-17 Sun 7:31>    <2017-12-17 Sun 9:01>
<2017-12-18 Mon 7:31>    <2017-12-18 Mon 9:01>
<2017-12-19 Tue 7:32>    <2017-12-19 Tue 9:02>
<2017-12-20 Wed 7:33>    <2017-12-20 Wed 9:02>
<2018-02-15 Thu 7:01>    <2018-02-15 Thu 8:33>
<2018-02-16 Fri 7:00>    <2018-02-16 Fri 8:31>
<2018-02-18 Sun 6:57>    <2018-02-18 Sun 8:28>
<2018-02-19 Mon 6:56>    <2018-02-19 Mon 8:26>
<2018-02-20 Tue 6:53>    <2018-02-20 Tue 8:24>
<2018-02-21 Wed 6:52>    <2018-02-21 Wed 8:22>
<2018-02-22 Thu 6:50>    <2018-02-22 Thu 8:20>
<2018-02-23 Fri 6:48>    <2018-02-23 Fri 8:18>
<2018-02-24 Sat 6:47>    <2018-02-24 Sat 8:16>
<2018-02-28 Wed 6:40>    <2018-02-28 Wed 8:09>
<2018-03-02 Fri 6:36>    <2018-03-02 Fri 8:05>
<2018-03-03 Sat 6:35>    <2018-03-03 Sat 8:03>
<2018-03-04 Sun 6:33>    <2018-03-04 Sun 8:01>
<2018-03-05 Mon 6:31>    <2018-03-05 Mon 7:59>
<2018-03-06 Tue 6:29>    <2018-03-06 Tue 7:57>
<2018-03-07 Wed 6:27>    <2018-03-07 Wed 7:55>
<2018-03-14 Wed 6:14>    <2018-03-14 Wed 7:41>
<2018-03-15 Thu 6:12>    <2018-03-15 Thu 7:39>
<2018-03-16 Fri 6:10>    <2018-03-16 Fri 7:37>
<2018-03-17 Sat 6:08>    <2018-03-17 Sat 7:35>
<2018-03-18 Sun 6:07>    <2018-03-18 Sun 7:33>
<2018-03-19 Mon 6:05>    <2018-03-19 Mon 7:31>
<2018-03-20 Tue 6:03>    <2018-03-20 Tue 7:29>
<2018-03-21 Wed 6:01>    <2018-03-21 Wed 7:27>
<2018-03-22 Thu 5:59>    <2018-03-22 Thu 7:25>
<2018-03-24 Sat 5:55>    <2018-03-24 Sat 7:21>
<2018-03-25 Sun 6:53>    <2018-03-25 Sun 8:20>
<2018-03-26 Mon 5:51>    <2018-03-26 Mon 7:18>
<2018-10-19 Fri 6:21>    <2018-10-19 Fri 7:51>
<2018-10-22 Mon 6:24>    <2018-10-22 Mon 7:55>
<2018-10-23 Tue 6:26>    <2018-10-23 Tue 7:57>
<2018-10-24 Wed 6:27>    <2018-10-24 Wed 7:58>
<2018-10-25 Thu 6:28>    <2018-10-25 Thu 8:00>
<2018-10-26 Fri 6:30>    <2018-10-26 Fri 8:02>
<2018-10-27 Sat 6:31>    <2018-10-27 Sat 8:03>
<2018-10-28 Sun 5:32>    <2018-10-28 Sun 7:05>
<2018-10-29 Mon 6:34>    <2018-10-29 Mon 8:06>
<2018-10-30 Tue 6:35>    <2018-10-30 Tue 8:08>
<2018-11-01 Thu 6:38>    <2018-11-01 Thu 8:11>
<2018-11-02 Fri 6:39>    <2018-11-02 Fri 8:13>
<2018-11-03 Sat 6:40>    <2018-11-03 Sat 8:16>
<2018-11-04 Sun 6:43>    <2018-11-04 Sun 8:17>
<2018-11-05 Mon 6:44>    <2018-11-05 Mon 8:19>
<2018-11-06 Tue 6:45>    <2018-11-06 Tue 8:21>
<2018-11-09 Fri 6:49>    <2018-11-09 Fri 8:26>
<2018-11-10 Sat 6:51>    <2018-11-10 Sat 8:27>
<2018-11-11 Sun 6:52>    <2018-11-11 Sun 8:29>
<2018-11-12 Mon 6:53>    <2018-11-12 Mon 8:31>
<2018-11-13 Tue 6:54>    <2018-11-13 Tue 8:32>
<2018-11-14 Wed 6:56>    <2018-11-14 Wed 8:34>
<2018-11-15 Thu 6:57>    <2018-11-15 Thu 8:36>
<2018-11-16 Fri 6:58>    <2018-11-16 Fri 8:37>
<2018-11-17 Sat 7:00>    <2018-11-17 Sat 8:39>
<2018-11-18 Sun 7:01>    <2018-11-18 Sun 8:41>
<2018-11-19 Mon 7:02>    <2018-11-19 Mon 8:42>
<2018-11-24 Sat 7:08>    <2018-11-24 Sat 7:30>
<2018-11-25 Sun 7:09>    <2018-11-25 Sun 8:52>
<2018-11-26 Mon 7:10>    <2018-11-26 Mon 8:53>
<2018-11-27 Tue 7:11>    <2018-11-27 Tue 8:55>
<2018-11-29 Thu 7:14>    <2018-11-29 Thu 8:58>
<2018-11-30 Fri 7:15>    <2018-11-30 Fri 8:59>
<2018-12-01 Sat 7:16>    <2018-12-01 Sat 9:00>
<2018-12-02 Sun 7:17>    <2018-12-02 Sun 9:02>
<2018-12-03 Mon 7:18>    <2018-12-03 Mon 9:03>
<2018-12-05 Wed 8:55>    <2018-12-05 Wed 9:04>
<2018-12-06 Thu 7:22>    <2018-12-06 Thu 9:04>
<2018-12-07 Fri 7:23>    <2018-12-07 Fri 9:03>
<2018-12-08 Sat 7:24>    <2018-12-08 Sat 9:04>
<2018-12-10 Mon 7:25>    <2018-12-10 Mon 9:03>
<2018-12-11 Tue 7:26>    <2018-12-11 Tue 9:02>
<2018-12-12 Wed 7:27>    <2018-12-12 Wed 9:03>
<2018-12-13 Thu 7:28>    <2018-12-13 Thu 9:03>
<2018-12-14 Fri 7:29>    <2018-12-14 Fri 9:06>
<2018-12-16 Sun 7:30>    <2018-12-16 Sun 9:00>
<2018-12-15 Sat 7:30>    <2018-12-15 Sat 9:01>
<2018-12-17 Mon 7:31>    <2018-12-17 Mon 9:01>
<2018-12-18 Tue 7:32>    <2018-12-18 Tue 9:01>
<2018-12-20 Thu 7:33>    <2018-12-20 Thu 9:03>
There are 120 solar trackings found in the log file directory
The total time of all trackings: 186 h (exact: 1 week, 18 hours, 46 minutes, and 19 seconds)
Total time the magnet was on (> 1 T): 0 h h
:END:

This lists 5 runs that were *not* part of our data taking, meaning we
lost them.


And finally (and most useful for information purposes) we can run the
log reader over the files and do a fake insert (aka dry run) of
inserting the files, which only outputs the run information including
which runs were missed:
#+begin_src sh
./cast_log_reader tracking -p ../resources/LogFiles/tracking-logs \
                  --startTime 2017/09/01 --endTime 2018/05/01 \
                  --h5out ~/CastData/data/DataRuns2017_Reco.h5 --dryRun
#+end_src
:RESULTS:
There are 70 solar trackings found in the log file directory
The total time of all trackings: 109 h (exact: 4 days, 13 hours, 3 minutes, and 22 seconds)
Total time the magnet was on (> 1 T): 0 h h
Filtering tracking logs to date: 2017-09-01T02:00:00+02:00 and 2018-05-01T02:00:00+02:00
========== Logs mapped to trackings in the output file: ==========
<2017-10-31 Tue 6:37>    <2017-10-31 Tue 8:10>  for run: 76
<2017-11-02 Thu 6:39>    <2017-11-02 Thu 8:14>  for run: 77
<2017-11-03 Fri 6:41>    <2017-11-03 Fri 8:15>  for run: 78
<2017-11-04 Sat 6:42>    <2017-11-04 Sat 8:17>  for run: 79
<2017-11-05 Sun 6:43>    <2017-11-05 Sun 8:18>  for run: 80
<2017-11-06 Mon 6:45>    <2017-11-06 Mon 8:20>  for run: 81
<2017-11-07 Tue 6:47>    <2017-11-07 Tue 8:23>  for run: 82
<2017-11-08 Wed 6:48>    <2017-11-08 Wed 8:24>  for run: 82
<2017-11-09 Thu 6:49>    <2017-11-09 Thu 8:26>  for run: 84
<2017-11-10 Fri 6:51>    <2017-11-10 Fri 8:28>  for run: 86
<2017-11-11 Sat 6:52>    <2017-11-11 Sat 8:30>  for run: 87
<2017-11-12 Sun 6:54>    <2017-11-12 Sun 8:31>  for run: 87
<2017-11-13 Mon 6:55>    <2017-11-13 Mon 8:33>  for run: 89
<2017-11-14 Tue 6:56>    <2017-11-14 Tue 8:34>  for run: 90
<2017-11-15 Wed 6:57>    <2017-11-15 Wed 8:36>  for run: 91
<2017-11-17 Fri 7:00>    <2017-11-17 Fri 8:39>  for run: 92
<2017-11-18 Sat 7:01>    <2017-11-18 Sat 8:41>  for run: 94
<2017-11-19 Sun 7:02>    <2017-11-19 Sun 8:43>  for run: 95
<2017-11-25 Sat 7:10>    <2017-11-25 Sat 8:53>  for run: 97
<2017-11-26 Sun 7:11>    <2017-11-26 Sun 8:54>  for run: 98
<2017-11-27 Mon 7:12>    <2017-11-27 Mon 8:56>  for run: 99
<2017-11-28 Tue 7:14>    <2017-11-28 Tue 8:57>  for run: 100
<2017-11-29 Wed 7:15>    <2017-11-29 Wed 8:59>  for run: 101
<2017-11-30 Thu 7:16>    <2017-11-30 Thu 9:00>  for run: 103
<2017-12-01 Fri 7:17>    <2017-12-01 Fri 9:01>  for run: 104
<2017-12-03 Sun 7:19>    <2017-12-03 Sun 8:58>  for run: 106
<2017-12-02 Sat 7:18>    <2017-12-02 Sat 8:59>  for run: 105
<2017-12-04 Mon 7:20>    <2017-12-04 Mon 9:01>  for run: 107
<2017-12-05 Tue 7:21>    <2017-12-05 Tue 8:59>  for run: 109
<2017-12-07 Thu 7:22>    <2017-12-07 Thu 9:02>  for run: 112
<2017-12-09 Sat 7:25>    <2017-12-09 Sat 9:05>  for run: 112
<2017-12-11 Mon 7:27>    <2017-12-11 Mon 9:00>  for run: 114
<2017-12-10 Sun 7:26>    <2017-12-10 Sun 9:01>  for run: 113
<2017-12-12 Tue 7:28>    <2017-12-12 Tue 9:02>  for run: 115
<2017-12-13 Wed 7:28>    <2017-12-13 Wed 9:00>  for run: 117
<2017-12-14 Thu 7:29>    <2017-12-14 Thu 8:59>  for run: 119
<2017-12-15 Fri 7:29>    <2017-12-15 Fri 9:00>  for run: 121
<2017-12-16 Sat 7:30>    <2017-12-16 Sat 9:01>  for run: 123
<2017-12-17 Sun 7:31>    <2017-12-17 Sun 9:01>  for run: 124
<2017-12-18 Mon 7:31>    <2017-12-18 Mon 9:01>  for run: 124
<2017-12-19 Tue 7:32>    <2017-12-19 Tue 9:02>  for run: 125
<2017-12-20 Wed 7:33>    <2017-12-20 Wed 9:02>  for run: 127
<2018-02-18 Sun 6:57>    <2018-02-18 Sun 8:28>  for run: 146
<2018-02-20 Tue 6:53>    <2018-02-20 Tue 8:24>  for run: 150
<2018-02-19 Mon 6:56>    <2018-02-19 Mon 8:26>  for run: 148
<2018-02-21 Wed 6:52>    <2018-02-21 Wed 8:22>  for run: 152
<2018-02-22 Thu 6:50>    <2018-02-22 Thu 8:20>  for run: 154
<2018-02-23 Fri 6:48>    <2018-02-23 Fri 8:18>  for run: 156
<2018-02-24 Sat 6:47>    <2018-02-24 Sat 8:16>  for run: 158
<2018-02-28 Wed 6:40>    <2018-02-28 Wed 8:09>  for run: 160
<2018-03-02 Fri 6:36>    <2018-03-02 Fri 8:05>  for run: 162
<2018-03-03 Sat 6:35>    <2018-03-03 Sat 8:03>  for run: 162
<2018-03-04 Sun 6:33>    <2018-03-04 Sun 8:01>  for run: 162
<2018-03-05 Mon 6:31>    <2018-03-05 Mon 7:59>  for run: 164
<2018-03-06 Tue 6:29>    <2018-03-06 Tue 7:57>  for run: 164
<2018-03-07 Wed 6:27>    <2018-03-07 Wed 7:55>  for run: 166
<2018-03-14 Wed 6:14>    <2018-03-14 Wed 7:41>  for run: 170
<2018-03-15 Thu 6:12>    <2018-03-15 Thu 7:39>  for run: 172
<2018-03-16 Fri 6:10>    <2018-03-16 Fri 7:37>  for run: 174
<2018-03-17 Sat 6:08>    <2018-03-17 Sat 7:35>  for run: 176
<2018-03-18 Sun 6:07>    <2018-03-18 Sun 7:33>  for run: 178
<2018-03-19 Mon 6:05>    <2018-03-19 Mon 7:31>  for run: 178
<2018-03-20 Tue 6:03>    <2018-03-20 Tue 7:29>  for run: 178
<2018-03-21 Wed 6:01>    <2018-03-21 Wed 7:27>  for run: 178
<2018-03-22 Thu 5:59>    <2018-03-22 Thu 7:25>  for run: 178
<2018-03-24 Sat 5:55>    <2018-03-24 Sat 7:21>  for run: 180
<2018-03-25 Sun 6:53>    <2018-03-25 Sun 8:20>  for run: 182
<2018-03-26 Mon 5:51>    <2018-03-26 Mon 7:18>  for run: 182
==================================================================

========== Logs *not* mapped to a run ============================
<2018-02-15 Thu 7:01>    <2018-02-15 Thu 8:33>
<2018-02-16 Fri 7:00>    <2018-02-16 Fri 8:31>
==================================================================
:END:

And for 2018:
#+begin_src sh
./cast_log_reader tracking -p ../resources/LogFiles/tracking-logs \
                  --startTime 2018/05/01 --endTime 2018/12/31 \
                  --h5out ~/CastData/data/DataRuns2018_Reco.h5 --dryRun
#+end_src
:RESULTS:
There are 50 solar trackings found in the log file directory
The total time of all trackings: 77 h (exact: 3 days, 5 hours, 42 minutes, and 57 seconds)
Total time the magnet was on (> 1 T): 0 h h
Filtering tracking logs to date: 2018-05-01T02:00:00+02:00 and 2018-12-31T01:00:00+01:00
========== Logs mapped to trackings in the output file: ==========
<2018-10-22 Mon 6:24>    <2018-10-22 Mon 7:55>  for run: 240
<2018-10-23 Tue 6:26>    <2018-10-23 Tue 7:57>  for run: 242
<2018-10-24 Wed 6:27>    <2018-10-24 Wed 7:58>  for run: 244
<2018-10-25 Thu 6:28>    <2018-10-25 Thu 8:00>  for run: 246
<2018-10-26 Fri 6:30>    <2018-10-26 Fri 8:02>  for run: 248
<2018-10-27 Sat 6:31>    <2018-10-27 Sat 8:03>  for run: 250
<2018-10-29 Mon 6:34>    <2018-10-29 Mon 8:06>  for run: 254
<2018-10-30 Tue 6:35>    <2018-10-30 Tue 8:08>  for run: 256
<2018-11-01 Thu 6:38>    <2018-11-01 Thu 8:11>  for run: 258
<2018-11-02 Fri 6:39>    <2018-11-02 Fri 8:13>  for run: 261
<2018-11-03 Sat 6:40>    <2018-11-03 Sat 8:16>  for run: 261
<2018-11-04 Sun 6:43>    <2018-11-04 Sun 8:17>  for run: 261
<2018-11-05 Mon 6:44>    <2018-11-05 Mon 8:19>  for run: 263
<2018-11-06 Tue 6:45>    <2018-11-06 Tue 8:21>  for run: 265
<2018-11-09 Fri 6:49>    <2018-11-09 Fri 8:26>  for run: 268
<2018-11-10 Sat 6:51>    <2018-11-10 Sat 8:27>  for run: 270
<2018-11-11 Sun 6:52>    <2018-11-11 Sun 8:29>  for run: 270
<2018-11-12 Mon 6:53>    <2018-11-12 Mon 8:31>  for run: 272
<2018-11-13 Tue 6:54>    <2018-11-13 Tue 8:32>  for run: 272
<2018-11-14 Wed 6:56>    <2018-11-14 Wed 8:34>  for run: 272
<2018-11-15 Thu 6:57>    <2018-11-15 Thu 8:36>  for run: 274
<2018-11-16 Fri 6:58>    <2018-11-16 Fri 8:37>  for run: 274
<2018-11-17 Sat 7:00>    <2018-11-17 Sat 8:39>  for run: 274
<2018-11-18 Sun 7:01>    <2018-11-18 Sun 8:41>  for run: 276
<2018-11-19 Mon 7:02>    <2018-11-19 Mon 8:42>  for run: 276
<2018-11-25 Sun 7:09>    <2018-11-25 Sun 8:52>  for run: 279
<2018-11-26 Mon 7:10>    <2018-11-26 Mon 8:53>  for run: 279
<2018-11-27 Tue 7:11>    <2018-11-27 Tue 8:55>  for run: 281
<2018-11-29 Thu 7:14>    <2018-11-29 Thu 8:58>  for run: 283
<2018-11-30 Fri 7:15>    <2018-11-30 Fri 8:59>  for run: 283
<2018-12-01 Sat 7:16>    <2018-12-01 Sat 9:00>  for run: 283
<2018-12-02 Sun 7:17>    <2018-12-02 Sun 9:02>  for run: 285
<2018-12-03 Mon 7:18>    <2018-12-03 Mon 9:03>  for run: 285
<2018-12-05 Wed 8:55>    <2018-12-05 Wed 9:04>  for run: 287
<2018-12-06 Thu 7:22>    <2018-12-06 Thu 9:04>  for run: 289
<2018-12-07 Fri 7:23>    <2018-12-07 Fri 9:03>  for run: 291
<2018-12-08 Sat 7:24>    <2018-12-08 Sat 9:04>  for run: 291
<2018-12-10 Mon 7:25>    <2018-12-10 Mon 9:03>  for run: 293
<2018-12-11 Tue 7:26>    <2018-12-11 Tue 9:02>  for run: 295
<2018-12-12 Wed 7:27>    <2018-12-12 Wed 9:03>  for run: 297
<2018-12-13 Thu 7:28>    <2018-12-13 Thu 9:03>  for run: 297
<2018-12-14 Fri 7:29>    <2018-12-14 Fri 9:06>  for run: 298
<2018-12-15 Sat 7:30>    <2018-12-15 Sat 9:01>  for run: 299
<2018-12-16 Sun 7:30>    <2018-12-16 Sun 9:00>  for run: 301
<2018-12-17 Mon 7:31>    <2018-12-17 Mon 9:01>  for run: 301
<2018-12-18 Tue 7:32>    <2018-12-18 Tue 9:01>  for run: 303
<2018-12-20 Thu 7:33>    <2018-12-20 Thu 9:03>  for run: 306
==================================================================

========== Logs *not* mapped to a run ============================
<2018-10-19 Fri 6:21>    <2018-10-19 Fri 7:51>
<2018-10-28 Sun 5:32>    <2018-10-28 Sun 7:05>
<2018-11-24 Sat 7:08>    <2018-11-24 Sat 7:30>
==================================================================
:END:

** Summary of CAST data taking [7/15]
:PROPERTIES:
:CUSTOM_ID: sec:cast:data_taking_campaigns
:END:

In summary then the data taken at CAST with the Septemboard detector
can be split into two periods. The first from October 2017 to April
2018 and the second from October 2018 to December 2018. The former
will from here on be called "Run-2" and the latter "Run-3". Run-1
refers to the data taking campaign with the single GridPix detector in
2014 and 2015. The distinction of run periods is mainly based on the
fact that the detector was dismounted between Run-2 and Run-3 and
additionally a full detector recalibration was performed, meaning the
datasets require slightly different parameters for calibration related aspects.

During Run-2 the scintillator vetoes were not working correctly. The
FADC was partially noisy. In Run-3 all detector features were working
as intended. The feature list is summarized in
tab. [[tab:cast:features_by_run]].

Run-2 ran with a Timepix shutter time of ~2/32~
(ref. sec. [[#sec:reco:event_duration]]) resulting in about $\SI{2.4}{s}$
long frames. This was changed with the start of 2018 (still in Run-2)
to ~2/30~ ($\sim\SI{2.2}{s}$).

In total 115 solar trackings were recorded between Run-2 and
Run-3. This amounts to about $\SI{180}{\hour}$ of tracking
data. Further, $\SI{3526}{\hour}$ of background data and
$\SI{194}{\hour}$ of \cefe calibration data were recorded. The total
active fraction of these times is about $\SI{90}{\%}$ in both run
periods. Two X-ray finger runs were done for alignment purposes (out
of which only 1 is directly useful).

The appendix [[#sec:appendix:cast_run_list]] lists the full run list with
additional information about each run.

Outside the issues mentioned in the previous section
[[#sec:cast:data_taking_woes]], the detector generally ran very
stable. Certain detector behaviors will be discussed later, which do
not affect data quality as they can be calibrated out. 

In total 5 (4) runs out of 120 (119) were missed in the course of the whole data
taking campaign (one of them was aborted after 30 min).

#+CAPTION: Overview of the total data taken with the Septemboard detector at CAST
#+CAPTION: in the time between October 2017 and December 2018. 'Active s' and
#+CAPTION: 'Active b.' refers to the total solar tracking and background time excluding
#+CAPTION: the dead time due to readout of the septemboard.
#+CAPTION: See the table below [[tab:cast:data_stats_overview]] for more details.
#+NAME: tab:cast:total_data_time
#+ATTR_LATEX: :align lrrrrr :booktabs t
|       | Solar tracking [h] | Active s. [h] | Background [h] | Active b. [h] | Active % |
|-------+--------------------+---------------+----------------+---------------+----------|
| Run-2 |             106.01 |         94.12 |        2401.43 |       2144.67 |     89.3 |
| Run-3 |              74.29 |         66.92 |        1124.93 |       1012.68 |     90.0 |
| Total |             180.30 |        161.05 |        3526.36 |       3157.35 |     89.5 |

#+CAPTION: Overview of working (\green{o}), mostly working (\orange{m}), not
#+CAPTION: working (\red{x}) features in each run. FADC was partially noisy
#+CAPTION: in Run-2.
#+NAME: tab:cast:features_by_run
#+ATTR_LATEX: :booktabs t
|-------------+------------+-----------|
| Feature     | Run 2      | Run 3     |
|-------------+------------+-----------|
| Septemboard | \green{o}  | \green{o} |
| FADC        | \orange{m} | \green{o} |
| Veto scinti | \red{x}    | \green{o} |
| SiPM        | \red{x}    | \green{o} |
|-------------+------------+-----------|

#+CAPTION: Overview of the data taken in each of the runs split by calibration data
#+CAPTION: ("calib") and background ("back"). First information about the total recorded
#+CAPTION: time and trackings and then event information regarding general activity and
#+CAPTION: activity split by chips, FADC and scintillators. Note that the scintillator
#+CAPTION: information for Run-2 is not useful, as the signals recorded were not actually
#+CAPTION: real signals.
#+NAME: tab:cast:data_stats_overview
#+ATTR_LATEX: :booktabs t
| Field                          | calib Run-2  | calib Run-3  | back Run-2    | back Run-3    |
|--------------------------------+--------------+--------------+---------------+---------------|
| total duration                 | 107.42 h      | 87.06 h       | 2507.43 h      | 1199.22 h      |
| active duration                | 2.6 h         | 3.53 h        | 2238.78 h      | 1079.6 h       |
| active fraction                | 2.422 %       | 4.049 %       | 89.29 %        | 90.02 %        |
| # trackings                    | \num{0}      | \num{0}      | \num{68}      | \num{47}      |
| tracking time                  | 0 h           | 0 h           | 106.01 h       | 74.3 h         |
| active tracking time           | 0 h           | 0 h           | 94.65 h        | 66.89 h        |
|--------------------------------+--------------+--------------+---------------+---------------|
| Events                         |              |              |               |               |
|--------------------------------+--------------+--------------+---------------+---------------|
| total # events                 | \num{532020} | \num{415927} | \num{3758960} | \num{1837330} |
| only center chip               | \num{480232} | \num{366917} | \num{23820}   | \num{9462}    |
| only any outer chip            | \num{7}      | \num{5}      | \num{1557934} | \num{741199}  |
| center + outer                 | \num{51368}  | \num{47825}  | \num{960499}  | \num{460726}  |
| center chip                    | \num{531600} | \num{414742} | \num{984319}  | \num{470188}  |
| any chip                       | \num{531607} | \num{414747} | \num{2542253} | \num{1211387} |
| fraction with center           | 99.92 %       | 99.72 %       | 26.19 %        | 25.59 %        |
| fraction with any              | 99.92 %       | 99.72 %       | 67.63 %        | 65.93 %        |
| with fadc readouts             | \num{531529} | \num{413853} | \num{542233}  | \num{211683}  |
| fraction with FADC             | 99.91 %       | 99.50 %       | 14.43 %        | 11.52 %        |
| with SiPM trigger <4095        | \num{1656}   | \num{20}     | \num{8585}    | \num{4304}    |
| with veto scinti trigger <4095 | \num{0}      | \num{2888}   | \num{0}       | \num{70016}   |
| with any SiPM trigger          | \num{531528} | \num{1312}   | \num{825460}  | \num{34969}   |
| with any veto scinti trigger   | \num{0}      | \num{216170} | \num{0}       | \num{206025}  |
| fraction with any SiPM         | 99.91 %       | 0.3154 %      | 21.96 %        | 1.903 %        |
| fraction with any veto scinti  | 0.000 %       | 51.97 %       | 0.000 %        | 11.21 %        |

*** TODOs for this section [/]                                   :noexport:

- [ ] *VERIFY EXACT SHUTTER TIMES*

- [ ] *UNIFORMLY DECIDE TO EITHER USE ~Run-2~ or ~Run 2~ STYLE*
- [ ] table for number of shifts in each run, column for number of
  trackings
- [ ] table of temperatures recovered from the shift logs!  
- [X] number of X-ray finger runs  
- [X] table of total time taking in each run
- [X] link to appendix containing total run list!  
- [X] table of working detector features in each run
- [ ] table of recorded temperatures (or as a plot? Table takes a lot
  of space! ~Tools/mapSeptemTempToFePeak.nim~)
  -> Done later when talking about detector activity & time behavior.
- [X] numbers of the total activity, number of events on the center &
  outside chips, # of FADC triggers, event durations etc.
- [ ] *THINK ABOUT WHETHER TO INCLUDE FADC NOISE HERE IN STATISTICS*
- [X] *MENTION SHUTTER TIMES USED, 2017 2/32, 2018 2/30?*
- [ ] plot of event durations already created!
  -> Not super interesting as a plot, imo.

- [ ] *FIX TABLE OF TIME! CODE IN NEXT NOEXPORT SECTION GIVES
  DIFFERENT NUMBERS THAN THE FIRST TABLE HERE! WHY?*
  -> Difference (among others) is different number of trackings
  known. In
  [[file:~/CastData/ExternCode/TimepixAnalysis/Tools/writeRunList/writeRunList.nim]]
  the generated run list that's commited to TPA is 115 trackings,
  whereas here (on voidRipper) only 109 trackings are known. Why?
  -> *Investigate*!

- [X] *FIX UP THE TABLE TO NOT USE FIELDS AS ROW NAMES, BUT RATHER
  TEXT*
- [X] *SEE NOEXPORT SECTION BELOW TO THINK ABOUT ADDING OTHER THINGS*  

*** Extended table about total time                              :extended:

This, tab. [[tab:cast:total_data_time_extended]], is an extended version
(wider + total times) of the table presented in the section above.

#+CAPTION: Overview of the total data taken with the Septemboard detector at CAST
#+CAPTION: in the time between October 2017 and December 2018. See the table below
#+CAPTION: [[tab:cast:data_stats_overview]] for more precise numbers including the
#+CAPTION: time the detector was active (shutter open).
#+NAME: tab:cast:total_data_time_extended
#+ATTR_LATEX: :align lrrrrrrr :booktabs t
|       | Solar tracking [h] | Background [h] | Active tracking [h] | Active background [h] | Total time [h] | Active time [h] |   Active % |
|-------+--------------------+----------------+---------------------+-----------------------+----------------+-----------------+------------|
| Run-2 |            106.006 |        2391.16 |             92.8017 |               2144.12 |        2497.16 |         2238.78 | 0.89653046 |
| Run-3 |            74.2981 |        1124.93 |             67.0066 |               1012.68 |        1199.23 |          1079.6 | 0.90024432 |
| Total |           180.3041 |        3516.09 |            159.8083 |                3156.8 |        3696.39 |         3318.38 | 0.89773536 |
#+TBLFM: $8=$7/$6

*** Code to compute statistics [10/17]                           :extended:

- [ ] code to compute the total run duration of the different parts
- [ ] code to produce the outer chip & central chip activity
- [ ] produce number of events with FADC trigger
- [ ] number of events with scintillator trigger + non trivial
  triggers (i.e. not maximum)


Tools we have for this and related:

- [[~/CastData/ExternCode/TimepixAnalysis/Tools/extractScintiRandomRate.nim]]
  -> works by single run
- [[~/CastData/ExternCode/TimepixAnalysis/Tools/outerChipActivity/outerChipActivity.nim]]
  -> works on full files
- [[~/CastData/ExternCode/TimepixAnalysis/Tools/countNonEmptyFrames/countNonEmptyFrames.nim]]
  -> works on full files
  -> only looks at center chip
- [[~/CastData/ExternCode/TimepixAnalysis/Tools/extractAlphasInBackground/extractAlphasInBackground.nim]]
  -> works on full files
  -> extracts all events with energy > 1 MeV
- [[~/CastData/ExternCode/TimepixAnalysis/Tools/extractScintiTriggers/extractScintiTriggers.nim]]
  -> (super old, still uses plotly)
  -> works on full files
  -> reads scinti trigger values and plots all != 0 & 4095 & and plots
  all < 300
- [[~/CastData/ExternCode/TimepixAnalysis/Tools/extractSparks/extractSparks.nim]]
  -> works on full files
  -> counts events with more than MinPix hits in a region. Written for
  Lucian iirc
- [[~/CastData/ExternCode/TimepixAnalysis/Tools/mapSeptemTempToFePeak.nim]]
  -> works on full files (all calibration)
  -> maps peak position by fit parameter to temperature from ~/resources~ directory
- [[~/CastData/ExternCode/TimepixAnalysis/Tools/writeRunList/writeRunList.nim]]
  -> works on full files
  -> uses the ~ExtendedRunInfo~ type
  Also outputs tracking & non tracking duration already.

- [ ] *CREATE PLOT OF DURATIONS, HIGHLIGHT 2017 USED 2/32 WHILE 2018 2/30*


Combined the above gives us more than we need. The one thing missing
(outside of details, like computing rates instead of numbers etc) is
_maybe_ FADC related, i.e. number of noisy events.
But we haven't even talked about noisy events, so I'm not sure if this
is the right place for that anyway.

So the information we want:
- [X] total duration (sum of all runs, background + calibration)
- [X] total active duration (event durations)
- [X] total # of trackings
- [X] total tracking time
- [X] total active tracking time 
- [ ] total # of events for each chip (non empty ones, & split between
     center & outside)
- [X] fraction of events with only center / only outer chips  
- [X] total # of FADC triggers
- [X] total # of scintillator triggers (run 3, by scintillator)
- [X] total # of non trivial scintillator triggers
- [ ] total # of non trivial center chip events w/o FADC trigger (some
  runs don't have FADC though) ?  
- ?

Better to just write a new piece of code that extracts exactly what we
need using ~ExtendedRunInfo~.
#+begin_src nim :tangle code/cast_run_information.nim
# 1. open file
# 2. get file info
# 3. for each run get extended run info
# ?
import std / [times, strformat, strutils]
import nimhdf5, unchained
import ingrid / tos_helpers

type
  CastInformation = object
    totalDuration: Second
    activeDuration: Second
    activeFraction: float # ratio of active / total    
    numTrackings: int
    trackingTime: Second
    activeTrackingTime: Second
    totalEvents: int # total number of recorded events
    onlyCenter: int  # events with activity only on center chip (> 3 hits)
    onlyOuter: int   # events with activity only on outer, but not center chip
    centerAndOuter: int # events with activity on center & any outer chip
    center: int      # events with activity on center (irrespective any other)
    anyActive: int # events with any active chip
    fractionWithCenter: float # fraction of events that have center chip activity
    fractionWithAny: float # fraction of events that have any activity
    # ... add mean of event durations?
    fadcReadouts: int
    fractionFadc: float # fraction of events having FADC readout
    scinti1NonTrivial: int # number of non trivial scinti triggers 0 < x < 4095
    scinti2NonTrivial: int # number of non trivial scinti triggers 0 < x < 4095
    scinti1Triggers: int # number of any scinti triggers != 0
    scinti2Triggers: int # number of any scinti triggers != 0
    fractionScinti1: float # fraction of events with any scinti 1 activity    
    fractionScinti2: float # fraction of events with any scinti 2 activity

proc fieldToStr(s: string): string =
  case s
  of "totalDuration":      result = "total duration"
  of "activeDuration":     result = "active duration"
  of "activeFraction":     result = "active fraction"
  of "numTrackings":       result = "# trackings"
  of "trackingTime":       result = "tracking time"
  of "activeTrackingTime": result = "active tracking time"
  of "totalEvents":        result = "total # events"
  of "center":             result = "center chip"
  of "onlyCenter":         result = "only center chip"
  of "onlyOuter":          result = "only any outer chip"
  of "centerAndOuter":     result = "center + outer"
  of "anyActive":          result = "any chip"
  of "fractionWithCenter": result = "fraction with center"
  of "fractionWithAny":    result = "fraction with any"
  of "fadcReadouts":       result = "with fadc readouts"
  of "fractionFadc":       result = "fraction with FADC"
  of "scinti1NonTrivial":  result = "with SiPM trigger <4095"
  of "scinti2NonTrivial":  result = "with veto scinti trigger <4095"
  of "scinti1Triggers":    result = "with any SiPM trigger"
  of "scinti2Triggers":    result = "with any veto scinti trigger"
  of "fractionScinti1":    result = "fraction with any SiPM"
  of "fractionScinti2":    result = "fraction with any veto scinti"  

proc `$`(castInfo: CastInformation): string =
  result.add &"Total duration: {pretty(castInfo.totalDuration.to(Hour), 4, true)}\n"
  result.add &"Active duration: {pretty(castInfo.activeDuration.to(Hour), 4, true)}\n"
  result.add &"Active fraction: {castInfo.activeFraction}\n"
  result.add &"Number of trackings: {castInfo.numTrackings}\n"
  result.add &"Tracking time: {pretty(castInfo.trackingTime.to(Hour), 4, true)}\n"
  result.add &"Active tracking time: {pretty(castInfo.activeTrackingTime.to(Hour), 4, true)}\n"
  result.add &"Number of total events:   {castInfo.totalEvents}\n"
  result.add &"Number of events without center: {castInfo.onlyOuter}\n"
  result.add &"\t| {(castInfo.onlyOuter.float / castInfo.totalEvents.float) * 100.0} %\n"
  result.add &"Number of events only center: {castInfo.onlyCenter}\n"
  result.add &"\t| {(castInfo.onlyCenter.float / castInfo.totalEvents.float) * 100.0} %\n"
  result.add &"Number of events with center activity and outer: {castInfo.centerAndOuter}\n"
  result.add &"\t| {(castInfo.centerAndOuter.float / castInfo.totalEvents.float) * 100.0} %\n"
  result.add &"Number of events any hit events: {castInfo.anyActive}\n"
  result.add &"\t| {(castInfo.anyActive.float / castInfo.totalEvents.float) * 100.0} %\n"

proc countEvents(df: DataFrame): int =
  for (tup, subdf) in groups(df.group_by("runNumber")):
    inc result, subDf["eventNumber", int].max

proc contains[T](t: Tensor[T], x: T): bool =
  for i in 0 ..< t.size:
    if x == t[i]:
      return true

proc countChipActivity(castInfo: var CastInformation, df: DataFrame) =
  for (tup, subDf) in groups(df.group_by(["eventNumber", "runNumber"])):
    let chips = subDf["chip"].unique.toTensor(int)
    if 3 in chips:
      inc castInfo.center
    # start new if 
    if 3 notin chips: 
      inc castInfo.onlyOuter
    elif [3].toTensor == chips:
      inc castInfo.onlyCenter
    elif 3 in chips and chips.len > 1:
      inc castInfo.centerAndOuter
    inc castInfo.anyActive

proc processFile(fname: string): CastInformation = # extend to both calib & both background
  let h5f = H5open(fname, "r")
  let fileInfo = getFileInfo(h5f)
  var castInfo: CastInformation
  for run in fileInfo.runs:
    let runInfo = getExtendedRunInfo(h5f, run, fileInfo.runType)
    castInfo.totalDuration  += runInfo.timeInfo.t_length.inSeconds().Second
    castInfo.activeDuration += runInfo.activeTime.inSeconds.Second
    castInfo.numTrackings   += runInfo.trackings.len
    for track in runInfo.trackings:
      castInfo.trackingTime += track.t_length.inSeconds.Second

    # read the data of all chips & FADC
    const names = ["eventNumber", "fadcReadout", "szint1ClockInt", "szint2ClockInt"]
    let dfNoChips = h5f.readRunDsets(run, commonDsets = names)
    let dfChips = h5f.readRunDsetsAllChips(run, fileInfo.chips,
                                           dsets = @[]) # don't need additional dsets
    castInfo.totalEvents       += dfNoChips.countEvents()
    castInfo.countChipActivity(dfChips)
    castInfo.fadcReadouts      += dfNoChips.filter(f{`fadcReadout` == 1}).len
    castInfo.scinti1Triggers   += dfNoChips.filter(f{`szint1ClockInt` != 0}).len
    castInfo.scinti2Triggers   += dfNoChips.filter(f{`szint2ClockInt` != 0}).len
    castInfo.scinti1NonTrivial += dfNoChips.filter(f{`szint1ClockInt` != 0 and `szint1ClockInt` < 4095}).len
    castInfo.scinti2NonTrivial += dfNoChips.filter(f{`szint2ClockInt` != 0 and `szint2ClockInt` < 4095}).len
      

# compute at the end as we need total information about fraction of total / active
  template fraction(arg, by: untyped): untyped = (castInfo.arg / castInfo.by) * 100.0
  castInfo.activeFraction = fraction(activeDuration, totalDuration)
  castInfo.activeTrackingTime = (castInfo.trackingTime * castInfo.activeFraction / 100.0)
  # fractions
  castInfo.fractionWithCenter = fraction(center         , totalEvents)
  castInfo.fractionWithAny    = fraction(anyActive      , totalEvents)
  castInfo.fractionFadc       = fraction(fadcReadouts   , totalEvents)
  castInfo.fractionScinti1    = fraction(scinti1Triggers, totalEvents)
  castInfo.fractionScinti2    = fraction(scinti2Triggers, totalEvents)
  echo castInfo
  result = castInfo

proc toTable(castInfos: Table[(string,string), CastInformation]): string =
  ## Turns the input into an Org table
  # | Field | Back Run-2 | Back Run-3 | Calib Run-2 | Calib Run-3 |
  # |-
  # ...
  # turn the input into a DF, then `toOrgTable` it
  proc toColName(tup: (string, string)): string =
    result = tup[1] & " "
    if "2017" in tup[0]:
      result.add "Run-2"
    else:
      result.add "Run-3"
      
  var df = newDataFrame()
  for k, v in pairs(castInfos):
    var fields = newSeq[string]()
    var vals = newSeq[string]()
    for field, val in fieldPairs(v):
      fields.add field.fieldToStr()
      when typeof(val) is Second:
        vals.add pretty(val.to(Hour), precision = 2, short = true,
                        format = ffDecimal)
      elif typeof(val) is float:
        vals.add $(val.formatFloat(precision = 4)) & " %"
      else:
        vals.add "\\num{" & $val & "}"
    let colName = k.toColName()
    let dfLoc = toDf({"Field" : fields, colName : vals})
    if df.len == 0:
      df = dfLoc
    else:
      df[colName] = dfLoc[colName]

  df = df.select(["Field", "calib Run-2", "calib Run-3", "back Run-2", "back Run-3"])
  echo df.toOrgTable(emphStrNumber = false)

proc main(background: seq[string], calibration: seq[string]) =

  var tab = initTable[(string, string), CastInformation]()
  for b in background:
    echo "--------------- Processing: ", b, " ---------------"
    tab[(b, "back")] = processFile(b)
    
  for c in calibration:
    echo "--------------- Processing: ", c, " ---------------"    
    tab[(c, "calib")] = processFile(c)
  echo tab
  echo tab.toTable()

when isMainModule:
  import cligen
  dispatch main
#+end_src

| Field                          | calib Run-2  | calib Run-3  | back Run-2    | back Run-3    |
|--------------------------------+--------------+--------------+---------------+---------------|
| total duration                 | 107.42 h     | 87.06 h      | 2507.43 h     | 1199.22 h     |
| active duration                | 2.6 h        | 3.53 h       | 2238.78 h     | 1079.6 h      |
| active fraction                | 2.422 %      | 4.049 %      | 89.29 %       | 90.02 %       |
| # trackings                    | \num{0}      | \num{0}      | \num{68}      | \num{47}      |
| tracking time                  | 0 h          | 0 h          | 106.01 h      | 74.3 h        |
| active tracking time           | 0 h          | 0 h          | 94.65 h       | 66.89 h       |
| total # events                 | \num{532020} | \num{415927} | \num{3758960} | \num{1837330} |
| only center chip               | \num{480232} | \num{366917} | \num{23820}   | \num{9462}    |
| only any outer chip            | \num{7}      | \num{5}      | \num{1557934} | \num{741199}  |
| center + outer                 | \num{51368}  | \num{47825}  | \num{960499}  | \num{460726}  |
| center chip                    | \num{531600} | \num{414742} | \num{984319}  | \num{470188}  |
| any chip                       | \num{531607} | \num{414747} | \num{2542253} | \num{1211387} |
| fraction with center           | 99.92 %      | 99.72 %      | 26.19 %       | 25.59 %       |
| fraction with any              | 99.92 %      | 99.72 %      | 67.63 %       | 65.93 %       |
| with fadc readouts             | \num{531529} | \num{413853} | \num{542233}  | \num{211683}  |
| fraction with FADC             | 99.91 %      | 99.50 %      | 14.43 %       | 11.52 %       |
| with SiPM trigger <4095        | \num{1656}   | \num{20}     | \num{8585}    | \num{4304}    |
| with veto scinti trigger <4095 | \num{0}      | \num{2888}   | \num{0}       | \num{70016}   |
| with any SiPM trigger          | \num{531528} | \num{1312}   | \num{825460}  | \num{34969}   |
| with any veto scinti trigger   | \num{0}      | \num{216170} | \num{0}       | \num{206025}  |
| fraction with any SiPM         | 99.91 %      | 0.3154 %     | 21.96 %       | 1.903 %       |
| fraction with any veto scinti  | 0.000 %      | 51.97 %      | 0.000 %       | 11.21 %       |

- [X] generate that table... then onto calibration finally!

* Data calibration                                                    :Part3:
:PROPERTIES:
:CUSTOM_ID: sec:calibration
:END:
#+LATEX: \minitoc

With the roughly $\SI{3500}{h}$ of data recorded at CAST it is time to
discuss the final calibrations [fn:calibration_term] necessary for the calculation of a
physics result. On the side of the septemboard detector this means the
'energy calibration'; the calculation of the energy of each event
recorded with the septemboard detector. This necessarily needs to
include a discussion of detector variability both due to external
factors as well as differing detector calibrations and
setups. Similarly, for the FADC the impact of the noise seen during
data taking and resulting differing amplifier settings will be discussed.


[fn:calibration_term] Note that the term 'calibration' is a heavily
loaded term implying very different things depending on context. This can at
times be confusing. I try to be explicit by fully specifying what
calibration is meant when it might be ambiguous. 

** Thoughts and TODOs [/]                                         :noexport:

This was our initial introduction for this chapter, but it seems outdated.
#+begin_quote
There are two different kinds of calibrations used for the data taking
campaign at CAST. One is a data taking campaign behind an X-ray tube
at the CAST Detector Lab (CDL) to characterize the geometric
properties of X-rays at different energies (as the foundation for
discrimination methods), discussed in section
[[#sec:cdl]]. The second are measurements using \cefe
source installed on a pneumatic manipulator at CAST to perform regular
calibration runs to monitor the detector behavior and serve as a basis
for the energy calibration of events, see section
[[#sec:preparation:55fe]].

*NOTE*: Should we therefore maybe split up the section rather by type
of calibration? Certainly clearer that way. Only question is how to
best present the other calibrations then (FADC, scintillators).
#+end_quote


- [ ] *INSERT REFERENCES TO SECTIONS!*

- [X] *RENAME* this to something like "data calibration"?
  -> Yup.

- [X] *MAYBE THIS CHAPTER CAN NOT ONLY CONTAIN STRICT CALIBRATIONS,
  BUT ALSO THINGS LiKE GAS GAIN TIME BINNING?*
  -> Yup.


On the one hand:
- [ ] talk about 55 fe calibration -> energy calibration
- [ ] talk about CDL -> definition of likelihood

On the other hand
- [ ] information about FADC data (i.e. spectrum etc)
- [ ] detector behavior over time
- [ ] ...
How do these two things go together?        
In some sense the CDL data is "less important" or "later staged data",
because it is _only relevant_ for the determination of the background
rate. All the other mentioned things are relevant already for the
understanding of the data itself, i.e. how does the detector behave,
what do we see in the calibration / background etc

I would propose the following for now:
We start writing about the general data related parts, i.e. anything
but CDL. The CDL can be mentioned afterwards as the motivation to "how
do we even get a background rate from all this?"

So then, where do we start? FADC first or GridPix?
-> GridPix first.

** Energy calibration - in principle [0/1]
:PROPERTIES:
:CUSTOM_ID: sec:calibration:energy
:END:

The reconstructed data from the GridPixes, as described in chapter
[[#sec:reco:data_reconstruction]] (cluster finding, cluster reconstruction
and charge calibration), still needs to be calibrated in energy.

The charge calibration [[#sec:operation_calibration:tot_calibration]]
computes the number of electrons recorded on each GridPix pixel in an
event from the ~ToT~ counts.

- [ ] GAS GAIN?

In order to calculate an equivalent energy based on a certain amount
of charge -- which depends on the gas gain -- the data recorded using
the \cefe calibration source at CAST is used. As the \cefe spectrum
(see sec. [[#sec:theory:escape_peaks_55fe]]) has a photo peak at
$\SI{5.9}{keV}$ and an escape peak at $\SI{2.9}{keV}$ it provides two
different lines relating charges to energies for calibration. While
the charge calibration for each pixel from ~ToT~ to electrons is
non-linear, the relation between energy and recorded charge is
linear. The position of the two peaks in the \cefe spectrum needs to
be determined precisely, which is done using a double gaussian fit

#+NAME: eq:calib:fe55_charge_fit_function
\begin{equation}
f(N_e, μ_e, σ_e, N_p, μ_p, σ_p) =
G^{\text{esc}}_{\text{K}_{α}}(N_e,μ_e,σ_e) + G_{\text{K}_{α}}(N_p,μ_p,σ_p),
\end{equation}

where $G$ is a regular gaussian, one for the escape peak
$G^{\text{esc}}$ and one for the photo peak $G$. An example spectrum
with such a fit can be seen in
fig. [[sref:fig:calib:fe55_example_fit_spectrum]].


#+begin_src subfigure
(figure ()
        (subfigure (linewidth 0.5) (caption "\\cefe spectrum") (label "fig:calib:fe55_example_fit_spectrum")
                   (includegraphics (list (cons 'width (linewidth 0.95)))
                                    "~/phd/Figs/energyCalibration/run_149/fe_spec_run_149_chip_3_charge.pdf"))
        (subfigure (linewidth 0.5) (caption "Energy calibration") (label "fig:calib:fe55_example_energy_calib")
                   (includegraphics (list (cons 'width (linewidth 0.95)))
                                    "~/phd/Figs/energyCalibration/run_149/energy_calib_run_149_charge.pdf"))
        (caption
         (subref "fig:calib:fe55_example_fit_spectrum")
          ": Fit to a \\cefe calibration run from the CAST data (run 149)
           using a double gaussian fit."
         (subref "fig:calib:fe55_example_energy_calib")
          ": Linear fit to the escape and photo peak energies to relate charges
           in electrons to energies in " ($ "\\si{keV}") ".")
        (label "fig:calib:fe55_calibration"))
#+end_src

Then, a linear function without y-offset

\[
Q(E) = m_c · E
\]

is fitted to the found peak positions of the spectra by charge $Q$,
against the known energies $E$ of the peaks in the \cefe
spectrum. This yields the calibration factor, $a = m_c⁻¹$, which can
be used to calibrate all events with _the same_ gas gain. Over the
time of data taking at CAST the gas gain varies by a significant
margin, requiring a more complex calibration routine as the
calibration factor would produce too imprecise energy values otherwise
(for example if each \cefe calibration run were used to deduce one
calibration factor $a = m_c⁻¹$ to be applied to the closest background
data in time). An example for this fit is seen in fig. sref:fig:calib:fe55_example_energy_calib.

Fortunately, the gas gain can be computed using raw data without
evaluating any physical events, allowing calculation of it also for
raw background data. This motivates the idea to map a gas gains to a
calibration factor needed to calibrate events at such gas gains in
energy. Taking a certain time interval in which the detector gas gain
is assumed constant, the gas gain of all time slices of this length is
computed for background and calibration data. For all time slices in
the calibration data the procedure above -- fitting the \cefe spectrum
and calculating the energy calibration -- is performed. A higher gas
gain leads to linearly more recorded electrons in the \cefe
spectra. Therefore, all energy calibration factors determined from
different time intervals should be on a slope depending on the gas
gain. As such a final fit

#+NAME: eq:gas_gain_vs_calib_factor
\begin{equation}
a(G) = m_g · G + b
\end{equation}

is performed to all time intervals of all calibration runs. This
yields the energy calibration factor $a$ valid for a given gas gain
$G$. Then in order to calibrate the energy of a given cluster in the
background data, the same time slicing is performed and one gas gain
calculated for each slice. The gas gain is inserted into the fit and
the resulting calibration factor is used to calibrate the energy of
every cluster in that time slice. We will come back to this fit in
sec. [[#sec:calib:final_energy_calibration]] to finalize the energy
calibration.

The remaining question is the stability of the gas gain over time,
which we will look at next in the context of the general detector
behavior over time. This allows us to find a suitable time interval to
use for all data and hence perform a temporally stable energy calibration.

# In sec. [[#sec:calib:final_energy_calibration]] we will see the final
# calibrations based on the chosen time interval.

*** TODOs for this section [/]                                   :noexport:

- [ ] Mention something about section [[#sec:large_events_few_pixels_tot]] in
=StatusAndProgress=, i.e. events that convert very close to grid and
thus have few pixels, but a lot of energy as determined by the
~ToT~. This is an argument in favor of using ~ToT~ over #pix.

- [X] *TALK ABOUT DETECTOR BEHAVIOR OVER TIME, REFER TO SUBSECTION*

- [ ] *INSERT REFERENCES TO THE FITS IN CODE*

- [ ] *CLARIFY WHAT IS BEING FIT / UNITS OF FIT FUNCTION / CLARIFY
  PIXEL VS CHARGE FITS / CHECK RELATION a to m IS CORRECT*
  -> maybe add pixel fit only as a footnote? 

- [ ] *INSERT PLOT OF ENERGY CALIBRATION FIT?*
  -> Maybe as a side-by-side?

- [X] *INSERT A 55 FE FIT W/ FIT PARAMETERS SHOWN*
  - [ ] *UPDATE PLOT*

*** Generate example plot for \cefe spectrum                     :extended:
:PROPERTIES:
:CUSTOM_ID: sec:calib:energy_gen_example_cefe
:END:

We use run number 149 (for no important reason) as an example
calibration run.

Desktop:
#+begin_src sh
raw_data_manipulation \
    -p /mnt/4TB/CAST/Data/2018/CalibrationRuns/Run_149_180219-17-25.tar.gz \
    -r calib \
    -o /tmp/run_149.h5
#+end_src
Laptop:
#+begin_src sh
raw_data_manipulation \
    -p /mnt/1TB/CAST/2018/CalibrationRuns/Run_149_180219-17-25.tar.gz \
    -r calib \
    -o /tmp/run_149.h5
#+end_src

We overwrite the default to use TikZ output via an environment
variable here just to make sure it is set independent of the
~config.toml~ file.
#+begin_src sh
USE_TEX=true PLOT_OUTPATH=~/phd/Figs/energyCalibration/run_149/ reconstruction -i /tmp/run_149.h5 --out /tmp/reco_149.h5
#+end_src

This produces the following plots:
- [[~/phd/Figs/energyCalibration/run_149/energy_calib_run_149.pdf]]
- [[~/phd/Figs/energyCalibration/run_149/energy_calib_run_149_charge.pdf]]
- [[~/phd/Figs/energyCalibration/run_149/fe_spec_run_149_chip_3.pdf]]
- [[~/phd/Figs/energyCalibration/run_149/fe_spec_run_149_chip_3_charge.pdf]]
  
*** On ~ToT~ vs. ~ToA~ for a Timepix1 detector                   :extended:

This is a good point to comment on the choice of using all pixels in
the CAST data taking to record ~ToT~ values. One might argue that due
to the single electron efficiency of GridPix detectors it would have
been a good idea to either just record only ~ToA~ values for all
pixels as to have access to time information (yielding longitudinal
information about events) or at least use a checkerboard pattern with
half the pixels recording ~ToT~ and half ~ToA~ values.

There are two major issues with that (outside of the fact that at the
time of making these choices I was not in a position to make an
educated choice anyway):

1. the ~ToA~ counter, as far as I'm aware, is too short for the
   Timepix1 as needed in the context for CAST like shutter times. Ref
   [[cite:&lupberger2016pixel]] page 30, but the gist is that Timepix1
   pixels can count to 11810. At a clock frequency of $\SI{40}{MHz}$
   this only yields a time window of $\SI{295}{μs}$ for ~ToA~
   values. For shutter lengths on the order of seconds such short
   ~ToA~ counters would run over pretty much always.
2. ignoring the practical limitation of 1, which may or may not be
   possible to circumvent in some way or another, there is a separate
   problem: Single electron efficiency is an ideal approximation of
   reality. Either for higher energies or in rare cases -- which are
   extremely important for low rate experiments like CAST where "rare"
   means precisely important for the selection of candidates! --
   conversion of photons can happen very close to the grid. In those
   cases _many_ primary electrons will enter single holes, resulting
   in events with very few pixels but very high charges. See
   sec. [[#sec:large_events_few_pixels_tot]] below.

Fortunately, we do have the FADC signal to get at least some time
information regardless of the choice.

At the same time in the future with a Timepix3 based GridPix detector
all these points will become mute: it records both ~ToT~ and ~ToA~ at
the same time at time high resolution. This _also_ means using an FADC
will become irrelevant, avoiding the difficulties of dealing with
analogue signals and associated EMI issues.

**** (While generating fake data) Events with large energy, but few pixels
:PROPERTIES:
:CUSTOM_ID: sec:large_events_few_pixels_tot
:END:

#+begin_quote
This section is taken out of my regular notes. It was written while
trying to understand certain behaviors while trying to generate fake
event data from existing data by removal of pixels. That approach is
the easiest way to generate lower energy 'simulated' data from
existing data without having to simulate full events (which we ended
up doing later anyway).
#+end_quote

While developing some fake data using existing events in the photo
peak & filtering out pixels to end up at ~3 keV, I noticed the
prevalence of events with <150 pixels & ~6 keV energy.

Code produced by splicing in the following code into the body of =generateFakeData=.
#+begin_src nim
    for i in 0 ..< xs.len:
      if xs[i].len < 150 and energyInput[i] > 5.5:
        # recompute from data
        let pp = toSeq(0 ..< xs[i].len).mapIt((x: xs[i][it], y: ys[i][it], ch: ts[i][it]))
        let newEnergy = h5f.computeEnergy(pp, group, a, b, c, t, bL, mL)
        echo "Length ", xs[i].len , " w/ energy ", energyInput[i], " recomp ", newEnergy
        let df = toDf({"x" : pp.mapIt(it.x.int), "y" : pp.mapIt(it.y.int), "ch" : pp.mapIt(it.ch.int)})
        ggplot(df, aes("x", "y", color = "ch")) +
          geom_point() +
          ggtitle("funny its real") + 
          ggsave("/tmp/fake_event.pdf")
        sleep(200)
    if true: quit()
#+end_src

This gives about 100 events that fit the criteria out of a total of
O(20000). A ratio of 1/200 seems probably reasonable for absorption of
X-rays at 5.9 keV.

While plotting them I noticed that they all share that they are
incredibly dense, like:
[[file:~/org/Figs/statusAndProgress/exampleEvents/event_few_pixels_large_energy.pdf]]

These events must be events where the X-ray to photoelectron
conversion happens very close to the grid!
This is one argument "in favor" of using ToT instead of ToA on the
Timepix1 and more importantly a good reason to keep using the ToT
values instead of pure pixel counting for at least some events!

- [ ] We should look at number of pixels vs. energy as a scatter plot to see
what this gives us.

**** Plotting low count / high energy events with ~plotData~

Alternatively to the above section we can also just use ~plotData~ to
create some event displays for such events for us. We can utilize the
~--cuts~ argument to create event displays only for events with fewer
than a certain number of pixels and more than some amount of energy.

Let's say < 100 pixels and > 5 keV for example:
#+begin_src sh
plotData \
    --h5file ~/CastData/data/DataRuns2018_Reco.h5 \
    --runType=rtBackground \
    --eventDisplay --septemboard \
    --cuts '("hits", 0, 100)' \
    --cuts '("energyFromCharge", 5.0, Inf)' \
    --cuts '("centerX", 3.0, 11.0)' \
    --cuts '("centerY", 3.0, 11.0)' \
    --applyAllCuts
#+end_src

Or we can produce a scatter plot of how the number of hits relates to
the energy if we make some similar cuts (producing the plot for all
background data obviously drowns it in uninteresting events). We do
this by utilizing the custom ~--x~ and ~--y~ argument:
#+begin_src sh
plotData \
  --h5file ~/CastData/data/DataRuns2018_Reco.h5 \
  --runType=rtBackground \
  --x energyFromCharge --y hits --z length \
  --cuts '("hits", 0, 150)' \
  --cuts '("energyFromCharge", 4.0, Inf)' \
  --cuts '("centerX", 3.0, 11.0)' \
  --cuts '("centerY", 3.0, 11.0)' \
  --applyAllCuts 
#+end_src
In addition we colored each point by the length of the cluster to see
if these clusters are commonly small.

This yields the following plot, fig. [[fig:calibration:large_energy_few_hits_scatter]].

#+CAPTION: Scatter plot of the energy of clusters against the number of hits
#+CAPTION: for clusters not at the edges of the chips and filtered to < 150 hits
#+CAPTION: and more than 4 keV. The color code is the length of the clusters in milli meter.
#+NAME: fig:calibration:large_energy_few_hits_scatter
[[~/phd/Figs/eventProperties/events_few_hits_large_energy_scatter.pdf]]

** Detector behavior over time [/]
:PROPERTIES:
:CUSTOM_ID: sec:calib:detector_behavior_over_time
:END:

Outside the detector related issues discussed in section
[[#sec:cast:data_taking_woes]] the detector generally ran very stable
during Run-2 and Run-3 at CAST. This allows and requires to assess the
data quality in more nuanced ways. Specifically, the stability of the
recorded signals over time is of interest, which is one of the main
purposes of the \cefe calibration runs. A fixed spectrum allows to verify
stable operation easily. In particular of interest for the energy
calibration of the data are the detected charge and gas gain of the
detector.

As the charge and gas gain can be computed purely from individual
pixel data without any physical interpretation, it serves as a great
reference over time. Longer time scale variations of the gas gain were
already evident from the calibration runs during data taking and
partially expected due to the power supply and grounding problems
encountered, as well as different sets of calibrations between Run-2
and Run-3. By binning the data into short intervals of order one hour,
significant fluctuations can be observed even on such time
scales. Fig. [[fig:calib:total_charge_over_time]] shows the normalized
median of the total charge in events for all CAST data normalized by
the datasets (background and calibration). Each data point represents
a $\SI{90}{min}$ time slice. Some data is removed prior to calculation
of the median as mentioned in the caption and title. The important
takeaway of the figure is the extreme variability of the median charge
(up to $\SI{30}{\%}$!). Fortunately though, the background and
calibration data behaves the same, evident by the strong correlation
(purple background, green calibration). While the causes for the
variability are not entirely certain (see
sec. [[#sec:calib:causes_variability]]), it allows us to take action and
calibrate the data accordingly.

#+CAPTION: The plot shows the median charge within $\SI{90}{min}$ time windows of both
#+CAPTION: background and calibration data. Some data is removed (only clusters with
#+CAPTION: less than 500 pixels active to remove the worst sparks and extremely large
#+CAPTION: events) and only events within the inner $\SI{4.5}{mm}$ radius are considered.
#+CAPTION: Each data type (calibration and background) is normalized to 1 as the median
#+CAPTION: charge is very different in the datasets. The median is used instead of the mean
#+CAPTION: to further remove effect of very rare, but extreme outliers. Each pane of
#+CAPTION: the plot shows a portion of data taking with significant time without data
#+CAPTION: between each. 
#+NAME: fig:calib:total_charge_over_time
#+ATTR_LATEX: :width 1.0\textwidth
[[~/phd/Figs/behavior_over_time/plotTotalChargeOverTime/background_median_charge_binned_90.0_min_filtered_crSilver.pdf]]

[fn:calib_amount_of_calibration] As is pretty evident in the top left
pane of fig. [[fig:calib:total_charge_over_time]] of the first data taking
campaign in 2017, the amount of calibration data is initially pretty
limited. The reason for this is plainly that too much was going on at
the time, leading to a neglect in taking regular calibration runs. (: 

*** TODOs for this section [/]                                   :noexport:

- [ ] *LOTS OF REPETITION BETWEEN NEXT PARAGRAPH AND PARTS OF
  PREVIOUS SECTION!*  

- [X] *CHECK NOTES W/ KLAUS AND STATUSANDPROGRESS FOR HOW WE WENT
  THROUGH GAS GAIN SLICING INVESTIGATION*  

- [ ] this should be in the detector energy calibration section. It
  only matters (and can be computed) once we discuss the energy calibration.

- [X] plot of total charge over time as *main motivation*
- [X] Plot of the peak position of the 55Fe calibration runs.
  -> Will be shown next.

- [X] Regarding 55Fe peak positions & temperature correlation:
  _Maybe_ the relevant temperature to correlate to isn't actually the
  septemboard temperature, but the ambient temperature in the CAST hall?
  Unlikely, but can easily check by extracting temps from log files
  and plotting against position!
  -> lol @ <2023-10-21 Sat 16:57>: What a fun idea right there. That
  is indeed the relevant parameter to look at, as we discuss in the
  next section anyway.

- [ ] SPLIT THIS BY: EXPLANATION TO A *WHY* SUCH VARIATION AND *WHAT
  IS DONE* AS A RESULT.

- [X] *FIGURE TOTAL CHARGE OVER TIME*
  what binning? 90min

- [ ] *INSERT TABLE OF THE MEAN POSITIONS OF THE 55FE PEAKS WITH*
  column charge and pixel mean of esc and photo. similar to that cdl
  table we have!
  -> AND OR have a plot of the positions?

- [X] *EXPLAIN HOW WE ENDED UP AT 90 MIN BINNING! AND HOW SLICING
  WORKS ETC*
  -> Section later.

*** Generate plot for median of charge over time                 :noexport:

Let's generate the plot for the median charge within 90 minutes,
filtered to only clusters with less than 500 hits, also showing the
calibration data, filtered to the silver region & each data type
(calibration & background) normalized to 1, as a facet plot.

- [X] We hand ~StartHue=285~ manually here for now, but we should change
  that to become a thesis wide setting for everything we compile.
  -> Done.

For a note on why the median and not the mean, see the whole section
on "Detector behavior over time" in the ~statusAndProgress~ and in
particular the 'Addendum' there (extreme outliers in some cases is the tl/dr).
#+begin_src sh
nim c -d:danger -d:StartHue=285 plotTotalChargeOverTime && \
    USE_TEX=true plotTotalChargeOverTime \
                 ~/CastData/data/DataRuns2017_Reco.h5 \
                 ~/CastData/data/DataRuns2018_Reco.h5 \
                 --interval 90 \
                 --cutoffCharge 0 \
                 --cutoffHits 500 \
                 --calibFiles ~/CastData/data/CalibrationRuns2017_Reco.h5 \
                 --calibFiles ~/CastData/data/CalibrationRuns2018_Reco.h5 \
                 --applyRegionCut \
                 --timeSeries \
                 --useMedian \
                 --normalizeMedian \
                 --outpath ~/phd/Figs/behavior_over_time/plotTotalChargeOverTime/
#+end_src

yielding
[[~/phd/Figs/behavior_over_time/plotTotalChargeOverTime/background_median_charge_binned_90.0_min_filtered_crSilver.pdf]]
among other things, with many more related plots to be found in:
[[file:Figs/behavior_over_time/plotTotalChargeOverTime/]]

*** Potential causes for the variability [0/6]
:PROPERTIES:
:CUSTOM_ID: sec:calib:causes_variability
:END:

# As mentioned in the previous section, variability of the data was
# already apparent during the data taking based on the position of the
# peaks in the \cefe spectra. The change in the median of the charge (and related the
# gas gain) is an easier data point to investigate possible reasons for
# such changes.

One possible cause for the variability seen in the previous section is
the electronics of the detector readout. Either a floating ground or
unstable power supply can result in the activation thresholds of the
pixels moving -- as was indeed observed -- as mentioned in section
[[#sec:cast:data_taking_woes]]. Lesser versions of the problems discussed
in that section could theoretically explain the variations. In
particular in regards to the \cefe spectra showing variation, the
number of pixels and the amount of charge are directly correlated. The
number of pixels is plainly a clamped version of the charge
information. If electronics caused threshold variations it would both
change the effective ~ToT~ value as well as the number of pixels
activated in the first place. Fortunately, the center chip also
contains the FADC, which allows for an independent measurement of the
effective charge generated below the grid and thus another indirect
measurement of the gas gain. By comparing how the mean position of the
\cefe spectra behave in the FADC data compared to the GridPix data we
can deduce if the GridPix behavior likely is due to real gas gain
changes or due to electronics.

Fig. [[fig:calib:fe55_peak_pos_charge_pixel_fadc]] shows the (normalized)
position of the \cefe photo peak based on a fit to the pixel, charge
and FADC spectrum (the latter based on the amplitudes of the FADC
signals). Aside from the variations in the FADC data in the 2017 data
(left) due to the changed FADC settings (more on that in
sec. [[#sec:calib:fadc]]), the 'temporally local' changes in all three
datasets are almost perfectly correlated. This implies a /real
physical origin/ in the observed variation and not an electronic or
power supply origin.

#+CAPTION: Normalized photo peak positions in the Run-2 data based on the charge (purple), pixel
#+CAPTION: (green) and FADC (orange) spectra. The empty range in the middle is the period between
#+CAPTION: Dec 2017 and Feb 2018. The strong changes in the FADC
#+CAPTION: on the left are due to the different FADC settings. Beyond that the three sets of 
#+CAPTION: data are fully correlated, implying a physical origin in the variation. Compare how
#+CAPTION: local (in time) features appear identical in each data.
#+NAME: fig:calib:fe55_peak_pos_charge_pixel_fadc
[[~/phd/Figs/behavior_over_time/mapSeptemTempToFePeak/time_vs_peak_pos.pdf]]

A physical change in the gas gain can either be caused by a change in
high voltage in the amplification region, a change in gas composition
or gas properties (assuming no change in the physical size of the
amplification gap, which is reasonable at least within Run-2 and Run-3
as the detector was not touched).

Firstly, the high voltage, while not logged to a file [fn:hv_logging],
was visually inspected regularly and was always kept at the desired
voltages by the Iseg HV module within the operating window. It is a
very unlikely source of the
variability. [fn:hv_variability_lab_course]

Secondly, there is no reason to believe the gas composition to be at
fault as a) the detector is used in an open loop at a constant gas
flow and b) it would then if anything show up as a sudden change in
detector properties upon a gas bottle change and not a continuous
change during operation.

This finally leaves the properties of the gas itself, for which three
variables are (partially) known:
1. the gas flow
2. the chamber pressure via the pressure controller on the outlet side
3. the temperature

The gas flow was at a relatively constant $\SI{2}{\liter\per\hour}$. The
absolute value should not be too relevant, as the flow is small in
absolute terms and thus should have no effect on the gas properties in
the chamber as such (via gas flow related effects causing turbulence
or similar in the chamber). Its secondary impact is only one on
absolute gas pressure, which is controlled by the pressure controller,
which provides granular control. While also no log files were written
for the chamber pressure, visual inspection was also done regularly
and the pressure was at a constant $\SI{1050}{mbar}$ at most varying
by $\SI{1}{mbar}$ in rare cases, but certainly not in a way
correlating to the gas gain variations.

This leaves the temperature inside the chamber and in the
amplification region as the final plausible source of the
variations. As the temperature log files for the Septemboard were lost
due to a software bug (as mentioned previously), there are two other
sources of temperature information. First of all the shift log of each
morning shift contains one temperature reading of the Septemboard,
which yields one value for every solar tracking. Second of all the
CAST slow control log files contain multiple different temperature
readings in one second intervals. Most notably the ambient temperature
in the CAST hall, which up to an offset (and some variation due to
detector load and cooling efficiency) should be equivalent to the gas
temperature. Fig. [[fig:calib:correlation_ambient_temperature_gasgain_and_spectra]]
shows the normalized temperature sensors in the CAST hall (excluding
the exterior temperature) during the Run-3 data taking period together
with the normalized peak position of the \cefe spectra in pixels
(black points), the temperature from the shift logs (blue points) and
the gas gain values of each chip (smaller points using the color
scale, based on $\SI{90}{min}$ intervals per point). The blue points of the
temperature of the Septemboard recorded during each solar tracking
nicely follows the temperature trend of the ambient temperature
(~T_amb~) in the hall, as expected. Comparing the \cefe spectra mean
positions with the shift log temperatures does not allow to draw
meaningful conclusions about possible correlations, due to lack of
statistics. But the gas gains of each chip compared to the temperature
lines does imply an (imperfect) inverse correlation between the
temperature and the gas gain.

As discussed in theory sec. [[#sec:theory:gas_gain_polya]] the expectation
for the gas gain given constant pressure is $G ∝ e^α$ where the first
Townsend coefficient $α$ scales with temperature by

#+NAME: eq:calib:townsend_scaling_prop
\begin{equation}
α ∝ \frac{1}{T} \exp\left(-\frac{1}{T}\right).
\end{equation}

The combination of the inverse relation to $T$ and its negative
exponential is a monotonically increasing sublinear function (and not
decreasing as $1/T$ would imply alone) in the relevant parameter
ranges. This should imply an increase in gas gain instead of the
apparent decrease we see for increasing temperatures. The kind of
scaling according to eq. [[eq:calib:townsend_scaling_prop]] was also
already experimentally measured for GridPix detectors by
L. Scharenberg in [[cite:&lucianMsc]]. The implications seem to be that
the assumptions going into the $α$ scaling must have been
violated. The septemboard detector in its -- essentially open -- gas
system is a non-trivial thermodynamic system due to the significant
heating of the Timepix ASICs and very small amplification region of
$\SI{50}{μm}$ height enclosing a gas mass, where gas flow is
potentially inhibited.

This is not meant as a definitive statement about the origins of the
gas gain variations in the Septemboard detector data. /However/, it
clearly motivates the need for an even more in depth study of the
behavior of these detectors for different gas temperatures at constant
pressures (continuing the work of [[cite:&lucianMsc]]). More precise
logging of temperatures and pressures in future detectors is highly
encouraged. Further, a significantly improved cooling setup (to more
closely approach a region where temperature changes have a smaller
relative impact), or theoretically even a temperature controlled setup
(to avoid temperature changes in the first place) with known inlet gas
temperatures might be useful. This behavior is one of the most
problematic from a data analysis point of view and thus it should be
taken seriously for future endeavors.

See appendix [[#sec:appendix:detector_time_behavior]] for plots similar to
fig. [[fig:calib:correlation_ambient_temperature_gasgain_and_spectra]] for
the other periods of CAST data taking and a scatter plot of the center
chip gas gains against the ambient temperature directly. 

#+CAPTION: Normalized data for Run-3 of the temperature sensors from the CAST slow control log
#+CAPTION: files compared to the behavior of the mean peak position in the \cefe pixel spectra
#+CAPTION: (black points), the recovered temperature values recorded during each solar tracking
#+CAPTION: (blue points) and the gas gain values computed based on \SI{90}{min} of data for each
#+CAPTION: chip (smaller points using Viridis color scale). The shift log temperatures nicely
#+CAPTION: follow the trend of the general temperatures. Gas gains and temperatures seem to be
#+CAPTION: inversely correlated, providing a possible explanation for the detector behavior.
#+NAME: fig:calib:correlation_ambient_temperature_gasgain_and_spectra
#+ATTR_LATEX: :float sideways
[[~/phd/Figs/behavior_over_time/correlation_fePixel_all_chips_gasgain_period_2018-10-19.pdf]]

[fn:hv_logging] Once again, in hindsight writing a log file of the
high voltage values would have been valuable, especially as it could
have been done straight from TOS. However, similar to what lead to
losses of the temperature log files, this was simply not prioritized
to implement at the time. The same holds for the gas pressure in the
chamber, which should have been logged using the ~FlowView~ software
of the Bronkhorst pressure controller used to control it.

[fn:hv_variability_lab_course] From other Iseg HV modules used in lab
course experiments we know that when they _are_ faulty it is very
evident. We have never experienced a module that reads correct values,
but actually supplies the wrong voltage. In each faulty case the
desired target voltage was simply not correctly held and this was
visible in the read out voltage.

**** TODOs about this section [/]                               :noexport:

- [X] *COME BACK TO THIS ONCE WE a) UNDERSTAND
  [[#sec:calib:behavior_over_time:thoughts_townsend]] OR TALKED TO LUCIAN*
  -> With our newfound understanding thanks to the talk with Lucian,
  we should now do the following:
  - [ ] Merge explanation about gas gain, Townsend coefficient
    etc. back into Polya / gas gain section of theory
  - [ ] Refer to theory in the above section saying "this is our
    expectation"
  - [ ] Finish / extend section above with adjusted explanation /
    interpretation of our data.

Text section about GridPix 1:
#+begin_quote

- [ ] *TAKE OUT THIS PART?*
This behavior was likely less relevant in single GridPix detectors as
the heat emission of a single GridPix is much lower and thus the
absolute temperatures are lower. For a fixed change in ambient
temperature this means a single GridPix detector undergoes temperature
changes in the amplification region at lower absolute temperatures. As
the behavior is highly non-linear the effect is likely less evident
for a single GridPix. In addition the single GridPix detector was
mostly built from acrylic glass, which is a good insulator potentially
leading to a more stable temperature.
- [ ] *MAYBE REWRITE ABOVE*

#+end_quote

Previous text about interpretation:
#+begin_quote

While an inverse correlation between temperature and gas gain may
appear counter intuitive, it is sensible when considering the
amplification region of the GridPix as an ideal gas. The pressure
controller keeps the pressure inside the gas chamber constant
independent of the temperature. This implies that an increase in
temperature results in a decrease in density in the amplification
region. The mobility of the charge carriers in the region is inversely
proportional to the density *CITE CITE CITE* (for now PDG chapter
detectors!, *REF THEORY SEC*) and a higher mobility implies a longer
mean free path. This in turn leads to _less_ additional ionization
events for higher temperatures, reducing the full amplification.
  
- [ ] *FIND GOOD REFERENCE FOR MOBILITY IN GAS INVERSELY PROP TO
  DENSITY!*
  -> Generally better understand this, add section to gaseous detector
  theory about this, show equation for mean free path based on
  mobility!
  Once section in theory there, reference theory instead!

#+end_quote



About \cefe spectrum peak plot:
- [ ] *REWRITE TEXT ABOVE PLOT BELOW IS REPLACED BY FACET PLOT*
- [ ] *UPDATE THE PLOT WITH ALL DATA, SPLIT BY RUN PERIOD AS FACET,
  AND UPDATE COLORS (!!!) IN THE CAPTION!*
-> Not sure if we really want a different plot. I think this one is
now quite nice actually.

- [ ] *(RE)MOVE THE PARAGRAPH BELOW?*
  -> Refers to paragraph about "what if we had temp log files"


- [ ] *WE COULD CREATE A PLOT SIMILAR TO ABOVE, BUT ACTUALLY PLOTTING
  TEMPERATURE AGAINST THE GAS GAIN DIRECTLY*
  -> Refers to fig. fig:calib:correlation_ambient_temperature_gasgain_and_spectra
  -> we'd just have to assign a temperature to each gas gain value
  (compute a mean of all temps in a gas gain slice interval?) and
  see what happens!  


- [X] Reasons for peak position moving:
  - [X] electronic causing the effective threshold to change
  - [X] change in gas temperature (seems uncorrelated going by
    plots!. *Maybe* correlated to ambient temperature?)
  - [X] change in pressure (unlikely, had pressure sensor!)
  - [X] change in gas flow (related to above 2) (unlikely, was constant at
    ~2 L/h. If changes smaller than what's commonly visible on the
    flow meter was responsible it would be Jochen's fault for not being
    aware of that!)
  - [X] change in grid voltage (real or effective due to charge up)
    -> either Iseg module broken (we *know* they are crappy), but in
    this case the module *showing* the right voltage, but maybe not
    really *applying* that voltage?
    -> a case of bad grounding causing the module to applying 300 V, but
    in reality not relative to the pixel layer, but rather to something
    else so that it was floating around that?

- [X] *WAS GAS PRESSURE CONTROLLER ON INLET OR OUTLET SIDE??? CHECK
  AND ADJUST ABOVE*
  -> Outlet

- [X] Plot of the (known) temperature at PCB from shift forms.
- [ ] *MENTION VARYING COOLING POWER OVER TIME, POSSIBLY CLOGGING*

- [X] *CHECK NAME OF BRONKHORST ? OR WHATEVER PRESSURE CONTROLLER AND
  ADJUST IN FOOTNOTE!*
  -> Bronkhorst.

**** Extended thoughts on missing temperature log data          :extended:

- [ ] *THINK ABOUT WHETHER TO PUT INTO MAIN AGAIN / REMOVE EXTENDED*

Note that even if the temperature logs were still available, it is not
obvious how they could lead to a correction that goes beyond the gas
gain binning in time that was eventually settled on. The variations
lead to gain and loss of information that cannot easily be corrected
for without introducing potential biases, especially because the
temperature sensor on the bottom side of the Septemboard does not
yield an absolute temperature inside the amplification region
anyway. While theoretically a fit correlating temperature to energy
calibration factors is thinkable it is not clear it would improve the
calibration over using gas gains binned in time, as the gas gain is
the physical result of temperature changes. The only interesting
aspect of it would be potentially higher time resolution than the time
binning required to have good statistics for a gas gain. Further,
temperature changes are not expected to usually occur on time scales
much shorter than of the order of one hour, if they are due to ambient
temperature changes. Still, it could be an interesting avenue to
explore by experimenting with the available slow control log
information on the ambient temperature as a proxy for the temperature
in the amplification region (same as the Septemboard temperature
sensors, but just with a larger offset and lack of detail regarding
local temperature changes due to water cooling related variations).

**** Further thoughts about variability                         :extended:

What if (/put on my crackpot helmet/):
At lower temperatures gas diffusion is less efficient. This might lead
to stronger effects of "over pressure" / less gas cycling between
below and above the grid. This could increase the pressure below the
grid as it is further away from an open system in a thermodynamic
sense. The higher the temperature the more flow via diffusion
exchanges gas below and above the grid, bringing the detector closer
to desired 1050 mbar operating window. Yeah right lol.

**** Thoughts about Townsend coefficient & gas gain temperature dependence :extended:
:PROPERTIES:
:CUSTOM_ID: sec:calib:behavior_over_time:thoughts_townsend
:END:

*NOTE*: This section was me trying to better understand the origin of
the Townsend coefficient and its temperature dependence. The results
have since been merged back into the theory part (about mean free path
and gas gain) and the main section above.

Some further discussions of the fact that our temperature vs. gain
data in
fig. [[fig:calib:correlation_ambient_temperature_gasgain_and_spectra]]
seems to imply an inverse proportionality between temperature and gas
gain from our data at CAST. So let's go back to our theoretical
expectation here and see what we learn.


The number of electrons after a distance $x$ should be

\[
n = n_0 e^{αx}
\]

where $α$ is the first Townsend coefficient, cite:&sauli2014gaseous
(eq. 5.2 p. 146). The gas gain is just this divided by the initial
number $n_0$.

Sauli on the definition of the first Townsend coefficient:
[[cite:&sauli2014gaseous]] page 145 eq. 5.1:
#+begin_quote
The mean free path for ionization λ is deﬁned as the average distance an electron
has to travel before having an ionizing collision; its inverse, α = λ⁻¹, is the
ionization or ﬁrst Townsend coefﬁcient, and represents the number of ion pairs
produced per unit length of drift; it relates to the ionization cross section through
the expression:
α = N σ_i                              (eq 5.1)
where N is the number of molecules per unit volume.
As for other quantities in gaseous electronics, the Townsend coefﬁcient is
proportional to the gas density and therefore to the pressure P; the ratio α/P is a
sole function of the reduced ﬁeld E/P, as shown in Figure 5.19 for noble gases
(Druyvesteyn and Penning, 1940).
#+end_quote

Also from Sauli [[cite:&sauli2014gaseous]] p. 151 is
fig. [[fig:sauli_gas_gain_T_over_P]]. The plot (data at least) is taken
from cite:&altunbas03_gas_gain and shows a linear (or very shallow
exponential) behavior of the gain vs $T/P$ with experimental data from
GEMs for COMPASS and Magboltz simulations as well.

#+CAPTION: Figure from cite:&sauli2014gaseous (page 151) showing the gas gain
#+CAPTION: dependence on the ratio of temperature and pressure. 
#+NAME: fig:sauli_gas_gain_T_over_P
[[~/phd/Figs/gas_physics/fig_5_25_sauli_p151_gas_gain_vs_T_P.png]]

Further papers of interest:
- cite:&aoyama85_gas_gain 
  -> Contains a mathematical derivation for a generalized first
  Townsend coefficient relationship with S = E/N (where N is the
  density and E the electric field).
- [[cite:&Davydov_2006]] contains a discussion about the first Townsend
  coefficient for very low densities and thus also discussions about
  the math etc.

Now, if we just go by our intuition from ideal gas physics we would
expect the following:

Assuming $α = 1 / λ$ where $λ$ is the mean free path. If the
temperature increases in a gas, the density decreases for constant
pressure $p$ via

\[
p = ρ R_s T
\]

with the specific gas constant $R_s$. A lower density necessarily
implies less particles per unit volume and thus a typically longer
path between interactions. This means $λ$ increases and due to the
inverse relationship with $α$, the first Townsend coefficient -- and
by extension the gas gain -- decreases.

This is even explicitly mentioned by that quote of Sauli above,
literally in the sentence
#+begin_quote
As for other quantities in gaseous electronics, the Townsend coefﬁcient is
proportional to the gas density [...]
#+end_quote

However, this is in stark contrast to
- the screenshot of the fig. above, [[fig:sauli_gas_gain_T_over_P]]
- the fact that Jochen kept going on about the gas gain being
  essentially $G ∝ e^{T/P}$
  -> This is clearly wrong, see both below and generally the fact that
  neither Magboltz nor Lucian's MSc measurements indicate anything of
  the sorts of an exponential increase with temperature.
- and my Magboltz simulations,
  sec. [[#sec:calib:behavior_over_time:magboltz_sim]]

After a discussion with Lucian today <2023-10-23 Mon>, I'm a little
bit more illuminated. In his MSc thesis [[cite:&lucianMsc]] goes through
a derivation based on [[cite:&engel65_gases]] for the temperature
dependence of the first Townsend coefficient. Starting from the
argument above about $α = 1/λ$ and then continuing with the
requirement to accumulate enough energy to produce secondary
ionization events, $e |\vec{E}| l \geq eV_i$ with the ionization
potential $V_i$ for the gas mixture and $l$ for the forward distance
of an electron under the electric field $|\vec{E}|$. This distance

\[
l = \frac{V_i}{|\vec{E}|} 
\]

can be compared to the mean free path $λ$ of the electron

\[
\mathcal{N} = e^{-l/λ}
\]

where $\mathcal{N}$ is the relative number of colliding electrons with
$l > λ$. This allows to define the probability of finding $1/l$
collisions per unit distance to be

\[
P(l) \frac{1}{λ} e^{-l / λ} = α
\]

which is precisely the definition of the first Townsend coefficient,
$α$.

The mean free path $λ$ can be related to the pressure $p$, temperature
$T$ and cross section of the electron in the gas, $σ$:

\[
λ = \frac{kT}{pσ}.
\]

Inserting this into the above definition of $α$ yields:

\[
α(T) = \frac{pσ}{kT} \exp\left( - \frac{V_i}{|\vec{E}|}\frac{pσ}{kT}\right)
\]

which allows to analytically compute the temperature dependence of the
first Townsend coefficient, which we'll do in
sec. [[#sec:calib:behavior_over_time:townsend_coefficient_temp_scaling]]. The
expression now is actually similar to (eq. 5.4) in
[[cite:&sauli2014gaseous]]. It seems to roughly match the Magboltz
simulations.

Note though that this dependence is 'fragile', as it is a higher order
dependence on $T$ assuming idealized constant parameters for $p$ and
$σ$ and gas composition. In reality it is easily thinkable that gas
contamination and slight variations in pressure can change the results
from this result.

***** Applying Lucians (eq. 5.17) formula and plotting it
:PROPERTIES:
:CUSTOM_ID: sec:calib:behavior_over_time:townsend_coefficient_temp_scaling
:END:

Lucian gives the following formula for the temperature dependence of
the first Townsend coefficient:

\[
α(T) = \frac{pσ}{kT} \exp\left( - \frac{V_i}{|\vec{E}|}\frac{pσ}{kT}\right)
\]

where $p$ is the gas pressure, $σ$ the cross section of electrons with
the gas at the relevant energies, $V_i$ the ionization potential for
the gas, $|\vec{E}|$ the electric field strength.

#+begin_src nim :tangle code/townsend_coefficient_temp_scaling.nim
import unchained, math, ggplotnim, sequtils

const V_i = 15.7.V # Lucian gives this ionization potential next to fig. 5.4
defUnit(kV•cm⁻¹)
defUnit(cm⁻¹)
proc townsend[P: Pressure; A: Area](p: P, σ: A, T: Kelvin, E: kV•cm⁻¹): cm⁻¹ =
  let arg = (V_i * p * σ) / (E * k_B * T)
  echo arg
  result = (p * σ / (k_B * T) * exp( -arg )).to(cm⁻¹)
echo townsend(1013.25.mbar, 500.MegaBarn, 273.15.K, 60.kV•cm⁻¹)

let temps = linspace(0.0, 100.0, 1000) # 0 to 100 °C
#let temps = linspace(-273.15, 10000.0, 1000) # all of da range!
var αs = temps.mapIt(townsend(1013.25.mbar, 500.MegaBarn, (273.15 + it).K, 60.kV•cm⁻¹).float)
let df = toDf(temps, αs)
ggplot(df, aes("temps", "αs")) +
  geom_line() +
  xlab("Gas temperature [°C]") +
  ylab("Townsend coefficient [cm⁻¹]") +
  theme_font_scale(1.0, family = "serif") +
  ggsave("~/phd/Figs/gas_physics/townsend_coefficient_temperature_scaling_lucian.pdf")
#+end_src

#+RESULTS:

***** Simulations with Magboltz
:PROPERTIES:
:CUSTOM_ID: sec:calib:behavior_over_time:magboltz_sim
:END:

I wrote a simple interfacing library with Magboltz for Nim:

https://github.com/SciNim/NimBoltz
[[file:~/CastData/ExternCode/NimBoltz/nimboltz.nim]]

and I ran simulations at different temperatures, but same pressure and
the first Townsend coefficient (based on the steady state simulation,
which should be the correct one for high fields
#+begin_quote
The simulation of avalanche gain detectors at high field requires the use of SST Townsend parameters.
#+end_quote
from https://magboltz.web.cern.ch/magboltz/usage.html and line 256 in
[[file:~/src/Magboltz/magboltz-11.17.f]].

These seem to indicate that the coefficient should increase.

*Why?*
-> See above!

**** TODO Note about variability in GridPix 1                   :extended:

- [ ] *CREATE TEMPERATURE PLOT OF TEMP IN CAST HALL DURING GRIDPIX1
  DATA!*
  -> for noexport this is very useful info.

Christoph *did* see variations in his gas gain as well!! Fig. 9.7 of
his thesis and he *even notes it is likely due to temperature effects
in the hall*!
The big difference is just that the absolute variations were quite a
bit smaller.

Why this has never been on the mind of people like Jochen I will never
understand...

Further: in fig. 7.26 he even sees significant differences in the gas
gain for different targets of CDL data. But he concludes (by taking a
cut) that this is due to multiple electrons in the same hole instead
of real changes. Likely a combination of both I assume.

**** Generate plot of \cefe peak position                       :extended:


From my zsh history:
#+begin_src 
: 1672709064:0;./mapSeptemTempToFePeak ~/CastData/data/CalibrationRuns2017_Reco.h5 --inputs fePixel --inputs feCharge --inputs feFadc
: 1672709066:0;evince /t/time_vs_peak_pos.pdf
: 1672709148:0;cp /t/time_vs_peak_pos.pdf ~/phd/Figs/time_vs_55fe_peak_pos_2017.pdf
#+end_src

First of all we need to make sure our calibration HDF5 file not only
has the reconstructed \cefe spectra including their fits, but also the
fits for the FADC spectra. If that is not the case:

1. Make sure the FADC data is fully reconstructed:
#+begin_src sh
reconstruction -i ~/CastData/data/CalibrationRuns2017_Reco.h5 --only_fadc
#+end_src
2. Now redo the \cefe fits:
#+begin_src sh
reconstruction -i ~/CastData/data/CalibrationRuns2017_Reco.h5 --only_fe_spec
#+end_src

With that done we can create a plot of all normalized \cefe peak
positions and compare it to the temperatures recovered from the CAST
shift forms of the septemboard. 

#+begin_src sh :dir ~/CastData/ExternCode/TimepixAnalysis/Tools :results drawer
WRITE_PLOT_CSV=true USE_TEX=true ./mapSeptemTempToFePeak \
             ~/CastData/data/CalibrationRuns2017_Reco.h5 \
             --inputs fePixel --inputs feCharge --inputs feFadc \
             --outpath ~/phd/Figs/behavior_over_time/mapSeptemTempToFePeak/
#+end_src

#+RESULTS:
:results:
[INFO] TeXDaemon ready for input.
shellCmd: command -v lualatex
shellCmd: lualatex -output-directory /home/basti/phd/Figs/behavior_over_time/mapSeptemTempToFePeak /home/basti/phd/Figs/behavior_over_time/mapSeptemTempToFePeak//time_vs_peak_pos_fePixel.tex
Generated: /home/basti/phd/Figs/behavior_over_time/mapSeptemTempToFePeak//time_vs_peak_pos_fePixel.pdf
[INFO] Writing CSV file: /home/basti/phd/Figs/behavior_over_time/mapSeptemTempToFePeak//time_vs_peak_pos_fePixel.pdf.csv
[INFO] TeXDaemon ready for input.
shellCmd: command -v lualatex
shellCmd: lualatex -output-directory /home/basti/phd/Figs/behavior_over_time/mapSeptemTempToFePeak /home/basti/phd/Figs/behavior_over_time/mapSeptemTempToFePeak//time_vs_peak_pos_feCharge.tex
Generated: /home/basti/phd/Figs/behavior_over_time/mapSeptemTempToFePeak//time_vs_peak_pos_feCharge.pdf
[INFO] Writing CSV file: /home/basti/phd/Figs/behavior_over_time/mapSeptemTempToFePeak//time_vs_peak_pos_feCharge.pdf.csv
[INFO] TeXDaemon ready for input.
shellCmd: command -v lualatex
shellCmd: lualatex -output-directory /home/basti/phd/Figs/behavior_over_time/mapSeptemTempToFePeak /home/basti/phd/Figs/behavior_over_time/mapSeptemTempToFePeak//time_vs_peak_pos_feFadc.tex
Generated: /home/basti/phd/Figs/behavior_over_time/mapSeptemTempToFePeak//time_vs_peak_pos_feFadc.pdf
[INFO] Writing CSV file: /home/basti/phd/Figs/behavior_over_time/mapSeptemTempToFePeak//time_vs_peak_pos_feFadc.pdf.csv
[INFO] TeXDaemon ready for input.
shellCmd: command -v lualatex
shellCmd: lualatex -output-directory /home/basti/phd/Figs/behavior_over_time/mapSeptemTempToFePeak /home/basti/phd/Figs/behavior_over_time/mapSeptemTempToFePeak//time_vs_peak_pos.tex
Generated: /home/basti/phd/Figs/behavior_over_time/mapSeptemTempToFePeak//time_vs_peak_pos.pdf
[INFO] Writing CSV file: /home/basti/phd/Figs/behavior_over_time/mapSeptemTempToFePeak//time_vs_peak_pos.pdf.csv
tot diff 171180   2017-11-09T06:33:00+01:00    2017-11-07T07:00:00+01:00   2017-11-08T17:27:34+01:00
Temp 98.3078354948008 peak 212.7571189854556 norm 0.5727625012999181
tot diff 84780   2017-11-13T06:40:00+01:00    2017-11-12T07:07:00+01:00   2017-11-12T15:30:08+01:00
Temp 68.86005708893606 peak 200.5267629392088 norm 0.5863183224669442
tot diff 86520   2017-11-18T06:38:00+01:00    2017-11-17T06:36:00+01:00   2017-11-17T20:18:47+01:00
Temp 85.04725936199722 peak 224.2078745321606 norm 0.625934087076792
tot diff 518880   2017-11-25T06:52:00+01:00    2017-11-19T06:44:00+01:00   2017-11-23T11:42:19+01:00
Temp 95.77865970937404 peak 213.4664809739825 norm 0.578611813845371
tot diff 86220   2017-11-30T07:32:00+01:00    2017-11-29T07:35:00+01:00   2017-11-29T21:19:07+01:00
Temp 83.69371537926236 peak 219.4480315730993 norm 0.6149695850461161
tot diff 85500   2017-12-05T07:35:00+01:00    2017-12-04T07:50:00+01:00   2017-12-04T14:39:45+01:00
Temp 60.80512631578948 peak 222.9204153830614 norm 0.6675160757145163
tot diff 173400   2017-12-07T07:45:00+01:00    2017-12-05T07:35:00+01:00   2017-12-05T12:20:37+01:00
Temp 45.89712508650519 peak 222.6633344356147 norm 0.6979010839707228
tot diff 86160   2017-12-13T07:41:00+01:00    2017-12-12T07:45:00+01:00   2017-12-12T21:59:03+01:00
Temp 87.18364798050139 peak 214.0834731995871 norm 0.5941256787963686
tot diff 86460   2017-12-14T07:42:00+01:00    2017-12-13T07:41:00+01:00   2017-12-13T22:30:09+01:00
Temp 89.09145350451077 peak 216.7236407119627 norm 0.598285035065055
tot diff 86580   2017-12-15T07:45:00+01:00    2017-12-14T07:42:00+01:00   2017-12-14T18:04:59+01:00
Temp 74.97479706629706 peak 209.0577735867058 norm 0.60052537293657
tot diff 86220   2017-12-16T07:42:00+01:00    2017-12-15T07:45:00+01:00   2017-12-15T20:22:45+01:00
Temp 82.12456158663883 peak 203.3763022789357 norm 0.5724482534597104
tot diff 86520   2017-12-20T07:46:00+01:00    2017-12-19T07:44:00+01:00   2017-12-19T17:21:16+01:00
Temp 73.18033009708738 peak 217.0091594597064 norm 0.6265958843364132
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2017-12-22T01:18:26+01:00
Temp 41.25354131187325 peak 217.00562158794 norm 0.6902136683399531
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-02-17T18:18:27+01:00
Temp 58.04207115915322 peak 235.8389880170459 norm 0.7120912864599173
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-02-18T19:12:58+01:00
Temp 58.34400536653413 peak 234.861220208172 norm 0.7084931142223374
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-02-19T18:25:04+01:00
Temp 58.62524853997592 peak 246.4517606092901 norm 0.7428274462722462
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-02-20T18:36:37+01:00
Temp 58.91850227778887 peak 245.8056920522876 norm 0.7402258581172541
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-02-21T19:05:40+01:00
Temp 59.21529150553447 peak 249.787911981088 norm 0.7515463207653517
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-02-22T18:23:52+01:00
Temp 59.49776704975278 peak 252.4084638125304 norm 0.758785985702344
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-02-23T18:41:43+01:00
Temp 59.79229356394149 peak 253.3111732989325 norm 0.7608260596375216
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-02-26T09:46:35+01:00
Temp 60.55694259230681 peak 248.9573437917113 norm 0.7460358536677646
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-03-01T11:26:13+01:00
Temp 61.44983225836223 peak 244.5913071945384 norm 0.7309965027288973
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-03-04T21:17:02+01:00
Temp 62.44195470899375 peak 238.8047287008442 norm 0.7115925317933264
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-03-06T20:15:43+01:00
Temp 63.01140765461126 peak 232.606294373343 norm 0.6919482399726685
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-03-07T19:29:18+01:00
Temp 63.292950502914 peak 234.8215091323142 norm 0.6979534235486392
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-03-13T17:55:16+01:00
Temp 65.01947502375376 peak 228.0743092639248 norm 0.6744378961108312
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-03-14T22:01:51+01:00
Temp 65.36021207492874 peak 229.4105013005594 norm 0.6777062939825866
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-03-15T18:59:25+01:00
Temp 65.61427574862168 peak 229.7085598544244 norm 0.6780778739045036
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-03-16T17:28:40+01:00
Temp 65.88686202242839 peak 228.9962799627942 norm 0.6754318058419422
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-03-17T21:55:56+01:00
Temp 66.23177768599042 peak 226.4263956710665 norm 0.6671731087476513
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-03-22T18:41:57+01:00
Temp 67.64718919415749 peak 242.4026851767026 norm 0.711281351087089
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-03-24T19:10:39+01:00
Temp 68.23482802656184 peak 234.522084598226 norm 0.6869727807000807
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-03-26T21:47:17+02:00
Temp 68.8361912941714 peak 235.4174324210365 norm 0.688382859934055
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-03-30T16:18:57+02:00
Temp 69.93354002659815 peak 226.9681348675651 norm 0.6615535529625496
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-04-11T18:04:09+02:00
Temp 73.44583714528223 peak 229.2245575721957 norm 0.6613598116474546
tot diff 171180   2017-11-09T06:33:00+01:00    2017-11-07T07:00:00+01:00   2017-11-08T17:27:34+01:00
Temp 98.3078354948008 peak 738.7759551121773 norm 1.988855489151521
tot diff 84780   2017-11-13T06:40:00+01:00    2017-11-12T07:07:00+01:00   2017-11-12T15:30:08+01:00
Temp 68.86005708893606 peak 663.3731310792177 norm 1.939630479657839
tot diff 86520   2017-11-18T06:38:00+01:00    2017-11-17T06:36:00+01:00   2017-11-17T20:18:47+01:00
Temp 85.04725936199722 peak 799.2321364326664 norm 2.231262567045371
tot diff 518880   2017-11-25T06:52:00+01:00    2017-11-19T06:44:00+01:00   2017-11-23T11:42:19+01:00
Temp 95.77865970937404 peak 744.378475487249 norm 2.017675927030548
tot diff 86220   2017-11-30T07:32:00+01:00    2017-11-29T07:35:00+01:00   2017-11-29T21:19:07+01:00
Temp 83.69371537926236 peak 773.8471437480873 norm 2.168588405503018
tot diff 85500   2017-12-05T07:35:00+01:00    2017-12-04T07:50:00+01:00   2017-12-04T14:39:45+01:00
Temp 60.80512631578948 peak 793.9408389528367 norm 2.377387787729548
tot diff 173400   2017-12-07T07:45:00+01:00    2017-12-05T07:35:00+01:00   2017-12-05T12:20:37+01:00
Temp 45.89712508650519 peak 791.8759007288726 norm 2.482002934563872
tot diff 86160   2017-12-13T07:41:00+01:00    2017-12-12T07:45:00+01:00   2017-12-12T21:59:03+01:00
Temp 87.18364798050139 peak 738.9641224049003 norm 2.050777457354989
tot diff 86460   2017-12-14T07:42:00+01:00    2017-12-13T07:41:00+01:00   2017-12-13T22:30:09+01:00
Temp 89.09145350451077 peak 757.4254599892301 norm 2.090940870133734
tot diff 86580   2017-12-15T07:45:00+01:00    2017-12-14T07:42:00+01:00   2017-12-14T18:04:59+01:00
Temp 74.97479706629706 peak 713.9061377206993 norm 2.050719005761439
tot diff 86220   2017-12-16T07:42:00+01:00    2017-12-15T07:45:00+01:00   2017-12-15T20:22:45+01:00
Temp 82.12456158663883 peak 686.4979444411518 norm 1.932302558830234
tot diff 86520   2017-12-20T07:46:00+01:00    2017-12-19T07:44:00+01:00   2017-12-19T17:21:16+01:00
Temp 73.18033009708738 peak 759.7617308758466 norm 2.193748756174088
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2017-12-22T01:18:26+01:00
Temp 41.25354131187325 peak 757.8227974988322 norm 2.4103507051376
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-02-17T18:18:27+01:00
Temp 58.04207115915322 peak 852.4346835501794 norm 2.573837835449101
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-02-18T19:12:58+01:00
Temp 58.34400536653413 peak 853.7613608914891 norm 2.575495626074692
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-02-19T18:25:04+01:00
Temp 58.62524853997592 peak 917.1271793854213 norm 2.764302591653144
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-02-20T18:36:37+01:00
Temp 58.91850227778887 peak 917.3399036896196 norm 2.762502006053641
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-02-21T19:05:40+01:00
Temp 59.21529150553447 peak 942.1628065404408 norm 2.834720804548125
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-02-22T18:23:52+01:00
Temp 59.49776704975278 peak 954.5304989241002 norm 2.869493180098019
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-02-23T18:41:43+01:00
Temp 59.79229356394149 peak 960.0623610256922 norm 2.883569854549922
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-02-26T09:46:35+01:00
Temp 60.55694259230681 peak 934.5722051533741 norm 2.800577650238313
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-03-01T11:26:13+01:00
Temp 61.44983225836223 peak 920.0562505712758 norm 2.749721194901412
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-03-04T21:17:02+01:00
Temp 62.44195470899375 peak 879.9295382823657 norm 2.622022149027353
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-03-06T20:15:43+01:00
Temp 63.01140765461126 peak 837.1503229848131 norm 2.490322517464415
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-03-07T19:29:18+01:00
Temp 63.292950502914 peak 847.7053821522342 norm 2.519611068934827
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-03-13T17:55:16+01:00
Temp 65.01947502375376 peak 820.5075832242663 norm 2.426320658204387
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-03-14T22:01:51+01:00
Temp 65.36021207492874 peak 832.4712512941716 norm 2.459220494978466
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-03-15T18:59:25+01:00
Temp 65.61427574862168 peak 829.1939738548302 norm 2.447701936759499
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-03-16T17:28:40+01:00
Temp 65.88686202242839 peak 815.0269686899784 norm 2.403947947807698
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-03-17T21:55:56+01:00
Temp 66.23177768599042 peak 799.709976352887 norm 2.356372760510467
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-03-22T18:41:57+01:00
Temp 67.64718919415749 peak 893.160508660574 norm 2.620797755910266
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-03-24T19:10:39+01:00
Temp 68.23482802656184 peak 857.0677293443778 norm 2.510561861518031
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-03-26T21:47:17+02:00
Temp 68.8361912941714 peak 849.9825612530595 norm 2.485429478998809
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-03-30T16:18:57+02:00
Temp 69.93354002659815 peak 809.3858421341336 norm 2.359150899723678
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-04-11T18:04:09+02:00
Temp 73.44583714528223 peak 829.1849688901491 norm 2.392368516943787
tot diff 171180   2017-11-09T06:33:00+01:00    2017-11-07T07:00:00+01:00   2017-11-08T17:27:34+01:00
Temp 98.3078354948008 peak 0.3156517706638187 norm 0.0008497647390944236
tot diff 84780   2017-11-13T06:40:00+01:00    2017-11-12T07:07:00+01:00   2017-11-12T15:30:08+01:00
Temp 68.86005708893606 peak 0.2937728612960976 norm 0.0008589597153855189
tot diff 86520   2017-11-18T06:38:00+01:00    2017-11-17T06:36:00+01:00   2017-11-17T20:18:47+01:00
Temp 85.04725936199722 peak 0.3454255849712654 norm 0.0009643445781425574
tot diff 518880   2017-11-25T06:52:00+01:00    2017-11-19T06:44:00+01:00   2017-11-23T11:42:19+01:00
Temp 95.77865970937404 peak 0.3330661587350751 norm 0.0009027928570186176
tot diff 86220   2017-11-30T07:32:00+01:00    2017-11-29T07:35:00+01:00   2017-11-29T21:19:07+01:00
Temp 83.69371537926236 peak 0.2360089584533551 norm 0.0006613790527388689
tot diff 85500   2017-12-05T07:35:00+01:00    2017-12-04T07:50:00+01:00   2017-12-04T14:39:45+01:00
Temp 60.80512631578948 peak 0.2419659658580049 norm 0.0007245463440773799
tot diff 173400   2017-12-07T07:45:00+01:00    2017-12-05T07:35:00+01:00   2017-12-05T12:20:37+01:00
Temp 45.89712508650519 peak 0.240435169531754 norm 0.0007536039369311459
tot diff 86160   2017-12-13T07:41:00+01:00    2017-12-12T07:45:00+01:00   2017-12-12T21:59:03+01:00
Temp 87.18364798050139 peak 0.2198524965217226 norm 0.0006101359053030192
tot diff 86460   2017-12-14T07:42:00+01:00    2017-12-13T07:41:00+01:00   2017-12-13T22:30:09+01:00
Temp 89.09145350451077 peak 0.2288293130553534 norm 0.0006317038285970326
tot diff 86580   2017-12-15T07:45:00+01:00    2017-12-14T07:42:00+01:00   2017-12-14T18:04:59+01:00
Temp 74.97479706629706 peak 0.217900953835728 norm 0.0006259277008475522
tot diff 86220   2017-12-16T07:42:00+01:00    2017-12-15T07:45:00+01:00   2017-12-15T20:22:45+01:00
Temp 82.12456158663883 peak 0.1503207381427924 norm 0.0004231114591246482
tot diff 86520   2017-12-20T07:46:00+01:00    2017-12-19T07:44:00+01:00   2017-12-19T17:21:16+01:00
Temp 73.18033009708738 peak 0.1622721196992897 norm 0.0004685472382791298
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2017-12-22T01:18:26+01:00
Temp 41.25354131187325 peak 0.1613966941531701 norm 0.00051334248170275
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-02-17T18:18:27+01:00
Temp 58.04207115915322 peak 0.1519729807227832 norm 0.0004588666032700798
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-02-18T19:12:58+01:00
Temp 58.34400536653413 peak 0.153636081513809 norm 0.0004634656404840052
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-02-19T18:25:04+01:00
Temp 58.62524853997592 peak 0.1658627644541513 norm 0.0004999250703120681
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-02-20T18:36:37+01:00
Temp 58.91850227778887 peak 0.16726297987167 norm 0.0005037002266831909
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-02-21T19:05:40+01:00
Temp 59.21529150553447 peak 0.1702751299217176 norm 0.0005123132116184948
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-02-22T18:23:52+01:00
Temp 59.49776704975278 peak 0.1721913218680174 norm 0.0005176385923019391
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-02-23T18:41:43+01:00
Temp 59.79229356394149 peak 0.1728023255392353 norm 0.0005190158441256989
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-02-26T09:46:35+01:00
Temp 60.55694259230681 peak 0.1699640802422223 norm 0.0005093213791775053
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-03-01T11:26:13+01:00
Temp 61.44983225836223 peak 0.1683157670935782 norm 0.0005030360175542845
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-03-04T21:17:02+01:00
Temp 62.44195470899375 peak 0.1609667718301883 norm 0.0004796502704296638
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-03-06T20:15:43+01:00
Temp 63.01140765461126 peak 0.155946231067982 norm 0.0004639028380920181
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-03-07T19:29:18+01:00
Temp 63.292950502914 peak 0.1575761287654471 norm 0.0004683591334872756
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-03-13T17:55:16+01:00
Temp 65.01947502375376 peak 0.1542088700265075 norm 0.0004560106142509626
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-03-14T22:01:51+01:00
Temp 65.36021207492874 peak 0.1574943337701327 norm 0.0004652572600535655
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-03-15T18:59:25+01:00
Temp 65.61427574862168 peak 0.1573456420773231 norm 0.0004644694064319835
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-03-16T17:28:40+01:00
Temp 65.88686202242839 peak 0.1540871661662128 norm 0.0004544849938943203
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-03-17T21:55:56+01:00
Temp 66.23177768599042 peak 0.1542531269493453 norm 0.0004545121072825144
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-03-22T18:41:57+01:00
Temp 67.64718919415749 peak 0.1716222596302931 norm 0.0005035905960260642
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-03-24T19:10:39+01:00
Temp 68.23482802656184 peak 0.1678832053427238 norm 0.0004917711379067538
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-03-26T21:47:17+02:00
Temp 68.8361912941714 peak 0.1651640919999399 norm 0.0004829554414899408
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-03-30T16:18:57+02:00
Temp 69.93354002659815 peak 0.1559328159354264 norm 0.0004545039261380404
tot diff 26437920   2018-10-22T08:38:00+02:00    2017-12-20T07:46:00+01:00   2018-04-11T18:04:09+02:00
Temp 73.44583714528223 peak 0.1591476212920055 norm 0.0004591734932618242
INFO: The integer column `Timestamp` has been automatically determined to be continuous. To overwrite this behavior add a `+ scale_x/y_discrete()` call to the plotting chain. Choose `x` or `y` depending on which axis this column refers to. Or apply a `factor` to the column name in the `aes` call, i.e. `aes(..., factor("Timestamp"), ...)`.
[INFO] TeXDaemon ready for input.
shellCmd: command -v lualatex
shellCmd: lualatex -output-directory /home/basti/phd/Figs/behavior_over_time/mapSeptemTempToFePeak /home/basti/phd/Figs/behavior_over_time/mapSeptemTempToFePeak//time_vs_peak_norm_by_temp.tex
Generated: /home/basti/phd/Figs/behavior_over_time/mapSeptemTempToFePeak//time_vs_peak_norm_by_temp.pdf
[INFO] Writing CSV file: /home/basti/phd/Figs/behavior_over_time/mapSeptemTempToFePeak//time_vs_peak_norm_by_temp.pdf.csv
[INFO] TeXDaemon ready for input.
shellCmd: command -v lualatex
shellCmd: lualatex -output-directory /home/basti/phd/Figs/behavior_over_time/mapSeptemTempToFePeak /home/basti/phd/Figs/behavior_over_time/mapSeptemTempToFePeak//time_vs_peak_temp_normed_comparison.tex
Generated: /home/basti/phd/Figs/behavior_over_time/mapSeptemTempToFePeak//time_vs_peak_temp_normed_comparison.pdf
[INFO] Writing CSV file: /home/basti/phd/Figs/behavior_over_time/mapSeptemTempToFePeak//time_vs_peak_temp_normed_comparison.pdf.csv
:end:

Note that the plot that we include in the thesis from the following created:
- [[~/phd/Figs/behavior_over_time/mapSeptemTempToFePeak/time_vs_peak_pos.pdf]]
- [[~/phd/Figs/behavior_over_time/mapSeptemTempToFePeak/time_vs_peak_pos_feFadc.pdf]]
- [[~/phd/Figs/behavior_over_time/mapSeptemTempToFePeak/time_vs_peak_norm_by_temp.pdf]]
- [[~/phd/Figs/behavior_over_time/mapSeptemTempToFePeak/time_vs_peak_pos_fePixel.pdf]]
- [[~/phd/Figs/behavior_over_time/mapSeptemTempToFePeak/time_vs_peak_temp_normed_comparison.pdf]]
- [[~/phd/Figs/behavior_over_time/mapSeptemTempToFePeak/time_vs_peak_pos_feCharge.pdf]]
is actually one of the ones that does not include the septemboard
temperatures from the shift forms. That's because of the much more in
depth plot below of course!
  

**** Generate plot of ambient CAST temp against 55Fe peaks      :extended:

First we run the CAST log reader to get the temperature data as a
simple CSV file (by default just written to
~/tmp/temperatures_cast.csv~):
#+begin_src sh
cd $TPA/LogReader # <- directory of TimepixAnalysis
./cast_log_reader sc -p ../resources/LogFiles/SClogs -s Version.idx
#+end_src
Note that this requires the slow control files for the relevant times
to be present in the ~SCLogs~ directory!


- [ ] *MOVE CODE OVER TO TPA, DEDUCTION TO STATUS, MAYBE KEEP HERE AS
  WELL?*
  -> well, the interesting stuff will go straight into the thesis, so
  there is less 

#+begin_src nim :tangle code/correlation_ambient_temps_fe55_peaks.nim
import std / [strutils, sequtils, times, stats, strformat]
import os except FileInfo
import ggplotnim, nimhdf5
import ingrid / tos_helpers
import ingrid / ingrid_types

type
  FeFileKind = enum
    fePixel, feCharge, feFadc

let UseTex = getEnv("USE_TEX", "false").parseBool

const Peak = "μ"
let PeakNorm = if UseTex: r"$μ/μ_{\text{max}}$" else: "μ/μ_max"
const TempPeak = "(μ/T) / max"

proc readFePeaks(files: seq[string], feKind: FeFileKind = fePixel): DataFrame =
  const kalphaPix = 10
  const kalphaCharge = 4
  const parPrefix = "p"
  const dateStr = "yyyy-MM-dd'.'HH:mm:ss" # example: 2017-12-04.13:39:45
  var dset: string
  var kalphaIdx: int
  case feKind
  of fePixel:
    kalphaIdx = kalphaPix
    dset = "FeSpectrum"
  of feCharge:
    kalphaIdx = kalphaCharge
    dset = "FeSpectrumCharge"
  of feFadc:
    kalphaIdx = kalphaCharge
    dset = "FeSpectrumFadcPlot" # raw dataset is `minvals` instead of `FeSpectrumFadc`

  var h5files = files.mapIt(H5open(it, "r"))
  var fileInfos = newSeq[FileInfo]()
  for h5f in mitems(h5files):
    let fi = h5f.getFileInfo()
    fileInfos.add fi
  var
    peakSeq = newSeq[float]()
    dateSeq = newSeq[float]()
  for (h5f, fi) in zip(h5files, fileInfos):
    for r in fi.runs:
      let group = h5f[(recoBase() & $r).grp_str]
      let chpGrpName = if feKind in {fePixel, feCharge}: group.name / "chip_3"
                       else: group.name / "fadc"
      peakSeq.add h5f[(chpGrpName / dset).dset_str].attrs[
        parPrefix & $kalphaIdx, float
      ]
      dateSeq.add parseTime(group.attrs["dateTime", string],
                            dateStr,
                            utc()).toUnix.float
  result = toDf({ Peak : peakSeq,
                  "Timestamp" : dateSeq })
    .arrange("Timestamp", SortOrder.Ascending)
    .mutate(f{float: PeakNorm ~ idx(Peak) / max(col(Peak))},
            f{"Type" <- $feKind})

proc toDf[T: object](data: seq[T]): DataFrame =
  ## Converts a seq of objects that (may only contain scalar fields) to a DF
  result = newDataFrame()
  for i, d in data:
    for field, val in fieldPairs(d):
      if field notin result:
        result[field] = newColumn(toColKind(type(val)), data.len)
      result[field, i] = val
    
proc readGasGainSliceData(files: seq[string]): DataFrame =     
  result = newDataFrame()
  for f in files:
    let h5f = H5file(f, "r")
    let fInfo = h5f.getFileInfo()
    for r in fInfo.runs:
      for c in fInfo.chips:
        let group = recoDataChipBase(r) & $c
        var gainSlicesDf = h5f[group & "/gasGainSlices90", GasGainIntervalResult].toDf
        gainSlicesDf["Chip"] = c
        gainSlicesDf["Run"] = r
        gainSlicesDf["File"] = f
        result.add gainSlicesDf
    discard h5f.close()

const periods = [("2017-10-30", "2017-12-23"),
                 ("2018-02-15", "2018-04-22"),
                 ("2018-10-19", "2018-12-21")]

proc toPeriod(x: int): string =
  let date = x.fromUnix()
  for p in periods:
    let t0 = p[0].parseTime("YYYY-MM-dd", utc())
    let t1 = p[1].parseTime("YYYY-MM-dd", utc())
    if date >= t0 and date <= t1: return p[0]
  
proc mapToPeriod(df: DataFrame, timeCol: string): DataFrame =
  result = df.mutate(f{int -> string: "RunPeriod" ~ toPeriod(idx(timeCol))})
    .filter(f{string -> bool: `RunPeriod`.len > 0})

proc readSeptemTemps(): DataFrame =
  const TempFile = "/home/basti/CastData/ExternCode/TimepixAnalysis/resources/cast_2017_2018_temperatures.csv"
  const OrgFormat = "'<'yyyy-MM-dd ddd H:mm'>'"
  result = toDf(readCsv(TempFile))
    .filter(f{c"Temp / °" != "-"})
  result["Timestamp"] = result["Date"].toTensor(string).map_inline(parseTime(x, OrgFormat, utc()).toUnix)

proc readCastTemps(): DataFrame =
  result = readCsv("/tmp/temperatures_cast.csv")
    #.filter(f{float: `Time` >= t0 and `Time` <= t1})
    .group_by("Temperature")
    .mutate(f{"TempNorm" ~ `TempVal` / max(col("TempVal"))})
    .filter(f{`Temperature` != "T_ext"})

proc toPeriod(v: float): string =
  result = v.int.fromUnix.format("dd/MM/YYYY")

proc keepEvery(df: DataFrame, num: int): DataFrame =
  ## Keeps only every `num` row of the data frame
  result = df
  result["idxMod"] = toSeq(0 ..< df.len)
  result = result.filter(f{int -> bool: `idxMod` mod num == 0})
    
proc plotCorrelationPerPeriod(df: DataFrame, kind: FeFileKind, gainDf, dfCastTemp, dfTemp: DataFrame,
                              period, outpath = "/tmp") =
  let t0 = df["Timestamp", float].min
  let t1 = df["Timestamp", float].max

  let dfCastTemp = dfCastTemp
    .keepEvery(50)
    .filter(f{float: `Time` >= t0 and `Time` <= t1})  
  let dfTemp = dfTemp
    .filter(f{float: `Timestamp` >= t0 and `Timestamp` <= t1})
  var gainDf = gainDf
    .filter(f{float: `tStart` >= t0 and `tStart` <= t1})
  echo gainDf

  ## XXX: combine point like data for legend?
  # let dfC = bind_rows([("Fe55", df), ("SeptemTemp", dfTemp)], "Type")
  var plt = ggplot(df, aes("Timestamp", PeakNorm)) +
    geom_line(data = dfCastTemp, aes = aes("Time", "TempNorm", color = "Temperature")) +
    geom_point() +
    scale_x_continuous(labels = toPeriod)

  if dfTemp.len > 0: # only if septemboard data available in this period
    plt = plt + geom_point(data = dfTemp, aes = aes("Timestamp", f{idx("Temp / °") / max(col("Temp / °"))}), color = "blue")

  block AllChips:
    plt + geom_point(data = gainDf, aes = aes("tStart", f{float: `G` / max(col("G"))}, color = "Chip"), alpha = 0.7, size = 1.5) +
      ggtitle("Correlation between temperatures (Septem = blue points) & 55Fe position " & $kind &
        " (black) and gas gains by chip", titleFont = font(11.0)) + 
      ggsave(&"{outpath}/correlation_{kind}_all_chips_gasgain_period_{period}.pdf",
              width = 1000, height = 600,
              useTeX = UseTeX, standalone = UseTeX)                                                                              

  block CenterChip:
    gainDf = gainDf.filter(f{`Chip` == 3})
    plt + geom_point(data = gainDf, aes = aes("tStart", f{float: `G` / max(col("G"))}), color = "purple", alpha = 0.7, size = 1.5) + 
      ggtitle("Correlation between temperatures (Septem = blue points) & 55Fe position " & $kind &
        " (black) and gas gains (chip3) in purple", titleFont = font(11.0)) +     
      ggsave(&"{outpath}/correlation_{kind}_period_{period}.pdf", width = 1000, height = 600,
             useTeX = UseTeX, standalone = UseTeX)                                   

proc plotCorrelation(files: seq[string], kind: FeFileKind, gainDf, dfCastTemp, dfTemp: DataFrame,
                     outpath = "/tmp") =
  let df = readFePeaks(files, feCharge)
    .mapToPeriod("Timestamp")

  for (tup, subDf) in groups(df.group_by("RunPeriod")):
    plotCorrelationPerPeriod(subDf, kind, gainDf, dfCastTemp, dfTemp, tup[0][1].toStr, outpath)

proc plotTempVsGain(dfCastTemp, gainDf: DataFrame, outpath: string) =    
  ## Now let's plot the actual gas gain against the temperature in each slice.
  ## Only for the center chip.
  ## 1. compute mean temperature within time associated with each gain value
  # dfCastTemp
  # gainDf
  ## NOTE: We do not compute the mean temperature associated with the
  proc mapGainToTemp(gainDf, dfCastTemp: DataFrame, period: string): DataFrame =
    let t0G = gainDf["tStart", int].min
    let t1G = gainDf["tStop", int].max
    # filter temperature data to relevant range
    echo dfCastTemp.isNil
    echo dfCastTemp
    let dfF = dfCastTemp
       .filter(f{int: `Time` >= t0G and `Time` <= t1G},
               f{string -> bool: `Temperature` == "T_amb"})
      
    var cT: RunningStat    
    let ambT = dfF["TempVal", float]
    let time = dfF["Time", int]    
  
    var j = 0
    let gDf = gainDf.filter(f{int -> bool: `Chip` == 3})
    var temps = newSeq[float](gDf.len)
    ## we now walk all temperatures and accumulate them in a `RunningStat` to compute
    ## the mean within `tStart` and `tStop` (by `tStart` of the next slice).
    ## First and last are just copied from ambient temperature values.
    temps[0] = ambT[0]    
    for i in 1 ..< gDf.high:
      while time[j] < gDf["tStart", int][i]:
        cT.push ambT[j]
        inc j
      temps[i] = cT.mean
      cT.clear()
    temps[gDf.high] = ambT[ambT.len - 1]
    let gains = gDf["G", float]
    result = toDf(temps, gains, period)

  var dfGT = newDataFrame()
  for (tup, subDf) in groups(gainDf.groupBy("RunPeriod")):
    dfGT.add mapGainToTemp(subDf, dfCastTemp, tup[0][1].toStr)
  echo dfGT
  echo dfGT.tail(100)
  ggplot(dfGT.filter(f{`temps` > 0.0}), aes("temps", "gains", color = "period")) +
    geom_point() +
    ggtitle("Gas gain (90 min slices) vs ambient T at CAST (center chip)") +
    xlab("Temperature [°C]") + ylab("Gas gain") + 
    ggsave(&"{outpath}/gain_vs_temp_center_chip.pdf",
           width = 600, height = 360,
           useTeX = UseTeX, standalone = UseTeX)            
      
proc main(calibFiles: seq[string], dataFiles: seq[string] = @[],
          outpath = "/tmp/") =
  ## NOTE: this file needs the CSV file containing the temperature data from the slow control
  ## CAST log files, which is written running the `cast_log_reader` on the slow control log
  ## directory!
  var gainDf = newDataFrame()
  if dataFiles.len > 0:
    gainDf = readGasGainSliceData(dataFiles)
      .mapToPeriod("tStart")
    ## Make a plot of the raw gas gains of all chips
    ggplot(gainDf, aes("tStart", "G", color = "Chip")) +
      geom_point(size = 2.0) +
      ggtitle("Raw gas gain values in 90 min bins for all chips") +
      ggsave(&"{outpath}/raw_gas_gain.pdf",
             width = 600, height = 360, 
             useTeX = UseTeX, standalone = UseTeX)              

  let dfCastTemp = readCastTemps()
  let dfTemp = readSeptemTemps()
  plotTempVsGain(dfCastTemp, gainDf, outpath)
      
  plotCorrelation(calibFiles, fePixel,  gainDf, dfCastTemp, dfTemp, outpath)
  plotCorrelation(calibFiles, feCharge, gainDf, dfCastTemp, dfTemp, outpath)

when isMainModule:
  import cligen
  dispatch main
#+end_src

Running the above as:
#+begin_src sh
USE_TEX=true WRITE_PLOT_CSV=true code/correlation_ambient_temps_fe55_peaks \
    ~/CastData/data/CalibrationRuns2017_Reco.h5 \
    ~/CastData/data/CalibrationRuns2018_Reco.h5 \
    -d ~/CastData/data/DataRuns2017_Reco.h5 \
    -d ~/CastData/data/DataRuns2018_Reco.h5 \
    --outpath ~/phd/Figs/behavior_over_time/
#+end_src

which generates the following plots:
- [[~/phd/Figs/behavior_over_time/raw_gas_gain.pdf]]
- [[~/phd/Figs/behavior_over_time/gain_vs_temp_center_chip.pdf]]
- [[~/phd/Figs/behavior_over_time/correlation_fePixel_period_2018-10-19.pdf]]
- [[~/phd/Figs/behavior_over_time/correlation_fePixel_period_2018-02-15.pdf]]
- [[~/phd/Figs/behavior_over_time/correlation_fePixel_period_2017-10-30.pdf]]
- [[~/phd/Figs/behavior_over_time/correlation_fePixel_all_chips_gasgain_period_2018-10-19.pdf]]
- [[~/phd/Figs/behavior_over_time/correlation_fePixel_all_chips_gasgain_period_2018-02-15.pdf]]
- [[~/phd/Figs/behavior_over_time/correlation_fePixel_all_chips_gasgain_period_2017-10-30.pdf]]
- [[~/phd/Figs/behavior_over_time/correlation_feCharge_period_2018-10-19.pdf]]
- [[~/phd/Figs/behavior_over_time/correlation_feCharge_period_2018-02-15.pdf]]
- [[~/phd/Figs/behavior_over_time/correlation_feCharge_period_2017-10-30.pdf]]
- [[~/phd/Figs/behavior_over_time/correlation_feCharge_all_chips_gasgain_period_2018-10-19.pdf]]
- [[~/phd/Figs/behavior_over_time/correlation_feCharge_all_chips_gasgain_period_2018-02-15.pdf]]
- [[~/phd/Figs/behavior_over_time/correlation_feCharge_all_chips_gasgain_period_2017-10-30.pdf]]

of which we insert only one of them (Run 3) correlation of gas gains
and temperature against the time. That is mainly because in that
period there was no worry about power supply effects anymore. It
should be noted that the apparent inverse correlation is not apparent
in the Run-2 data of 2017. Generally the water cooling was working
better at those times, which may be relevant. I don't want to
introduce even more speculation into the main section and as the
scatter plot of gas gain and temperature clearly shows an inverse
correlation for a large chunk of the data, the existing text is justified.

Also, we chose to include the ~fePixel~ version and not the ~feCharge~
version as the link between gas gain of the center chip and the \cefe
charge spectrum is much more direct, offering less additional information.

***** Initial interpretation upon seeing the correlation plot

Note: this text was written after I created the first version of the
above plot for the first time.

The first thing that jumps out is that the (normalized) temperature
recovered from the shift forms of the Septem board sensor is strongly
correlated with the ambient CAST temperature (~T_amb~). This is
interesting and reassuring as it partially explains why the
temperatures were higher on the Septemboard during Run-3 than Run-2:
it was hotter in the hall in Run-3 (not shown in this plot, see full
version of all data).

*Next paragraph was written before gas gain information was in the plot*
However, the peak position of the 55Fe data is either uncorrelated
*or* actually inversely proportional to the temperatures. When the
temperatures are lower the peak position is higher and vice versa. The
data is imo not good enough to make final statements about this, but
_something_ might be going on there. This is something that one might
want to investigate in the future!

*UPDATE*: Having added the gas gain slice information to the plot now,
it seems pretty evident that there *is* an inverse correlation between
the gas gain and the temperature!

- ideal gas, temp + constant pressure, lower density, higher mobility
- less visible in old detector, as absolute temperatures under grid
  much lower, therefore on a "less steep" part of the exponential that
  makes up the gas gain temperature dependence!

PDG 2016 page 467 says: (detectors at accelerators chapter)
#+begin_quote
For different temperatures and pressures, the mobility can be scaled
inversely with the density assuming an ideal gas law
#+end_quote
This *should* imply:
- A higher temperature in the CAST hall, while keeping the same
  pressure in the detector, means a lower gas density according to the
  ideal gas law, p·V = nRT ⇔ n₁RT₁ = n₂RT₂ ⇔ T₁/T₂ = n₁/n₂ ⇔ T₁ > T₂ ⇒
  n₁ < n₂. n ∝ ρ.
- A lower density according to the quote then implies a _higher_
  mobility.
- The 'mobility' should be proportional to the mean free path.
  - [ ] *CHECK THIS*
- *Assuming* the mean free path is _long enough_ in 'both'
  temperatures as to have enough kinetic energy to cause an ionization
  ~typically, *then* a _higher mobility_ means *less* gas gain, as
  there will be *less* collisions!
  *However* if the mean free path would lead to typical collisions
  that do *not* have enough energy to cause ionization, then the gas
  gain would be *lower* for a *lower* mobility, as the gas would then
  act as a dampener. But the former should always be true in the
  amplification region I guess.

This explanation is not meant as a definitive statement about the
origins of the gas gain variations in the Septemboard detector
data. *However*, it clearly motivates the need for an in depth study
of the behavior of these detectors for different gas temperatures at
constant pressures and more precise logging of temperatures in future
detectors. Further, a significantly improved cooling setup (to more
closely approach a region where temperature changes have a smaller
relative impact), or theoretically even a temperature controlled setup
with known inlet gas temperatures might be useful. This behavior is
one of the most problematic from a data analysis point of view and
thus it should be taken seriously for future endeavors!


- [X] *INSERT THE PIXEL TEMP GASGAIN PLOT INTO THESIS ROTATED FULL
  PAGE?*

- [X] *ADD VERSION OF PLOTS THAT SHOW FULL DATA WITHOUT CUT TO RUN-3*

- [X] *ADD A SIMILAR PLOT, BUT NOT USING 55FE POSITIONS, BUT GAS GAIN
  SLICES*
  -> done by *adding* Gas gain data as well for all chips!

*** TODO Section covering our plots of logL variables over time? :noexport:

We have those plots in ~statusAndProgress~! Maybe add them here?
Section: ~Time behavior of logL variables~

*** Gas gain binning
:PROPERTIES:
:CUSTOM_ID: sec:calib:gas_gain_time_binning
:END:

Motivated by the strong variation seen over timescales much shorter
than the typical length of a background run, the gas gain needs to be
computed in time slices of a fixed length. This is naturally a
trade-off between assigning accurate gas gains to a time slice and
acquiring enough statistics to compute said gas gain correctly.

To determine a suitable time window the gas gain was computed for a
fixed set of different time intervals and figures similar to
fig. [[fig:calib:total_charge_over_time]] were considered not only for the
median charge, but also different geometric cluster
distributions. Further, by applying the energy calibration based on each
different set of time intervals to the background data (as will be
explained in sec. [[#sec:calib:final_energy_calibration]]), the histograms
of the median cluster energy in the background data was studied. The
ideal time interval is one in which the resulting median energy
distribution has low variance and is unimodal approaching a normal
distribution, (background in all slices is equivalent over large enough
times) while at the same time provides enough statistics in the \cefe
spectrum of the slice to perform a good fit.

Unimodality can be quantitatively checked using different goodness of
fit tests (Anderson-Darling, Cramér-von Mises,
Kolmogorov-Smirnov). See appendix
[[#sec:appendix:choice_gas_gain_binning]] for a comparison and further
plots comparing the intervals. The goodness of fit tests tend to favor
shorter intervals, in particular $\SI{45}{min}$. However, looking at
fig. [[fig:calib:median_energy_ridgeline_30_10_2017]] shows that the
variance grows significantly below $\SI{90}{min}$.

#+CAPTION: Ridgeline plot of a kernel density estimation (bandwidth based on Silverman's rule of thumb)
#+CAPTION: of the median cluster energies split by the used time intervals. The underlying data is the background data
#+CAPTION: from Oct 2017 to Dec 2017. The overlap of the individual ridges is for
#+CAPTION: easier visual comparison and a KDE was selected over a histogram due to strong
#+CAPTION: binning dependence of the resulting histograms. For the dataset and binning the $\SI{90}{min}$
#+CAPTION: interval (olive) strikes an acceptable balance between unimodality and
#+CAPTION: variance. 
#+NAME: fig:calib:median_energy_ridgeline_30_10_2017
[[~/phd/Figs/behavior_over_time/optimizeGasGainSliceTime/medianEnergy_kde_ridges_30_10_2017.pdf]]

As the different ways to look at the data are not entirely conclusive,
the choice was made to choose an interval length that is not too long,
while still providing enough statistics for the \cefe spectra. As such
$\SI{90}{min}$ was selected as the final interval time. Of course no
data taking run is a perfect multiple of $\SI{90}{min}$. The last
slice smaller than the time interval is either added to the second to
last slice (making it longer than $\SI{90}{min}$) if it is smaller
than some fixed interval or will be kept as a single shorter
interval. This is controlled by an additional parameter that is set to
$\SI{25}{min}$ by default. [fn:configuration_slicing]

[fn:configuration_slicing] Both the gas gain time slicing and the
minimum length for the last slice in a run to be kept as a shorter
slice can be configured from the TPA configuration file, via the
~gasGainInterval~ and ~minimumGasGainInterval~ fields, respectively.
  
[fn:optimize_gas_gain_window] The code for this optimization can be
found here *GITHUB/TPA/Tools/optimizaGasGainSliceTime.nim*. Further see
the ~statusAndProgress~ notes on this!

**** TODOs for this section [/]                                 :noexport:

- [X] *REWORD ABOVE DUE TO CHANGE IN PREVIOUS SECTION*
  -> And in general after a full read probably rephrase the why
  selection was chosen.

- [X] *INSERT THE PLOT SHOWING THE DIFFERENT DISTRIBUTIONS?*

- [X] *INSERT FIG OF MEDIAN ENERGY OF BACKGROUND DATA USING 90 MIN*
  -> Must be in the section that actually explains the energy calibration!
- [X] *INSERT FIG OF RIDGELINE FOR MEDIAN ENERGY HISTO FOR DIFFERENT 
  TIMES*
  -> to appendix
- [X] *NOTE: The ~optimizeGasGainSliceTime.nim~ tool writes the CSV
  files to the ~TPA/Tools/out/~ directory! So we needn't rerun the
  whole thing and can just generate the plot we want to show here!
  -> Also part of ~resources~ in this repo now for reference.

- [X] *EXPLAIN ~minimumGasGainSlice~*: how we deal with last pieces in
  a run!
  #+begin_src
  # minutes the gas gain interval has to be long at least. This comes into play
  # at the end of a run `(tRun mod gasGainInterval) = lastSliceLength`. If the
  # remaining time `lastSliceLength` is less than `minimumGasGainInterval`, the
  # slice will be absorbed into the second to last, making that longer.
  minimumGasGainInterval = 25
  #+end_src


**** Ridgeline plot of the median energies                      :extended:

Finally, let's recreate the plot of the histograms of the median
energies in the time slices as a ridgeline plot to better explain why
we chose 90 min instead of anything else that we tested.

First, if one is not happy with using the provided CSV files that
contain the precomputed medians of the cluster energy in the
~phd/resources/optimize_gas_gain_length~ directory, run the
~optimizeGasGainSliceTime~ tool:
#+begin_src sh
cd $TPA/Tools
nim c -d:release -d:StartHue=285 optimizeGasGainSliceTime.nim
./optimizeGasGainSliceTime --path <PathToDataRunsH5> --genCsv
#+end_src
Note that this takes some time, as the fitting of all gas gains in
each time slice is somewhat time consuming (for the 90 min case it
takes maybe 15 minutes for all data; more for shorter time scales,
less for longer).
(It may be necessary to modify the code as I may forget to change the
input data file paths; they are hardcoded as of right now)

- [ ] *CHANGE CODE TO NOT USE HARDCODED PATHS, THEN ADJUST SCRIPT
  ABOVE*
  -> Change code to not point to hardcoded config file!

This generates CSV files in an ~out~ directory from wherever you
actually ran the code from.

To produce the plots as used in the thesis (and many more), just run:
#+begin_src sh :dir ~/CastData/ExternCode/TimepixAnalysis/Tools/optimizeGasGainSliceTime/
USE_TEX=true WIDTH=800 HEIGHT=480 LINE_WIDTH=1.0 WRITE_PLOT_CSV=true \
             ./optimizeGasGainSliceTime \
             --path out \
             --plot \
             --outpath ~/phd/Figs/behavior_over_time/optimizeGasGainSliceTime/
#+end_src

Among others this generates a plot of the scores for different
goodness of fit tests for each interval setting and period of data
taking. They all use the mean of the full data and the variance of the
full data (that's clearly not ideal, but it should be passable to have
something comparable).

The resulting plot is 
[[~/phd/Figs/behavior_over_time/optimizeGasGainSliceTime/gofs_for_different_binnings.pdf]]
At least for the 2017 data set the 45 minute interval seems to be the
clear winner. However, in beginning 2018 it's one of the worst at
least for Anderson-Darling and Cramér-von-Mises (which are probably
the most interesting tests to look at).

The 90 min result is sort of mostly in the middle. The big advantage
of it though is that it definitely captures enough statistics, which
is extremely important for the \cefe spectrum, as the data rate is
very low there. As much statistics as possible is needed to get a nice
fit there.

At the same time comparing the ridgeline plot / histograms it is also
evident that the variance itself is quite a bit smaller in the 90 min
case, which is another important aspect.

In addition all these plots for the distribution of properties / the
energy are created:
- [[~/phd/Figs/behavior_over_time/optimizeGasGainSliceTime/medianEnergy_intervals.pdf]]
- [[~/phd/Figs/behavior_over_time/optimizeGasGainSliceTime/medianEnergy_vs_time_30.pdf]]
- [[~/phd/Figs/behavior_over_time/optimizeGasGainSliceTime/medianEnergy_vs_time_45.pdf]]
- [[~/phd/Figs/behavior_over_time/optimizeGasGainSliceTime/medianEnergy_vs_time_60.pdf]]
- [[~/phd/Figs/behavior_over_time/optimizeGasGainSliceTime/medianEnergy_vs_time_90.pdf]]
- [[~/phd/Figs/behavior_over_time/optimizeGasGainSliceTime/medianEnergy_vs_time_120.pdf]]
- [[~/phd/Figs/behavior_over_time/optimizeGasGainSliceTime/medianEnergy_vs_time_180.pdf]]
- [[~/phd/Figs/behavior_over_time/optimizeGasGainSliceTime/medianEnergy_vs_time_300.pdf]]
- [[~/phd/Figs/behavior_over_time/optimizeGasGainSliceTime/medianEnergy_kde_intervals.pdf]]
- [[~/phd/Figs/behavior_over_time/optimizeGasGainSliceTime/medianEnergy_ridges_17_02_2018.pdf]]
- [[~/phd/Figs/behavior_over_time/optimizeGasGainSliceTime/medianEnergy_ridges_21_10_2018.pdf]]
- [[~/phd/Figs/behavior_over_time/optimizeGasGainSliceTime/medianEnergy_ridges_30_10_2017.pdf]]
- [[~/phd/Figs/behavior_over_time/optimizeGasGainSliceTime/medianEnergy_intervals_17_02_2018.pdf]]
- [[~/phd/Figs/behavior_over_time/optimizeGasGainSliceTime/medianEnergy_intervals_21_10_2018.pdf]]
- [[~/phd/Figs/behavior_over_time/optimizeGasGainSliceTime/medianEnergy_intervals_30_10_2017.pdf]]
- [[~/phd/Figs/behavior_over_time/optimizeGasGainSliceTime/medianEnergy_kde_ridges_17_02_2018.pdf]]
- [[~/phd/Figs/behavior_over_time/optimizeGasGainSliceTime/medianEnergy_kde_ridges_21_10_2018.pdf]]
- [[~/phd/Figs/behavior_over_time/optimizeGasGainSliceTime/medianEnergy_kde_ridges_30_10_2017.pdf]]

** Energy calibration dependence on the gas gain [/]
:PROPERTIES:
:CUSTOM_ID: sec:calib:final_energy_calibration
:END:

With the final choice of time interval for the gas gain binning in
place, the actual calibration used for further analysis can be
presented. Fig. sref:fig:calib:gasgain_vs_energy_calib_comparison
shows the fits according to the linear relation as explained in
sec. [[#sec:calibration:energy]], eq. [[eq:gas_gain_vs_calib_factor]], for the
two data taking campaigns, Run-2 in
sref:fig:calib:gasgain_vs_energy_calib_2017 and Run-3 in
sref:fig:calib:gasgain_vs_energy_calib_2018. Each point represents one
$\SI{90}{min}$ slice of calibration data for which a \cefe spectrum
was fitted and then the linear energy calibration performed. The
resulting energy calibration factor is then plotted against the gas
gain computed for this time slice. The uncertainty of each point is
the uncertainty extracted from the fit parameter of the calibration
factor after error propagation. Calibrations need to be performed
separately for each data taking campaign as the detector behavior
changes due to different detector calibrations. These have an impact
on the ~ToT~ calibration as well as the activation threshold.

To compare the energy calibration using single gas gain values for
each full run against the method of time slicing them to
$\SI{90}{min}$ chunks, we will look at the median cluster energy in
each time slice for background and calibration data. This is the same
idea as behind fig. [[fig:calib:total_charge_over_time]] previously, just
for the energy instead of charge. This yields
fig. [[fig:calib:median_energy_binned_vs_unbinned]]. The points represent
background and crosses calibration data. Green is the unbinned (full
run) approach and purple the binned approach using $\SI{90}{min}$
slices. The effect is a slight, but visible reduction in variance. It
represents an important aspect of increasing data reliability and
lowering associated systematic uncertainties.

As such the final energy calibration works by first deducing the gas
gain at the time of an event, computing the calibration factor
required for this gas gain and finally using that factor to convert
the charge into energy.

#+begin_src subfigure
(figure ()
        (subfigure (linewidth 0.5) (caption "Run-2") (label "fig:calib:gasgain_vs_energy_calib_2017")
                   (includegraphics (list (cons 'width (linewidth 0.95)))
                                    "~/phd/Figs/energyCalibration/Run_2/gasgain_vs_energy_calibration_factors_4316675118229057340.pdf"))
        (subfigure (linewidth 0.5) (caption "Run-3") (label "fig:calib:gasgain_vs_energy_calib_2018")
                   (includegraphics (list (cons 'width (linewidth 0.95)))
                                    "~/phd/Figs/energyCalibration/Run_3/gasgain_vs_energy_calibration_factors_-4542617296427170283.pdf"))
        (caption
         "Fit of the gas gain values vs the calculated energy calibration factors
          for all calibration runs in Run-2 "
         (subref "fig:calib:gasgain_vs_energy_calib_2017")
         " and Run-3 "
         (subref "fig:calib:gasgain_vs_energy_calib_2018")
         ". Each run was further sliced into " ($ (SI 90 "min")) " parts for the gas
         gain determination and \\cefe fits.")
        (label "fig:calib:gasgain_vs_energy_calib_comparison"))
#+end_src

#+CAPTION: Median of the cluster energy after calibration using two different approaches.
#+CAPTION: Green corresponds to calculating the energy based on a single gas gain for each
#+CAPTION: run and purple implies calculation based on $\SI{90}{min}$ time intervals for the
#+CAPTION: gas gain. Both cases use the same $\SI{90}{min}$ intervals to compute a local, temporal
#+CAPTION: median of all clusters. Each subplot corresponds to a data taking period with
#+CAPTION: significant times between for clarity. The energies from unbinned gas gains has a
#+CAPTION: wider distribution than the binned data. The latter approaches a flat distribution
#+CAPTION: of the background energies (points) better than the former. The impact for the
#+CAPTION: calibration data (crosses) is much smaller, as they are not much longer than the
#+CAPTION: $\SI{90}{min}$ binning anyway.
#+NAME: fig:calib:median_energy_binned_vs_unbinned
#+ATTR_LATEX: :width 1\textwidth
[[~/phd/Figs/behavior_over_time/median_energy_binned_vs_unbinned.pdf]]


*** TODOs for this section [/]                                   :noexport:

- [X] *REWRITE ME WITH NEW STRUCTURE IN MIND!*
- [ ] *ADD TABLE OF ALL PARAMETERS & GAS GAIN SLICE DATA WITH ERRORS
  TO APPENDIX!*
- [ ] *HAVE TABLE OF FIT PARAMETERS SOMEWHERE FOR RUN-2 AND RUN-3!*
  -> Appendix.
  
- [X] *INSERT PLOT OF GAS GAIN VS CALIB?*
- [X] *REPHRASE WORD 'UNDERSTANDING' IN SECTION ABOVE*
- [X] *MOVE EXPLANATION OF THIS FIT TO AFTER DISCUSSION OF GAS GAIN VARIANCE?*  
- [ ] *HERE (and related sections) IMPORTANT TO CHECK THAT OUR
  EXPLANATION OF HOW GAS GAIN CALC'D IS GOOD*

- [X] *Instead of single plot as now, have both plots in a subfig for
  each of the two run periods!*

- [X] *THINK ABOUT INSTEAD OF EXPLAINING HOW THE CALIBRATION IS DONE*
  in as much detail as done above, instead just start by saying how
  the calibration is gas gain dependent and then say that is
  varying. therefore first study gas gain behavior & discover slicing

- [X] *MENTION IF GAS GAIN VS ENERGY CALIB FITS USE INDIVIDUAL SLICES
  OR MEAN VALUE!*
  -> Note that this is somewhat of an internal implementation detail
  to some extent, but practically the ~gcMean~ approach is not
  mentioned at all in the thesis. We can't and don't need to explain
  every possibility of the software after all.

*** Generate plot for the gas gain vs. energy calibration factors :extended:

In principle the plots shown in the section above are produced during
the regular data reconstruction and calibration, in particular by the
~reconstruction~ program using the ~--only_gain_fit~ argument.

Let's recreate them in the style we want for the thesis.

#+begin_src sh
USE_TEX=true WIDTH=600 HEIGHT=360 reconstruction \
   -i ~/CastData/data/CalibrationRuns2017_Reco.h5 \
   --only_gain_fit \
   --plotOutPath ~/phd/Figs/energyCalibration/Run_2/
USE_TEX=true WIDTH=600 HEIGHT=360 reconstruction \
   -i ~/CastData/data/CalibrationRuns2018_Reco.h5 \
   --only_gain_fit \
   --plotOutPath ~/phd/Figs/energyCalibration/Run_3/
#+end_src

which produces the plots in:
- [[~/phd/Figs/energyCalibration/Run_2]]
- [[~/phd/Figs/energyCalibration/Run_3]]

*** Generate plot of median cluster energy                       :extended:

Let's now generate the plots of the median cluster energy.

*Note*: As the gas gain calculation is time consuming:
If the gas gain values were computed at some point in the past and the
~gasGainSlices*~ datasets are still present, the calculation of the
gas gain bins isn't needed. One can simply change the current
~gasGainInterval~ in the ~config.toml~ and recompute the gas gain vs
energy calibration fit (~--only_gain_fit~) and the energy again to
re-generate the plots.

First here are two sections which cover how to compute the relevant
gas gain slices and calibrate the energy accordingly. They also show
how to use the tool to plot the median energy over the time bins. When
running them, they generate CSV files that we will use further down to
generate a plot that combines the data from the full run and 90 min
binned gas gain data into one plot.

**** Full run gas gain
First, the case of computing it from the full run gas gain, calculate
the median of 90 min data in the plot and second the 90 minute binned
gas gain version, with the same bins used for the plot.

To start, first need to recompute the gas gain based on the full
runs. Set the ~fullRunGasGain~ value in the ~config.toml~ file to
~true~ and ~gasGainInterval~ to ~0~ (the latter because
~fullRunGasGain~ appends a ~0~ suffix to the generated dataset & the
~--only_gain_fit~ step reads from the dataset with suffix of ~gasGainInterval~) then run:
#+begin_src sh
cd $TPA/Analysis/ingrid
./runAnalysisChain -i ~/CastData/data --outpath ~/CastData/data \
                   --years 2017 --years 2018 \
                   --back --calib \
                   --only_gas_gain
#+end_src

Then check that the ~gasGainInterval~ datasets in the H5 files now
actually contain a single slice in the dataset without a numbered
suffix (indicating the minutes used for the slice).

With that done, again, run the ~only_gain_fit~ argument on the
calibration files:
#+begin_src sh
cd $TPA/Analysis/ingrid
./runAnalysisChain -i ~/CastData/data --outpath ~/CastData/data \
                   --years 2017 --years 2018 \
                   --calib \
                   --only_gain_fit
#+end_src

Side note:
to check if the fit was done correctly on the full run slices, check
the output directory, e.g. something like
~out/CalibrationRuns2018_Raw_2020-04-28_15-06-54~ in case of the Run-3
data and compare the ~gasgain_vs_energy_calibration_factors_*~ files
present there. The latest (full run) version should have less data
points than the 90 min version that should already be present from the
initial reconstruction & calibration of the data.

Then recompute the energy for all data:
#+begin_src sh
cd $TPA/Analysis/ingrid
./runAnalysisChain -i ~/CastData/data --outpath ~/CastData/data \
                   --years 2017 --years 2018 \
                   --back --calib \
                   --only_energy_from_e
#+end_src


*Note*: The ~plotTotalChargeOverTime~ should be compiled using
#+begin_src sh
nim c -d:danger -d:StartHue=285 plotTotalChargeOverTime
#+end_src
as was done in the previous section where we generated the median
charge plot.

And again, generate the plot using our tool:
#+begin_src sh
./plotTotalChargeOverTime ~/CastData/data/DataRuns2017_Reco.h5 \
                          ~/CastData/data/DataRuns2018_Reco.h5 \
                          --interval 90 \
                          --cutoffCharge 0 --cutoffHits 500 \
                          --calibFiles ~/CastData/data/CalibrationRuns2017_Reco.h5 \
                          --calibFiles ~/CastData/data/CalibrationRuns2018_Reco.h5 \
                          --applyRegionCut --timeSeries
#+end_src

which yields the output file:
~out/background_median_energyFromCharge_90.0_min_filtered_crSilver.pdf~
and
[[~/phd/Figs/behavior_over_time/median_energy_full_run_gasgain_binned_90min_crSilver.pdf]]

Finally, it generates the following CSV file from the used data frame:
~out/data_90.0_min_filtered_crSilver.csv~
which we store in:
[[~/phd/resources/behavior_over_time/data_full_run_gasgain_90.0_min_filtered_crSilver.csv]]

**** 90 min binning gas gains

Now to generate the 90 minute version, set the ~fullRunGasGain~ back
to ~false~ and make sure the gas gain time slice interval is set to 90
min in the ~config.toml~. Then again run:
#+begin_src sh
cd $TPA/Analysis/ingrid
./runAnalysisChain -i ~/CastData/data --outpath ~/CastData/data \
                   --years 2017 --years 2018 \
                   --back --calib \
                   --only_gas_gain
#+end_src
and again check the gas gain slice dataset without a suffix has
_multiple_ slices each about 90 min in length.

With that done, again, run the ~only_gain_fit~ argument on the
calibration files:
#+begin_src sh
cd $TPA/Analysis/ingrid
./runAnalysisChain -i ~/CastData/data --outpath ~/CastData/data \
                   --years 2017 --years 2018 \
                   --calib \
                   --only_gain_fit
#+end_src
and then recompute the energy for all data:
#+begin_src sh
cd $TPA/Analysis/ingrid
./runAnalysisChain -i ~/CastData/data --outpath ~/CastData/data \
                   --years 2017 --years 2018 \
                   --back --calib \
                   --only_energy_from_e
#+end_src

And again, generate the plot using our tool:
#+begin_src sh
./plotTotalChargeOverTime ~/CastData/data/DataRuns2017_Reco.h5 \
                          ~/CastData/data/DataRuns2018_Reco.h5 \
                          --interval 90 \
                          --cutoffCharge 0 --cutoffHits 500 \
                          --calibFiles ~/CastData/data/CalibrationRuns2017_Reco.h5 \
                          --calibFiles ~/CastData/data/CalibrationRuns2018_Reco.h5 \
                          --applyRegionCut --timeSeries
#+end_src

which - among others - generates the output file
~out/background_median_energyFromCharge_90.0_min_filtered_crSilver.pdf~
which for our purposes:
[[~/phd/Figs/behavior_over_time/median_energy_binned_90min_crSilver.pdf]]

And again, it generates the CSV file:
~out/data_90.0_min_filtered_crSilver.csv~
which we store in:
[[~/phd/resources/behavior_over_time/data_gas_gains_binned_90.0_min_filtered_crSilver.csv]]

**** Comparison of the files

If you ran the code in the order above, your ~config.toml~ file should
again be in the 90 min & binned gas gain setting. Otherwise change it
back and rerun the 90 min binning & fitting & energy calculation
commands from above to make sure we don't mess up plots that are
generated further!


- ~~/phd/Figs/behavior_over_time/median_energy_full_run_gasgain_binned_90min_crSilver.pdf~
- ~~/phd/Figs/behavior_over_time/median_energy_binned_90min_crSilver.pdf~

Comparing the two median energy files, both binned by the same
intervals (the ones used for the 90 min gas gain calculations), it's
evident that _more or less_ they do agree. However, in some areas
significant spikes can be seen in the version from the full run gas
gain values, which is precisely expected: It's those runs in which the
temperature varied significantly within the one run, changing the gas
gain as a result. So there are times in which the average gas gain of
that run does not match locally within the run.


Now we use the CSV files from ~phd/resources/behavior_over_time~ to
generate the same plot as the individual ones here, but showing the
binned and unbinned data with different shapes / colors.

#+begin_src nim :tangle code/median_energy_binned_vs_unbinned.nim
import std / [times, strformat]
import ggplotnim

let df1 = readCsv("/home/basti/phd/resources/behavior_over_time/data_full_run_gasgain_90.0_min_filtered_crSilver.csv")
let df2 = readCsv("/home/basti/phd/resources/behavior_over_time/data_gas_gains_binned_90.0_min_filtered_crSilver.csv")
let df = bind_rows([("Unbinned", df1), ("Binned", df2)], "Data")

proc toPeriod(v: float): string =
  result = v.int.fromUnix.format("dd/MM/YYYY")

let name = "energyFromChargeMedian"
ggplot(df, aes("timestamp", name, shape = "runType", color = "Data")) +
  facet_wrap("runPeriods", scales = "free") +
  facetMargin(0.5, ukCentimeter) +
  theme_scale(1.2) +
  facetHeaderText(font = font(14.0)) + 
  scale_x_continuous(labels = toPeriod) +
  geom_point(alpha = some(0.8), size = 2.0) +
  ylim(2.0, 6.5) +
  margin(top = 1.5, left = 2.0, bottom = 1.0, right = 2.0) +
  legendPosition(0.5, 0.175) +
  xlab("Date", rotate = -20, alignTo = "right", margin = 0.0) +
  ylab("Energy [keV]") + 
  ggtitle(&"Median of cluster energy, binned vs. unbinned. 90 min intervals.") +
  ggsave(&"Figs/behavior_over_time/median_energy_binned_vs_unbinned.pdf",
         width = 1200, height = 800,
         useTeX = true, standalone = true)
#+end_src

#+RESULTS:

which results in

[[~/phd/Figs/behavior_over_time/median_energy_binned_vs_unbinned.pdf]]

** FADC
:PROPERTIES:
:CUSTOM_ID: sec:calib:fadc
:END:

As touched on multiple times previously, in particular in
sec. [[#sec:cast:data_taking_woes]], the FADC suffered from noise. This
meant multiple changes to the amplifier settings to mitigate this.

We will now go over what the FADC noise looks like and explain our
noise filter used to determine if an FADC event is noisy (to ignore
it), in sec. [[sec:calibration:fadc_noise]]. Then we check the impact of
the different amplification settings on the FADC data and finally
discuss the impact of the FADC data quality on the rest of the
detector data in sec. .....

*** TODOs for this section [/]                                   :noexport:

Initial text for this section:
#+begin_quote
As described over the course of previous chapters, the FADC serves
essentially a 4-way purpose. First of all, it is used as a trigger to
start the readout process of the GridPixes, if the center chip records
a large enough signal. This brings up the question of the activation
threshold of the FADC, sec. [[#sec:fadc:activation_threshold]]. Secondly,
thanks to the high temporal resolution the recorded signal can be used
to gain information about the longitudinal shape of recorded
events. This theoretically allows it to act as an additional
background suppression mechanism,
sec. [[#sec:fadc:fadc_as_veto]]. Finally, its trigger clock acts as the
reference time and readout flag for the scintillators, see
sec. [[#sec:fadc:scintillator_trigger_reference]] (*I THINK THIS MAY NOT
BE NEEDED, AS THERE IS ENOUGH STUFF ABOUT THIS BEFORE*). Furthermore,
in principle the FADC provides an independent measurement about the
total charge collected on the central GridPix. Thus, it serves as a
reference as to whether changes in the computed gas gain on the
GridPix as well as possible changes in the recorded number of pixels
are due to physical changes in the gas gain or due to changes in the
readout electronics. Such a secondary insight is valuable in the
context of studying the GridPix behavior over time, sec.
[[#sec:calib:detector_behavior_over_time]]. 
#+end_quote

- trigger
- longitudinal shape information -> acting as a veto
- reference clock for scintillators
- [ ] 
  -> independent information of total collected charge
  -> allows to check whether seen changes in gas gain on gridPix are
  really changes in gas gain or changes in the readout electronics of
  a timepix!!
  This is actually an important insight as it allows us to see what
  happened in the cases where pixel information was lost / peak
  position changed drastically. Did gas gain change based on FADC information?
  -> Used in time dependence of gas gain.

problems:  
- noise
- different thresholds due to gain changes
- different shapes due to 50ns 100ns signal integration, 50ns 20ns
  differentiation
- [ ] *REGARDING 50ns vs 100ns ~fadc_analysis.nim~ CONTAINS CODE TO 
  CHECK NOISE COMPARED TO 50 vs 100!!*


*** FADC noise example and detection
:PROPERTIES:
:CUSTOM_ID: sec:calibration:fadc_noise
:END:

An example of the most common type of noise events seen in the FADC
data is shown in fig. [[fig:calib:fadc_noise_example]]. As the FADC
registers effectively correspond to $\SI{1}{ns}$ time resolution, the
periodicity of these noise events is about $\SI{150}{ns}$,
corresponding to roughly $\SI{6.6}{MHz}$ frequency. Other types of
less common noise events are events with a noise frequency of about
$\SI{1.5}{MHz}$. [fn:frequencies] A final type of noise events are
events, in which the FADC input is fully at a low input voltage (in
the tens of $\si{mV}$ range), but contains no real 'activity'. The values
though are lower than the threshold in these cases triggering the
FADC.

#+CAPTION: Example of the most common type of noise example. Noise has a
#+CAPTION: periodicity of about $\sim\SI{150}{ns}$, or about $\SI{6.6}{MHz}$. 
#+NAME: fig:calib:fadc_noise_example
[[~/phd/Figs/FADC/fadcNoise/figs/DataRuns2017_Reco_2023-10-25_15-12-30/fadc_event_run109_event19157_region_crAll_fadc_noisy_0.5_1.5_applyAll_true.pdf]]

For data analysis purposes, in particular when the FADC data is used
in conjunction with the GridPix data it is important to not
accidentally use an FADC event, which contains noise. While generally
noise events are unlikely to be part of physical ionization events on
the center GridPix it is better to be on the conservative side. The
noise analysis is kept very simple [fn:implementation]. The FADC
spectrum consisting of $\num{2560}$ registers is being sliced into
$\num{150}$ register wide intervals. In each interval we check for the
minimum of the signal, $m_s$. The slice is adjusted around the found
minimum to check if the minimum is contained fully in the slice (if
not it is part of the next slice). If that minimum is below $m_s < B -
σ$, where $B$ is the signal baseline and $σ$ the standard deviation of
the full FADC signal (including the peaks!), it will be counted as a
peak. The noise filter detection is then defined by signals with at
least $\num{4}$ peaks within slices of $\SI{150}{ns}$. 

[fn:frequencies] The frequencies are on the low end in terms of common
radio communication frequencies. The leading assumption has always
been that the source likely points to noise produced by e.g. the
motors moving the CAST magnet and similar.

[fn:implementation] I started with a simple implementation, intending
to replace it later. But it worked well enough that I simply kept it
so far.

**** TODOs for this section [/]                                 :noexport:

- [X] *EXAMPLE OF NOISE*

- [ ] Add examples of other types of noise too?

**** Notes on FADC noise analysis                               :extended:

The ~fadc_analysis.nim~ program in ~TimepixAnalysis~ on the one hand
contains the code to detect noisy events and is used as a library for
that purpose. But it can also be used to perform a standalone FADC
noise analysis if compiled on its own.


To determine if an event is noisy:
- check for dips in the signal of width 150 ns
- if more than 4 dips in one event -> noisy

~fadc_helpers.nim~ -> ~isFadcFileNoisy~ using ~findPeaks~ from
~NimUtil/helpers/utils.nim~.

Note: the implementation is rather simple. Instead of slicing the FADC
data into chunks of the desired width it would be smarter to work with
a running version of the data and see if the running mean crosses some
lower threshold. The difficulty in that is detecting separate
peaks. One would need to track the 'last return to baseline' and only
count more dips if the next minimum had a baseline return (or maybe
50% of baseline return) in it.

**** Find a good noisy FADC event                               :extended:

Types of noise:
- O(4) periods in 2560 ns
- O(16) periods in 2560 ns
- signal at negative voltage (with no periodicity) over entire range
-> appendix one example of each? Then again, we also don't show
examples of all sorts of fun GridPix events. Then again again, those
don't cause data loss because they are extremely infrequent :)

Run 109 (based on our notes taken during the CAST data taking) was a
run with serious amounts of noise. We'll find a good FADC event using
~plotData~ by filtering to ~noisy == 1~ and producing FADC event
displays.

First let's generate some events:
#+begin_src sh
F_LEFT=0.8 \
    plotData \
    --h5file ~/CastData/data/DataRuns2017_Reco.h5 \
    --runType=rtBackground \
    --cuts '("fadc/noisy", 0.5, 1.5)' \
    --applyAllCuts \
    --fadc \
    --eventDisplay \
    --runs 109 \
    --head 50
#+end_src

Good examples for the three main types of noise we had are:
High frequency event:
event #: 19157

Full on negative value event:
event #: 4147

Low frequency event:
event #: 9497

For each of these let's generate a prettier version:
#+begin_src sh :dir ~/phd/Figs/FADC/fadcNoise
F_LEFT=-0.8 L_MARGIN=2.5 B_MARGIN=1.5 T_MARGIN=1.0 USE_TEX=true WIDTH=600 HEIGHT=420 FONT_SCALE=1.2 \
    plotData \
    --h5file ~/CastData/data/DataRuns2017_Reco.h5 \
    --runType=rtBackground \
    --cuts '("fadc/noisy", 0.5, 1.5)' \
    --applyAllCuts \
    --fadc \
    --eventDisplay \
    --runs 109 \
    --events 19157 --events 4147 --events 9497
#+end_src

- [[~/phd/Figs/FADC/fadcNoise/figs/DataRuns2017_Reco_2023-10-25_15-12-30/fadc_event_run109_event19157_region_crAll_fadc_noisy_0.5_1.5_applyAll_true.pdf]]
- [[~/phd/Figs/FADC/fadcNoise/figs/DataRuns2017_Reco_2023-10-25_15-12-30/fadc_event_run109_event4147_region_crAll_fadc_noisy_0.5_1.5_applyAll_true.pdf]]
- [[~/phd/Figs/FADC/fadcNoise/figs/DataRuns2017_Reco_2023-10-25_15-12-30/fadc_event_run109_event9497_region_crAll_fadc_noisy_0.5_1.5_applyAll_true.pdf]]

*** Amplifier settings impact and activation threshold [0/4]
:PROPERTIES:
:CUSTOM_ID: sec:calib:fadc:amplifier_settings
:END:

Now let's look at the impact of the different amplifier settings on
the FADC data properties. This includes differences in the rise time
and fall time, but because changing the integration and
differentiation times on the amplifier has a direct impact on the
absolute amplification on the signal, we also need to consider the
change in activation threshold of the signals.

As a short reminder, the FADC settings were changed twice during the
Run-2 period in 2017. Starting from 2018 the settings were left like
at the end of 2017. An overview of the setting changes is shown in
tab. [[tab:calib:fadc_amplifier_settings]]. Note that the Ortec amplifier
has a coarse and a fine gain. Only the coarse gain was
changed. [fn:fine_gain] The gain changes were performed to counteract
the resulting amplification changes due to the integration and
differentiation setting changes (this is documented as a side effect
in the FADC manual [[cite:&fadc_manual]]).

#+CAPTION: Overview of the different FADC amplifier settings and the associated run
#+CAPTION: numbers.
#+NAME: tab:calib:fadc_amplifier_settings
#+ATTR_LATEX: :booktabs t
| Runs      | Integration [$\si{ns}$] | Differentiation [$\si{ns}$] | Coarse gain |
|-----------+-------------------------+-----------------------------+-------------|
| 76 - 100  |                      50 |                          50 |          6x |
| 101 - 120 |                      50 |                          20 |          10 |
| 121 - 306 |                     100 |                          20 |          10 |

The \cefe calibration spectra come in handy for the FADC data, as they
also give a known baseline to compare against for this type of
data. To get an idea of the rise and fall times of the FADC for
different settings, we can compute a truncated mean of all rise and
fall times in each calibration run. [fn:trunc_mean] This is done in
fig. [[fig:fadc:peak_position_55fe_run2_settings]], which shows the mean
rise time for each run in 2017 with the fall time color coded in each
point. The shaded region indicate the FADC amplifier
settings. Changing the differentiation time from $\SI{50}{ns}$ down to
$\SI{20}{ns}$ decreased the rise time by about $\SI{10}{ns}$. The
change in the fall time is much more pronounced. The change of the
integration time from $\SI{50}{ns}$ to $\SI{100}{ns}$ then brings the
rise time back up by about $\SI{5}{ns}$ with now a drastic extension
in the fall time from the mid $\SI{200}{ns}$ to over
$\SI{400}{ns}$. Clearly the fall time is much more determined by the
amplifier settings.

#+CAPTION: The mean rise time of the FADC signals recorded during the \cefe data
#+CAPTION: during Run-2 of the FADC. Again, the FADC amplifier settings are visible
#+CAPTION: as expected.
#+CAPTION: '∫': integration time, '∂': differentiation time, 'G': coarse gain.
#+NAME: fig:fadc:mean_rise_times_55fe_fadc_run2
[[~/phd/Figs/FADC/fadc_mean_riseTime_run2.pdf]]

A direct scatter plot of the rise times against the fall times is
shown in fig. [[fig:fadc:riseTime_vs_fallTime_55fe_fadc_run2]], where the
drastic changes to the fall time are even more pronounced. Each point
once again represents one \cefe calibration run. The different
settings manifest as separate 'islands' in this space.

#+CAPTION: The mean rise time of the FADC signals recorded during the \cefe data
#+CAPTION: against the fall time during Run-2 at CAST. One point for each calibration
#+CAPTION: run.
#+CAPTION: The different settings create three distinct blobs.
#+CAPTION: '∫': integration time, '∂': differentiation time, 'G': coarse gain.
#+NAME: fig:fadc:riseTime_vs_fallTime_55fe_fadc_run2
[[~/phd/Figs/FADC/fadc_mean_riseTime_vs_fallTime_run2.pdf]]

The FADC pulses contain a measure of the total charge that was induced
on the grid and therefore an indirect measure of the charge seen on
the center GridPix. The \cefe calibration runs could be used to fully
calibrate the FADC signals in charge if desired. Ideally one would
fully (numerically) integrate the FADC signal for each event to
compute an effective charge. As we only use the FADC signals in the
context of this thesis for their sensitivity to longitudinal shape
information, this is not implemented. For \cefe calibration data the
amplitude of the FADC pulse is a direct proxy for the charge anyway,
because the signal shape is (generally) the same for
X-rays. [fn:shape] For the determination of whether the gas gain
variations discussed in sec. [[#sec:calib:detector_behavior_over_time]]
have a physical origin due to changing gas gain or are caused by
electronic effects, we already included the FADC data in
fig. [[fig:calib:fe55_peak_pos_charge_pixel_fadc]] of
sec. [[#sec:calib:causes_variability]]. Computing the histogram of all
amplitudes of the FADC signals in a \cefe calibration run yields a
spectrum like for the center GridPix. The fitted position of the
photopeak in these spectra is then a direct counterpart to those
computed for the GridPix. Due to its independence and only being
sensitive to induced charge it acts as a good validator.

In the context of the FADC amplifier settings it is interesting to see
how the photopeak position changes between runs when computed like
that. This is shown in
fig. [[fig:fadc:peak_position_55fe_run2_settings]]. We can see that the
initial change in differentiation time resulted in a larger gain
change than the attempt at compensation from 6x to 10x on the coarse
gain. The final increase in the integration time then caused another
drop in signal amplitudes, implying an even lower absolute gain. In
addition though the gain variation within a single setting is very
visible. 

#+CAPTION: The peak position in \si{V} of the photopeak in the \cefe calibration
#+CAPTION: runs during Run-2 as seen on the FADC. The different FADC amplifier settings are clearly visible.
#+CAPTION: '∫': integration time, '∂': differentiation time, 'G': coarse gain.
#+NAME: fig:fadc:peak_position_55fe_run2_settings
[[~/phd/Figs/FADC/peak_positions_fadc_run2.pdf]]

Finally, we can look at the activation threshold of the FADC. The
easiest way to do this is the following: we read the energies of all
events on the center GridPix, then map them to their FADC
events. Although not common in calibration data some events may not
trigger the FADC. By then computing -- for example -- the first
quantile of the energy data (the absolute lowest value may be some
outlier) we automatically get the lowest equivalent GridPix energy
that triggers the FADC. Doing this leads to yet another similar plot
to the previous,
fig. [[fig:fadc:activation_threshold_gridpix_55fe_run2_settings]]. With
the first FADC settings the activation threshold was at a low
$\sim\SI{1.1}{keV}$. Unfortunately, both amplifier settings moved the
threshold further up to about $\SI{2.2}{keV}$ with the final
settings. In hindsight it likely would have been a better idea to try
to run with a lower activation threshold so that the FADC trigger is
available for more events at low energies. However, at the time of the
data taking campaign not all information was available for an educated
assessment, nor was there enough time to test and implement other
ideas. Especially because there is a high likelihood that other
settings might have run back into noise problems. 

#+CAPTION: The activation threshold of the FADC for each calibration run in 2017.
#+CAPTION: Computed by the first quantile of the corresponding energies recorded
#+CAPTION: by the GridPix.
#+CAPTION: '∫': integration time, '∂': differentiation time, 'G': coarse gain.
#+NAME: fig:fadc:activation_threshold_gridpix_55fe_run2_settings
[[~/phd/Figs/FADC/activation_threshold_gridpix_energy_fadc_run2.pdf]]



[fn:trunc_mean] We use a truncated mean of all data within the 5-th
and 95-h percentile if the rise and fall time values. This is just to
make the numbers less susceptible to extreme outliers. Alternatively,
we could of course also look at the median for example.

[fn:fine_gain] At least to my memory and notes, which should otherwise
contain that.

[fn:shape] As with everything this is only an approximation and
completely neglects possible nonlinearities in amplitude vs. integral
and so on. But it works well for its purpose here.

**** TODOs for this section [/]                                 :noexport:

- [ ] *MAKE SURE TO ADD 50/100 ns DATASETS TO ALL RAW DATA!*

What is it that we actually want from this? Aside from providing an
*overview* the FADC is barely used for anything. Things that are
important:
- [ ] *barely used not true anymore!* Important veto.
- [ ] *trigger threshold* deduced from calibration data / real data
  - [ ] define how to compute & compute for every run, plot trigger
    threshold vs run (correlate with settings)
- [ ] show rise time / fall time histograms! Required for our cuts we
  actually perform on that in background calc later! This is required
  because otherwise we cannot explain why we do / do not use the FADC
  as a veto and why it's not super helpful.

- [ ] *MOST LIKELY THESE RISE/FALL TIME PLOTS CHANGE SIGNIFICANTLY NOW
  AFTER HAVING CHANGED FADC BASELINE & THRESHOLD CALCS!!!*

- [X] show spectra (already one shown in previous chapter!)
- [ ] show 55Fe spectrum of FADC data
- [X] create plot showing position of 55 Fe photo peak in FADC data,
  highlighting when amplifier settings were changed
- [X] create plot of mean rise times in FADC   
- [ ] show how FADC events differ in 50ns and 100ns data (*we have 
  those datasets*)
  -> partially done in plots comparing rise time between different runs!
- [ ] show detector pointing to Zenith data for muon related stuff
- [ ] plot histograms of 50ns runs vs 100ns runs at CAST to see
  difference in background
  -> code is there!
- [ ] plot histograms of the different properties (rise & fall time in
  particular) for the different FADC settings used at CAST
  -> partially done! Not the histograms shown above, but the mean
  value of those histograms!
  -> For the actual histograms we could split the data by the datasets
  with different FADC settings (similar to how done in
  ~fadc_analysis.nim~!) and then plot them by 3 different colors.
- [X] finish trying to deduce the pedestals from real data. Didn't
  work last time only due to our confusion of using all 4
  channels. Try again focusing on the single channel we actually use!
  -> Now that it's done correctly it works extremely well!
- [ ] create a plot comparing 55Fe data from GridPix with FADC (by
  peak position over time. Same correlation or not?)
  -> also see detector behavior over time / FADC as proxy for charge
- [ ] *REGARDING 50ns vs 100ns ~fadc_analysis.nim~ CONTAINS CODE TO 
  CHECK NOISE COMPARED TO 50 vs 100!!*
- [ ] noise analysis
- [ ] apply Savitzky Golay filter to FADC spectra & describe
  -> Still necessary after correct pedestals in use w/ trunc mean?
  Maybe much more helpful now? Try it!
- [ ] train NN on FADC data and see what we can gain <- this goes
  towards analysis chapter of course!
  Best use a CNN with 1D input data & kernel!

**** Further thoughts on understanding impact of $\SI{50}{ns}$ vs $\SI{100}{ns}$ :extended:

We did actually take a measurement of the FADC in the laboratory at
some point comparing $\SI{50}{ns}$ integration time with
$\SI{100}{ns}$ integration time. This was during the course of the
master thesis of Hendrik Schmick [[cite:&SchmickMaster]].

Unfortunately, the exact values of the amplifier gain and
differentiation times were not recorded (dummy me!). These two
datasets may still be valuable (and are part of the raw data hosted at
Zenodo), but I haven't attempted to use them for a deeper
understanding in the last years (we looked into them back in 2019 though).

**** Generate plots of FADC fall & rise times and FADC \cefe spectrum [/] :extended:
:PROPERTIES:
:CUSTOM_ID: sec:reco:fadc_rise_fall_plots
:END:

- [ ] *REPLACE THE ORIGIN OF THIS PLOT*

- [ ] *CREATE A NEW FADC \cefe SPECTRUM*
  -> Follow sec. [[#sec:calib:energy_gen_example_cefe]]
  -> Use the same run too!

#+begin_src sh :results drawer
raw_data_manipulation -p ~/CastData/data/2018_2/Run_240_181021-14-54/ \
                      --out /t/raw_240.h5 \
                      --runType rtBackground
#+end_src

#+begin_src sh :results drawer
reconstruction /t/raw_240.h5 --out /t/reco_240.h5
#+end_src

#+RESULTS:
:results:
{"<HDF5file>": /t/raw_240.h5, "--only_gain_fit": false,
"--create_fe_spec": false, "--version": false, "--runNumber": nil,
"--only_charge": false, "--only_energy_from_e": false,
"--only_energy": nil, "--out": /t/reco_240.h5, "--config": nil,
"--only_gas_gain": false, "--only_fadc": false, "--help": false,
"--only_fe_spec": false}
...
INFO Writing data to datasets
INFO Writing of FADC data took 0.9212641716003418 seconds
INFO Reconstruction of all runs in /t/raw_240.h5 with flags: {rfReadAllRuns} took 5.32500171661377 seconds
INFO Performed reconstruction of the following runs:
INFO {240}
INFO while iterating over the following:
INFO {240}
:end:
#+begin_src sh :results drawer
reconstruction /t/reco_240.h5 --only_fadc
#+end_src

#+RESULTS:
:results:
{"<HDF5file>": /t/reco_240.h5, "--only_gain_fit": false, "--create_fe_spec": false, "--version": false, "--runNumber": nil, "--only_charge": false, "--only_energy_from_e": false, "--only_energy": nil, "--out": nil, "--config": nil, "--only_gas_gain": false, "--only_fadc": true, "--help": false, "--only_fe_spec": false}
INFO Reading config file: /home/basti/CastData/ExternCode/TimepixAnalysis/Analysis/ingrid/config.toml
Start fadc calc
FADC minima calculations took: 0.5971462726593018
INFO Reconstruction of all runs in /t/reco_240.h5 with flags: {rfOnlyFadc, rfReadAllRuns} took 1.252563714981079 seconds
INFO Performed reconstruction of the following runs:
INFO {240}
INFO while iterating over the following:
INFO {240}
:end:

#+begin_src sh :results drawer
raw_data_manipulation -p ~/CastData/data/2017/Run_96_171123-10-42 \
                      --out /t/raw_96.h5 \
                      --runType rtCalibration
#+end_src

#+RESULTS:
:results:
Flags are is {}
...
INFO Closing h5file with code 0
INFO Processing all given runs took 0.3456798712412516 minutes
:end:

#+begin_src sh :results drawer
reconstruction /t/raw_96.h5 --out /t/reco_96.h5
#+end_src

#+RESULTS:
:results:
{"<HDF5file>": /t/raw_96.h5, "--only_gain_fit": false,
"--create_fe_spec": false, "--version": false, "--runNumber": nil,
"--only_charge": false, "--only_energy_from_e": false,
"--only_energy": nil, "--out": /t/reco_96.h5, "--config": nil,
"--only_gas_gain": false, "--only_fadc": false, "--help": false,
"--only_fe_spec": false}
...
INFO Writing data to datasets
INFO Writing of FADC data took 6.967457056045532 seconds
INFO Reconstruction of all runs in /t/raw_96.h5 with flags: {rfReadAllRuns} took 15.26722311973572 seconds
INFO Performed reconstruction of the following runs:
INFO {96}
INFO while iterating over the following:
INFO {96}
:end:
#+begin_src sh :results drawer
reconstruction /t/reco_96.h5 --only_fadc
#+end_src

#+RESULTS:
:results:
{"<HDF5file>": /t/reco_96.h5, "--only_gain_fit": false, "--create_fe_spec": false, "--version": false, "--runNumber": nil, "--only_charge": false, "--only_energy_from_e": false, "--only_energy": nil, "--out": nil, "--config": nil, "--only_gas_gain": false, "--only_fadc": true, "--help": false, "--only_fe_spec": false}
INFO Reading config file: /home/basti/CastData/ExternCode/TimepixAnalysis/Analysis/ingrid/config.toml
Start fadc calc
FADC minima calculations took: 4.997849464416504
INFO Reconstruction of all runs in /t/reco_96.h5 with flags: {rfOnlyFadc, rfReadAllRuns} took 10.23849511146545 seconds
INFO Performed reconstruction of the following runs:
INFO {96}
INFO while iterating over the following:
INFO {96}
:end:

#+begin_src nim :results drawer :tangle code/fadc_rise_fall_different_settings.nim
import nimhdf5, ggplotnim
import std / [strutils, os, sequtils, strformat]
import ingrid / [tos_helpers]
import ingrid / calibration / [calib_fitting, calib_plotting]
import ingrid / calibration 

proc stripPrefix(s, p: string): string =
  result = s
  result.removePrefix(p)

let useTeX = getEnv("USE_TEX", "false").parseBool

from ginger import transparent

const settings = @["∫: 50 ns, ∂: 50 ns, G: 6x",
                   "∫: 50 ns, ∂: 20 ns, G: 10x",
                   "∫: 100 ns, ∂: 20 ns, G: 10x"]
const runs = @[80, 101, 121]

const riseTimeS = "riseTime [ns]"
const fallTimeS = "fallTime [ns]"

proc fadcSettings(plt: GgPlot, allRuns: seq[int], hideText: bool, minVal, maxVal, margin: float): GgPlot =
  ## This is a bit of a mess, but:
  ## It handles drawing the colored rectangles for the different FADC settings and
  ## adjusting the margin if any given via the R_MARGIN environment variable.
  ## The rectangle drawing is a bit ugly to look at, because we use the numbers initially
  ## intended for the peak position plot, but rescale them to map the completely different
  ## values for the other plots using min/max value and a potential margin.
  let mRight = getEnv("R_MARGIN", "6.0").parseFloat
  let widths = @[101 - 80, 121 - 101, allRuns.max - 121 + 1]
  let Δ = (maxVal - minVal) 
  let min = minVal - Δ * margin
  let ys = @[min, min, min]
  let heights = @[0.25, 0.25, 0.25].mapIt(it / 0.25 * (Δ * (1 + 2 * margin))) 
  let textYs = @[0.325, 0.27, 0.22].mapIt((it - 0.1) / (0.35 - 0.1) * Δ + minVal)
  let dfRects = toDf(settings, ys, textYs, runs, heights, widths)
  echo dfRects
  result = plt +
    geom_tile(data = dfRects, aes = aes(x = "runs", y = "ys", height = "heights", width = "widths", fill = "settings"),
              alpha = 0.3) +
    xlim(80, 200) +
    margin(right = mRight) 
  if not hideText:
    result = result + geom_text(data = dfRects, aes = aes(x = f{`runs` + 2}, y = "textYs", text = "settings"), alignKind = taLeft)

proc getSetting(run: int): string =
  result = settings[lowerBound(runs, run) - 1]

proc plotFallTimeRiseTime(df: DataFrame, suffix: string, allRuns: seq[int], hideText: bool) =
  ## Given a full run of FADC data, create the
  ## Note: it may be sensible to compute a truncated mean instead
  let dfG = df.group_by("runNumber").summarize(f{float: riseTimeS << truncMean(col("riseTime").toSeq1D, 0.05)},
                                               f{float: fallTimeS << truncMean(col("fallTime").toSeq1D, 0.05)})
    .mutate(f{int -> string: "settings" ~ getSetting(`runNumber`)})

  let width = getEnv("WIDTH_RT", "600").parseFloat
  let height = getEnv("HEIGHT_RT", "450").parseFloat
  let mRight = getEnv("R_MARGIN", "4.0").parseFloat
  let fontScale = getEnv("FONT_SCALE", "1.0").parseFloat

  let (rMin, rMax) = (dfG[riseTimeS, float].min, dfG[riseTimeS, float].max)
  let perc = 0.025
  let Δr = (rMax - rMin) * perc
  var plt = ggplot(dfG, aes(runNumber, riseTimeS)) + 
    ggtitle("FADC signal rise times in ⁵⁵Fe data for all runs in $#" % suffix) +
    margin(right = mRight) +
    theme_font_scale(fontScale) +
    ylim(rMin - Δr, rMax + Δr)
  plt = plt.fadcSettings(allRuns, hideText, rMin, rMax, perc)
  plt + geom_point(aes = aes(color = fallTimeS)) +
    ggsave("Figs/FADC/fadc_mean_riseTime_$#.pdf" % suffix,
               width = width, height = height, useTeX = useTeX, standalone = useTeX)

  let (fMin, fMax) = (dfG[fallTimeS, float].min, dfG[fallTimeS, float].max)
  let Δf = (fMax - fMin) * 1.025
  var plt2 = ggplot(dfG, aes(runNumber, fallTimeS)) + 
    margin(right = mRight) +
    ylim(fMin - Δf, fMax + Δf) + 
    theme_font_scale(fontScale) +
    ggtitle("FADC signal fall times in ⁵⁵Fe data for all runsin $#" % suffix)
  plt2 = plt2.fadcSettings(allRuns, hideText, fMin, fMax, perc)
  plt2 + geom_point(aes = aes(color = riseTimeS)) +
    ggsave("Figs/FADC/fadc_mean_fallTime_$#.pdf" % suffix,
                width = width, height = height, useTeX = useTeX, standalone = useTeX)

  ggplot(dfG, aes(riseTimeS, fallTimeS, color = "settings")) + 
    geom_point() +
    ggtitle("FADC signal rise vs fall times for ⁵⁵Fe data in $#" % suffix) +
    margin(right = mRight) +
    theme_font_scale(fontScale) + 
    ggsave("Figs/FADC/fadc_mean_riseTime_vs_fallTime_$#.pdf" % suffix,
           width = width, height = height, useTeX = useTeX, standalone = useTeX)    

proc fit(fname: string, year: int): (DataFrame, DataFrame) =
  var h5f = H5open(fname, "r")
  let fileInfo = h5f.getFileInfo()
  let is2017 = year == 2017
  let is2018 = year == 2018
  if not is2017 and not is2018:
    raise newException(IOError, "The input file is neither clearly a 2017 nor 2018 calibration file!")
  
  var peakPos = newSeq[float]()
  var actThr = newSeq[float]()  
  var dfProp = newDataFrame()
  for run in fileInfo.runs:
    var df = h5f.readRunDsets(
      run,
      commonDsets = @["fadc/eventNumber",
                      "fadc/baseline",
                      "fadc/riseStart",
                      "fadc/riseTime",                  
                      "fadc/fallStop",
                      "fadc/fallTime",
                      "fadc/minVal",
                      "fadc/argMinval"]                 
    )
    df = df.rename(df.getKeys.mapIt(f{it.stripPrefix("fadc/") <- it}))
    df["runNumber"] = run
    let dset = h5f[(recoBase() & $run / "fadc/fadc_data").dset_str]
    let fadcData = dset[float].toTensor.reshape(dset.shape)

    let feSpec = fitFeSpectrumFadc(df["minVal", float].toSeq1D)
    let ecData = fitEnergyCalib(feSpec, isPixel = false)
    let texts = buildTextForFeSpec(feSpec, ecData)
    plotFeSpectrum(feSpec, run, 3, texts = texts, pathPrefix = "Figs/FADC/fe55_fits/", useTeX = false)

    # add fit to peak positions
    peakPos.add feSpec.pRes[feSpec.idx_kalpha]
    
    ggplot(df, aes("minVal")) +
      geom_histogram(bins = 300) +
      ggsave("/t/fadc_run_$#_minima.pdf" % $run)

    # Now get the activation threshold as a function of gridpix energy on center
    # chip. Get GridPix data on center chip...
    var dfGP = h5f.readRunDsets(
      run,
      chipDsets = some((chip: 3, dsets: @["energyFromCharge", "eventNumber"]))
    )
    # ...sum all clusters for each event (for multiple clusters, the FADC sees all)...
    dfGP = dfGP.group_by("eventNumber").summarize(f{float -> float: "energyFromCharge" << sum(col("energyFromCharge"))})
    # ... join with FADC DF to only have events left with FADC trigger...
    df = innerJoin(dfGP, df.clone(), "eventNumber")
    # ...compute activation threshold as 1-th percentile of data
    actThr.add percentile(df["energyFromCharge", float], 1)

    dfProp.add df
  doAssert h5f.close() >= 0

  let df = toDf({ "runs" : fileInfo.runs,
                  "peaks" : peakPos,
                  "actThr" : actThr })
  result = (df, dfProp)

proc main(path: string, year: int, fit = false, hideText = false) =
  ##    - run 101 <2017-11-29 Wed 6:40> was the first with FADC noise
  ##      significant enough to make me change settings:
  ##      - Diff: 50 ns -> 20 ns (one to left)
  ##      - Coarse gain: 6x -> 10x (one to right)
  ##    - run 112: change FADC settings again due to noise:
  ##      - integration: 50 ns -> 100 ns
  ##        This was done at around <2017-12-07 Thu 8:00>
  ##      - integration: 100 ns -> 50 ns again at around
  ##        <2017-12-08 Fri 17:50>.
  ##    - run 121: Jochen set the FADC main amplifier
  ##      integration time from 50 -> 100 ns again, around
  ##      <2017-12-15 Fri 10:20>
  
  let is2017 = year == 2017
  let yearToRun = if is2017: 2 else: 3
  let suffix = "run$#" % $yearToRun
  
  var dfProp = newDataFrame()
  var df = newDataFrame()  
  var peakPos: seq[float]
  if fit:
    (df, dfProp) = fit(path, year)
    dfProp.writeCsv(&"resources/properties_fadc_{suffix}.csv")
    df.writeCsv(&"resources/peak_positions_fadc_{suffix}.csv")    
  else:
    dfProp = readCsv(&"{path}/properties_fadc_{suffix}.csv")
    df = readCsv(&"{path}/peak_positions_fadc_{suffix}.csv")     

  let allRuns = df["runs", int].toSeq1D

  let width = getEnv("WIDTH", "600").parseFloat
  let height = getEnv("HEIGHT", "450").parseFloat

  plotFallTimeRiseTime(dfProp, suffix, allRuns, hideText)

  block Fe55PeakPos:
    let outname = "Figs/FADC/peak_positions_fadc_$#.pdf" % $suffix
    var plt = ggplot(df, aes("runs", "peaks"))
    if is2017:
      plt = plt.fadcSettings(allRuns, hideText, 0.1, 0.35, 0.0)
    plt + geom_point() +
      ylim(0.1, 0.35) +
      ylab("⁵⁵Fe peak position [V]") + xlab("Run number") +
      ggtitle("Peak position of the ⁵⁵Fe runs in the FADC data") + 
      ggsave(outname, width = width, height = height, useTeX = useTeX, standalone = useTeX)
  block ActivationThreshold:
    let outname = "Figs/FADC/activation_threshold_gridpix_energy_fadc_$#.pdf" % $suffix
    var plt = ggplot(df, aes("runs", "actThr"))
    if is2017:
      plt = plt.fadcSettings(allRuns, hideText, 0.9, 2.4, 0.0)
    plt + geom_point() +
      ylim(0.9, 2.4) + 
      ylab("Activation threshold [keV]") + xlab("Run number") +
      ggtitle("Activation threshold based on center GridPix energy") + 
      ggsave(outname, width = width, height = height, useTeX = useTeX, standalone = useTeX)
    

when isMainModule:
  import cligen
  dispatch main
#+end_src

- [X] *REGENERATE FADC DATA IN H5 FILES ON ~voidRipper~*
- [X] *GENERATE HISTOGRAMS OF MINVALS FOR ALL CALIBRATION RUNS*
- [X] *GENERATE PLOT OF ALL FIT TO MINVALS HISTO FIND PEAK FOR ALL
  CALIBRATION RUNS*
  -> start by just computing maximum of the above histogram for each
  run as a basis
- [ ] *ALL THESE PLOTS SHOULD REALLY BE GENERATED WHEN RUNNING
  ~reconstruction --only_fadc~! Replace that!*
  -> Well.. Not today.
- [X] *GENERATE PLOT OF FADC & GridPix PEAK POSITIONS AGAINST ALL
  RUNS*
  -> Done and in previous section (at least for relevant runs)

Run the code for 2017 calibration data to generate the plot of the
FADC settings (and generate the CSV containing the peak positions by run):
#+begin_src sh :dir ~/phd
./code/fadc_rise_fall_different_settings.nim -p ~/CastData/data/CalibrationRuns2017_Reco.h5 --fit --year 2017
#+end_src
and now for 2018 to generate the CSV for the peak positions of the run
3 data:
#+begin_src sh
./code/fadc_rise_fall_different_settings.nim -p ~/CastData/data/CalibrationRuns2018_Reco.h5 --fit --year 2018
#+end_src

To generate the final plots we use the generated CSV files (in order
to more quickly change parameters about the size of plots etc):
#+begin_src sh :dir ~/phd/
USE_TEX=true FONT_SCALE=1.2 R_MARGIN=7.5 RT_MARGIN=3.0 WIDTH=600 HEIGHT=360 HEIGHT_RT=420 \
             code/fadc_rise_fall_different_settings \
             -p resources \
             --year 2017
USE_TEX=true FONT_SCALE=1.2 R_MARGIN=7.5 RT_MARGIN=3.0 WIDTH=600 HEIGHT=360 HEIGHT_RT=420 \
             code/fadc_rise_fall_different_settings \
             -p resources \
             --year 2018
#+end_src

The CSV files are found in:
- [[~/phd/resources/peak_positions_fadc_run2.csv]]
- [[~/phd/resources/peak_positions_fadc_run3.csv]]
- [[~/phd/resources/properties_fadc_run2.csv]]
- [[~/phd/resources/properties_fadc_run3.csv]]

and the plots we generated are all in:
[[file:Figs/FADC/]]
with the \cefe fits in
[[file:Figs/FADC/fe55_fits/]]

**** Initial study of activation threshold                      :extended:

This section was the ideas and code that I initially wrote when I
first thought about including something about the activation threshold
for the FADC. At that point I had never actually tried to quantify
what the threshold was (I obviously had a pretty good idea based on
other aspects).

- How to compute? :: The fits we perform for the 55 Fe in one of our
  scripts here ideally should be done for each run again... That way
  we could compute the energy similar to what we do for GridPix
  data. Ideally we could compute it more by using an integral approach
  though, as that gives us a better proxy for the amount of charge. Or
  at least using a peak finding to detect multiple signals within one
  FADC event to sum energies of both.
  As an easier approach we can of course compute a lower percentile
  (not the total minimum, but maybe 1% of each run and plot that?).
  
#+begin_src nim :tangle code/fadc_compute_activation_threshold.nim
import nimhdf5, ggplotnim
import std / [strutils, os, sequtils]
import ingrid / [tos_helpers, fadc_helpers, ingrid_types, fadc_analysis]

proc fadcSettingRuns(): seq[int] =
  result = @[0, 101, 121]

proc stripPrefix(s, p: string): string =
  result = s
  result.removePrefix(p)

proc minimum(h5f: H5File, runNumber: int, percentile: int): (float, float) =
  var df = h5f.readRunDsets(
    runNumber,
    chipDsets = some((chip: 3, dsets: @["energyFromCharge", "eventNumber"]))
  )
  # sum all energies of all same events to get a combined energy of all
  # clusters on the center chip in each event (to correlate w/ FADC)
  df = df.group_by("eventNumber").summarize(f{float -> float: "energyFromCharge" << sum(col("energyFromCharge"))})
  var run = h5f.readRecoFadc(runNumber)
  let fEvs = h5f.readRunDsets(runNumber, fadcDsets = @["eventNumber"])
  let minVals = run.minVal.toSeq1D
  let dfFadc = toDf({ "eventNumber" : fEvs["eventNumber", int],
                      "minVals" : minVals })
  # join both by `eventNumber` (dropping center chip events w/ no FADC)
  df = innerJoin(df, dfFadc, "eventNumber")
  # percentile based on minvals & gridpix energy
  result = (percentile(minVals, 100 - percentile), percentile(df["energyFromCharge", float], percentile))

proc main(fname: string, percentile: int) =
  var h5f = H5open(fname, "r")
  let fileInfo = h5f.getFileInfo()
  echo fileInfo
  var minimaFadc = newSeq[float]()
  var minimaGP = newSeq[float]()  
  var idxs = newSeq[int]()
  for run in fileInfo.runs:
    let idx = lowerBound(fadcSettingRuns(), run)
    echo "idx ", idx, " for run ", run
    let (minFadc, minEnergy) = minimum(h5f, run, percentile)
    minimaFadc.add minFadc
    minimaGP.add minEnergy
    idxs.add idx

  let df = toDf(minimaFadc, minimaGP, idxs)
  ggplot(df, aes("minimaFadc", fill = "idxs")) +
    geom_histogram(position = "identity", alpha = 0.5, hdKind = hdOutline) +
    xlab("Pulse amplitude [V]") + ylab("Counts") +
    ggtitle("Activation threshold by smallest pulses triggering FADC") +
    theme_font_scale(1.0, family = "serif") +
    ggsave("~/phd/Figs/FADC/fadc_minima_histo_activation_threshold_mV.pdf")

  ggplot(df, aes("minimaGP", fill = "idxs")) +
    geom_histogram(position = "identity", alpha = 0.5, hdKind = hdOutline) +
    xlab("Energy on GridPix [keV]") + ylab("Counts") +
    ggtitle("Activation threshold by energy recorded on center GridPix") +
    theme_font_scale(1.0, family = "serif") +
    ggsave("~/phd/Figs/FADC/fadc_minima_histo_gridpix_energy.pdf")    

when isMainModule:
  import cligen
  dispatch main
#+end_src
  
#+begin_src sh
./code/fadc_compute_activation_threshold -f ~/CastData/data/CalibrationRuns2017_Reco.h5 --percentile 1
#+end_src

The produced fig. [[fig:fadc:activation_threshold_histo_mV_histo_run2]]
of the largest values found for the activation of the
FADC shows us the actual activation thresholds for the FADC in
volt. We can see that generally the threshold remained constant in
terms of milli volt. That's good to know.

#+CAPTION: Different activation thresholds of the Run-2 data due to different FADC settings, 
#+CAPTION: determined based on the 1-th percentile of the data in the \cefe calibration runs,
#+CAPTION: by using the energy of the center GridPix.
#+NAME: fig:fadc:activation_threshold_histo_mV_histo_run2
[[~/phd/Figs/FADC/fadc_minima_histo_activation_threshold_mV.pdf]]

The second plot fig. [[fig:fadc:activation_threshold_histo_run2]] shows
the activation in terms of the sum of all clusters energies (for one
event number, to take into account multiple clusters) based on the
GridPix energy. Here we see the 'real' activation energy in keV and
can see that unfortunately for the later settings the threshold was
very high. :(

#+CAPTION: Different activation thresholds of the Run-2 data due to different FADC settings, 
#+CAPTION: determined based on the 1-th percentile of the data in the \cefe calibration runs,
#+CAPTION: by using the energy of the center GridPix.
#+NAME: fig:fadc:activation_threshold_histo_run2
[[~/phd/Figs/FADC/fadc_minima_histo_gridpix_energy.pdf]]


Of course this itself does not imply a difference in activation
threshold of the equivalent physical energy!

- [ ] *COMPUTE IN ENERGY INSTEAD OF VOLTAGE*

*NOTE*: From the FADC plots here (pure FADC & GridPix energy
correlated) as well as the raw FADC spectra it's clearly evident that
the actual gain of the FADC went _down_ instead of up after changing
diff 50->20ns and coarse gain 6x->10x and further when going from
50ns-100ns integration time.
To some extent this may make sense: differentiation

According to FADC manual the "differentiation" setting adjusts the RC
differentiation time and therefore
#+begin_quote
DIFFERENTIATE Front panel 6-position switch
selects a differentiation time constant to control the
decay time of the pulse; settings select Out
(equivalent to 150 µs), 20, 50, 100, 200, or 500 ns.

INTEGRATE Front panel 6-position switch selects
an integration time constant to control the risetime
of the pulse; settings select 20, 50, 100,
#+end_quote
And further:
#+begin_quote
Generally speaking, the Integrate time constant can
be selected so that the rise time of the output
pulses is normalized at a rate that is slower than
the rise times of the input pulses. This function is of
greatest value when the pulses originate in a large
detector so that they generate a wide variety of rise
times and are difficult to observe for timing
measurements. The Differentiate time constant is
also selectable and determines the total interval
before the pulse returns to the baseline and allows
a new pulse to be observed. The combination of
integration and differentiation time constants also
contributes to the amount of electronic noise that is
seen in the system, so the resulting waveforms
should be considered from each of these points of
view and adjusted for optimum results.

When the shaping time constants impose
considerable changes in the input waveform, the
nominal gain, which is the product of the Coarse
and Fine control settings, may be degraded
somewhat. This is not normally a problem, since
the gain is constant even though it may be less
than the nominal settings indicate. 
#+end_quote

I.e. this means the differentiation time is responsible for getting
the signal back to baseline. And it's expected that it has an effect
on the amplitude on the signal!

- [ ] *ADD ABOVE AS EXPLANATION FOR THE SEEN BEHAVIOR*
  -> maybe 4 fold plot:
  - histogram as above for lower percentile energies of min vals &
    gridpix energies together with three spectra, one for each setting
    combined in one plot?

All this is effectively good news. This explains also _why_ the
changes to the integration / differentiation had such an effect on the
noise! They simply reduced the effective gain, ergo made the FADC less
sensitive to the same noise!    

Ref:
https://www.ortec-online.com/-/media/ametekortec/manuals/4/474-mnl.pdf?la=en&revision=07c47ecb-5c63-48ff-a393-ba39e45be57b
from here:
https://www.ortec-online.com/products/electronics/amplifiers/474


*** How to compute an effective charge based on FADC signals? [0/1] :noexport:

- [X] The ideas of this section have been merged into the general FADC section

This section should cover our ideas about how we compute an effective
charge (in arbitrary units) based on the FADC signals as a measure
about the effective charge recorded in a signal. To cross correlate
changes in FADC "charge" to GridPix charge.

- [ ] -> is detector behavior over time visible in FADC data?

-> Looking at [[file:Figs/time_vs_55fe_peak_pos_2017.pdf]] generated via
#+begin_src sh
./mapSeptemTempToFePeak ~/CastData/data/CalibrationRuns2017_Reco.h5 --inputs fePixel --inputs feCharge --inputs feFadc
#+end_src
shows a _very_ strong correlation between all three kinds of
calibration.

Note the strong fall in the FADC data in the "left hump" of points is
the change of the integration / differentiation time during the 2017
data taking (up to Dec 2017). But looking closely even there a strong
correlation is visible inside of each "block".

This puts to rest at least _most_ theories that the change might _not_
be a change in the gas gain, but some other effect like electronics!

- [ ] *EXPAND ON THE ABOVE, EXPLAIN THAT IN DETECTOR BEHAVIOR OVER
  TIME!*
  -> given that also temperature is not properly correlated it leaves
  charge up effects (changing effective voltage) and gas flow
  (unlikely as flow constant & pressure always stable).


** Scintillators [/]                                              :noexport:

- [X] *DO THE SCINTILLATORS DESERVE A SECTION HERE TO EXPLAIN THEIR
  USAGE?*
  -> probably not, as this really should be about the stuff we do to
  calibrate and use data. But none of this is needed for
  scintillators.
  -> As a matter of fact the explanation for the FADC above about
  being of 4-fold use might actually be better placed somewhere else?
  -> No, they don't. 

- [X] Well, at the very least we should present the information
  about the number of triggers, clock cycle distributions
  etc. somewhere. Either do it here somewhere or do it before we
  perform the cuts. Both can work.
  -> In principle this chapter here could really be about the detector
  energy etc. while other calculations could be presented before?
  Or we have another short chapter/section about "data overview"?
  -> We have an overview of the CAST data. Later in the background
  rate chapter we then present the scintillator in more detail when
  discussing the cuts.


* Chapter about analysis principle [/]                             :noexport:Software:
:PROPERTIES:
:CUSTOM_ID: sec:analysis_principle
:END:
#+LATEX: \minitoc

- [ ] *This chapter does not serve a proper purpose anymore I
  think. All aspects are already explained in other chapters.*

*IDEA*: Maybe this chapter should only be about everything from raw
data to clustering, charge & energy calibration, gas gain, computation
of geometric properties? Well, two of these are already mentioned in
the previous chapter.

*NOTE:* We need to better understand how to:
- explain the theoretical foundations of what we do, e.g. cluster
  finding algorithms. Certain things, e.g. cluster finding algos could
  also just go to the appendix. Interesting, but technically just a detail.
- introduce the software stack we use
- introduce the physics for the calibration (e.g. ~ToT~ calib, ...)
- explain the algorithms used in the software

Can we disentangle this from the purely detector focused things? I'm
not so sure.

After introducing detector specific calibrations etc. we can go on to
what the steps are that are required to turn a calibrated detector
(one that is sensitive to N electrons essentially) into something that
can do physics.

Need some chapter that talks about the detector specific details that
explain how a limit / physics result is obtained.

- [ ] turn the ingrid reconstruction schematic into a generalized flow chart?

** Take data. Output data is ASCII files
- [X] In [[#sec:reco:tos_data_parsing]]
Parsing of data in format.

Present format.
#+begin_quote
Generic header.

Data.
#+end_quote

Store data in HDF5 files. Not much going on here aside from making it
fast.

** Reconstruct & calibrate data

Read data from HDF5 files. 

What does reconstruction mean?

Multiple things.

*** 1. perform cluster finding
- [X] In [[#sec:reco:data_reconstruction]] (subsection)
Present our current two clustering algorithms. 

- dumb search in radius around each pixel (add foot note that the
  implementation in MarlinTPC had a bug), based on *rectangular*
  search, not circular
- optional: DBSCAN, short introduction give full reference to
  implementation.

**** Investigation of buggy clustering in MarlinTPC             :noexport:

*** 2. for each cluster, compute geometric properties

- [X] In [[#sec:reco:data_reconstruction]] (subsection)

Table of the computed properties.

As they are geometric easy to explain.

Highlight the ones used for likelihood. Done here? 

Show our sketch explaining what each property means from one of the
talks. Maybe need to fix the radius variable?



*** 3. (optional / required for analysis) charge calibration

- [X] In [[#sec:operation_calibration:tot_calibration]]

Use Timepix ~ToT~ calibration (ref theory section before where we
explain how it works).

Given ~ToT~ calibration apply function to get number of electrons (given
that we ran in ~ToT~ mode).

*** 4. (optional / required for analysis) compute gas gain

- [X] [[#sec:daq:polya_distribution_threshold]]

Computing gas gain. Polya fit. Explain not fit parameter used, but
mean of data.

Heavy gas gain variation over time. 

Explain that thus behavior chosen that minimizes effect by binning in
time.

90 minutes.

Show plot with old way (full runs) vs. new length.

Results in stable operation. This section probably belongs somewhere
else? 

**** Study for optimal gas gain time length                     :noexport:



*** 5. (optional / required for analysis) energy computation

- [X] [[#sec:calibration:energy]]

Requires: charge calibration, gas gain

Very easy in theory. In practice complicated.

Theoretically, two ways:

1. pixel counting. Due to single electron detection efficiency (or a
   slight correction for under/overcounting) can just multiply hits by
   eV per hit
2. charge calibration plus reference spectrum of 55Fe runs. 

Both cases are very simple *iff* the detector is stable over
time. Then just take closest 55Fe run and compute correction factor
for hits / conversion factor from peak in charge values.

But: instability means we need to average over more data.

Compute for all runs.

Fit.

Apply fit.

*** 6. (optional) FADC reconstruction
- [X] In [[#sec:calib:fadc]]
Apply pedestals.

Determine lowest point. Determine rising / falling times in ns.

Compute other properties.

** Compute reference spectra 

Possibly explain in chapter about CDL? Or talk there only about the
*data* we took there, but not in detail about *what* this data is *for*?

If so explain here.

** Log file reader to get tracking (maybe no export)

Talk about log file reader (full section definitely :noexport:), used
to mark times in runs that correspond to tracking.

** Likelihood method

Explain likelihood method in theory (maybe do in section before).

Explain our methods for linear interpolation between the reference
spectra.

Apply reference data for limit at specific custom software efficiency.

Started at 80% reference and then tweaked for optimal ε = S/√B (or
something). 

Likelihood method gives us everything we need for background
rate. Whatever comes out gives us left over clusters.

*** Septem veto
:PROPERTIES:
:CUSTOM_ID: sec:septem_veto
:END:

Talk about the septem veto we finally use.

In the septem veto the main idea is to go back to the raw data for any
event, which contains a cluster on the central chip, which is
signal-like based on the likelihood method presented above. For these,
a so called 'septem event' is built based on the raw pixel data of all
chips. This is simply an 'event' in the same notion as understood by
the =reconstruction= tool (c/f [[sec:reconstruction]]), i.e. a two
dimensional array of the ~ToT~ values. Except in this case it is not a
$256 \times 256$ array, but rather a $3 \cdot 256 \times 3 \cdot 256$
array, where the full septemboard detector is merged into a single
event without any spacing between the chips (more on that below). 

These septem events are then pushed through the whole reconstruction
and calibration pipeline. Thanks to starting from an event that now
includes information that was previously not taken into account (pixel
activity outside the center chip), the cluster finding algorithm can
detect larger clusters than previously. This can change the shape of
the cluster that was previously considered signal like. In the case
that this cluster now looks more like background, it will be vetoed by
this technique.

The decision to merge the different chips into a single event
*without* any spacing between the chips is made to ensure good cluster
finding. The spacing between chips is of course a dead zone where no
activity can be measured. For a cluster finding algorithm this may
cause a cutoff that should not happen, as the information is simply
not *available*. Ideally, one could imagine an algorithm that
interpolates data between the chips based on the information on the
two neighboring chips. But this is too experimental. 

If there is data on the neighboring chip, it is extremely likely there
was ionization between the chips as well, meaning the merging of the
chips "only" makes the event less eccentric than the physical
event. If there is no data on the neighboring chip, the merging has no
effect, the cluster will simply look the same as in the original
single chip event. 

This excludes two possibilities:
1. a physical event may have no active pixels between the chips, despite
   having active pixels on both chip borders. This is extremely
   unlikely, if the events are significantly close to the border. The
   main case of this would be two real X-rays (extremely low
   probability) or either of the clusters is a track parallel to
   border of the chip. 
2. a more likely loss information for events without information on
   the neighboring chips, despite the physical event being more
   eccentric than the recorded one. This is a real limitation that
   cannot be worked around.

**** Explain why no spacing between chips

**** Hough transformation experiments                           :noexport:

*** Scintillator veto

Talk about scintillator veto. Main ideas of course.

*** FADC veto

Explain the veto we use based on FADC.



*** Comparison with other attempts                               :noexport:

Show the other attempts we did about the different ways to
interpolate.

** Compute limit

Limit computation done. Needs to be after ray tracing introduction I'd
say. Input for theory is required.

Perform signal / background rejection.

Get background rate + signals in tracking.

Get expected flux from theory.

Get detector efficiencies.

Combine efficiencies & theory flux in raytracing simulation.

Compute 'real' expected signal.

Use suitable limit calculation method to compute a limit.

We can split this into two pieces?:

** How to compute background rate

** How to compute a limit

For =mclimit= use the notes in StatusAndProgress about how limit
calculation works. Might be good in general, because a lot of it
applies elsewhere anyway.

Describe unbinned likelihood method from Nature paper adapted to our
work. We can plot some funny plots explaining how it works.

* Finding signal and defining background | Background rate computation :Analysis:
:PROPERTIES:
:CUSTOM_ID: sec:background
:END:
#+LATEX: \minitoc

With the CAST data fully reconstructed and energy calibrated it is
time to define the methods used to extract axion candidates and derive
a background rate from the data sec. [[#sec:background:likelihood_method]]
and sec. [[#sec:background:mlp]]. A few extra inputs are necessary,
sec. [[#sec:cdl]]. Next apply it to the data
(sec. [[#sec:background:likelihood_cut]]) and then improve it further
using additional detector features (sec. *???*). Finally, we compare
the resulting background rate to other detectors and try to understand
the remaining background (sec. *???*).

The methods discussed in this chapter are generally classifiers that
predict how 'signal-like' a cluster is. Based on this prediction we
will usually define a cut value as to keep a cluster as a potential
signal. This means that if we apply the method to background data
(that is, CAST data taken outside of solar trackings) we recover the
'background rate'; the irreducible amount of background left (at a
certain efficiency), which is appears signal-like. If instead we apply
the same methods to CAST solar tracking data we get instead a set of
'axion induced X-ray candidates'. [fn:candidates_contain_background]
In the context of the chapter we commonly talk about "background
rates", but the equivalent meaning in terms of tracking data and
candidates should be kept in mind.

- [ ] *REFER TO CAST DATA SUMMARY TABLE AND PRESENT AN ENERGY SPECTRUM
  WITHOUT ANY CUTS WHATSOEVER*

Likelihood method demands info about X-ray properties, hence:

[fn:candidates_contain_background] Of course the set of candidates
contains background itself. The terminology 'candidate' intends
to communicate that each candidate may be a background event or
potentially a signal due to axions. But that is part of chapter
[[#sec:limit]]. 
  
** Likelihood method [/]
:PROPERTIES:
:CUSTOM_ID: sec:background:likelihood_method
:END:

The detection principle of the GridPix detector implies physically
different kinds of events will have different geometrical shapes. An
example can be seen in
fig. sref:fig:background:eccentricity_signal_background, comparing
the cluster eccentricity of \cefe calibration events with background
data. This motivates usage of a geometry based approach to determine
likely signal-like or background-like clusters. The method to
distinguish the two types of events is a likelihood cut, based on the
one in cite:krieger2018search. It effectively assigns a single value
to each cluster for the likelihood that it is a signal-like event. 

Specifically, this likelihood method is based on three different
geometric properties:  
1. the eccentricity $ε$ of the cluster, determined by
   computing the long and short axis of the two dimensional cluster and
   then computing the ratio of the RMS of the projected positions of
   all active pixels within the cluster along each axis.
2. the fraction of all pixels within a circle of the radius of one
   transverse RMS from the cluster center, $f$.
3. the length of the cluster (full extension along the long axis)
   divided by the transverse RMS, $l$. 

These variables are obviously highly correlated, but still provide a
very good separation between the typical shapes of X-rays and
background events. They mainly characterize the "spherical-ness" as
well as the density near the center of the cluster, which is precisely
the intuitive sense in which these type of events differ. For each of
these properties we define a probability density function
$\mathcal{P}_i$, which can then be used to define the likelihood of a
cluster with properties $(ε, f, l)$ to be signal-like:

#+NAME: eq:background:likelihood_def
\begin{equation}
\mathcal{L}(ε, f, l) = \mathcal{P}_{ε}(ε) \cdot \mathcal{P}_{f}(f)
\cdot \mathcal{P}_l(l)
\end{equation}

where the subscript is denoting the individual probability density 
and the argument corresponds to the individual value of each
property.

This raises the important question of what defines each individual
probability density $\mathcal{P}_i$. In principle it can be
defined by computing a normalized density distribution of a known
dataset, which contains representative signal-like data. The \cefe
calibration data from CAST contains such representative data, if not
for one problem: the properties used in the likelihood method are
energy dependent, as seen in
fig. sref:fig:background:eccentricity_photo_escape, a comparison of
the eccentricity of X-rays from the photopeak of the \cefe calibration
source compared to those from the escape peak. The CAST calibration
data can only characterize two different energies, but the expected
axion signal is a (model dependent) continuous spectrum. 

For this reason data was taken using an X-ray tube with 8 different
target / filter combinations to provide the needed data to compute
likelihood distributions for X-rays of a range of different
energies. The details will be discussed in the next section,
[[#sec:cdl]]. 

#+begin_src subfigure
(figure ()
        (subfigure (linewidth 0.5) (caption "Calibration \\& background") (label "fig:background:eccentricity_signal_background")
                   (includegraphics (list (cons 'width (linewidth 0.95)))
                                    "~/phd/Figs/background/eccentricity_calibration_background.pdf"))
        (subfigure (linewidth 0.5) (caption "Photo \\& Escapepeak") (label "fig:background:eccentricity_photo_escape")
                   (includegraphics (list (cons 'width (linewidth 0.95)))
                                    "~/phd/Figs/background/eccentricity_photo_escape_peak.pdf"))
        (caption
         (subref "fig:background:eccentricity_signal_background")
          "Comparison of the eccentricity of clusters from a calibration and a background run.
           Background events are clearly much more eccentric on average. "
         (subref "fig:background:eccentricity_photo_escape")
          "The X-ray properties are energy dependent as can be seen in the comparison between
           the eccentricity of X-rays from the photo peak compared to those from the escape
           peak. A kernel density estimation is used in both figures.")
        (label "fig:background:properties_energy_type_dependent"))
#+end_src

*** TODOs for this section [/]                                   :noexport:

- [X] *REPHRASE SUCH THAT THIS XRAY DATA BECOMES A REQUIREMENT*
  -> Relates to second paragraph with numbers 

- [X] *REPLACE EVENT FIGURES BY EXAMPLE HISTOGRAM OF DIFFERENT
  PROPERTIES BACKGROUND / CALIBRATION*

- [X] *ADD NOTE THAT THERE IS _ALSO_ AN ENERGY DEPENDENCE ON SIGNAL
  CLUSTER SHAPES*

*** Generate plot of eccentricity signal vs. background          :noexport:

We will now generate a plot comparing the eccentricity of signal
events from calibration data to background events. In addition another
comparison will be made between photopeak photons and escape peak
photons to show that events at different energies have different
shapes, motivating the need for the X-ray tube data at different energies.

#+begin_src nim :tangle code/eccentricity_background_signal.nim
import ggplotnim, nimhdf5
import ingrid / [tos_helpers, ingrid_types]

proc read(f: string, run: int): DataFrame =
  withH5(f, "r"):
    result = h5f.readRunDsets(
      run = run,
      chipDsets = some((chip: 3, dsets: @["eccentricity", "centerX", "centerY", "energyFromCharge"]))
    )

proc main(calib, back: string) =

  # read data from each file, one fixed run with good statistics
  # cut on silver region
  let dfC = read(calib, 128)
  let dfB = read(back, 124)
  var df = bind_rows([("Calibration", dfC), ("Background", dfB)], "Type")
    .filter(f{float -> bool: inRegion(`centerX`, `centerY`, crSilver)},
            f{float: `eccentricity` < 10.0})
  ggplot(df, aes("eccentricity", fill = "Type")) +
    #geom_histogram(bins = 100,
    #               hdKind = hdOutline, 
    #               position = "identity",
    #               alpha = 0.5,
    #               density = true) +
    geom_density(color = "black", size = 1.0, alpha = 0.7, normalize = true) + 
    ggtitle("Comparison of the eccentricity of calibration and background data") + 
    ggsave("Figs/background/eccentricity_calibration_background.pdf", useTeX = true, standalone = true)

  proc splitPeaks(x: float): string =
    if x >= 2.75 and x <= 3.25:
      "Escapepeak"
    elif x >= 5.55 and x <= 6.25:
      "Photopeak"
    else:
      "Unclear"

  let dfP = dfC
      .mutate(f{float: "Peak" ~ splitPeaks(`energyFromCharge`)})
      .filter(f{string: `Peak` != "Unclear"},
              f{`eccentricity` <= 2.0})
  ggplot(dfP, aes("eccentricity", fill = "Peak")) +
    #geom_histogram(bins = 50,
    #               hdKind = hdOutline, 
    #               position = "identity",
    #               alpha = 0.5,
    #               density = true) +
    geom_density(color = "black", size = 1.0, alpha = 0.7, normalize = true) +
    ggtitle(r"Comparison of the X-rays from the $^{55}\text{Fe}$ photopeak (5.9 keV) and escapepeak (3 keV)",
      titleFont = font(10.0)) + 
    ggsave("Figs/background/eccentricity_photo_escape_peak.pdf", useTeX = true, standalone = true)
    
when isMainModule:
  import cligen
  dispatch main
#+end_src

yielding
[[~/phd/Figs/background/eccentricity_calibration_background.pdf]]
and
[[~/phd/Figs/background/eccentricity_photo_escape_peak.pdf]]

** CAST Detector Lab [/]
:PROPERTIES:
:CUSTOM_ID: sec:cdl
:END:

In this section we will cover the X-ray tube data taken at the CAST
Detector Lab (CDL) at CERN. First, we will show the setup in
sec. [[#sec:cdl:setup]]. Next the different target / filter combinations
that were used will be discussed and the measurements presented in
sec. [[#sec:cdl:measurements]]. These measurements then are used to define
the probability densities for the likelihood method, see
sec. [[#sec:cdl:derive_probability_density]]. Further, they can be used to
measure the energy resolution of the detector at different energies,
sec. [[#sec:cdl:energy_resolution]]. And finally, in
sec. [[#sec:cdl:cdl_morphing]] we cover a few more details on the linear
interpolation we perform to compute a likelihood distribution at an
arbitrary energy.

The data presented here was also part of the master thesis of Hendrik
Schmick cite:SchmickMaster. Further, the selection of lines and
approach follows the ideas used for the single GridPix detector in
cite:krieger2018search with some notable differences. For a reasoning
for one particular difference in terms of data treatment, see appendix
[[#sec:appendix:fit_by_run_justification]]

#+begin_quote
Note: in the course of the CDL related sections the terms
target/filter combination (implicitly including the used high voltage
setting) is used interchangeably with the main fluorescence line (or
just 'the fluorescence line') targeted in a specific measurement. Be
careful while reading about applied high voltages in $\si{kV}$ and
energies in $\si{keV}$. The produced fluorescence lines typically have
about 'half' the energy in $\si{keV}$ as the applied voltage in
$\si{kV}$. See tab. [[tab:cdl:run_overview_tab]] in
sec. [[#sec:cdl:measurements]] for the precise relation.
#+end_quote

*** TODOs for this section [/]                                   :noexport:

- [ ] *(RE)WRITE THE FIT BY RUN JUSTIFICATION SECTION AND REFERENCE
  HERE*

- [X] *FIND OUT WHAT CHIP CALIBRATION USED FOR CDL*
  -> It was the Run-3 calibration. Checked by comparing the ~fsr~
  files in the CDL TOS run directories with those of the
  ~ChipCalibration~ directory of the TPA resources. The ones from
  Run-3 match and the ones from Run-2 don't.
- [X] *REFERENCE HENDRIK MSC FOR CDL*

- [X] *CREATE PLOT SIMILAR TO*:
  ~/org/Figs/statusAndProgress/cdl_vs_background/eccentricity_ridgeline_XrayReferenceFile2018.h5_2018.pdf~
  but using our custom colors and shown as a KDE similar to the median
  energy cluster KDE plot!

- [ ] *INCLUDE ALL CDL PLOTS GENERATED BY ~cdl_spectrum_creation~ WE
  DON'T PUT INTO MAIN THESIS INTO EXTENDED!*

- [ ] *reintroduce something along these lines?*
  #+begin_comment
The distributions which the previous background rate plots were based
on were obtained in 2014 with the Run-1 detector at the CAST Detector
Lab (CDL).
Using a different detector for this extremely sensitive part of the
analysis chain will obviously introduce systematic errors.
Thus, new calibration data was taken with the current Run-2 and Run-3
detector from 15-19 Feb 2019.
  #+end_comment

*** CDL setup [/]
:PROPERTIES:
:CUSTOM_ID: sec:cdl:setup
:END:

The CAST detector lab provides a vacuum test stand, which contains an
X-ray tube. A Micromegas-like detector can easily be mounted to the
rear end of the test stand. An X-ray tube uses a filament to produce
free electrons, which are then accelerated with a high voltage of the
order of multiple $\si{kV}$. The main part of the setup is a
rotateable wheel inside the vacuum chamber, which contains a set of 18
different positions with 8 different target materials, as seen in
tab. [[tab:cdl:targets]]. The highly energetic electrons interacting with
the target material generate a continuous Bremsstrahlung spectrum with
characteristic lines depending on the target. A second rotateable
wheel contains a set of 11 different filters, see
tab. [[tab:cdl:filters]]. These can be used to filter out undesired parts
of the generated spectrum by choosing a filter that is opaque in the
undesired energy ranges.

As mentioned previously in sec. [[#sec:cast:timeline]] the detector was
dismounted from CAST in Feb. 2019 and installed in the CAST detector
lab on <2019-02-14 Thu>. The week from <2019-02-15 Fri> to
<2019-02-21 Thu> X-ray tube data was taken in the CDL.

Fig. sref:fig:cdl:cdl_setup shows the whole vacuum test stand, which
contains the X-ray tube on the front left end and the Septemboard
detector installed on the rear right, visible by the red HV cables and
yellow HDMI cable. In fig. sref:fig:cdl:detector_setup we see the
whole detector part from above, with the Septemboard detector
installed to the vacuum test stand on the left side. The water cooling
system is seen in the bottom right, with the power supply above
that. The copper shielded box slightly right of the center is the
Ortec pre-amplifier of the FADC, which is connected via a heavily
shielded LEMO cable to the Septemboard detector. This cable was
available in the CAST detector lab and proved invaluable for the
measurements as it essentially removed any noise visible in the FADC
signals (compared to the significant noise issues encountered at CAST,
sec. [[#sec:cast:data_taking_woes_2017]]). Given the activation threshold
of the FADC with CAST amplifier settings (see
sec. [[#sec:calib:fadc:amplifier_settings]]) at around $\SI{2}{keV}$, the
amplification needed to be adjusted in the CDL on a per-target
basis. The shielded cable allowed the FADC to even act as a trigger
for $\SI{277}{eV}$ $\ce{C}_{Kα}$ X-rays without any noise problems. [fn:shielded_lemo_cable]

#+begin_src subfigure
(figure ()
        (subfigure (linewidth 0.5) (caption "Full setup") (label "fig:cdl:cdl_setup")
                   (includegraphics (list (cons 'width (linewidth 0.95)))
                                    "~/phd/Figs/CDL/IMG_20190214_155459.jpg"))
        (subfigure (linewidth 0.5) (caption "Detector setup") (label "fig:cdl:detector_setup")
                   (includegraphics (list (cons 'width (linewidth 0.95)))
                                    "~/phd/Figs/CDL/IMG_20190214_191752.jpg"))
        (caption
         (subref "fig:cdl:cdl_setup")
         " shows the full vacuum test stand containing the X-ray tube with 
          the Septemboard detector installed at the rear, visible by the red HV and yellow HDMI cables. "
         (subref "fig:cdl:detector_setup")
         " is a view of the detector setup from above. On the left hand side
           is the detector mounted to the vacuum setup. The water cooling is seen in the bottom right, connected via
           the blue tubes. The gas supply is in red tubing and the power supply is visible on the right above the
           water cooling (with a green Phoenix connector). The copper shielded cable is a LEMO cable for the FADC
           signal going to the pre-amplifier (with the University of Bonn sticker).")
        (label "fig:cdl:cdl"))
#+end_src

#+CAPTION: Table of all available target materials in the CAST detector lab and their respective
#+CAPTION: position on a rotateable wheel.
#+NAME: tab:cdl:targets
#+ATTR_LATEX: :booktabs t  
| Target Material | Position |
|-----------------+----------|
| Ti              |        1 |
| Ag              |        2 |
| Mn              |        3 |
| C               |        4 |
| BN              |        5 |
| Au              |        6 |
| Al              |   7 - 11 |
| Cu              |  12 - 18 |

#+CAPTION: Table of all available filters in the CAST detector lab and their respective
#+CAPTION: position on a rotateable wheel.
#+NAME: tab:cdl:filters
#+ATTR_LATEX: :booktabs t  
| Filter material                             | Position |
|---------------------------------------------+----------|
| Ni 0.1 mm                                   |        1 |
| Al 5 µm                                     |        2 |
| PP G12 (EPIC)                               |        3 |
| PP G12 (EPIC)                               |        4 |
| Cu 10 µm                                    |        5 |
| Ag 5 µm (99.97%) AG000130/24                |        6 |
| Fe 50 µm                                    |        7 |
| Mn foil 50 µm (98.7% + permanent polyester) |        8 |
| Co 50 µm (99.99%) CO0000200                 |        9 |
| Cr 40 µm (99.99% + permanent polyester)     |       10 |
| Ti 50 µm (99.97%) TI000315                  |       11 |

[fn:shielded_lemo_cable] In hindsight it seems obvious that a
similarly shielded LEMO cable should have been used for the
measurements at CAST. Unfortunately, due to lack of experience with
electromagnetic interference this was not considered before.

**** TODOs for this section [/]                                 :noexport:

- [ ] *SOMEWHERE HERE ADD REFERENCE TO WHAT EPIC REFERS TO!*
  -> See the extra info subsection below. Link to a website and we
  need a link to some XMM Newton paper!!!



- [ ] *CHECK XRAY TUBE EXPLANATION FOR CORRECTNESS!*
- [ ] *SAY SOMETHING ABOUT TURBO AND SPUTTER ION PUMPS?*  
- [ ] *CONSIDER ADDING PICTURE OF TARGET AND FILTER WHEEL?* See
  Hendrik's thesis. He has pictures from Tobi.  

- [X] 2 pictures of setup, side by side
- [X] one paragraph about how X-ray tube works. Bremsstrahlung via HV, hit
  target & then filter  
- [X] table of available targets & filters
- [X] measurement dates (which week)
- ?

- [ ] *CONSIDER MOVING THESE TABLES TO AN APPENDIX!*
  
*** CDL measurements (from =StatusAndProgress=) [0/9]
:PROPERTIES:
:CUSTOM_ID: sec:cdl:measurements
:END:

The measurements were performed with 8 different target and filter
combinations, with a higher density towards lower energies due to the
nature of the expected solar axion flux. For each target and filter at
least two runs were measured, one with and one without using the FADC
as a trigger. The latter was taken to collect more statistics in a
shorter amount of time, as the FADC readout slows down the data taking
speed due to increased dead time. Table [[tab:cdl:run_overview_tab]]
provides an overview of all data taking runs, the target, filter and
HV setting, the main X-ray fluorescence line targeted and its
energy. Finally, the mean position of the main fluorescence line in
the charge spectrum and its width as determined by a fit is shown. As
can be seen the position moves in some cases significantly between
different runs, for example in case of the $\ce{Cu}-\ce{Ni}$
measurements. Also within a single run some stronger variation can be
seen, evident by the much larger line width for example in run
$\num{320}$ compared to $\num{319}$. 

The variability both between runs for the same target and filter as
well as within a run shows the detector was undergoing gas gain
changes similar to the variations at CAST
(sec. [[#sec:calib:detector_behavior_over_time]]). With an understanding
that the variation is correlated to the temperature this can be
explained. The small laboratory underwent significant temperature
changes due to the presence of 3 people, all running machines and
freely opening and closing windows, in particular due to very warm
temperatures relative to a typical February in
Geneva. [fn:weather_at_cdl] Fig. [[fig:cdl:gas_gain_by_cdl_run]] shows the
calculated gas gains (based on $\SI{90}{min}$ intervals) for each run
colored by the target/filter combination. It undergoes a strong,
almost exponential change over the course of the data taking
campaign. Because of this variation between runs, each run is treated
fully separately in contrast to [[cite:&krieger2018search]], where all
runs for one target / filter combination were combined.

#+CAPTION: Gas gain of each gas gain slice of $\SI{90}{min}$ in all CDL runs.
#+CAPTION: A significant decrease over the runs (equivalent to the week of data taking)
#+CAPTION: is visible. Multiple points for the same run correspond to multiple
#+CAPTION: gas gain time slices as explained in sec. [[#sec:calib:gas_gain_time_binning]].
#+NAME: fig:cdl:gas_gain_by_cdl_run
[[~/phd/Figs/CDL/gas_gain_by_run_and_tfkind.pdf]]

Fortunately, this significant change of gas gain does not have an
impact on the distributions of the cluster properties. See appendix
[[#sec:appendix:fit_by_run:gas_gain_var_cluster_prop]] for comparisons of the
properties of the different runs (and thus different gas gains) for
each target and filter combination.

As the main purpose is to use the CDL data to generate reference
distributions for certain cluster properties, the relevant clusters
that correspond to known X-ray energies must be extracted from the
data. This is done in two different ways:
1. A set of fixed cuts (one set for each target/filter combination) is
   applied to each run, as presented in
   tab. [[tab:cdl:cdl_cleaning_cuts]]. This is the same set as used in
   cite:krieger2018search. Its main purpose is to remove events with
   multiple clusters and potential background contributions.
2. By cutting around the main fluorescence line in the charge spectrum
   in a $3σ$ region, for which the spectrum needs to be fitted with
   the expected lines, see sec. [[#sec:cdl:fits_to_spectra]]. This is done
   on a run-by-run basis.

The remaining data after both sets of cuts can then be combined for
each target/filter combination to make up the distributions for the
cluster properties as needed for the likelihood method.

For a reference of the X-ray fluorescence lines (for more exact values
and $\alpha_1$, $\alpha_2$ values etc.) see
tab. [[tab:theory:xray_fluorescence]]. The used EPIC filter refers to a
filter developed for the EPIC camera of the XMM-Newton telescope. It
is a bilayer of $\SI{1600}{\angstrom}$ polyimide and
$\SI{800}{\angstrom}$ aluminium. For more information about the EPIC
filters see references
cite:struder2001xmm_pnccd,turner2001xmm_mos,barbera2003monitoring,barbera2016thin,
in particular cite:barbera2016thin for an overview of the materials and production.

#+CAPTION: Overview of all runs taken behind the X-ray tube, whether they ran with or without FADC,
#+CAPTION: their targets, filters and high voltage setting and information about the major fluorescence
#+CAPTION: line and its energy. Finally, the mean $μ$ and width $σ$ of the main fluorescence line as
#+CAPTION: determined by the fit plus the resulting energy resolution $μ/σ$ is shown.
#+NAME: tab:cdl:run_overview_tab
#+ATTR_LATEX: :float sideways :booktabs t
|-----+-------+--------+--------+---------+-------------------------------+--------------+------------------------+-----------------------+-----------------------|
| Run | FADC? | Target | Filter | HV [kV] | Line                          | Energy [keV] | μ [e⁻]                 | σ [e⁻]                | σ/μ                   |
|-----+-------+--------+--------+---------+-------------------------------+--------------+------------------------+-----------------------+-----------------------|
| 319 | y     | Cu     | Ni     |      15 | $\ce{Cu}$ $\text{K}_{\alpha}$ |         8.04 | $\num{9.509(21)e+05}$  | $\num{7.82(18)e+04}$  | $\num{8.22(19)e-02}$  |
| 320 | n     | Cu     | Ni     |      15 |                               |              | $\num{9.102(22)e+05}$  | $\num{1.010(19)e+05}$ | $\num{1.110(21)e-01}$ |
| 345 | y     | Cu     | Ni     |      15 |                               |              | $\num{6.680(12)e+05}$  | $\num{7.15(11)e+04}$  | $\num{1.070(16)e-01}$ |
|-----+-------+--------+--------+---------+-------------------------------+--------------+------------------------+-----------------------+-----------------------|
| 315 | y     | Mn     | Cr     |      12 | $\ce{Mn}$ $\text{K}_{\alpha}$ |         5.89 | $\num{6.321(29)e+05}$  | $\num{9.44(26)e+04}$  | $\num{1.494(41)e-01}$ |
| 323 | n     | Mn     | Cr     |      12 |                               |              | $\num{6.328(11)e+05}$  | $\num{7.225(89)e+04}$ | $\num{1.142(14)e-01}$ |
| 347 | y     | Mn     | Cr     |      12 |                               |              | $\num{4.956(10)e+05}$  | $\num{6.211(82)e+04}$ | $\num{1.253(17)e-01}$ |
|-----+-------+--------+--------+---------+-------------------------------+--------------+------------------------+-----------------------+-----------------------|
| 325 | y     | Ti     | Ti     |       9 | $\ce{Ti}$ $\text{K}_{\alpha}$ |         4.51 | $\num{4.83(31)e+05}$   | $\num{4.87(83)e+04}$  | $\num{1.01(18)e-01}$  |
| 326 | n     | Ti     | Ti     |       9 |                               |              | $\num{4.615(87)e+05}$  | $\num{4.93(25)e+04}$  | $\num{1.068(57)e-01}$ |
| 349 | y     | Ti     | Ti     |       9 |                               |              | $\num{3.90(23)e+05}$   | $\num{4.57(57)e+04}$  | $\num{1.17(16)e-01}$  |
|-----+-------+--------+--------+---------+-------------------------------+--------------+------------------------+-----------------------+-----------------------|
| 328 | y     | Ag     | Ag     |       6 | $\ce{Ag}$ $\text{L}_{\alpha}$ |         2.98 | $\num{3.0682(97)e+05}$ | $\num{3.935(79)e+04}$ | $\num{1.283(26)e-01}$ |
| 329 | n     | Ag     | Ag     |       6 |                               |              | $\num{3.0349(51)e+05}$ | $\num{4.004(40)e+04}$ | $\num{1.319(13)e-01}$ |
| 351 | y     | Ag     | Ag     |       6 |                               |              | $\num{2.5432(63)e+05}$ | $\num{3.545(49)e+04}$ | $\num{1.394(20)e-01}$ |
|-----+-------+--------+--------+---------+-------------------------------+--------------+------------------------+-----------------------+-----------------------|
| 332 | y     | Al     | Al     |       4 | $\ce{Al}$ $\text{K}_{\alpha}$ |         1.49 | $\num{1.4868(50)e+05}$ | $\num{2.027(38)e+04}$ | $\num{1.364(26)e-01}$ |
| 333 | n     | Al     | Al     |       4 |                               |              | $\num{1.3544(30)e+05}$ | $\num{2.539(24)e+04}$ | $\num{1.875(18)e-01}$ |
|-----+-------+--------+--------+---------+-------------------------------+--------------+------------------------+-----------------------+-----------------------|
| 335 | y     | Cu     | EPIC   |       2 | $\ce{Cu}$ $\text{L}_{\alpha}$ |        0.930 | $\num{8.885(99)e+04}$  | $\num{1.71(11)e+04}$  | $\num{1.93(13)e-01}$  |
| 336 | n     | Cu     | EPIC   |       2 |                               |              | $\num{7.777(94)e+04}$  | $\num{2.39(14)e+04}$  | $\num{3.08(19)e-01}$  |
| 337 | n     | Cu     | EPIC   |       2 |                               |              | $\num{7.86(15)e+04}$   | $\num{2.47(11)e+04}$  | $\num{3.14(15)e-01}$  |
|-----+-------+--------+--------+---------+-------------------------------+--------------+------------------------+-----------------------+-----------------------|
| 339 | y     | Cu     | EPIC   |     0.9 | $\ce{O }$ $\text{K}_{\alpha}$ |        0.525 | $\num{5.77(11)e+04}$   | $\num{1.38(22)e+04}$  | $\num{2.39(39)e-01}$  |
| 340 | n     | Cu     | EPIC   |     0.9 |                               |              | $\num{4.778(31)e+04}$  | $\num{1.230(50)e+04}$ | $\num{2.58(11)e-01}$  |
|-----+-------+--------+--------+---------+-------------------------------+--------------+------------------------+-----------------------+-----------------------|
| 342 | y     | C      | EPIC   |     0.6 | $\ce{C }$ $\text{K}_{\alpha}$ |        0.277 | $\num{4.346(36)e+04}$  | $\num{1.223(29)e+04}$ | $\num{2.814(70)e-01}$ |
| 343 | n     | C      | EPIC   |     0.6 |                               |              | $\num{3.952(20)e+04}$  | $\num{1.335(14)e+04}$ | $\num{3.379(40)e-01}$ |
|-----+-------+--------+--------+---------+-------------------------------+--------------+------------------------+-----------------------+-----------------------|

#+CAPTION: Cuts applied to the CDL datasets in order to roughly clean of potential
#+CAPTION: background events and double hits (recognized as single cluster).
#+CAPTION: RMS refers to the transverse RMS of the clusters.
#+NAME: tab:cdl:cdl_cleaning_cuts
#+ATTR_LATEX: :align lllrrrrr
|--------+--------+-------------------------------+-----+--------+-----------+-----------+--------------|
| Target | Filter | line                          |  HV | length | rms_T_min | rms_T_max | eccentricity |
|--------+--------+-------------------------------+-----+--------+-----------+-----------+--------------|
| Cu     | Ni     | $\ce{Cu}$ $\text{K}_{\alpha}$ |  15 |        |       0.1 |       1.0 |          1.3 |
| Mn     | Cr     | $\ce{Mn}$ $\text{K}_{\alpha}$ |  12 |        |       0.1 |       1.0 |          1.3 |
| Ti     | Ti     | $\ce{Ti}$ $\text{K}_{\alpha}$ |   9 |        |       0.1 |       1.0 |          1.3 |
| Ag     | Ag     | $\ce{Ag}$ $\text{L}_{\alpha}$ |   6 |    6.0 |       0.1 |       1.0 |          1.4 |
| Al     | Al     | $\ce{Al}$ $\text{K}_{\alpha}$ |   4 |        |       0.1 |       1.1 |          2.0 |
| Cu     | EPIC   | $\ce{Cu}$ $\text{L}_{\alpha}$ |   2 |        |       0.1 |       1.1 |          2.0 |
| Cu     | EPIC   | $\ce{O }$ $\text{K}_{\alpha}$ | 0.9 |        |       0.1 |       1.1 |          2.0 |
| C      | EPIC   | $\ce{C }$ $\text{K}_{\alpha}$ | 0.6 |    6.0 |       0.1 |       1.1 |              |
|--------+--------+-------------------------------+-----+--------+-----------+-----------+--------------|

[fn:weather_at_cdl] The measurement campaign at the CDL was done in
February. However, the weather was nice and sunny most of the week, if
I remember correctly. The laboratory has windows towards the
south-east. With reasonably cold outside temperatures and sunshine
warming the lab in addition to machines and people, opening and
closing windows changed the temperature on short time scales, likely
significantly contributing to the detector instability. In the
extended version of this thesis you'll find a section below this one
containing the weather data at the time of the CDL data taking.

[fn:epic_filter] The filter in the CDL was indicated as 'G12', which
based on the references should be the 'medium filter'. Hence the used numbers.

**** TODOs from the section above [9/15]                        :noexport:

This section contains the more generic TODOs that are longer / done
and have no specific relevance to an existing paragraph / fig / tab

- [X] *THIS STRONGER VARIATION IS NOT REALLY VISIBLE IN THE WAY WE
  LOOK AT THE DATA NOW, I.E. VIA FITS TO FULL SPECTRA!* 

- [X] *HAVE EXPLICIT SUBSECTION IN ABOVE APPENDIX ONLY ABOUT
  PROPERTIES PER ENERGY*
  -> Note: this is too important as a general concept to not show up
  in the thesis at all! So not only for extended version, but at least
  in appendix!
  - [ ] *JUST NEED TO CAPTION ALL PLOTS IN APPENDIX*

- [ ] *CONSIDER TAKING EXPONENTIAL PART 10⁵ OF CHARGES AND PUT INTO HEADER!*

- [X] *ADD SCHMICK THESIS REFERENCE*

- [X] *PLOT OF ALL SPECTRA AS SINGLE W/ CALIBRATED ENERGY*
- [X] *FOR APPENDIX/EXTENDED: HISTOGRAMS OF ALL SPECTRA BY RUN*
  -> what we hacked into cdl_spectrum_creation!

- [X] *REMOVE PIXEL MEAN*
  -> replace by some measure of charge position? More importantly, we
  need to replace the table anyway due to our new fit by run approach.
  Better to show the mean charges and then the σ of the line width?
  The variance of that over the different runs is a good enough
  approximator for "run internal" variation. 

- [X] *LOOK INTO THE CDL TEMPERATURES THAT WE HAVE. CREATE A PLOT OF
  ALL TEMP DATA NOW THAT IT'S PART OF RAW_DATA*.
  -> possibly just add them to the gas gain plot here.
  - [ ] once we have that maybe decide if it's worth including them. For
  sure include those in the extended version!

- [ ] *CREATE PLOT LIKE RIDGE LINE PLOT BUT NOT COMPARING BETWEEN
  RUNS, BUT BETWEEN ENERGIES!*
  -> As in combine all Mn-Cr data and all Cu-Ni data and create same
  ridges. If my understanding is correct there _should_ be a
  difference there, despite there not being a difference in the cases
  of different gas gains! However, I'm not even sure how different the
  change was in the first place between Mn-Cr and Cu-Ni. Need to check
  that of course (well I guess that plot _would_ be such a
  comparison).

- [ ] GENERAL OUTLINE OF MEASUREMENT / IDEAS SECTION:  
  - [X] explain idea behind it: different targets & filters for different
    energies. Same ones as krieger
  - data is handled on a per run basis, where we have 2-3 runs per
    target & filter. Always at least one with and one without FADC to
    collect more data (FADC readout slows out readout a bit)
  - clean the data slightly, depending on target/filter, cleaning cuts  
  - perform fit according to expected lines on cleaned data _per run_
  - main fluorescence line is the important one, as that one defines
    a further cut to extract clusters matching that energy
  - optional: each run is energy calibrated based on the mean position
    of the main peak in charge and its known charge position, *different
    from regular energy calibration!*
  - cleaned data + individual fits extract that data which is used to
    define reference probability distributions for each energy!
  - combine PDFs to get likelihood, done.  


- [ ] *MAYBE TEMPERATURE REFERENCE ONLY IN FOOTNOTE?*

- [X] *HOW VARIANCE IN TABLE DETERMINED?*
  -> I can't figure out anymore how the variance was calculated in the
  table. For that reason I suppose the better idea is to make a plot
  of:
  gas gain in our 90 min bins with colors based on target/filter and
  shape based on run? For run we need to replace real run numbers by
  run count (1, 2, 3, ...) to all target/filters start at same shape.
  That's easier to understand & clearer to have all data and doesn't
  rely on pixel information. Ah, we can have discrete X-axis actually
  or run number as x! then we don't even need different shapes?
  -> Not used anymore now, so doesn't matter.
- [X] *UPDATE PEAK POSITIONS IN THE TABLE!*

- [X] *AT LEAST THE Cu EPIC 2 KV ROW FIT FUNCTION IS WRONG!! NOT ONLY
  ONE AND NOT Kα!*
  -> Also: Cu EPIC 2kV is Cu Lα and Lβ, but their energies: α = 929.7,
  929.7, β = 949.8
  That's a single pixel difference! Our plot for Cu EPIC 2 kV shows 2
  very clear peaks though. Those *cannot* be two lines, but must be
  detector changing its behavior again! CHeck runs above.
  Also explains why the charge spectrum of this only shows a single
  one!
  -> *NOTE*: When plotting the hits information by run for the Cu EPIC
  2kV we see that the 2 peak structure is only visible in *2* of 3
  runs. 335 has one, 336 and 337 have two. And the latter two are the
  runs *without* the FADC! So likely a double hit thing? The thing is
  though the main peak present in the FADC run is the *larger* one of
  the two present in 336, 337. If it was double hits we'd expect the
  smaller one to be that.
  -> Looking at event displays of 336:
  There's a *large* number of multi hit events and some that at least
  _look_ like higher energetic X-rays. Quite possibly though those
  could also be higher energy events.
  The run with FADC looks much better in this regard.
  It's still unclear though, why the peaks in the runs w/o FADC are at
  even lower pixel counts...
  -> how to proceed?
  -> CDL program cleaned up. Almost everything looks but, but Ti-Ti
  9kV charge is a bit off in NLOPT fit.
  - [X] Investigate. Fixed.
  - [X] Also: the error bars on feCharge are huge in the energy
    resolution plot now. Fixed.
- [ ] *NOTE: ALSO OTHER FUNCTIONS ARE NOT QUITE CORRECT. THEY
  CURRENTLY CONTAIN THE STUFF THAT IS CONTAINED IN THE SPECTRUM, BUT
  NOT WHAT WE FIT! ALSO THE FIXED PARAMETERS ARE NOT SHOWN!*
- [ ] *APPENDIX WITH THE EXACT FIT FUNCTIONS AND THE FIXED
  PARAMETERS!* (well, this is what the table is for, no?)

- [X] *MAKE NOTES ABOUT VARIABILITY, TEMPERATURE CHANGES & SLIGHTLY
  HIGHER PRESSURE IN DETECTOR THAN NORMAL*
  -> in hindsight shouldn't have opened / closed windows etc.

- [X] *CHANGE THE USED CHARGES AGAIN IN ~cdl_cuts.nim~ AFTER OUR NEW
  FITS!*
  -> We now do everything on a per run basis and write the charge
  bounds to the H5 file and use those!

- [ ] *NOTE*: We do have some information about FADC gains in CDL run
  in ~CDL_measurements.org~ in the text portion of the notes for each
  target / filter!

- [X] *LOOK UP LOCAL WEATHER IN GENEVA DURING WEEK OF CDL DATA TAKING
  AND CREATE A PLOT!*

  
- [ ] *NOTE THAT GAS PRESSURES DURING CDL WERE:*
  - <2019-02-15 Fri 15:06>: 1052 mbar
  - <2019-02-16 Sat 15:01>: 1053 mbar
  - <2019-02-17 Sun 10:25>: 1052 mbar
  - <2019-02-19 Tue 16:17>: 1052 mbar
  
**** Table of fit lines                                         :extended:  

This is the full table.

#+CAPTION: The fun table! 
#+NAME: test
#+ATTR_LATEX: :float sideways 
| cuNi15                            | mnCr12                           | tiTi9                                          | agAg6                                        | alAl4                | cuEpic2                                          | cuEpic0_9                               | cEpic0_6                                   |
|-----------------------------------+----------------------------------+------------------------------------------------+----------------------------------------------+----------------------+--------------------------------------------------+-----------------------------------------+--------------------------------------------|
| $\text{EG}(\ce{Cu}_{Kα})$         | $\text{EG}(\ce{Mn}_{Kα})$        | $\text{EG}(\ce{Ti}_{Kα})$                      | $\text{EG}(\ce{Ag}_{Lα})$                    | $\text{EG}(Al_{Kα})$ | $\text{G}(\ce{Cu}_{Lα})$                         | $\text{G}(O_{Kα})$                      | $\text{G}(\ce{C}_{Kα})$                    |
| $\text{EG}(\ce{Cu}^{\text{esc}})$ | $\text{G}(\ce{Mn}^{\text{esc}})$ | $\text{G}(\ce{Ti}^{\text{esc}}_{Kα})$          | G:                                           |                      | G:                                               | #G:                                     | G:                                         |
|                                   |                                  | #name = $\ce{Ti}^{\text{esc}}_{Kα}$            | name = $\ce{Ag}_{Lβ}$                        |                      | name = $\ce{Cu}_{Lβ}$                            | #  name = $\ce{C}_{Kα}$                 | name = $\ce{O}_{Kα}$                       |
|                                   |                                  | # $μ = eμ(\ce{Ti}_{Kα}) · \frac{1.537}{4.511}$ | $N = eN(\ce{Ag}_{Lα}) · 0.1$                 |                      | $N = N(\ce{Cu}_{Lα}) / 5.0$                      | #  $μ = μ(O_{Kα}) · (0.277/0.525)$      | $μ = μ(\ce{C}_{Kα}) · \frac{0.525}{0.277}$ |
|                                   |                                  | G:                                             | $μ = eμ(\ce{Ag}_{Lα}) · \frac{3.151}{2.984}$ |                      | $μ = μ(\ce{Cu}_{Lα}) · \frac{0.9498}{0.9297}$    | #  $σ = σ(O_{Kα})$                      | $σ = σ(\ce{C}_{Kα})$                       |
|                                   |                                  | name = $\ce{Ti}^{\text{esc}}_{Kβ}$             | $σ = eσ(\ce{Ag}_{Lα})$                       |                      | $σ = σ(\ce{Cu}_{Lα})$                            | #G:                                     |                                            |
|                                   |                                  | $μ = eμ(\ce{Ti}_{Kα}) · \frac{1.959}{4.511}$   |                                              |                      | G:                                               | #  name = $\ce{Fe}_{Lα}β$               |                                            |
|                                   |                                  | $σ = σ(\ce{Ti}^{\text{esc}}_{Kα})$             |                                              |                      | name = $\ce{O}_{Kα}$                             | #  $μ = μ(O_{Kα}) · \frac{0.71}{0.525}$ |                                            |
|                                   |                                  | G:                                             |                                              |                      | $N = N(\ce{Cu}_{Lα}) / 3.5$                      | #  $σ = σ(O_{Kα})$                      |                                            |
|                                   |                                  | name = $\ce{Ti}_{Kβ}$                          |                                              |                      | $μ = μ(\ce{Cu}_{Lα}) · \frac{0.5249}{0.9297}$    | #G:                                     |                                            |
|                                   |                                  | $μ = eμ(\ce{Ti}_{Kα}) · \frac{4.932}{4.511}$   |                                              |                      | $σ = σ(\ce{Cu}_{Lα}) / 2.0$                      | #  name = $\ce{Ni}_{Lα}β$               |                                            |
|                                   |                                  | $σ = eσ(\ce{Ti}_{Kα})$                         |                                              |                      |                                                  | #  $μ = μ(O_{Kα}) · \frac{0.86}{0.525}$ |                                            |
|-----------------------------------+----------------------------------+------------------------------------------------+----------------------------------------------+----------------------+--------------------------------------------------+-----------------------------------------+--------------------------------------------|
| cuNi15 Q                          | mnCr12 Q                         | tiTi9 Q                                        | agAg6 Q                                      | alAl4 Q              | cuEpic2 Q                                        | cuEpic0_9 Q                             | cEpic0_6 Q                                 |
|-----------------------------------+----------------------------------+------------------------------------------------+----------------------------------------------+----------------------+--------------------------------------------------+-----------------------------------------+--------------------------------------------|
| $\text{G}(\ce{Cu}_{Kα})$          | $\text{G}(\ce{Mn}_{Kα})$         | $\text{G}(\ce{Ti}_{Kα})$                       | $\text{G}(\ce{Ag}_{Lα})$                     | $\text{G}(Al_{Kα})$  | $\text{G}(\ce{Cu}_{Lα})$                         | $\text{G}(O_{Kα})$                      | $\text{G}(\ce{C}_{Kα})$                    |
| $\text{G}(\ce{Cu}^{\text{esc}})$  | $\text{G}(\ce{Mn}^{\text{esc}})$ | $\text{G}(\ce{Ti}^{\text{esc}}_{Kα})$          | G:                                           |                      | G:                                               | G:                                      | G:                                         |
|                                   |                                  | #name = $\ce{Ti}^{\text{esc}}_{Kα}$            | name = $\ce{Ag}_{Lβ}$                        |                      | name = $\ce{Cu}_{Lβ}$                            | name = $\ce{C}_{Kα}$                    | name = $\ce{O}_{Kα}$                       |
|                                   |                                  | # $μ = eμ(\ce{Ti}_{Kα}) · \frac{1.537}{4.511}$ | $N = N(\ce{Ag}_{Lα}) · 0.1$                  |                      | $N = N(\ce{Cu}_{Lα}) / 5.0$                      | $N = N(O_{Kα}) / 10.0$                  | $μ = μ(\ce{C}_{Kα}) · \frac{0.525}{0.277}$ |
|                                   |                                  | G:                                             | $μ = μ(\ce{Ag}_{Lα}) · \frac{3.151}{2.984}$  |                      | $μ = μ(\ce{Cu}_{Lα}) · \frac{0.9498}{0.9297}$    | $μ = μ(O_{Kα}) · \frac{277.0}{524.9}$   | $σ = σ(\ce{C}_{Kα})$                       |
|                                   |                                  | name = $\ce{Ti}^{\text{esc}}_{Kβ}$             | $σ = σ(\ce{Ag}_{Lα})$                        |                      | $σ = σ(\ce{Cu}_{Lα})$                            | $σ = σ(O_{Kα})$                         |                                            |
|                                   |                                  | $μ = μ(\ce{Ti}_{Kα}) · \frac{1.959}{4.511}$    |                                              |                      | # $\text{G}:                                     |                                         |                                            |
|                                   |                                  | $σ = σ(\ce{Ti}^{\text{esc}}_{Kα})$             |                                              |                      | #  name = $\ce{O}_{Kα}$                          |                                         |                                            |
|                                   |                                  | G:                                             |                                              |                      | #  $N = N(\ce{Cu}_{Lα}) / 4.0$                   |                                         |                                            |
|                                   |                                  | name = $\ce{Ti}_{Kβ}$                          |                                              |                      | #  $μ = μ(\ce{Cu}_{Lα}) · \frac{0.5249}{0.9297}$ |                                         |                                            |
|                                   |                                  | $μ = μ(\ce{Ti}_{Kα}) · \frac{4.932}{4.511}$    |                                              |                      | #  $σ = σ(\ce{Cu}_{Lα}) / 2.0$                   |                                         |                                            |
|                                   |                                  | $σ = σ(\ce{Ti}_{Kα})$                          |                                              |                      |                                                  |                                         |                                            |


**** Extra info on target materials                             :extended:

- [X] *EXTEND THIS TO INCLUDE CITATIONS*
- [ ] *ADD OUR OWN POLYIAMID + ALUMINUM TRANSMISSION PLOT*
  -> 1600 Å polyimide + 800 Å Al
  -> Q: What do we use for the polyimide? Guess below text can tell
  us the atomic fractions in principle.

EPIC filters:
https://www.cosmos.esa.int/web/xmm-newton/technical-details-epic
section 6 about filters contains:
#+begin_quote
There are four filters in each EPIC camera. Two are thin filters made
of 1600 Å of poly-imide film with 400 Å of aluminium evaporated on to
one side; one is the medium filter made of the same material but with
800 Å of aluminium deposited on it; and one is the thick filter. This
is made of 3300 Å thick Polypropylene with 1100 Å of aluminium and 450
Å of tin evaporated on the film.
#+end_quote
i.e. the EPIC filters contain aluminum. That could explain why the
Cu-EPIC 2kV data contains something that might be either aluminum
fluorescence or at least just continuous spectrum that is not filtered
due to the absorption edge of aluminum there!

Relevant references:
cite:struder2001xmm_pnccd,turner2001xmm_mos,barbera2003monitoring,barbera2016thin

In particular cite:barbera2016thin contains much more details about
the EPIC filters and its actual composition:
#+begin_quote
Filter manufacturing process The EPIC Thin and Medium filters
manufactured by MOXTEX consist of a thin film of polyimide, with
nominal thickness of 160 nm, coated with a single layer of aluminum
whose nominal thickness is 40 nm for the Thin and 80 nm for the Medium
filters, respectively. The polyimide thin films are produced by
spin-coating of a polyamic acid (PAA) solution obtained by dissolving
two precursor monomers (an anhydride and an amine) in an organic polar
solvent. For the EPIC Thin and Medium filters the two precursors are
the Biphenyldianhydride (BPDA) and the p-Phenyldiamine (PDA) (Dupont
PI-2610), and the solvent is N-methyl-2-pyrrolidone (NMP) and
Propylene Glycol Monomethyl Ether (Dupont T9040 thinner). To convert
the PAA into polyimide, the solution is heated up to remove the NMP
and to induce the imidization through the evaporation of water
molecules. The film thickness is controlled by spin coating
parameters, PAA viscosity, and curing temperature [19]. The polyimide
thin membrane is attached with epoxy onto a transfer ring and the
aluminum is evaporated in a few runs, distributed over 2–3 days, each
one depositing a metal layer of about 20 nm thickness.

The EPIC Thin and Medium flight qualified filters have been
manufactured during a period of 1 year, from January’96 to
January’97. Table 1 lists the full set of flight-qualified filters
(Flight Model and Flight Spare) delivered to the EPIC consortium,
together with their most relevant parameters. Along with the
production of the flight qualified filters, the prototypes and the
qualification filters (not included in this list) have been
manufactured and tested for the construction of the filter
transmission model and to assess the stability in time of the
Optical/UV transparency (opacity). Among these qualification filters
are T4, G12, G18, and G19 that have been previously mentioned.
#+end_quote

Further it states that 'G12' refers to the *medium filter*
#+begin_quote
UV/Vis transmission measurements in the range 190–1000 nm have been
performed between May 1997 and July 2002 on one Thin (T4) and one
medium (G12) EPIC on-ground qualification filters to monitor their
time stability [16].
#+end_quote

PP G12 is the name written in the CDL documentation! Mystery solved.


**** Reconstruct all CDL data                                   :extended:

Reconstructing all CDL data is done by either using ~runAnalysisChain~
on the directory (currently not tested) or by manually running
~raw_data_manipulation~ and ~reconstruction~ as follows:

Take note that you may change the paths of course. The paths chosen
here are those in use during the writing process of the thesis.

#+begin_src sh
cd ~/CastData/data/CDL_2019
raw_data_manipulation -p . -r Xray -o ~/CastData/data/CDL_2019/CDL_2019_Raw.h5
#+end_src

And now for the reconstruction:
#+begin_src sh
cd ~/CastData/data/CDL_2019
reconstruction -i ~/CastData/data/CDL_2019/CDL_2019_Raw.h5 -o ~/CastData/data/CDL_2019/CDL_2019_Reco.h5
reconstruction -i ~/CastData/data/CDL_2019/CDL_2019_Reco.h5 --only_charge
reconstruction -i ~/CastData/data/CDL_2019/CDL_2019_Reco.h5 --only_fadc
reconstruction -i ~/CastData/data/CDL_2019/CDL_2019_Reco.h5 --only_gas_gain
reconstruction -i ~/CastData/data/CDL_2019/CDL_2019_Reco.h5 --only_energy_from_e
#+end_src

At this point the reconstructed CDL H5 file is generally done and
ready to be used in the next section.

**** Generate plots and tables for this section                 :extended:

To generate the plots of the above section (and much more) as well as
the table summarizing all the runs and their fits, we continue
with the ~cdl_spectrum_creation~ tool as follows:

Make sure the config file uses ~fitByRun~ to reproduce the same plots!

#+begin_src sh
cd /tmp
cdl_spectrum_creation -i ~/CastData/data/CDL_2019/CDL_2019_Reco.h5 --cutcdl --dumpAccurate --hideNloptFit
#+end_src

This generates lots of plots, they should all land in something like:
~out/CDL_2019_raw_2020-09-18_17-28-35/~
relative to where the command was run.

Among them are:
- plots of the target / filter kinds with all fits and the raw / cut
  data as histogram
- plots of histograms of the raw data split *by run*
  -> useful to see the detector variability!
- energy resolution plot
- peak position of hits / charge vs energy (to see if indeed linear)
- calibrated, normalized histogram / kde plot of the target/filter
  combinations in energy (calibrated using the main line that was
  fitted)
- ridgeline plots of KDEs of all the geometric cluster properties
  split by target/filter kind and run  
- gas gain of each gas gain slice in the CDL data, split by run &
  target filter kind
- temperature data of those CDL runs that contain it
- finally it generates the (almost complete) tables as shown in the
  above section for the data overview including μ, σ and μ/σ

Note about temperature plots:
- [[~/phd/Figs/CDL/septem_temperature_cdl.pdf]]
- [[~/phd/Figs/CDL/septem_imb_temperature_facet_cdl.pdf]]
  -> This plot including the IMB temperature shows us precisely what
  we expect: the temperature of the IMB and the Septemboard is
  directly related and just an offset of one another. This is very
  valuable information to have as a reference 
- [[~/phd/Figs/CDL/septem_temperature_facet_cdl.pdf]]
- [[~/phd/Figs/CDL/septem_temperature_facet_cdl_time_since_start.pdf]]

As we ran the command from ~/tmp/~ the output plots will be in a
~/tmp/out/CDL*~ directory. We copy over the generated files files
including ~calibrated_cdl_energy_histos.pdf~ and
~calibrated_cdl_energy_kde.pdf~ here to ~/phd/Figs/CDL/~.

Finally, running the code snippet as mentioned above also produces
table [[tab:cdl:run_overview_tab]] as well as the equivalent for the pixel
spectra and writes them to stdout at the end!

***** TODOs for this section                                   :noexport:

- [X] *DISCUSS HOW ~CDL_2019_RECO.h5~ IS GENERATED*

- [X] *GENERATE A PLOT OF THE GAS GAINS ENCOUNTERED DURING THE CDL
  MEASUREMENTS*
  -> As a simple point plot where the points can be colored by the
  target/filter kind

- [X] *ADD THE ANNOTATIONS FOR THE FIT PARAMETERS STILL AND NO
  NLOPT VERSIONS!*
  
- [X] *INSERT RIDGELINE PLOT OF DIFFERENT PROPERTIES PER RUN, SHOWING
  NO SIGNIFICANT CHANGE DESPITE DIFFERENT GAS GAINS*


**** Generate the ~calibration-cdl-2018.h5~ file                :extended:

This is also done using the ~cdl_spectrum_creation~. Assuming the
reconstructed CDL H5 file exists, it is as simple as:
#+begin_src sh
cdl_spectrum_creation -i ~/CastData/data/CDL_2019/CDL_2019_Reco.h5 --genCdlFile \
                      --outfile ~/CastData/data/CDL_2019/calibration-cdl
#+end_src
which generates ~calibration-cdl-2018.h5~ for us.

Make sure the config file uses ~fitByRun~ to reproduce the same plots!

**** Get the table of fit lines from code                       :extended:

Our code ~cdl_spectrum_creation~ can be used to output the final
fitting functions in a format like the table inserted in the main
text, thanks to using a CT declarative definition of the fit
functions.

We do this by running:
#+begin_src sh :results drawer
cdl_spectrum_creation --printFunctions
#+end_src

#+RESULTS:
:results:
Charge functions:
| Target | Filter | HV [kV] | Fit functions                                                                                                                                                                                                                                                                                                  |
| Cu     | Ni     |      15 | $G^{\ce{Cu}}_{Kα} + G^{\ce{Cu}, \text{esc}}_{Kα}$                                                                                                                                                                                                                                                              |
| Mn     | Cr     |      12 | $G^{\ce{Mn}}_{Kα} + G^{\ce{Mn}, \text{esc}}_{Kα}$                                                                                                                                                                                                                                                              |
| Ti     | Ti     |       9 | $G^{\ce{Ti}}_{Kα} + G^{\ce{Ti}, \text{esc}}_{Kα} + G^{\ce{Ti}}_{Kβ}\left( μ^{\ce{Ti}}_{Kα}·(\frac{4.932}{4.511}), σ^{\ce{Ti}}_{Kα} \right) + G^{\ce{Ti}, \text{esc}}_{Kβ}\left( μ^{\ce{Ti}}_{Kα}·(\frac{1.959}{4.511}), σ^{\ce{Ti}, \text{esc}}_{Kα} \right)$                                                  |
| Ag     | Ag     |       6 | $G^{\ce{Ag}}_{Lα} + G^{\ce{Ag}}_{Lβ}\left( N^{\ce{Ag}}_{Lα}·0.56, μ^{\ce{Ag}}_{Lα}·(\frac{3.151}{2.984}), σ^{\ce{Ag}}_{Lα} \right)$                                                                                                                                                                            |
| Al     | Al     |       4 | $G^{\ce{Al}}_{Kα}$                                                                                                                                                                                                                                                                                             |
| Cu     | EPIC   |       2 | $G^{\ce{Cu}}_{Lα} + G^{\ce{Cu}}_{Lβ}\left( N^{\ce{Cu}}_{Lα}·(\frac{0.65}{1.11}), μ^{\ce{Cu}}_{Lα}·(\frac{0.9498}{0.9297}), σ^{\ce{Cu}}_{Lα} \right) + G^{\ce{O}}_{Kα}\left( \frac{N^{\ce{Cu}}_{Lα}}{3.5}, μ^{\ce{Cu}}_{Lα}·(\frac{0.5249}{0.9297}), \frac{σ^{\ce{Cu}}_{Lα}}{2.0} \right) + G_{\text{unknown}}$ |
| Cu     | EPIC   |     0.9 | $G^{\ce{O}}_{Kα} + G^{\ce{C}}_{Kα}\left( \frac{N^{\ce{O}}_{Kα}}{10.0}, μ^{\ce{O}}_{Kα}·(\frac{277.0}{524.9}), σ^{\ce{O}}_{Kα} \right) + G_{\text{unknown}}$                                                                                                                                                    |
| C      | EPIC   |     0.6 | $G^{\ce{C}}_{Kα} + G^{\ce{O}}_{Kα}\left( μ^{\ce{C}}_{Kα}·(\frac{0.525}{0.277}), σ^{\ce{C}}_{Kα} \right)$                                                                                                                                                                                                       |

Pixel functions:
| Target | Filter | HV [kV] | Fit functions                                                                                                                                                                                                                                                                                                  |
| Cu     | Ni     |      15 | $EG^{\ce{Cu}}_{Kα} + EG^{\ce{Cu}, \text{esc}}_{Kα}$                                                                                                                                                                                                                                                            |
| Mn     | Cr     |      12 | $EG^{\ce{Mn}}_{Kα} + G^{\ce{Mn}, \text{esc}}_{Kα}$                                                                                                                                                                                                                                                             |
| Ti     | Ti     |       9 | $EG^{\ce{Ti}}_{Kα} + G^{\ce{Ti}, \text{esc}}_{Kα} + G^{\ce{Ti}}_{Kβ}\left( eμ^{\ce{Ti}}_{Kα}·(\frac{4.932}{4.511}), eσ^{\ce{Ti}}_{Kα} \right) + G^{\ce{Ti}, \text{esc}}_{Kβ}\left( eμ^{\ce{Ti}}_{Kα}·(\frac{1.959}{4.511}), σ^{\ce{Ti}, \text{esc}}_{Kα} \right)$                                              |
| Ag     | Ag     |       6 | $EG^{\ce{Ag}}_{Lα} + G^{\ce{Ag}}_{Lβ}\left( eN^{\ce{Ag}}_{Lα}·0.56, eμ^{\ce{Ag}}_{Lα}·(\frac{3.151}{2.984}), eσ^{\ce{Ag}}_{Lα} \right)$                                                                                                                                                                        |
| Al     | Al     |       4 | $EG^{\ce{Al}}_{Kα}$                                                                                                                                                                                                                                                                                            |
| Cu     | EPIC   |       2 | $G^{\ce{Cu}}_{Lα} + G^{\ce{Cu}}_{Lβ}\left( N^{\ce{Cu}}_{Lα}·(\frac{0.65}{1.11}), μ^{\ce{Cu}}_{Lα}·(\frac{0.9498}{0.9297}), σ^{\ce{Cu}}_{Lα} \right) + G^{\ce{O}}_{Kα}\left( \frac{N^{\ce{Cu}}_{Lα}}{3.5}, μ^{\ce{Cu}}_{Lα}·(\frac{0.5249}{0.9297}), \frac{σ^{\ce{Cu}}_{Lα}}{2.0} \right) + G_{\text{unknown}}$ |
| Cu     | EPIC   |     0.9 | $G^{\ce{O}}_{Kα} + G_{\text{unknown}}$                                                                                                                                                                                                                                                                         |
| C      | EPIC   |     0.6 | $G^{\ce{C}}_{Kα} + G^{\ce{O}}_{Kα}\left( μ^{\ce{C}}_{Kα}·(\frac{0.525}{0.277}), σ^{\ce{C}}_{Kα} \right)$                                                                                                                                                                                                       |

:end:

**** Historical weather data for Geneva during CDL data taking  :extended:

More or less location of CDL (side of building 17 at Meyrin CERN site):
#+begin_src
46.22965, 6.04984
#+end_src
(https://www.openstreetmap.org/search?whereami=1&query=46.22965%2C6.04984#map=19/46.22965/6.04984)

Here we can see historic weather data plots from Meyrin from the
relevant time range:
https://meteostat.net/en/place/ch/meyrin?s=06700&t=2019-02-15/2019-02-21

This at least proves the weather was indeed very nice outside, sunny
and over 10°C peak temperatures during the day!

I exported the data and it's available here:
[[~/phd/resources/weather_data_meyrin_cdl_data_taking.csv]]

Legend of the columns:
|----+--------+------------------------|
|  # | Column | Description            |
|----+--------+------------------------|
|  1 | time   | Time                   |
|  2 | temp   | Temperature            |
|  3 | dwpt   | Dew Point              |
|  4 | rhum   | Relative Humidity      |
|  5 | prcp   | Total Precipitation    |
|  6 | snow   | Snow Depth             |
|  7 | wdir   | Wind Direction         |
|  8 | wspd   | Wind Speed             |
|  9 | wpgt   | Peak Gust              |
| 10 | pres   | Air Pressure           |
| 11 | tsun   | Sunshine Duration      |
| 12 | coco   | Weather Condition Code |

#+begin_src nim
import ggplotnim, times
var df = readCsv("/home/basti/phd/resources/weather_data_meyrin_cdl_data_taking.csv")
  .mutate(f{string -> int: "timestamp" ~ parseTime(`time`, "yyyy-MM-dd HH:mm:ss", local()).toUnix()})
  .rename(f{"Temperature [°C]" <- "temp"}, f{"Pressure [mbar]" <- "pres"})
df = df.gather(["Temperature [°C]", "Pressure [mbar]"], "Data", "Value")
echo df
ggplot(df, aes("timestamp", "Value", color = "Data")) +
  facet_wrap("Data", scales = "free") +
  facetMargin(0.5) + 
  geom_line() +
  xlab("Date", rotate = -45.0, alignTo = "right", margin = 2.0) +
  margin(bottom = 2.5, right = 4.75) +
  legendPosition(0.84, 0.0) + 
  scale_x_date(isTimestamp = true,
               dateSpacing = initDuration(hours = 12),
               formatString = "yyyy-MM-dd HH:mm", 
               timeZone = local()) +
  ggtitle("Meyrin, Geneva weather during CDL data taking campaign") + 
  ggsave("/home/basti/phd/Figs/CDL/weather_meyrin_during_cdl_data_taking.pdf", width = 1000, height = 600)
#+end_src

#+RESULTS:
| DataFrame | with         |      13 | columns |         and |  336 | rows: |               |              |            |      |             |       |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |           |        |
|       Idx | Data         |   Value | snow    |        dwpt | wdir |  coco |          prcp |         time |  timestamp | rhum |        wspd | tsun  |      wpgt |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |           |        |
|    dtype: | string       |   float | float   |       float |  int |   int |           int |       string |        int |  int |       float | float |     float |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |           |        |
|         0 | Temperatu... |    -0.3 | -nan    |        -1.7 |  200 |     1 |             0 | 2019-02-1... | 1550185200 |   90 |         1.8 | -nan  |        12 |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |           |        |
|         1 | Temperatu... |    -0.7 | -nan    |        -2.6 |  230 |     1 |             0 | 2019-02-1... | 1550188800 |   87 |         3.6 | -nan  |       3.7 |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |           |        |
|         2 | Temperatu... |    -1.7 | -nan    |        -2.5 |  280 |     1 |             0 | 2019-02-1... | 1550192400 |   94 |         1.8 | -nan  |         7 |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |           |        |
|         3 | Temperatu... |    -1.9 | -nan    |        -2.9 |  240 |     1 |             0 | 2019-02-1... | 1550196000 |   93 |         1.8 | -nan  |        13 |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |           |        |
|         4 | Temperatu... |    -2.1 | -nan    |        -3.4 |  320 |     1 |             0 | 2019-02-1... | 1550199600 |   91 |         1.8 | -nan  |       3.7 |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |           |        |
|         5 | Temperatu... |    -2.5 | -nan    |        -3.8 |  330 |     1 |             0 | 2019-02-1... | 1550203200 |   91 |         1.8 | -nan  |        13 |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |           |        |
|         6 | Temperatu... |    -2.7 | -nan    |        -3.5 |  290 |     1 |             0 | 2019-02-1... | 1550206800 |   94 |         1.8 | -nan  |        12 |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |           |        |
|         7 | Temperatu... |      -2 | 0       |        -3.3 |  240 |     1 |             0 | 2019-02-1... | 1550210400 |   91 |         1.8 | 0     |       5.5 |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |           |        |
|         8 | Temperatu... |    -1.8 | -nan    |        -2.8 |  330 |     1 |             0 | 2019-02-1... | 1550214000 |   93 |         1.8 | -nan  |        12 |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |           |        |
|         9 | Temperatu... |    -0.1 | -nan    |        -1.8 |  300 |     2 |             0 | 2019-02-1... | 1550217600 |   88 |         1.8 | -nan  |        13 |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |           |        |
|        10 | Temperatu... |     3.6 | -nan    |         0.3 |  110 |     1 |             0 | 2019-02-1... | 1550221200 |   79 |         1.8 | -nan  |       5.5 |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |           |        |
|        11 | Temperatu... |     5.2 | -nan    |           0 |   70 |     2 |             0 | 2019-02-1... | 1550224800 |   69 |         3.6 | -nan  |        10 |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |           |        |
|        12 | Temperatu... |     7.4 | -nan    |         0.8 |   20 |     2 |             0 | 2019-02-1... | 1550228400 |   63 |         3.6 | -nan  |        11 |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |           |        |
|        13 | Temperatu... |     8.7 | -nan    |         0.4 |   70 |     1 |             0 | 2019-02-1... | 1550232000 |   56 |         5.4 | -nan  |       9.3 |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |           |        |
|        14 | Temperatu... |     9.7 | -nan    |         0.3 |   80 |     1 |             0 | 2019-02-1... | 1550235600 |   52 |         7.6 | -nan  |        15 |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |           |        |
|        15 | Temperatu... |     9.8 | -nan    |        -0.4 |   90 |     1 |             0 | 2019-02-1... | 1550239200 |   49 |         7.6 | -nan  |        19 |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |           |        |
|        16 | Temperatu... |    10.5 | -nan    |        -0.3 |   80 |     1 |             0 | 2019-02-1... | 1550242800 |   47 |         5.4 | -nan  |       9.3 |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |           |        |
|        17 | Temperatu... |      10 | -nan    |        -0.2 |   80 |     1 |             0 | 2019-02-1... | 1550246400 |   49 |         5.4 | -nan  |        11 |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |           |        |
|        18 | Temperatu... |     7.3 | -nan    |        -0.4 |   60 |     1 |             0 | 2019-02-1... | 1550250000 |   58 |         3.6 | -nan  |         6 |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |           |        |
|        19 | Temperatu... |     4.7 | 0       |        -0.7 |   50 |     1 |             0 | 2019-02-1... | 1550253600 |   68 |         1.8 | -nan  |       7.4 |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |           |        |
|           |              |         |         |             |      |       |               |              |            |      |             |       |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |           |        |
|     INFO: | The          | integer | column  | `timestamp` |  has |  been | automatically |   determined |         to |   be | continuous. | To    | overwrite | this | behavior | add | a | `+ | scale_x/y_discrete()` | call | to | the | plotting | chain. | Choose | `x` | or | `y` | depending | on | which | axis | this | column | refers | to. | Or | apply | a | `factor` | to | the | column | name | in | the | `aes` | call, | i.e. | `aes(..., | timestamp | ...)`. |

The weather during the data taking campaign is shown in
fig. [[fig:cdl:weather_meyrin_during_cdl_data_taking]]. It's good to see
my memory served me right.

#+CAPTION: Weather during the data taking campaign at the CDL in Meyrin, Geneva.
#+CAPTION: The weather was sunny and warm for a Februray during the day. As a result
#+CAPTION: the small laboratory heat up significantly and opening / closing windows
#+CAPTION: lead to significant temperature changes.
#+NAME: fig:cdl:weather_meyrin_during_cdl_data_taking
[[~/phd/Figs/CDL/weather_meyrin_during_cdl_data_taking.pdf]]

*** Charge spectra of the CDL data [/]
:PROPERTIES:
:CUSTOM_ID: sec:cdl:fits_to_spectra
:END:

To the spectra of each run of the charge data a mixture of Gaussian
functions is fitted. [fn:pixel_spectra_fitting]

Specifically the Gaussian expressed as
\begin{equation}
G(E; \mu, \sigma, N) = \frac{N}{\sqrt{2 \pi}} \exp\left(-\frac{(E - \mu)^2}{2\sigma^2}\right),
\end{equation}
is used and will be referenced as $G$ with possible arguments from
here on.

Note that while the physical X-ray transition lines are Lorentzian
shaped [[cite:&weisskopf97_lorentzian]], the lines as detected by a
gaseous detector are entirely dominated by detector resolution,
resulting in Gaussian lines. For other types of detectors used in
X-ray fluorescence (XRF) analysis convolutions of Lorentzian and
Gaussian distributions are used
cite:huang86_profile_xrf,heckel87_low_peak_distor, called pseudo Voigt
functions cite:roberts75_lorentz,gunnink77_algo_lorentz.

The functions fitted to the different spectra then depend on which
fluorescence lines are visible. The full list of all combinations is
shown in tab. [[tab:cdl:fit_func_charge]]. Typically each line that is
expected from the choice of target, filter and chosen voltage is
fitted if it can be visually identified in the
data. [fn:choice_of_params] If no 'argument' is shown in the table to
$G$ it means each parameter ($N, μ, σ$) is fitted. Any specific
argument given implies that parameter is _fixed_ relative to another
parameter. For example $μ^{\ce{Ag}}_{Lα}·(\frac{3.151}{2.984})$ fixes
the $Lβ$ line of silver to the fit parameter $μ^{\ce{Ag}}_{Lα}$ of the
$Lα$ line with a multiplicative shift based on the relative eneries of
$Lα$ to $Lβ$.  In some cases the amplitude is fixed between different
lines where relative amplitudes cannot be easily predicted or
determined, e.g. in one of the $\ce{Cu}-\text{EPIC}$ runs, the
$\ce{C}_{Kα}$ line is fixed to a tenth of the $\ce{O}_{Kα}$ line. This
is done to get a good fit based on trial and error.

Finally, in both $\ce{Cu}-\text{EPIC}$ lines two 'unknown' Gaussians
are added to cover the behavior of the data at higher charges. The
physical origin of this additional contribution is not entirely
clear. The used EPIC filter contains an aluminum coating
cite:barbera2016thin. As such it has the aluminum absorption edge at
about $\SI{1.5}{keV}$, possibly matching the additional contribution
for the $\SI{2}{kV}$ dataset. Whether it is from a continuous part of
the spectrum or a form of aluminum fluorescence is not clear
however. This explanation doesn't work in the $\SI{0.9}{kV}$ case,
which is why the line is deemed 'unknown'. It may also be a
contribution of the specific polyimide used in the EPIC filter
cite:barbera2016thin. Another possibility is it is a case of
multi-cluster events, which are too close to be split, but with
properties close enough to a single X-ray as to not be removed by the
cleaning cuts (which gets more likely the lower the energy is).

The full set of all fits (including the pixel spectra) is shown in
appendix
[[#sec:appendix:cdl:all_spectra_fits]]. Fig. [[fig:cdl:ti_ti_charge_spectrum_run_326]]
shows the charge spectrum of the $\ce{Ti}$ target and $\ce{Ti}$ filter
at $\SI{9}{kV}$ for one of the runs. These plots show the raw data in
the green histogram and the data left after application of the
cleaning cuts (tab. [[tab:cdl:cdl_cleaning_cuts]]) in the purple
histogram. The black line is the result of the fit as described in
tab. [[tab:cdl:fit_func_charge]] with the resulting parameters shown in
the box (parameters that were fixed are not shown). The black straight
lines with grey error bands represent the $3σ$ region around the main
fluorescence line, which is used to extract those clusters likely from
the fluorescence line and therefore known energy.

#+CAPTION: Charge spectrum of the $\ce{Ti}-\ce{Ti}$ spectrum at $\SI{9}{kV}$ from run
#+CAPTION: 326. The green histograms shows the raw data of this run and the purple
#+CAPTION: histogram indicating the data left after the cleaning cuts are applied.
#+CAPTION: The purple line indicates the result of the fit as described in tab. [[tab:cdl:fit_func_charge]]
#+CAPTION: with the resulting parameters shown in the box. The black lines represent the
#+CAPTION: $3σ$ region around the main fluorescence line (with grey error bands),
#+CAPTION: which is later used to extract those clusters likely from the
#+CAPTION: fluorescence line and therefore known energy.
#+NAME: fig:cdl:ti_ti_charge_spectrum_run_326
[[~/phd/Figs/CDL/Ti-Ti-9kVCharge-2019_run_326.pdf]]

\footnotesize
#+CAPTION: All fit functions for the charge spectra used for each target / filter combination. Typically each
#+CAPTION: line that is expected and visible in the data is fit. $G$ is a normal Gaussian. No 'argument'
#+CAPTION: to $G$ means each parameter ($N, μ, σ$) is fit. Specific arguments imply this
#+CAPTION: parameter is _fixed_ relative to another parameter, e.g. $μ^{\ce{Ag}}_{Lα}·(\frac{3.151}{2.984}$ fixes
#+CAPTION: $Lβ$ line of silver to the fit parameter $μ^{\ce{Ag}}_{Lα}$ of $Lα$ with a multiplicative
#+CAPTION: shift based on the relative eneries of $Lα$ to $Lβ$.
#+CAPTION: In some cases the amplitude is fixed between different lines, e.g. in one of the $\ce{Cu}-\text{EPIC}$
#+CAPTION: runs the $\ce{C}_{Kα}$ line is fixed to a tenth of the
#+CAPTION: $\ce{O}_{Kα}$ line. In both $\ce{Cu}-\text{EPIC}$
#+CAPTION: lines two 'unknown' Gaussians are added to cover the behavior of the data at higher charges. It is unclear
#+CAPTION: what the real cause is, in particular in the lower energy case.
#+NAME: tab:cdl:fit_func_charge
#+ATTR_LATEX: :environment longtable :width \textwidth :spread 
|--------+--------+---------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Target | Filter | HV [kV] | Fit functions                                                                                                                                                                                                                                                                                                  |
|--------+--------+---------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Cu     | Ni     |      15 | $G^{\ce{Cu}}_{Kα} + G^{\ce{Cu}, \text{esc}}_{Kα}$                                                                                                                                                                                                                                                              |
| Mn     | Cr     |      12 | $G^{\ce{Mn}}_{Kα} + G^{\ce{Mn}, \text{esc}}_{Kα}$                                                                                                                                                                                                                                                              |
| Ti     | Ti     |       9 | $G^{\ce{Ti}}_{Kα} + G^{\ce{Ti}, \text{esc}}_{Kα} + G^{\ce{Ti}}_{Kβ}\left( μ^{\ce{Ti}}_{Kα}·(\frac{4.932}{4.511}), σ^{\ce{Ti}}_{Kα} \right) + G^{\ce{Ti}, \text{esc}}_{Kβ}\left( μ^{\ce{Ti}}_{Kα}·(\frac{1.959}{4.511}), σ^{\ce{Ti}, \text{esc}}_{Kα} \right)$                                                  |
| Ag     | Ag     |       6 | $G^{\ce{Ag}}_{Lα} + G^{\ce{Ag}}_{Lβ}\left( N^{\ce{Ag}}_{Lα}·0.56, μ^{\ce{Ag}}_{Lα}·(\frac{3.151}{2.984}), σ^{\ce{Ag}}_{Lα} \right)$                                                                                                                                                                            |
| Al     | Al     |       4 | $G^{\ce{Al}}_{Kα}$                                                                                                                                                                                                                                                                                             |
| Cu     | EPIC   |       2 | $G^{\ce{Cu}}_{Lα} + G^{\ce{Cu}}_{Lβ}\left( N^{\ce{Cu}}_{Lα}·(\frac{0.65}{1.11}), μ^{\ce{Cu}}_{Lα}·(\frac{0.9498}{0.9297}), σ^{\ce{Cu}}_{Lα} \right) + G^{\ce{O}}_{Kα}\left( \frac{N^{\ce{Cu}}_{Lα}}{3.5}, μ^{\ce{Cu}}_{Lα}·(\frac{0.5249}{0.9297}), \frac{σ^{\ce{Cu}}_{Lα}}{2.0} \right) + G_{\text{unknown}}$ |
| Cu     | EPIC   |     0.9 | $G^{\ce{O}}_{Kα} + G^{\ce{C}}_{Kα}\left( \frac{N^{\ce{O}}_{Kα}}{10.0}, μ^{\ce{O}}_{Kα}·(\frac{277.0}{524.9}), σ^{\ce{O}}_{Kα} \right) + G_{\text{unknown}}$                                                                                                                                                    |
| C      | EPIC   |     0.6 | $G^{\ce{C}}_{Kα} + G^{\ce{O}}_{Kα}\left( μ^{\ce{C}}_{Kα}·(\frac{0.525}{0.277}), σ^{\ce{C}}_{Kα} \right)$                                                                                                                                                                                                       |
|--------+--------+---------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
\normalsize

[fn:pixel_spectra_fitting] Note that similar fits can be performed for
the pixel spectra as well. However, as these are not needed for
anything further they are only presented in the extended version of
the thesis.

[fn:choice_of_params] The restriction to what is 'visually
identifiable' is to avoid the need to fit lines lost in detector
resolution and signal to noise ratio. In addition it avoids
overfitting to large numbers of parameters (which brings its own
challenges and problems).

**** TODOs of the above section                                 :noexport:

- [X] *REWRITE BELOW, FIND CITATION FOR LORENTZ SHAPE OF X-ray
  transitions*
  -> And also this could maybe go to where we first show. Done.

- [X] *REWRITE ABOVE PART ABOUT UNKNOWN LINE ACCORDING TO*:
  #+begin_quote
  The physical origin of this additional contribution modeled by a Gaussian is not entirely clear. The used
  EPIC filter contains an aluminum coating. As such it has the aluminum absorption edge at about $\SI{1.5}{keV}$, matching the additional
  contribution. Whether it is from a continuous part of the spectrum or a form of aluminum fluorescence is not clear however.  
  #+end_quote
  -> Done.

- [X] *HAVE
  [[file:~/CastData/ExternCode/TimepixAnalysis/Tools/calcCdlCutsFromFitParams.nim]]
  TO COMPUTE CHARGE CUT VALUES FROM DUMPED FITS!*
  -> We now handle this in code, write them to CDL H5 file. The above
  could still be useful to generate a table of the final used values!
  -> We generate the table from the code now. The script above might
  still serve as some reference though.

- [X] Only mention pixel spectra in passing? We don't use them for
  anything. So there's no need for why they should really appear here!
  -> Yes, this is better. See section below now.

- [X] *SHOW EXAMPLE FIT WITH RAW+CUTS & FIT PARAMETERS PLOT*
  
- [X] *PROBABLY REPLACE PURPLE LINE FOR PLOT WITHOUT NLOPT & START BY
  BLACK LINE!*

- [X] *THINK THIS TABLE THROUGH AGAIN. POSSIBLY REPLACE IT BY AN
  ENUMERATION WITH ONE ITEM BY TARGET / FILTER*
  -> I think that's better, yes. (hence it's now here)
#+CAPTION: The fun table!
#+CAPTION: ~(*)~: The ratio used in these amplitudes is not physical, but motivated by the very rough ratios seen in the data.
#+CAPTION: ~(**)~:  The physical origin of this additional contribution modeled by a Gaussian is not entirely clear. The used
#+CAPTION: EPIC filter contains an aluminum coating. As such it has the aluminum absorption edge at about $\SI{1.5}{keV}$, matching the additional
#+CAPTION: contribution. Whether it is from a continuous part of the spectrum or a form of aluminum fluorescence is not clear however.
#+NAME: test
#+ATTR_LATEX: :float sideways :booktabs t
| $\ce{CuNi} \SI{15}{kV}$                | $\ce{MnCr} \SI{12}{kV}$               | tiTi9                                          | agAg6                                          | alAl4                | cuEpic2                                         | cuEpic0.9                               | cEpic0.6                                     |
|----------------------------------------+---------------------------------------+------------------------------------------------+------------------------------------------------+----------------------+-------------------------------------------------+-----------------------------------------+----------------------------------------------|
| $\text{EG}(\ce{Cu}_{Kα})$              | $\text{EG}(\ce{Mn}_{Kα})$             | $\text{EG}(\ce{Ti}_{Kα})$                      | $\text{EG}(\ce{Ag}_{Lα})$                      | $\text{EG}(Al_{Kα})$ | $\text{G}(\ce{Cu}_{Lα})$                        | $\text{G}(O_{Kα})$                      | $\text{G}(\ce{C}_{Kα})$                      |
| $\text{EG}(\ce{Cu}^{\text{esc}}_{Kα})$ | $\text{G}(\ce{Mn}^{\text{esc}}_{Kα})$ | $\text{G}(\ce{Ti}^{\text{esc}}_{Kα})$          | G($\ce{Ag}_{Lβ}$)                              |                      | G($\ce{Cu}_{Lβ}$):                              |                                         | G($\ce{O}_{Kα}$):                            |
|                                        |                                       | G($\ce{Ti}^{\text{esc}}_{Kβ}$):                | $\ N = eN(\ce{Ag}_{Lα}) · 0.56$                |                      | $\ N = N(\ce{Cu}_{Lα}) · \frac{0.65}{1.11}$     |                                         | $\ μ = μ(\ce{C}_{Kα}) · \frac{0.525}{0.277}$ |
|                                        |                                       | $\ μ = eμ(\ce{Ti}_{Kα}) · \frac{1.959}{4.511}$ | $\ μ = eμ(\ce{Ag}_{Lα}) · \frac{3.151}{2.984}$ |                      | $\ μ = μ(\ce{Cu}_{Lα}) · \frac{0.9498}{0.9297}$ |                                         | $\ σ = σ(\ce{C}_{Kα})$                       |
|                                        |                                       | $\ σ = σ(\ce{Ti}^{\text{esc}}_{Kα})$           | $\ σ = eσ(\ce{Ag}_{Lα})$                       |                      | $\ σ = σ(\ce{Cu}_{Lα})$                         |                                         |                                              |
|                                        |                                       | G($\ce{Ti}_{Kβ}$):                             |                                                |                      | G($\ce{O}_{Kα}$):                               |                                         |                                              |
|                                        |                                       | $\ μ = eμ(\ce{Ti}_{Kα}) · \frac{4.932}{4.511}$ |                                                |                      | $\ N = N(\ce{Cu}_{Lα}) / 3.5$ ~(*)~             |                                         |                                              |
|                                        |                                       | $\ σ = eσ(\ce{Ti}_{Kα})$                       |                                                |                      | $\ μ = μ(\ce{Cu}_{Lα}) · \frac{0.5249}{0.9297}$ |                                         |                                              |
|                                        |                                       |                                                |                                                |                      | $\ σ = σ(\ce{Cu}_{Lα}) / 2.0$                   |                                         |                                              |
|                                        |                                       |                                                |                                                |                      | G($\ce{Al}_K$) ~(**)~                           |                                         |                                              |
|----------------------------------------+---------------------------------------+------------------------------------------------+------------------------------------------------+----------------------+-------------------------------------------------+-----------------------------------------+----------------------------------------------|
| cuNi15 Q                               | mnCr12 Q                              | tiTi9 Q                                        | agAg6 Q                                        | alAl4 Q              | cuEpic2 Q                                       | cuEpic0.9 Q                             | cEpic0.6 Q                                   |
|----------------------------------------+---------------------------------------+------------------------------------------------+------------------------------------------------+----------------------+-------------------------------------------------+-----------------------------------------+----------------------------------------------|
| $\text{G}(\ce{Cu}_{Kα})$               | $\text{G}(\ce{Mn}_{Kα})$              | $\text{G}(\ce{Ti}_{Kα})$                       | $\text{G}(\ce{Ag}_{Lα})$                       | $\text{G}(Al_{Kα})$  | $\text{G}(\ce{Cu}_{Lα})$                        | $\text{G}(O_{Kα})$                      | $\text{G}(\ce{C}_{Kα})$                      |
| $\text{G}(\ce{Cu}^{\text{esc}}_{Kα})$  | $\text{G}(\ce{Mn}^{\text{esc}}_{Kα})$ | $\text{G}(\ce{Ti}^{\text{esc}}_{Kα})$          | G($\ce{Ag}_{Lβ}$):                             |                      | G($\ce{Cu}_{Lβ}$):                              | G($\ce{C}_{Kα}$):                       | G($\ce{O}_{Kα}$):                            |
|                                        |                                       | G($\ce{Ti}^{\text{esc}}_{Kβ}$):                | $\ N = N(\ce{Ag}_{Lα}) · 0.56$                 |                      | $\ N = N(\ce{Cu}_{Lα}) · \frac{0.65}{1.11}$     | $\ N = N(O_{Kα}) / 10.0$ ~(*)~          | $\ μ = μ(\ce{C}_{Kα}) · \frac{0.525}{0.277}$ |
|                                        |                                       | $\ μ = μ(\ce{Ti}_{Kα}) · \frac{1.959}{4.511}$  | $\ μ = μ(\ce{Ag}_{Lα}) · \frac{3.151}{2.984}$  |                      | $\ μ = μ(\ce{Cu}_{Lα}) · \frac{0.9498}{0.9297}$ | $\ μ = μ(O_{Kα}) · \frac{277.0}{524.9}$ | $\ σ = σ(\ce{C}_{Kα})$                       |
|                                        |                                       | $\ σ = σ(\ce{Ti}^{\text{esc}}_{Kα})$           | $\ σ = σ(\ce{Ag}_{Lα})$                        |                      | $\ σ = σ(\ce{Cu}_{Lα})$                         | $\ σ = σ(O_{Kα})$                       |                                              |
|                                        |                                       | G($\ce{Ti}_{Kβ}$):                             |                                                |                      |                                                 |                                         |                                              |
|                                        |                                       | $\ μ = μ(\ce{Ti}_{Kα}) · \frac{4.932}{4.511}$  |                                                |                      |                                                 |                                         |                                              |
|                                        |                                       | $\ σ = σ(\ce{Ti}_{Kα})$                        |                                                |                      |                                                 |                                         |                                              |
  

**** Notes on implementation details of fit functions           :extended:

- [ ] *REWRITE THE BELOW* (much of that is irrelevant for the full
  thesis)
  -> Place into :noexport: section!
The exact implementation in use for both the gaussian:
- Gauss:
  https://github.com/Vindaar/seqmath/blob/master/src/seqmath/smath.nim#L997-L1009

The fitting was performed both with [[https://www.physics.wisc.edu/~craigm/idl/cmpfit.html][MPFit]] (Levenberg Marquardt C
implementation) as a comparison, but mainly using [[https://nlopt.readthedocs.io/en/latest/][NLopt]] (via
[[https://github.com/Vindaar/nimnlopt]]). Specifically the gradient based
[[http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.146.5196]["Method of Moving Asymptotes"]] algorithm was used (NLopt provides a
large number of different minimization / maximization algorithms to
choose from) to perform maximum likelihood estimation written in the
form of a poisson distributed log likelihood $\chi^2$:
#+BEGIN_EXPORT latex
\begin{equation}
\chi^2_{\lambda, P} = 2 \sum_i y_i - n_i + n_i \ln\left(\frac{n_i}{y_i}\right),
\end{equation}
#+END_EXPORT
where $n_i$ is the number of events in bin $i$ and $y_i$ the model
prediction of events in bin $i$.

The required gradient was calculated simply using the [[https://en.wikipedia.org/wiki/Symmetric_derivative][symmetric
derivative]]. Other algorithms and minimization functions were tried,
but this proved to be the most reliable.  See the implementation:
https://github.com/Vindaar/TimepixAnalysis/blob/master/Analysis/ingrid/calibration.nim#L131-L162

**** Pixel spectra                                              :extended:

In case of the pixel spectra the fit functions are generally very
similar, but in some cases the regular Gaussian is replaced by an
'exponential Gaussian' defined as follows:
#+BEGIN_EXPORT latex
\begin{equation}
EG(E; \mu, \sigma, N, a, b) =
\begin{cases}
N \exp\left(-\frac{(E-\mu)^2}{2\sigma^2}\right) & \mathrm{for}: E \geq c\\
   \exp(aE + b) & \mathrm{for}: E < c \\
\end{cases}
\end{equation}
#+END_EXPORT
where the constant $c$ is chosen such that the resulting function is
continuous. The idea being that in the pixel spectra can have a longer
exponential tail on the left side due to threshold effects and
multiple electrons entering a single grid hole.

The implementation of the exponential Gaussian is found here:
https://github.com/Vindaar/TimepixAnalysis/blob/master/Analysis/ingrid/calibration.nim#L182-L194
- [ ] *REPLACE LINKS SUCH AS THESE BY TAGGED VERSION AND NOT DIRECT
  INLINE LINKS*

The full list of all fit combinations for the pixel spectra is shown
in tab. [[tab:cdl:fit_func_pixel]]. The fitting otherwise works the same
way, using a non-linear least square fit both implemented by hand
using MMA as well as a standard Levenberg-Marquardt fit.
  
#+CAPTION: All fit functions for the pixel spectra for the different combinations. If a function misses
#+CAPTION: a parameter below (out of $(N, μ, σ)$ for the Gaussian and $(a, b, N, μ, σ)$ for the exponential
#+CAPTION: Gaussian) means that parameter has been fixed relative to another.
#+NAME: tab:cdl:fit_funcs_pixel
|--------+--------+---------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Target | Filter | HV [kV] | Fit functions                                                                                                                                                                                                                                                                                                  |
|--------+--------+---------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Cu     | Ni     |      15 | $EG^{\ce{Cu}}_{Kα} + EG^{\ce{Cu}, \text{esc}}_{Kα}$                                                                                                                                                                                                                                                            |
| Mn     | Cr     |      12 | $EG^{\ce{Mn}}_{Kα} + G^{\ce{Mn}, \text{esc}}_{Kα}$                                                                                                                                                                                                                                                             |
| Ti     | Ti     |       9 | $EG^{\ce{Ti}}_{Kα} + G^{\ce{Ti}, \text{esc}}_{Kα} + G^{\ce{Ti}, \text{esc}}_{Kβ}\left( eμ^{\ce{Ti}}_{Kα}·(\frac{1.959}{4.511}), σ^{\ce{Ti}, \text{esc}}_{Kα} \right) + G^{\ce{Ti}}_{Kβ}\left( eμ^{\ce{Ti}}_{Kα}·(\frac{4.932}{4.511}), eσ^{\ce{Ti}}_{Kα} \right)$                                              |
| Ag     | Ag     |       6 | $EG^{\ce{Ag}}_{Lα} + G^{\ce{Ag}}_{Lβ}\left( eN^{\ce{Ag}}_{Lα}·0.56, eμ^{\ce{Ag}}_{Lα}·(\frac{3.151}{2.984}), eσ^{\ce{Ag}}_{Lα} \right)$                                                                                                                                                                        |
| Al     | Al     |       4 | $EG^{\ce{Al}}_{Kα}$                                                                                                                                                                                                                                                                                            |
| Cu     | EPIC   |       2 | $G^{\ce{Cu}}_{Lα} + G^{\ce{Cu}}_{Lβ}\left( N^{\ce{Cu}}_{Lα}·(\frac{0.65}{1.11}), μ^{\ce{Cu}}_{Lα}·(\frac{0.9498}{0.9297}), σ^{\ce{Cu}}_{Lα} \right) + G^{\ce{O}}_{Kα}\left( \frac{N^{\ce{Cu}}_{Lα}}{3.5}, μ^{\ce{Cu}}_{Lα}·(\frac{0.5249}{0.9297}), \frac{σ^{\ce{Cu}}_{Lα}}{2.0} \right) + G_{\text{unknown}}$ |
| Cu     | EPIC   |     0.9 | $G^{\ce{O}}_{Kα} + G_{\text{unknown}}$                                                                                                                                                                                                                                                                         |
| C      | EPIC   |     0.6 | $G^{\ce{C}}_{Kα} + G^{\ce{O}}_{Kα}\left( μ^{\ce{C}}_{Kα}·(\frac{0.525}{0.277}), σ^{\ce{C}}_{Kα} \right)$                                                                                                                                                                                                       |
|--------+--------+---------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|

*** Overview of CDL data in energy

With the fits to the charge spectra performed on a run-by-run basis,
they can be utilized to calibrate the energy of each cluster in the
data. [fn:using_regular_energy_calib] This is done by using the linear
relationship between charge and energy and therefore using the charge
of the main fluorescence line as computed from the fit. Each run is
therefore self-calibrated (in contrast to our normal energy
calibration approach
[[#sec:calibration:energy]]). Fig. [[fig:cdl:calibrated_energy_histos]] shows
normalized histograms of all CDL data after applying basic cuts and
performing said energy calibrations.

#+CAPTION: Normalized histograms of all CDL data after applying basic cuts
#+CAPTION: and calibrating the data in energy using the charge of the main fitted line
#+CAPTION: and its known energy as a baseline. Some targets show a wider distribution, due
#+CAPTION: to detector variability which results in different gas gains and thus different
#+CAPTION: charges in different runs.
#+NAME: fig:cdl:calibrated_energy_histos
[[~/phd/Figs/CDL/calibrated_cdl_energy_histos.pdf]]

[fn:using_regular_energy_calib] We can of course apply the regular
energy calibration based on the multiple fits to the CAST calibration
data as explained in
sec. [[#sec:calib:final_energy_calibration]]. However, due to the very
different gas gains observed in the CDL the applicability is not
ideal. Add to that the fact that we know for certain the energy of the
clusters in the main fluorescence peak there is simply no need either.

*** Definition of the reference distributions
:PROPERTIES:
:CUSTOM_ID: sec:cdl:derive_probability_density
:END:

Having performed fits to all charge spectra of each run and the
position of the main fluorescence line and its width determined, the
reference distributions for the cluster properties entering the
likelihood can be computed. By taking all clusters within the
$3σ$ bounds around the main fluorescence line of the data used for the
fit, the dataset is selected. This guarantees to leave mainly X-rays
of the targeted energy for each line in the dataset. As the fit is
performed for each run separately, the $3σ$ charge cut is performed
run by run and then all data combined for each fluorescence line
(target, filter & HV setting). 

The desired reference distributions then are simply the normalized
histograms of the clusters in each of the properties. With 8 targeted
fluorescence lines and 3 properties this yields a total of 24
reference distributions. Each histogram is then interpreted as a
probability density function (PDF) for clusters 'matching' (more on
this in sec. [[#sec:cdl:cdl_morphing]]) the energy of its fluorescence
line. An overview of all the reference distributions is shown in
fig. [[fig:cdl:reference_distributions_overview_ridgeline]]. We can see
that all distributions tend to get wider towards lower energies
(towards the top of the plot). This is expected and due to smaller
clusters having less primary electrons and therefore statistical
variations in geometric properties playing a more important
role. [fn:diffusion] The binning shown in the figure is the exact
binning used to define the PDF. In case of the fraction of pixels
within a transverse RMS radius bins with significantly higher counts
are observed at low energies. This is _not_ a binning artifact, but a
result of the definition of the variable. The property computes the
fraction of pixels that lie within a circle of the radius
corresponding to the transverse RMS of the cluster around the cluster
center (see fig. sref:fig:reco:property_explanations). At energies with few
pixels in total, the integer nature of $N$ or $N+1$ primary electrons
(active pixels) inside the radius becomes apparent.

The binning in the histograms is chosen by hand such that the binning
is as fine as possible without leading to significant statistical
fluctuations, as those would have a direct effect on the PDFs leading
to unphysical effects on the probabilities. Ideally an approach of
either an automatic bin selection algorithm or something like a kernel
density estimation should be used. However, the latter is slightly
problematic due to the integer effects in the low energies of the
fraction in transverse RMS variable.

See appendix [[#sec:appendix:cdl_reference_distributions]] for an overview
of the distributions with each histogram in a separate plot.
- [ ] *Do we want this?*

The summarized 'recipe' of the approach is therefore:
1. apply cleaning cuts according to tab. [[tab:cdl:cdl_cleaning_cuts]],
2. perform fits according to tab. [[tab:cdl:fit_func_charge]],
3. cut to the $3σ$ region around the main fluorescence line of the
   performed fit (i.e. the first term in tab. [[tab:cdl:fit_func_charge]]),
4. combine all remaining clusters for the same fluorescence line from
   each run,
5. compute a histogram for each desired cluster property and each
   fluorescence line,
6. normalize the histogram to define the reference distribution.   

#+CAPTION: Overview of all reference distributions for each target/filter combination and property.
#+CAPTION: The binning is the same as used to compute the probabilities. Towards lower energies
#+CAPTION: (towards the top) the distributions all become wider, as the clusters have fewer electrons
#+CAPTION: and statistical fluctuations play a larger role. The 'fraction in transverse RMS' property
#+CAPTION: becomes partially discrete, which is _not_ a binning effect, but due to integer counting effects
#+CAPTION: of the number of electrons within the transverse RMS radius.
#+NAME: fig:cdl:reference_distributions_overview_ridgeline
[[~/phd/Figs/CDL/ridgeline_all_properties_side_by_side.pdf]]

[fn:diffusion] Keep in mind that all energies undergo roughly the same
amount of diffusion (aside from the energy dependent absorption length
to a lesser extent), so lower energy events are more sparse.

**** TODOs for the above section [/]                            :noexport:

- [ ] mention what kind of binning we use
  
- [ ] *ADD THAT APPENDIX*
  -> Appendix about all the individual histograms


- [X] present plot of reference distributions. How?
  -> ridge line of all targets & then show one property?
  -> ridge line of all properties
  -> side-by-side facet of all properties of one line?
  -> Finally: side by side of 3 ridge line plots!

- [X] *REGARDING CORRELATION BETWEEN THE THREE VARIABLES!*
  -> Create a point plot of the eccentricity, lengthDivRmsTrans and
  fracRms data (using one as color scale). Should give a nice
  correlation likely!
  (we could also *compute* the correlation, but well)
  -> Done here:
  [[~/phd/Figs/background/correlation_ecc_ldiv_frac.pdf]]
  where we create plot of the logL interpolation based on CDL
  data. This one shows the CDL data after cleaning cut! Clearly strong
  correlation visible.
  The 'inverse' plot
  [[~/phd/Figs/background/correlation_ecc_frac_ldiv.pdf]] looks a bit more
  funny. I think because of some ldiv values being _much_ larger the
  color scale is not very useful. The line at the bottom is the
  diagonal outliers in the first plot.
  - [X] *ALSO DO SAME PLOT BUT LDIV vs FRAC with ECC AS COLOR!*
  Done here:
  [[~/phd/Figs/background/correlation_ldiv_frac_ecc.pdf]]

  And as a bonus the same plots, but cut to $ε < 2.5$:
  - [[~/phd/Figs/background/correlation_ecc_ldiv_frac_ecc_smaller_2_5.pdf]]
  - [[~/phd/Figs/background/correlation_ecc_frac_ldiv_ecc_smaller_2_5.pdf]]
  - [[~/phd/Figs/background/correlation_ldiv_frac_ecc_ecc_smaller_2_5.pdf]]

  In some ways the correlation becomes nicer, in others it gets harder
  to see. Interesting!

**** Generate ridgeline plot of all reference distributions     :extended:

- [ ] Does this also fall out of ~cdl_spectrum_creation~ ?

*** Define likelihood distribution and interpolating it [/]
:PROPERTIES:
:CUSTOM_ID: sec:cdl:cdl_morphing
:END:

With our reference distributions defined it is time to look back at
the equation for the definition of the likelihood, eq. [[eq:background:likelihood_def]].

To avoid numerical issues dealing with very small probabilities, the
actual relation evaluated numerically is the negative log of the
likelihood:

\[
-\ln \mathcal{L}(ε, f, l) = - \ln \mathcal{P}_ε(ε) -
  \ln \mathcal{P}_f(f) - \ln \mathcal{P}_l(l)
\]

By considering a single fluorescence line the three reference
distributions make up the $\mathcal{P}_i(i)$ of the likelihood
$\mathcal{L}(ε, l, f)$ for that energy. However, the reference
distributions do not strictly speaking define the likelihood
distribution. They only represent a PDF to look up a probability-like
value for a given cluster with properties $(ε, l, f)$ for a single
likelihood value $\mathcal{L}$. For that reason the definition of the
likelihood distributions for each energy is a two-step process:
1. compute the reference distributions as described in
   sec. [[#sec:cdl:derive_probability_density]],
2. take the raw cluster data (unbinned data!) of those
   clusters that define the reference distributions and feed each of
   these into eq. [[eq:background:likelihood_def]] for a single likelihood
   value each.
The resulting distribution made up of all those likelihood values is
the likelihood distribution for that energy. All these distributions
are shown in fig. [[fig:cdl:likelihood_distributions]] as a negative log
likelihood. [fn:logL_instead_of_L] Note that due to the usage of
negative log likelihoods the raw data often contains infinities, which
are just a side effect of picking up a zero probability from one of
the reference distributions for a particular cluster. In reality the
reference distributions should be a continuous distribution that is
nowhere exactly zero. However, due to limited statistics there is a
small range of non-zero probabilities (most bins are empty outside the
main range). For all practical purposes this does not matter, but it
does explain the rather hard cutoff from 'sensible' likelihood values
to infinities.

#+CAPTION: $-\ln\mathcal{L}$ distributions for each of the targeted fluorescence lines
#+CAPTION: and thus target/filter combinations.
#+NAME: fig:cdl:likelihood_distributions
[[~/phd/Figs/CDL/logL_ridgeline.pdf]]

- [ ] *BELOW MUST GO TO PART WHERE WE FINISH HOW ALL THIS WORKS WITH
  CDL DATA, INCL LINEAR INTERPOLATION. ALTERNATIVELY, FINISH THAT
  EXPLANATION ABOVE BEFORE TALKING ABOUT CDL DETAILS?*
To finally classify events as signal or background using the
likelihood distribution, one sets a desired "software efficiency"
$ε_{\text{eff}}$, which is defined as:

#+NAME: eq:background:lnL:cut_condition
\begin{equation}
ε_{\text{eff}} = \frac{∫_0^{\mathcal{L'}} \mathcal{L}(ε, f, l) \, \mathrm{d}L}{∫_0^{∞}\mathcal{L}(ε, f, l) \, \mathrm{d} L}
\end{equation}

where $\mathcal{L}'$ denotes a specific $\mathcal{L}$ value, which
yields the desired $ε_{\text{eff}}$. In practical terms one computes
the normalized cumulative sum of the log likelihood and searches for
the point at which the desired $ε_{\text{eff}}$ is reached. The
typical software efficiency we aim for is
$\SI{80}{\percent}$. Classification as X-ray like then is simply any
cluster with a $\ln\mathcal{L}$ value smaller than the cut value
$\ln\mathcal{L}'$.

Note that this value $\mathcal{L}'$ has to be determined for _each_
likelihood distribution.

[fn:logL_instead_of_L] The $-\ln\mathcal{L}$ values in the
distributions give a good idea of why. Roughly speaking the values go
from 5 to 20, meaning the actual likelihood values are in the range
from $e^{-5} \approx \num{7e-3}$ to $e^{-20} \approx \num{2e-9}$!
While 64-bit floating point numbers nowadays in principle provide
enough precision for these numbers human readability is improved. But
32-bit floats would already accrue serious floating point errors due
to only about 7 decimal digits accuracy. But even with 64 bit floats
slight differences in the likelihood definition might run into trouble
as well!

**** TODOs for this section [/]                                 :noexport:

- [ ] *REWRITE TEXT AROUND SO THAT THIS MAKES SENSE HERE*
  Maybe we could create a "box" environment that is a side-note of
  sorts?
  I think that could work well. Something like this
  https://tex.stackexchange.com/questions/179197/framed-or-colored-box-with-text-and-margin-notes
  possibly.
  -> related to lnL discussion.

- [ ] *THINK ABOUT OPTIMIZING CUT EFFICIENCY AGAIN!*
  -> Well, we calc multiple different ones, but optimizing fully is
  not worth it (would maybe be for MLP though)

- [ ] *INCLUDE THE CUT VALUE INTO THE RIDGELINE PLOT. VERTICAL LINE
  WHERE IT IS FOR EACH TFKIND AND THEN A GEOM_TEXT NEXT WITH THE VALUE!*

- [ ] *BELOW MUST GO TO PART WHERE WE FINISH HOW ALL THIS WORKS WITH
  CDL DATA, INCL LINEAR INTERPOLATION. ALTERNATIVELY, FINISH THAT
  EXPLANATION ABOVE BEFORE TALKING ABOUT CDL DETAILS?*

**** Interpolation between lines

- [ ] *MAYBE MOVE NEXT PARAGRAPH BACK TO INTRODUCTION OF METHOD*
Due to the energy dependence eq. [[eq:background:likelihood_def]] needs to
be understood to be valid in the energy range that defines the
used likelihood distributions.  

For the GridPix detector used in 2014/15 similar X-ray tube data was
taken and each of the 8 X-ray tube energies were assigned an energy
interval. In that interval the likelihood distributions of the
associated X-ray tube data were used to compute a likelihood value for
the given cluster. This leads to discontinuities of the properties at
the interval boundaries, while the actual properties of the clusters
change continuously. It can then lead to jumps in the efficiency of
the background suppression method and thus in the achieved background
rate. It seems a safe assumption that the reference distributions
undergo a continuous change for changing energies of the
X-rays. Therefore, to avoid such issues we perform a linear
interpolation for each cluster with energy $E_α$ between the closest
two neighboring X-ray tube energies $E_β$ and $E_γ$ in each
probability density $\mathcal{P}_i$ at the cluster's properties. With
$ΔE = E_β - E_γ$ the difference in energy between the closest two
X-ray tube energies, each probability density is then interpolated to:

\[
\mathcal{P}_i(E_α, x_i) = \left(1 - \frac{|E_α - E_β|}{ΔE}\right) · \mathcal{P}_i(E_β, x_i) +
  \left( 1 - \frac{|E_α - E_γ|}{ΔE} \right) · \mathcal{P}_i(E_γ, x_i) .
\]

Each probability density of the closest neighbors are evaluated at the
cluster's property $x_i$ and the linear interpolation weighted by the
distance to each energy is computed.

The choice for a linear interpolation was made after different ideas
were tried. Most importantly, a linear interpolation does not yield
unphysical results (for example negative bin counts in interpolated
data, which can happen in a spline interpolation) and yields very good
results in the cases that can be tested, namely reconstructing a known
likelihood distribution $B$ by doing a linear interpolation between
the two outer neighbors $A$ and
$C$. \ref{fig:cdl:cdl_morphing_frac_known_lines} shows this idea using
the fraction in transverse RMS variable. This variable is shown here
as it has the strongest obvious shape differences going from line to
line. The blue histogram corresponds to interpolated bins based on the
difference in energy between the line above and below the target
(hence it is not done for the outer lines, as there is no 'partner'
above / below). While the interpolation (blue) sometimes over- or
undershoots it needs to be kept in mind it covers almost twice the
energy that is needed when interpolating in the space between two
lines. To give an idea of what this results in for the interpolation,
fig. \ref{fig:cdl:cdl_morphing_frac_all_energies} shows a heatmap of
the same variable showing how the interpolation describes the energy
and fraction in transverse RMS space continuously. 

\begin{figure}[htbp]
  \centering
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{/home/basti/phd/Figs/CDL/cdlMorphing/fractionInTransverseRms_ridgeline_morph_all_XrayReferenceFile2018.h5_2018.pdf}
    \caption{Known lines}
    \label{fig:cdl:cdl_morphing_frac_known_lines}
  \end{subfigure}%
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{/home/basti/phd/Figs/CDL/cdlMorphing/cdl_as_raster_interpolated_fractionInTransverseRms.pdf}
    \caption{All energies}
    \label{fig:cdl:cdl_morphing_frac_all_energies}
  \end{subfigure}%
  \label{fig:cdl:cdl_morphing_examples}
  \caption{\subref{fig:cdl:cdl_morphing_frac_known_lines} shows recovering the known lines (aside from the outer ones)
  using a binwise linear interpolation from the neighbors. While the interpolation (blue) sometimes over- or undershoots
  it needs to be kept in mind it covers almost twice the energy that is needed.
  \subref{fig:cdl:cdl_morphing_frac_all_energies} is a heatmap of the full energy vs. fraction in transverse RMS
  space interpolated using this binwise linear interpolation.}
\end{figure}
- [ ] *REDO THESE TWO PLOTS!* 

Given that such an interpolation works as well as it does on
recovering a known line, implies that a linear interpolation on
'half' [fn:why_not_half_energy] the energy interval in practice should
yield reasonably realistic distributions, certainly much better than
allowing a discrete jump at specific boundaries.

See appendix [[#sec:appendix:morphing_cdl_spectra]] for a lot more
information about the ideas considered and comparisons between the not
interpolated data. In particular
fig. [[fig:appendix:cdl_morphing_logL_vs_energy]] which computes the
$\ln\mathcal{L}$ values for all of the (cleaned, cuts
tab. [[tab:cdl:cdl_cleaning_cuts]] applied) CDL data comparing the case of
no interpolation with interpolation showing a clear improvement in the
smoothness of the point cloud.

The need to define cut values $\mathcal{L}'$ for each likelihood
distribution to have a variable to cut on is one reason why the
practical implementation does not use full interpolation to compute
the reference distributions on the fly each time for every individual
cluster energy, but instead uses a pre-calculated high resolution mesh
of \num{1000} interpolated distributions. This allows to compute the
cut value as well as the distributions before starting to classify all
clusters, saving significant performance. With a mesh of \num{1000}
energies the largest error on the energy is $<\SI{5}{eV}$ anyhow. 

- [X] *EXPAND ON THIS, GIVE AN EQUATION DEFINING WHAT WE MEAN
  INCLUDING THE BINNING!*

- [ ] *WE STILL HAVEN'T TALKED ABOUT BINNING ETC OF ALL THIS!*

- [X] *ADD PLOT OF LINEAR INTERPOLATION 'RECOVERING' MIDDLE
  DISTRIBUTION*
- [X] *IN APPENDIX PUT PLOTS THAT SHOW HEATMAP OF KDE(?)/INTERPOLATION
  OF FULL ECCENTRICITY/... BEHAVIOR*
  -> Partially done, notes of CDL morphing study added in appendix as :noexport:
- [X] What about 2D heatmap showing energy ranges?

[fn:why_not_half_energy] It's not half the energy as all the lines are
not evenly spaced.

**** TODOs for the above sections                               :noexport:


- [X] explain why Inf is a common value
  
- [ ] explain why if interpreted as probabilities in reference
  distributions it's not to be thought of as "is 0.2% probability to
  be an X-ray", but rather is a generalized concept of probability,
  which when combined into a likelihood is just a distribution where
  all we care about is the cumulative fraction below a certain cutoff
  (our software efficiency)
- [ ] potentially rephrase the footnote, in particular after actually
  generating the likeilhood distributions as non-log plot!
- [ ] *REFERENCE OF 1e-5 to 1e-20 IS NOT TRUE, AS WE USE LN INSTEAD OF LOG10!*


- [ ] Note: an interesting plot could be to compute the likelihood
  distributions for each CDL _run_ separately and produce such a
  ridgeline plot, with all runs in each target/filter row. Similar to
  the ridgeline plots we have comparing the properties. Given the
  similarity of the properties there won't be a significant
  difference, but that alone would be valuable insight!

- [ ] *IMPORTANT* <2023-02-04 Sat 14:31>
  If I'm not too confused right now: We do *TWO* different
  morphings. One for the reference distributions which are used to
  compute the LogL *values* for a new cluster and another based on the
  existing logL *distributions* in order to determine _the cut values_
  in the morphed case!
  This means we *CAN* create an interpolation heatmap of the data
  using that.
  It still makes me wonder about whether we can get by with an
  unbinned approach somehow.

*** Study of interpolation of CDL distributions [/]              :extended:

The full study and notes about how we ended up at using a linear
interpolation, can be found in appendix [[#sec:appendix:morphing_cdl_spectra]].

- [X] This is now in the appendix. Good enough? Link
  
Put here all our studies of how and why we ended up at linear
interpolation!

The actual linear interpolation results (i.e. reproducing the "middle"
one will be shown in the actual thesis).

*** Generate plots of the likelihood and reference distributions  :extended:

In order to generate plots of the reference distributions as well as
the likelihood distributions, we use the ~plotCdl~ tool:

#+begin_src sh
cd $TPA/Plotting/plotCdl
./plotCdl
#+end_src
which generates a range of figures in the local ~out~ directory.

Among them:
- ridgeline plots for each property, as well as facet plots
- a combined ridgeline plot of all reference distributions side by
  side as a ridgeline each (used in the above section)
- plots of the CDL energies using ~energyFromCharge~
- plots of the likelihood distributions. Both as a outline and
  ridgeline plot.

Note that this tool can also create comparisons between a background
dataset and their properties and the reference data!

For now we use:
- ~out/ridgeline_all_properties_side_by_side.pdf~
- ~out/eccentricity_facet_calibration-cdl-2018.h5.pdf~
- ~out/fractionInTransverseRms_facet_calibration-cdl-2018.h5.pdf~
- ~out/lengthDivRmsTrans_facet_calibration-cdl-2018.h5.pdf~
- ~out/logL_ridgeline.pdf~

which also all live in ~phd/Figs/CDL~.

*** Energy resolution [/]
:PROPERTIES:
:CUSTOM_ID: sec:cdl:energy_resolution
:END:

- [ ] this should be a small aside, as it is not extremely important
  for us. Only serves as an input to our systematics. I guess it is
  important in that sense after all, heh.

On a slight tangent, with the main fluorescence lines fitted in all
the charge spectra, the position and line width can be used to compute
the energy resolution of the detector:

\[
ΔE = \frac{σ}{μ}
\]

where $σ$ is a measure of the line width and $μ$ the position (in this
case in total cluster charge). In some cases the full width at half
maximum (FWHM) is used and in others the standard deviation (for a normal
distribution the FWHM is about $2.35 σ$).

- [ ] *COMPUTE ENERGY RESOLUTION FOR EVERY 55FE SPECTRUM AND ADD NOTE
  ABOUT IT HERE!*

#+CAPTION: Energy resolutions depending on the energy of the fluorescence lines based on the
#+CAPTION: charge and pixel spectra. As expected the behavior of the energy resolution is
#+CAPTION: more or less $1/E$. The uncertainty for each point is the error propagated uncertainty
#+CAPTION: based on the fit parameter uncertainties for the mean position and the line width.
#+CAPTION: There are multiple data points for each energy owing to the fact that each run
#+CAPTION: is fit separately.
#+NAME: fig:cdl:energy_resolution
[[~/phd/Figs/CDL/energyresoplot-2019.pdf]]

**** NOTE on energy resolutions                                 :noexport:

Our energy resolutions, if converted to FWHM seem to be rather bad
compared to best in class gaseous detectors, for which sometimes
almost as low as 10% was achieved. 



** Application of likelihood cut for background rate [0/0]
:PROPERTIES:
:CUSTOM_ID: sec:background:likelihood_cut
:END:

- [ ] This *COULD* become its own chapter after all. That way make the
  CDL part a likelihood method & stuff chapter and this an apply
  method for background rate chapter.

- [X] most of it moved to likelihood method section
- [ ] *REWRITE ME*
- [ ] *BETTER CLARIFY DISTINCTION BETWEEN BACKGROUND AND TRACKING*  

By applying the likelihood cut method introduced in the first part of
this chapter to the background data of CAST we can extract all
clusters that are X-ray like and therefore describe the background
rate. Unless otherwise stated the following plots use a software
efficiency of $\SI{80}{\percent}$.

Fig. [[fig:background:cluster_centers_no_vetoes_2017_18]] shows the
cluster centers and their distribution over the whole center GridPix,
which highlights the extremely uneven distribution of background. The
increase towards the edges and in particular the corners is due to
events being cut off. Statistically by cutting of a piece of a
track-like event, the resulting event likely more spherical than
before. In particular in a corner where two sides are cut off
potentially. This is an aspect the detector vetoes help with, see
sec. [[#sec:background:septem_veto]]. In addition the plot shows some
smaller regions of few pixel diameter that have more activity. With
about \num{95000} clusters left on the center chip, the likelihood cut
at $\SI{80}{\percent}$ software efficiency represents a background
suppression of about a factor \num{15} (compare
tab. [[tab:cast:data_stats_overview]], $\sim\num{1.5e6}$ events on the
center chip). In the regions towards the center of the chip, the
suppression is of course much
higher. Fig. [[fig:background:background_suppression_tiles_no_vetoes_2017_18]]
shows what the background suppression looks like locally when
comparing the number of clusters in a small region of the chip to the
total number of raw clusters that were detected. Note that this is
based on the assumption that the raw data is homogeneously
distributed. See appendix [[#sec:appendix:background_occupancy]] for
occupancy maps of the raw data.

- [ ] *ADD OCCUPANCIES TO APPENDIX!*
- [ ] *FIX TITLE OF SUPPRESSION PLOT*
\begin{figure}[htbp]
  \centering
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{/home/basti/phd/Figs/backgroundClusters/background_cluster_centers_no_vetoes.pdf}
    \caption{Cluster centers $\ln\mathcal{L}$}
    \label{fig:background:cluster_centers_no_vetoes_2017_18}
  \end{subfigure}%
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{/home/basti/phd/Figs/backgroundClusters/background_suppression_tiles_no_vetoes.pdf}
    \caption{Background suppression}
    \label{fig:background:background_suppression_tiles_no_vetoes_2017_18}
  \end{subfigure}%
  \label{fig:background:background_no_vetoes_clusters}
  \caption{\subref{fig:background:background_clusters_no_vetoes}
  Cluster centers of all X-ray like clusters in the 2017/18 CAST background
  data. The number of these clusters increases drastically towards the edges
  and in particular corners, due to geometric effects. Some regions with
  minor sparking are visible as small yellow points of few pixel sizes.
  \subref{fig:background:background_suppression_tiles_no_vetoes_2017_18} shows the local
  background suppression over the total number of raw clusters detected. It assumes a
  homogeneous background distribution in the raw data. 
}
\end{figure}

- [ ] *REWRITE BELOW TO MAKE A BIT MORE SENSE IN CONTEXT OF ABOVE*
- [ ] *EXPLAIN THAT THIS USES GOLD REGION!!!*
  -> cluster distribution motivates that!

The distribution of the X-ray like clusters in the background data
motivate on the one hand to consider local background rates for a
physics analysis and at the same time the selection of a specific
region in which a benchmark background rate can be defined. For this
purpose cite:krieger2018search defines different detector regions in
which the background rate is computed and treated as constant. One of
these, termed the 'gold region' is a square around the center of
$\SI{5}{mm}$ side length (slightly larger than the 4 center tiles in
fig. \ref{fig:background:background_suppression_tiles_no_vetoes_2017_18}). All
background rate plots unless otherwise specified in the remainder of
the thesis always refer to this region of low background.

- [ ] *SLIGHTLY LARGER THAN TILES: ACTUALLY CORRECT?*

With the software efficiency defined, the $-\ln\mathcal{L}$ of each
cluster is simply computed using the linear interpolation of the log
likelihood distribution determined based on the cluster energy. A
cluster is considered a signal if its $L$ is smaller than the $L$
computed for the desired $ε_{\text{eff}}$ at the energy of the
cluster.

Using this approach for the single GridPix data taken at CAST in
2014/15, a background rate shown in
fig. [[fig:background_rate_eff80_only_center]] is achieved.

Note that by applying the exact same method to the CAST data taken
during the solar tracking time, the dataset containing candidates for
the solar axion search is extracted. 

#+begin_center
#+CAPTION: Background rate achieved based on the $\ln\mathcal{L}$ method at
#+CAPTION: $ε_{\text{eff}} = \SI{80}{\percent}$ using the GridPix1 CAST 2017/18 data
#+CAPTION: without the application of any vetoes.
#+NAME: fig:background_rate_eff80_only_center
[[~/phd/Figs/background/background_rate_crGold_no_vetoes.pdf]]
#+end_center
- [ ] *THINK ABOUT IF EVERYTHING IS CORRECT HERE* (see below)

The background rate presented here uses a comparable technique to one
presented in cite:krieger2018search and shown in
fig. [[fig:detector:background_rate_2014]] and uses the same detector
features. As a result the absolute background rate is comparable. The
Septemboard detector of this thesis however has multiple aspects to
improve on this. We will now go through the different detector
features and discuss how they can improve the background rate.

The average background rate between $\SIrange{0}{8}{keV}$ in this case
is $\SI{2.07e-5}{keV⁻¹.cm⁻².s⁻¹}$. 
#+begin_comment
Integrated background rate/keV in range: 0.0 .. 8.0: 2.0739e-05 keV⁻¹·cm⁻²·s⁻¹
#+end_comment

- [ ] *!!!*

*** TODOs for above                                              :noexport:

1. show all clusters left over after likelihood cut?
  Introduces discussion of different chip regions. For a pure
   "background rate" thus taking a center region is good idea and also
   introduces talking point again that vetoes can help.
2. show that background rate in center.

- [ ] What to do with raw data vs. applying rate?
- [ ] how to treat different chip regions?

- [ ] Try to replace the binning of reference distributions and
  therefore logL distribution by a KDE.


Note on background rate plot above:
- [ ] *THINK ABOUT THIS*
-> The slight 'cut' at 3 keV is stronger than the I would have thought
and in particular different from the background rate in
[[~/org/Figs/statusAndProgress/IAXO_TDR/background_rate_2017_2018_no_vetoes.pdf]]
which is much more "smooth" than the new background.
My first intuition was that this was due to some bug related to the
morphing kind. I.e. we weren't actually using the linear
morphing. However, I've computed the equivalent crGold, no vetoes case
[[~/org/Figs/statusAndProgress/backgroundRates/background_rate_crGold_no_morphing_no_vetoes.pdf]]
and there _is_ a difference. So it's definitely related to the
morphing.

The other big difference between the old plot is the revamp of the
calculation of the reference distributions (no use of the X-ray
reference H5 file anymore) and obviously the fitting by run + adapted
charge cuts for the reference data.
I *ASSUME* that this is the reason we get the new results. *However*
we should somehow maybe perform some check that we're actually doing
everything correctly?

*** Generate plots for interpolated likelihood distribution and logL variable correlations :noexport:

- [X] Create a plot of the likelihood distributions interpolated over
  all energies!
  Well, it doesn't work in the way I thought, because obviously we cannot just compute
  the likelihood distributions directly! We can compute *a* likelihood value for a cluster
  at an arbitrary energy, but to get a likelihood distribution we'd need actual X-ray data
  at all energies! The closest we have to that is the general CDL data (not just the main
  peak & using correct energies for each cluster)
  -> Attempt to use the CDL data with only the cleaning cuts
  -> Question: Which tool do we add this to? Or separate here using
  CDL reconstructed data?

  How much statistics do we even have if we end up splitting
  everything over 1000 energies? Like nothing...
  Done in:
  [[~/phd/Figs/background/logL_of_CDL_vs_energy.pdf]]
  now where we compare the no morphing vs. the linear morphing
  case. Much more interesting than I would have thought, because one
  can clearly see the effect of the morphing on the logL values that
  are computed!
  This is a pretty nice result to showcase the morphing is actually
  useful. Will be put into appendix and linked.

- [X] *ADD LINE SHOWING WHERE THE CUT VALUES ARE TO THE PLOT*
  -> The hard corners in the interpolated data is because the
  reference distributions are already pre-binned of course! So if by
  just changing the bins slightly the ε cut value still lies in the
  same bin, of course we see a 'straight' line in energy. Hence a non
  smooth curve.
  We could replace this all:
  - either by a smooth KDE and interpolate based on
    that as an alternative. I should really try this, the only
    questionable issue is the fracRms distribution and its discrete
    features. *However* we don't actually guarantee in any way that
    in current approach the bins _actually_ correspond to any fixed
    integer values. It is quite likely that the bins that show
    smaller/larger values are too wide/small!
  - or by keeping everything as is, but then performing a spline
    interpolation on the *distinct* (logL, energy) pairs such that the
    result is a smooth logL value. "Better bang for buck" than doing
    full KDE and avoids the issue of discreteness in the fracRms
    distribution. Even though the interpolated values probably
    correspond to something as if the fracRms distribution _had_ been smooth.
  
#+begin_src nim :tangle code/generate_interp_likelihood.nim
import std / [os, strutils]
import ingrid / ingrid_types
import ingrid / private / [cdl_utils, cdl_cuts, hdf5_utils, likelihood_utils]
import pkg / [ggplotnim, nimhdf5]


const TpxDir = "/home/basti/CastData/ExternCode/TimepixAnalysis"
const cdl_runs_file = TpxDir / "resources/cdl_runs_2019.org"
const fname = "/home/basti/CastData/data/CDL_2019/CDL_2019_Reco.h5"
const cdlFile = "/home/basti/CastData/data/CDL_2019/calibration-cdl-2018.h5"
const dsets = @["totalCharge", "eccentricity", "lengthDivRmsTrans", "fractionInTransverseRms"]

proc calcEnergyFromFits(df: DataFrame, fit_μ: float, tfKind: TargetFilterKind): DataFrame =
  ## Given the fit result of this data type & target/filter combination compute the energy
  ## of each cluster by using the mean position of the main peak and its known energy
  result = df
  result["Target"] = $tfKind
  let invTab = getInverseXrayRefTable()
  let energies = getXrayFluorescenceLines()
  let lineEnergy = energies[invTab[$tfKind]]
  result = result.mutate(f{float: "energy" ~ `totalCharge` / fit_μ * lineEnergy})

let h5f = H5open(fname, "r")
var df = newDataFrame()
for tfKind in TargetFilterKind:
  for (run, grp) in tfRuns(h5f, tfKind, cdl_runs_file):
    var dfLoc = newDataFrame()
    for dset in dsets:
      if dfLoc.len == 0:
        dfLoc = toDf({ dset : h5f.readCutCDL(run, 3, dset, tfKind, float64) })
      else:
        dfLoc[dset] = h5f.readCutCDL(run, 3, dset, tfKind, float64)
    dfLoc["runNumber"] = run
    dfLoc["tfKind"] = $tfKind
    # calculate energy from fit
    let fit_μ = grp.attrs["fit_μ", float]
    dfLoc = dfLoc.calcEnergyFromFits(fit_μ, tfKind)
    df.add dfLoc

proc calcInterp(ctx: LikelihoodContext, df: DataFrame): DataFrame =
  # walk all rows
  # feed ecc, ldiv, frac into logL and return a DF with
  result = df.mutate(f{float: "logL" ~ ctx.calcLikelihoodForEvent(`energy`,
                                                            `eccentricity`,
                                                            `lengthDivRmsTrans`,
                                                            `fractionInTransverseRms`)
  })

# first make plots of 3 logL variables to see their correlations  
ggplot(df, aes("eccentricity", "lengthDivRmsTrans", color = "fractionInTransverseRms")) +
  geom_point(size = 1.0) +
  ggtitle("lnL variables of all (cleaned) CDL data for correlations") + 
  ggsave("/home/basti/phd/Figs/background/correlation_ecc_ldiv_frac.pdf")

ggplot(df, aes("eccentricity", "fractionInTransverseRms", color = "lengthDivRmsTrans")) +
  geom_point(size = 1.0) +
  ggtitle("lnL variables of all (cleaned) CDL data for correlations") +   
  ggsave("/home/basti/phd/Figs/background/correlation_ecc_frac_ldiv.pdf")

ggplot(df, aes("lengthDivRmsTrans", "fractionInTransverseRms", color = "eccentricity")) +
  geom_point(size = 1.0) +
  ggtitle("lnL variables of all (cleaned) CDL data for correlations") +   
  ggsave("/home/basti/phd/Figs/background/correlation_ldiv_frac_ecc.pdf")


df = df.filter(f{`eccentricity` < 2.5})
ggplot(df, aes("eccentricity", "lengthDivRmsTrans", color = "fractionInTransverseRms")) +
  geom_point(size = 1.0) +
  ggtitle("lnL variables of all (cleaned) CDL data for correlations (ε < 2.5)") +   
  ggsave("/home/basti/phd/Figs/background/correlation_ecc_ldiv_frac_ecc_smaller_2_5.pdf")

ggplot(df, aes("eccentricity", "fractionInTransverseRms", color = "lengthDivRmsTrans")) +
  geom_point(size = 1.0) +
  ggtitle("lnL variables of all (cleaned) CDL data for correlations (ε < 2.5)") +   
  ggsave("/home/basti/phd/Figs/background/correlation_ecc_frac_ldiv_ecc_smaller_2_5.pdf")

ggplot(df, aes("lengthDivRmsTrans", "fractionInTransverseRms", color = "eccentricity")) +
  geom_point(size = 1.0) +
  ggtitle("lnL variables of all (cleaned) CDL data for correlations (ε < 2.5)") +   
  ggsave("/home/basti/phd/Figs/background/correlation_ldiv_frac_ecc_ecc_smaller_2_5.pdf")


from std/sequtils import concat
# now generate the plot of the logL values for all cleaned CDL data. We will compare the
# case of no morphing with the linear morphing case
proc getLogL(df: DataFrame, mk: MorphingKind): (DataFrame, DataFrame) = 
  let ctx = initLikelihoodContext(cdlFile, yr2018, crGold, igEnergyFromCharge, Timepix1, mk)
  var dfMorph = ctx.calcInterp(df)
  dfMorph["Morphing?"] = $mk
  let cutVals = ctx.calcCutValueTab()
  case cutVals.kind
  of mkNone:
    let lineEnergies = getEnergyBinning()
    let tab = getInverseXrayRefTable()
    var cuts = newSeq[float]()
    var energies = @[0.0]
    var lastCut = Inf
    var lastE = Inf
    for k, v in tab:
      let cut = cutVals[k]
      if classify(lastCut) != fcInf:
        cuts.add lastCut
        energies.add lastE
      cuts.add cut
      lastCut = cut
      let E = lineEnergies[v]
      energies.add E
      lastE = E
    cuts.add cuts[^1] # add last value again to draw line up 
    echo energies.len, " vs ", cuts.len
    let dfCuts = toDf({energies, cuts, "Morphing?" : $cutVals.kind})
    result = (dfCuts, dfMorph)
  of mkLinear:
    let energies = concat(@[0.0], cutVals.cutEnergies, @[20.0])
    let cutsSeq = cutVals.cutValues.toSeq1D
    let cuts = concat(@[cutVals.cutValues[0]], cutsSeq, @[cutsSeq[^1]])
    let dfCuts = toDf({"energies" : energies, "cuts" : cuts, "Morphing?" : $cutVals.kind})
    result = (dfCuts, dfMorph)

var dfMorph = newDataFrame()
let (dfCutsNone, dfNone) = getLogL(df, mkNone)
let (dfCutsLinear, dfLinear) = getLogL(df, mkLinear)
dfMorph.add dfNone
dfMorph.add dfLinear

var dfCuts = newDataFrame()
dfCuts.add dfCutsNone
dfCuts.add dfCutsLinear

ggplot(dfMorph, aes("logL", "energy", color = factor("Target"))) +
  facet_wrap("Morphing?") +
  geom_point(size = 1.0) +
  geom_line(data = dfCuts, aes = aes("cuts", "energies")) + # , color = "Morphing?")) + 
  ggtitle("lnL values of all (cleaned) CDL data against energy") + 
  ggsave("/home/basti/phd/Figs/background/logL_of_CDL_vs_energy.pdf", width = 1000, height = 600)
#+end_src

*** Generate background rate and cluster plot [0/2]              :noexport:

- [ ] *NOTE: THE PLOT WE CURRENTLY GENERATE: DOES IT USE TRACKING INFO
  OR NOT?*

- [ ] *EXPLAIN AND SHOW HOW TO INSERT TRACKING INFO*
#+begin_src sh
./cast_log_reader tracking \
                  -p ../resources/LogFiles/tracking-logs \
                  --startTime 2018/05/01 \
                  --endTime 2018/12/31 \
                  --h5out ~/CastData/data/DataRuns2018_Reco.h5 \
                  --dryRun
#+end_src
With the ~dryRun~ option you are only presented with what would be
written. Run without to actually add the data.  

To generate the background rate plot we need:
1. the reconstructed data files for the background data at CAST
   - (optional for this part): the slow control and tracking logs of
     CAST
   - the tracking information added to the reconstructed background
     data files to compute the background rate only for tracking or
     only for signal candidate data, done by running the
     ~cast_log_reader~ with the reconstructed data files as input
2. the reconstructed CDL data and the ~calibration-cdl-2018.h5~ file
   generated from it for the reference and likelihood distributions

To compute the background rate using these inputs we use the
~likelihood~ tool first in order to apply the likelihood cut method
according to a desired software efficiency defined in the
~config.toml~ file. Afterwards we can use another tool to generate the
plot of the background rate (which takes care of scaling the data to a
rate etc.).

The relevant section of the ~config.toml~ file should look like this:
#+begin_src toml
[Likelihood]
# the signal efficiency to be used for the logL cut (percentage of X-rays of the
# reference distributions that will be recovered with the corresponding cut value)
signalEfficiency = 0.8
# the CDL morphing technique to be used (see `MorphingKind` enum), none or linear
morphingKind = "Linear"
# clustering algorithm for septem veto
clusterAlgo = "dbscan" # choose from {"default", "dbscan"}
# the search radius for the cluster finding algorithm in pixel
searchRadius = 50 # for default clustering algorithm
epsilon = 65 # for DBSCAN algorithm

[CDL]
# whether to fit the CDL spectra by run or by target/filter combination.
# If `true` the resulting `calibration-cdl*.h5` file will contain sub groups
# for each run in each target/filter combination group!
fitByRun = true
#+end_src
(linear morphing, 80% efficiency, and CDL based on fits per run)

In total we'll want the following files:
- for years 2017 & 2018:
  - chip region crAll & crGold:
    - no vetoes
    - scinti veto
    - FADC veto
    - septem veto
    - line veto
- [X] *USE ~createAllLikelihoodCombinations~ for it after short
  rewrite*

In order to generate all likelihood output files (after having
computed the ~likelihood~ datasets in the data files and added the
tracking information!) we will use the
~createAllLikelihoodCombinations~ tool (rename please)


- [ ] *INSERT TRACKING INFORMATION AND ADD THE ~--tracking~ FLAG TO ~LIKELIHOOD~*

- [ ] *RERUN THE BELOW AND CHANGE PATHS TO DIRECTLY IN PHD DIRECTORY!*

  
- *UPDATE*: The current files we will likely use are generated by:
All standard variants including the different FADC percentiles:
#+begin_src sh
./createAllLikelihoodCombinations \
    --f2017 ~/CastData/data/DataRuns2017_Reco.h5 \
    --f2018 ~/CastData/data/DataRuns2018_Reco.h5 \
    --c2017 ~/CastData/data/CalibrationRuns2017_Reco.h5 \
    --c2018 ~/CastData/data/CalibrationRuns2018_Reco.h5 \
    --regions crGold --regions crAll \
    --vetoSets "{fkScinti, fkFadc, fkSeptem, fkLineVeto, fkExclusiveLineVeto}" \
    --fadcVetoPercentiles 0.9 --fadcVetoPercentiles 0.95 --fadcVetoPercentiles 0.99 \
    --out /t/lhood_outputs_adaptive_fadc \
    --cdlFile ~/CastData/data/CDL_2019/calibration-cdl-2018.h5 \
    --multiprocessing
#+end_src

The septem + line variants _without_ the FADC:
#+begin_src sh
./createAllLikelihoodCombinations \
    --f2017 ~/CastData/data/DataRuns2017_Reco.h5 \
    --f2018 ~/CastData/data/DataRuns2018_Reco.h5 \
    --c2017 ~/CastData/data/CalibrationRuns2017_Reco.h5 \
    --c2018 ~/CastData/data/CalibrationRuns2018_Reco.h5 \
    --regions crGold --regions crAll \
    --vetoSets "{+fkScinti, fkSeptem, fkLineVeto, fkExclusiveLineVeto}" \
    --out /t/lhood_outputs_adaptive_fadc \
    --cdlFile ~/CastData/data/CDL_2019/calibration-cdl-2018.h5 \
    --multiprocessing
#+end_src
and the lnL cut efficiency variants for 70 and 90% for the septem +
line + FADC@90% variants:
#+begin_src sh
./createAllLikelihoodCombinations \
    --f2017 ~/CastData/data/DataRuns2017_Reco.h5 \
    --f2018 ~/CastData/data/DataRuns2018_Reco.h5 \
    --c2017 ~/CastData/data/CalibrationRuns2017_Reco.h5 \
    --c2018 ~/CastData/data/CalibrationRuns2018_Reco.h5 \
    --regions crGold --regions crAll \
    --signalEfficiency 0.7 --signalEfficiency 0.9 \
    --vetoSets "{+fkScinti, +fkFadc, +fkSeptem, fkLineVeto}" \
    --fadcVetoPercentile 0.9 \
    --out /t/lhood_outputs_adaptive_fadc \
    --cdlFile ~/CastData/data/CDL_2019/calibration-cdl-2018.h5 \
    --multiprocessing \
    --dryRun
#+end_src
which for the time being are here:
[[file:~/org/resources/lhood_limits_automation_correct_duration/]]

#+begin_src sh :var TPA='/home/basti/CastData/ExternCode/TimepixAnalysis' :results drawer
cd $TPA/Analysis
./createAllLikelihoodCombinations --f2017 ~/CastData/data/DataRuns2017_Reco.h5 \
                                  --f2018 ~/CastData/data/DataRuns2018_Reco.h5 \
                                  --c2017 ~/CastData/data/CalibrationRuns2017_Reco.h5 \
                                  --c2018 ~/CastData/data/CalibrationRuns2018_Reco.h5 \
                                  --regions crGold \
                                  --regions crAll \
                                  --vetoes fkNoVeto \
                                  --vetoes fkScinti \
                                  --vetoes fkFadc \
                                  --vetoes fkSeptem \
                                  --vetoes fkLineVeto \
                                  --vetoes fkExclusiveLineVeto \
                                  --out ~/phd/resources/background/autoGen \
                                  --cdlFile ~/CastData/data/CDL_2019/calibration-cdl-2018.h5 \
                                  --multiprocessing \
                                  --dryRun 
#+end_src
#+RESULTS:
:results:
Command: (fname: "/home/basti/CastData/data/DataRuns2017_Reco.h5", year: 2017, region: crGold, vetoes: {fkNoVeto})
As filename: /home/basti/phd/resources/background/autoGen/likelihood_cdl2018_Run2_crGold.h5
Command: (fname: "/home/basti/CastData/data/DataRuns2017_Reco.h5", year: 2017, region: crGold, vetoes: {fkNoVeto, fkScinti})
As filename: /home/basti/phd/resources/background/autoGen/likelihood_cdl2018_Run2_crGold_scinti.h5
Command: (fname: "/home/basti/CastData/data/DataRuns2017_Reco.h5", year: 2017, region: crGold, vetoes: {fkNoVeto, fkScinti, fkFadc})
As filename: /home/basti/phd/resources/background/autoGen/likelihood_cdl2018_Run2_crGold_scinti_fadc.h5
Command: (fname: "/home/basti/CastData/data/DataRuns2017_Reco.h5", year: 2017, region: crGold, vetoes: {fkNoVeto, fkScinti, fkFadc, fkSeptem})
As filename: /home/basti/phd/resources/background/autoGen/likelihood_cdl2018_Run2_crGold_scinti_fadc_septem.h5
Command: (fname: "/home/basti/CastData/data/DataRuns2017_Reco.h5", year: 2017, region: crGold, vetoes: {fkNoVeto, fkScinti, fkFadc, fkSeptem, fkLineVeto})
As filename: /home/basti/phd/resources/background/autoGen/likelihood_cdl2018_Run2_crGold_scinti_fadc_septem_line.h5
Command: (fname: "/home/basti/CastData/data/DataRuns2017_Reco.h5", year: 2017, region: crGold, vetoes: {fkNoVeto, fkScinti, fkFadc, fkExclusiveLineVeto})
As filename: /home/basti/phd/resources/background/autoGen/likelihood_cdl2018_Run2_crGold_scinti_fadc_line.h5
Command: (fname: "/home/basti/CastData/data/DataRuns2017_Reco.h5", year: 2017, region: crAll, vetoes: {fkNoVeto})
As filename: /home/basti/phd/resources/background/autoGen/likelihood_cdl2018_Run2_crAll.h5
Command: (fname: "/home/basti/CastData/data/DataRuns2017_Reco.h5", year: 2017, region: crAll, vetoes: {fkNoVeto, fkScinti})
As filename: /home/basti/phd/resources/background/autoGen/likelihood_cdl2018_Run2_crAll_scinti.h5
Command: (fname: "/home/basti/CastData/data/DataRuns2017_Reco.h5", year: 2017, region: crAll, vetoes: {fkNoVeto, fkScinti, fkFadc})
As filename: /home/basti/phd/resources/background/autoGen/likelihood_cdl2018_Run2_crAll_scinti_fadc.h5
Command: (fname: "/home/basti/CastData/data/DataRuns2017_Reco.h5", year: 2017, region: crAll, vetoes: {fkNoVeto, fkScinti, fkFadc, fkSeptem})
As filename: /home/basti/phd/resources/background/autoGen/likelihood_cdl2018_Run2_crAll_scinti_fadc_septem.h5
Command: (fname: "/home/basti/CastData/data/DataRuns2017_Reco.h5", year: 2017, region: crAll, vetoes: {fkNoVeto, fkScinti, fkFadc, fkSeptem, fkLineVeto})
As filename: /home/basti/phd/resources/background/autoGen/likelihood_cdl2018_Run2_crAll_scinti_fadc_septem_line.h5
Command: (fname: "/home/basti/CastData/data/DataRuns2017_Reco.h5", year: 2017, region: crAll, vetoes: {fkNoVeto, fkScinti, fkFadc, fkExclusiveLineVeto})
As filename: /home/basti/phd/resources/background/autoGen/likelihood_cdl2018_Run2_crAll_scinti_fadc_line.h5
Command: (fname: "/home/basti/CastData/data/DataRuns2018_Reco.h5", year: 2018, region: crGold, vetoes: {fkNoVeto})
As filename: /home/basti/phd/resources/background/autoGen/likelihood_cdl2018_Run3_crGold.h5
Command: (fname: "/home/basti/CastData/data/DataRuns2018_Reco.h5", year: 2018, region: crGold, vetoes: {fkNoVeto, fkScinti})
As filename: /home/basti/phd/resources/background/autoGen/likelihood_cdl2018_Run3_crGold_scinti.h5
Command: (fname: "/home/basti/CastData/data/DataRuns2018_Reco.h5", year: 2018, region: crGold, vetoes: {fkNoVeto, fkScinti, fkFadc})
As filename: /home/basti/phd/resources/background/autoGen/likelihood_cdl2018_Run3_crGold_scinti_fadc.h5
Command: (fname: "/home/basti/CastData/data/DataRuns2018_Reco.h5", year: 2018, region: crGold, vetoes: {fkNoVeto, fkScinti, fkFadc, fkSeptem})
As filename: /home/basti/phd/resources/background/autoGen/likelihood_cdl2018_Run3_crGold_scinti_fadc_septem.h5
Command: (fname: "/home/basti/CastData/data/DataRuns2018_Reco.h5", year: 2018, region: crGold, vetoes: {fkNoVeto, fkScinti, fkFadc, fkSeptem, fkLineVeto})
As filename: /home/basti/phd/resources/background/autoGen/likelihood_cdl2018_Run3_crGold_scinti_fadc_septem_line.h5
Command: (fname: "/home/basti/CastData/data/DataRuns2018_Reco.h5", year: 2018, region: crGold, vetoes: {fkNoVeto, fkScinti, fkFadc, fkExclusiveLineVeto})
As filename: /home/basti/phd/resources/background/autoGen/likelihood_cdl2018_Run3_crGold_scinti_fadc_line.h5
Command: (fname: "/home/basti/CastData/data/DataRuns2018_Reco.h5", year: 2018, region: crAll, vetoes: {fkNoVeto})
As filename: /home/basti/phd/resources/background/autoGen/likelihood_cdl2018_Run3_crAll.h5
Command: (fname: "/home/basti/CastData/data/DataRuns2018_Reco.h5", year: 2018, region: crAll, vetoes: {fkNoVeto, fkScinti})
As filename: /home/basti/phd/resources/background/autoGen/likelihood_cdl2018_Run3_crAll_scinti.h5
Command: (fname: "/home/basti/CastData/data/DataRuns2018_Reco.h5", year: 2018, region: crAll, vetoes: {fkNoVeto, fkScinti, fkFadc})
As filename: /home/basti/phd/resources/background/autoGen/likelihood_cdl2018_Run3_crAll_scinti_fadc.h5
Command: (fname: "/home/basti/CastData/data/DataRuns2018_Reco.h5", year: 2018, region: crAll, vetoes: {fkNoVeto, fkScinti, fkFadc, fkSeptem})
As filename: /home/basti/phd/resources/background/autoGen/likelihood_cdl2018_Run3_crAll_scinti_fadc_septem.h5
Command: (fname: "/home/basti/CastData/data/DataRuns2018_Reco.h5", year: 2018, region: crAll, vetoes: {fkNoVeto, fkScinti, fkFadc, fkSeptem, fkLineVeto})
As filename: /home/basti/phd/resources/background/autoGen/likelihood_cdl2018_Run3_crAll_scinti_fadc_septem_line.h5
Command: (fname: "/home/basti/CastData/data/DataRuns2018_Reco.h5", year: 2018, region: crAll, vetoes: {fkNoVeto, fkScinti, fkFadc, fkExclusiveLineVeto})
As filename: /home/basti/phd/resources/background/autoGen/likelihood_cdl2018_Run3_crAll_scinti_fadc_line.h5
:end:

using the ~--dryRun~ option first to see the generated commands that
will be run.

We will run this over night <2023-02-13 Mon 00:37> now using the new
default eccentricity cutoff for the line veto of 1.5 (motivated by the
ratio of fraction passing real over fake!) as well as the
~lvRegularNoHLC~ line veto kind (although this should not matter, as
we won't use the line veto without the septem veto!). The files are
now in [[file:resources/background/autoGen/]].

- [ ] *WARNING: THE GENERATED FILES HERE STILL USE OUR HACKED IN
  CHANGED NUMBER OF BINS FOR THE REFERENCE DISTRIBUTIONS!*
  -> TWICE AS MANY BINS!



- [ ] *THESE ARE OUTDATED. WE USE OUR TOOL*
First let's apply the likelihood tool to the 2017 data for the gold
region:
#+begin_src sh
likelihood -f ~/CastData/data/DataRuns2017_Reco.h5 --computeLogL \
           --region crGold \
           --cdlYear 2018 \
           --h5out /home/basti/phd/resources/background/lhood_2017_crGold_80eff_no_vetoes.h5 \
           --cdlFile ~/CastData/data/CDL_2019/calibration-cdl-2018.h5
#+end_src


and the same for the end of 2018 data:
#+begin_src sh
likelihood -f ~/CastData/data/DataRuns2018_Reco.h5 --computeLogL \
           --region crGold \
           --cdlYear 2018 \
           --h5out /home/basti/phd/resources/background/lhood_2018_crGold_80eff_no_vetoes.h5 \
           --cdlFile ~/CastData/data/CDL_2019/calibration-cdl-2018.h5
#+end_src           

and now the same for the whole chip:
#+begin_src sh
likelihood -f ~/CastData/data/DataRuns2017_Reco.h5 --computeLogL \
           --region crAll \
           --cdlYear 2018 \
           --h5out /home/basti/phd/resources/background/lhood_2017_crAll_80eff_no_vetoes.h5 \
           --cdlFile ~/CastData/data/CDL_2019/calibration-cdl-2018.h5
#+end_src
and the same for the end of 2018 data:
#+begin_src sh
likelihood -f ~/CastData/data/DataRuns2018_Reco.h5 --computeLogL \
           --region crAll \
           --cdlYear 2018 \
           --h5out /home/basti/phd/resources/background/lhood_2018_crAll_80eff_no_vetoes.h5 \
           --cdlFile ~/CastData/data/CDL_2019/calibration-cdl-2018.h5
#+end_src           


With this we can first create the plot of the cluster centers from the
all chip files using ~plotClusterCenters~:

#+begin_src sh
plotBackgroundClusters ~/phd/resources/background/autoGen/likelihood_cdl2018_Run2_crAll.h5 \
                       ~/phd/resources/background/autoGen/likelihood_cdl2018_Run3_crAll.h5 \
                       --zMax 30 \
                       --title "X-ray like clusters of CAST data" \
                       --outpath ~/phd/Figs/backgroundClusters/
#+end_src

From the logL output files created by
~createAllLikelihoodCombinations~, let's now create the 'classical'
background rate in the gold region without any vetoes:

#+begin_src sh :results drawer
plotBackgroundRate \
 ~/phd/resources/background/autoGen/likelihood_cdl2018_Run2_crGold.h5 \
 ~/phd/resources/background/autoGen/likelihood_cdl2018_Run3_crGold.h5 \
 --combName 2017/18 \
 --combYear 2017 \
 --centerChip 3 \
 --title "Background rate from CAST data" \
 --showNumClusters \
 --showTotalTime \
 --topMargin 1.5 \
 --energyDset energyFromCharge \
 --outfile background_rate_crGold_no_vetoes.pdf \
 --outpath ~/phd/Figs/background/ \
 --useTeX
#+end_src

which generates
[[file:Figs/background/background_rate_crGold_no_vetoes.pdf]] from the TikZ ~.tex~
file of the same name with the following output about the background
rates:
#+begin_src
[INFO] total time: 8059622.573708649 of file: likelihood_cdl2018_Run2_crGold.h5
[INFO] total time: 3886553.012294403 of file: likelihood_cdl2018_Run3_crGold.h5
# ...
Dataset: 2017/18
         Integrated background rate in range: 0.0 .. 12.0: 2.3355e-04 cm⁻² s⁻¹
         Integrated background rate/keV in range: 0.0 .. 12.0: 1.9462e-05 keV⁻¹·cm⁻²·s⁻¹
Dataset: 2017/18
         Integrated background rate in range: 0.5 .. 2.5: 6.1610e-05 cm⁻² s⁻¹
         Integrated background rate/keV in range: 0.5 .. 2.5: 3.0805e-05 keV⁻¹·cm⁻²·s⁻¹
Dataset: 2017/18
         Integrated background rate in range: 0.5 .. 5.0: 1.1669e-04 cm⁻² s⁻¹
         Integrated background rate/keV in range: 0.5 .. 5.0: 2.5931e-05 keV⁻¹·cm⁻²·s⁻¹
Dataset: 2017/18
         Integrated background rate in range: 0.0 .. 2.5: 8.9066e-05 cm⁻² s⁻¹
         Integrated background rate/keV in range: 0.0 .. 2.5: 3.5626e-05 keV⁻¹·cm⁻²·s⁻¹
Dataset: 2017/18
         Integrated background rate in range: 4.0 .. 8.0: 2.6285e-05 cm⁻² s⁻¹
         Integrated background rate/keV in range: 4.0 .. 8.0: 6.5711e-06 keV⁻¹·cm⁻²·s⁻¹
Dataset: 2017/18
         Integrated background rate in range: 0.0 .. 8.0: 1.6591e-04 cm⁻² s⁻¹
         Integrated background rate/keV in range: 0.0 .. 8.0: 2.0739e-05 keV⁻¹·cm⁻²·s⁻¹
Dataset: 2017/18
         Integrated background rate in range: 2.0 .. 8.0: 8.3039e-05 cm⁻² s⁻¹
         Integrated background rate/keV in range: 2.0 .. 8.0: 1.3840e-05 keV⁻¹·cm⁻²·s⁻¹
#+end_src


- [ ] *FINISH THIS*

*** Background suppression [/]                                   :noexport:

In tab. [[tab:cast:data_stats_overview]] we see that the total CAST data
contains $\num{984319} + \num{470188} = \num{1454507}$
events. Compared to our $\num{94625}$ clusters left on the whole chip,
this represents a:
#+begin_src nim
let c17 = 984319'f64
let c18 = 470188'f64
let cCut = 94625'f64
echo (c17 + c18) / cCut
#+end_src

#+RESULTS:
: 15.37127608982827
background suppression of about 15.3. 

- [X] We could make a plot showing the background suppression? A tile
  map with text on taking X times Y squares and printing the
  suppression? Would be a nice way to give better understanding how
  the vetoes help to improve things over the chip.

Running ~plotBackgroundClusters~ now also produces a tile map of the
background suppression over the total raw number of clusters on the
chip!
~plots/background_suppression_tile_map.pdf~ which is in
[[file:Figs/backgroundClusters/background_suppression_tiles_no_vetoes.pdf]].

*** Verification of software efficiency using calibration data [/] 

- [ ] *SHOW OUR VERIFICATION USING 55FE DATA*
  -> Check and if not possibly do:
  Using fake energies (by leaving out pixel info) to generate
  arbitrary energies check the efficiency. I think this is what we did
  in that context. Right, that was it:
  [[file:~/CastData/ExternCode/TimepixAnalysis/Tools/determineEffectiveEfficiency.nim]]
- [ ] *CREATE PLOT OF THE LOGL CUT VALUES TO ACHIEVE THE USED
  EFFICIENCY!*
  -> essentially take our morphed likelihood distributions and for
  each of those calculate the logL value for the cut value. Then
  create a plot of energy vs. cut value and show that. Should be a
  continuous function through the logL values that fixes the desired efficiency.  
- [ ] *THINK ABOUT OPTIMIZING EFFICIENCY*
  -> We can think about writing a short script to optimize the
  efficiency

- [ ] *AFAIR WE STILL COMPUTE LOGL CUT VALUE BASED ON BINNED
  HISTOGRAM. CHANGE THAT TO UNBINNED DATA*  
  

** Artificial neural networks as cluster classification [/]
:PROPERTIES:
:CUSTOM_ID: sec:background:mlp
:END:

*THIS WILL STILL BE A DECENTLY LARGE CHAPTER*

- super short introduction to ANNs and MLPs in particular
- including math of an MLP, effectively just simple matrix math
- introduced backpropagation, very simply, mention SGD
- show the MLP layout we use, refer to appendix / extended version for
  study of different activation functions, layouts etc    
- Highlight that one of the main points was to use a network that
  remains relatively small

- describe training input data!
  - background is fine
  - X-ray training data: simulated events!


- [ ] *THIS IS FINALLY BECOMING A PROPER SECTION!*
  -> See further above after likelihood cut method

- [ ] *CITE REFERENCE FOR MLP*
- [ ] *CITE REFERENCE FOR SGD*
- [ ] *CITE REFERENCE FOR BACKPROPAGATION, AUTOGRAD*?

The likelihood cut based method presented in section
[[#sec:likelihood_method]] works well, but does not use the full potential
of the data. It mainly uses length / eccentricity related properties,
which are hand picked and ignores the possible separation power of all
other properties.

Multiple different ways to use all separation power exist. One
promising approach is the usage of artificial neural networks (ANN). A
multi-layer perceptron (MLP; (*reference something?*) ) is a simple
supervised ANN model, which consists of an input and output layer plus
one or more fully connected hidden layers. By training such a network
on the already computed geometric properties of the clusters,
computational requirements remain relatively moderate.

As each neuron on a given layer in an MLP is fully connected to all
neurons on the previous layer, the output of neuron $k$ on layer $i$
is described by

#+NAME: eq:mlp:neuron_output
\begin{equation}
y_{k,i} = φ \left( \sum_{j = 0}^m w_{kj} y_{j,i-1} \right)
\end{equation}

where $w_{kj}$ is the weight between neuron $k$ on layer $i$ and
neuron $j$ of $m$ neurons on layer $i-1$. $φ$ is a (typically
non-linear) activation function which is applied to saturate a
neuron's output.

If $y_{j,i-1}$ is considered a vector for all $j$, $w_{kj}$ can be
considered a weight matrix and eq. [[eq:mlp:neuron_output]] is simply a
matrix product. Each layer is computed iteratively starting from the
input layer towards the output layer.

Given that an MLP is a supervised learning algorithm, the desired
target output for a given input during training is known. A loss
function is defined to evaluate the accuracy of the network. Many
different loss functions are used in practice, but in many cases the
mean squared error (MSE; also known as L2 norm) is used as a loss

\[
l(\mathbf{y}, \mathbf{\hat{y}}) = \frac{1}{N} \sum_{i = 1}^N \left( y_i - \hat{y}_i \right)²
\]

where $\mathbf{y}$ is a vector $∈ \mathbb{R}^N$ of the network
outputs and $\mathbf{\hat{y}}$ the target outputs. The sum runs over
all $N$ output neurons. [fn:loss_of_minibatch]

In order to train a neural network, the initially random weights must
be modified. This is done by computing the gradients of the loss
(given an input) with respect to all the weights of the network,
$\frac{∂ l(\mathbf{y})}{∂ w_{ij}}$. Effectively the chain rule is used to express
these partial derivatives using the intermediate steps of the
calculation. This leads to an iterative equation for the weights
further up the network (towards the input
layer). Each weight is updated according to

\[
w^{n+1}_{ij} = w^n_{ij} - η \frac{∂ l(\mathbf{y})}{∂ w^n_{ij}}.
\]

where $η$ is the learning rate. This approach to updating the weights
during training is referred to as
backpropagation. [fn:reverse_mode_autograd]

[fn:loss_of_minibatch] For performance reasons to utilize the
parallel nature of GPUs, training and inference of NNs is done in
'mini-batches'. As still only a single loss value is needed for
training, the loss is computed as the mean of all losses for each
mini-batch element.

[fn:reverse_mode_autograd] The gradients in a neural network are
usually computed using 'automatic differentiation' (or 'autograd',
'autodiff'). There are two forms of automatic differentiation: forward
mode and reverse mode. These differ by the practical evaluation order
of the chain rule. Forward mode computes the chain rule from
left-to-right (input to output), while reverse mode computes it from
right-to-left (output to input). Computationally these differ in their
complexity in terms of the required number of evaluations given N
inputs and M outputs. Forward mode computes all M output derivatives
for a single input, whereas reverse mode computes all input
derivatives for a single output. Thus, forward mode is efficient for
cases with few inputs and many outputs, while the opposite is true for
reverse mode. Neural networks are classical cases of many inputs to
few outputs (scalar loss function!). As such reverse mode autograd is
the standard way to compute the gradients during NN training. In the
context it is effectively synonymous with 'backpropagation' (due to
its output-to-input evaluation of the chain rule).

*** MLP for CAST data
:PROPERTIES:
:CUSTOM_ID: sec:background:mlp_for_cast
:END:

The simplest approach to training using a neural network for data
classification of CAST like data is an MLP that uses the pre-computed
geometric properties for each cluster as an input.

The choice remains between a single or two output neurons. The more
classical approach is treating signal (X-rays) and background events
as two different classes that the classifier learns to predict. We use
the latter approach. By our convention the target outputs for signal
and background events are

\begin{align*}
\mathbf{\hat{y}}_{\text{signal}} &= \vektor{ 1 \\ 0 } \\
\mathbf{\hat{y}}_{\text{background}} &= \vektor{ 0 \\ 1 }
\end{align*}

where the first entry of $\mathbf{\hat{y}}$ corresponds to output
neuron 1 and the second to output neuron 2.

For the network to generalize to all real CAST data, the training
dataset must be representative of the wide variety of the real
data. For background like clusters it can be sourced from the
extensive non-tracking dataset recorded at CAST. For the signal like
X-ray data it is more problematic. The only source of X-rays with
enough statistics are the $\cefe$ datasets from CAST, but these only
describe X-rays of two energies. The CAST detector lab data from the
X-ray tube are both limited in statistics as well as suffering from
systematic differences to the CAST data due to different gas gains.

We will now describe how we generate X-ray clusters for the training
by simulating them from a target energy, a transverse diffusion and a
desired gas gain.

*** Generation of simulated X-rays as MLP training input
:PROPERTIES:
:CUSTOM_ID: sec:background:mlp:event_generation
:END:

To generate simulated events as MLP training input we only use the
relationships that were introduced in chapter
[[#sec:theory_detector]]. The idea is to generate events using the
underlying gaseous detector physics and make as few heuristic
modifications to better match the observed (imperfect) data as
possible.

The basic Monte Carlo algorithm will now be described in a series of
steps. The actual implementation is found at *CITE ME*.
- [ ] *CITE LINK TO IMPLEMENTATION OF FAKE GENERATION*
- [ ] *MENTION MARKUS GRIDPIX SIMULATION* -> slower, but more accurate.

1. (optional) sample from different fluorescence lines for an element
   given their relative intensities to define a target energy $E$ 
2. sample from the exponential distribution of the absorption length
   for the target photon energy to get the conversion point of the
   X-ray in the gas (Note: we only sample X-rays that convert, those
   that would traverse the whole chamber without conversion are ignored)
3. sample a target charge to achieve for the cluster, based on target energy by sampling from a
   normal distribution with a mean roughly matching detector energy resolution (target energy
   inverted to a charge)
4. sample center position of the cluster within a uniform radius of
   $\SI{4.5}{mm}$ around the chip center.
5. begin the sampling of each electron in the cluster
6. sample a charge in number of electrons (after amplification) for
   the electron based on a Pòlya distribution of input gas gain (and
   matching normalization & $θ$ constant).  Reject the electron if not
   crossing activation threshold (based on real data).
7. sample a radial distance from the cluster center based on gas
   diffusion constant and *INSERT DIFFUSION LAW* and the remaining
   drift distance to the readout plane.
8. sample a random angle, uniform in $(0, 2π]$
9. convert radius and angle to an $(x, y)$ position of the electron,
   add to the cluster.
10. based on a linear approximation activate between 0 to 4
    neighboring pixels with a slightly reduced gas gain. We never
    activate any neighbors at a charge of $\SI{1000}{e⁻}$ and always
    activate at least one at $\SI{10000}{e⁻}$. Number of activated
    pixels depends on uniform random number being below one of 4
    different thresholds:
    \[
    N_{\text{neighbor}} = \text{rand}(0, 1) · N < \text{threshold}
    \]
    where the threshold is determined by the linear function described
    by the mentioned condition and $\text{rand}(0, 1)$ is a uniform random
    number in $(0, 1)$.
11. continue sampling electrons until the total charge adds up to the
    target charge. We stop at the value, which is closer to the target
    (before or after adding a final pixel that crosses the target
    charge) to avoid biasing us towards values always larger than the
    target.
12. The final cluster is reconstructed just like any other real
    cluster, using the same calibration functions as the real chip
    (depending on which dataset it should correspond to).

Note: neighboring pixels are added to achieve matching eccentricity
distributions between real data and simulated. Activating neighboring
pixels increases the local pixel density randomly, which effectively
increases the weight of some pixels, leading to a slight increase of
eccentricity of a cluster. The approach of activating up to 4 neighbor
pixels and giving them slightly lower charges stems from empirically
matching the simulated data to real data. From a physical perspective
the most likely cause of neighboring pixels is due to UV photons that
are emitted in the amplification region and travel towards the grid
and produce a new electron, which starts an avalanche from there. From
that point of view neighboring pixels should see the full gas
amplification and neighbors other than the direct { up, down, left,
right } neighbors can be activated (depending on an exponential
distribution due to possible absorption of the UV photons in the gas).
- [ ] *CITE MARKUS MSC THESIS*


Based on the above algorithm fake events can be generated that either
match the gas gain and gas diffusion of an existing data taking run
(background or calibration) or any arbitrary combination of parameters
can be used. The former is important for verification of the validity
of the generated events as well as to check the MLP cut efficiency
(more on that in sec. [[#sec:background:mlp:mlp_cut_value]]). The latter
is used to generate a wide variety of MLP training data.

**** Generate some example and comparison plots :noexport:

- [ ] Comparison of the different properties as a ridgeline plot?
  -> similar to plots in appendix
  [[#sec:appendix:fit_by_run:gas_gain_var_cluster_prop]]
  like
  [[~/phd/Figs/CDL/C-EPIC-0.6kV_ridgeline_kde_by_run.pdf]]

Let's generate plots comparing the properties used by the MLP of
generated events with those of different runs. The idea is to generate
fake data based on the gas properties of each run, i.e. gas gain, gas
diffusion and the target energy of the X-ray fluorescence line we want
(i.e. the Kα line for $\ce{Mn}$ in case of the \cefe source or each
target in the X-ray tube data).

We'll generate a ridgeline plot of all these properties normalized to
$x/\text{max}(\{x\})$ where $\{x\}$ is the set of all values in the
run to get values between 0 and 1 for all properties such that they
can be compared in a single plot. Each ridge will use a KDE based
density to best highlight any differences in the underlying
distribution.

We will first use a tool to generate HDF5 files of simulated events
following a real run. For one run, e.g. 241 CAST calibration, we do
this by:
#+begin_src sh
fake_event_generator \
    like \
    -p /mnt/1TB/CAST/2018_2/CalibrationRuns2018_Reco.h5 \
    --run 241 \
    --outpath /tmp/test_fakegen_run241.h5 \
    --outRun 241 \
    --tfKind Mn-Cr-12kV \
    --nmc 50000
#+end_src
where we specify we want X-rays like the 55Fe source (via ~tfKind~)
and want to simulate 50k X-rays.

Then we can compare the properties using ~plotDatasetGgplot~, which
can be found here [[file:~/CastData/ExternCode/TimepixAnalysis/Plotting/plotDsetGgplot/plotDatasetGgplot.nim]]:
#+begin_src sh
plotDatasetGgplot \
    -f /mnt/1TB/CAST/2018_2/CalibrationRuns2018_Reco.h5 \
    -f /t/test_fakegen_run241.h5 \
    --run 241
#+end_src

Let's automate this quickly for all \cefe calibration runs.

#+begin_src nim :tangle /tmp/generate_all_fake_data_plots.nim
import shell, strutils, sequtils
import nimhdf5
import ingrid / [ingrid_types, tos_helpers]

const filePath = "/mnt/1TB/CAST/$#/CalibrationRuns$#_Reco.h5"
const genData = """
fake_event_generator \
    like \
    -p $file \
    --run $run \
    --outpath /home/basti/org/resources/fake_events_for_runs.h5 \
    --outRun $run \
    --tfKind Mn-Cr-12kV \
    --nmc 50000
"""
const plotData = """
plotDatasetGgplot \
    -f $file \
    -f /home/basti/org/resources/fake_events_for_runs.h5 \
    --names 55Fe --names Simulation \
    --run $run \
    --plotPath /home/basti/phd/Figs/fakeEventSimulation/runComparisons/ \
    --prefix ingrid_properties_run_$run \
    --suffix "Run $run"
"""

const years = ["2017", "2018_2"]
const yearFile = ["2017", "2018"]
for (year, fYear) in zip(years, yearFile):
  #if year == "2017": continue ## skip for now, already done
  let file = filePath % [year, fYear]
  var runs = newSeq[int]()
  withH5(file, "r"):
    let fileInfo = getFileInfo(h5f)
    runs = fileInfo.runs
  for run in runs:
    echo "Working on run: ", run
    let genCmd = genData % ["file", file, "run", $run]
    shell:
      ($genCmd)
    let plotCmd = plotData % ["file", file, "run", $run, "run", $run, "run", $run]
    shell:
      ($plotCmd)
#+end_src

which yields all the plots in:
[[file:Figs/fakeEventSimulation/runComparisons/]]


*** Determination of gas diffusion
:PROPERTIES:
:CUSTOM_ID: sec:background:mlp:determine_gas_diffusion
:END:

To generate X-ray events with similar physical properties as those
seen in a particular data taking run, we need the three inputs
required: gas gain, gas diffusion and target energy. The determination
of the gas gain is a well defined procedure, explained in sections
[[#sec:daq:polya_distribution_threshold]] and
[[#sec:calib:gas_gain_time_binning]]. The target energy is a matter of the
purpose the generated data is for. The gas diffusion however is more
complicated.

In theory the gas diffusion is a fixed parameter for a specific gas
mixture at fixed temperature, pressure and electromagnetic fields and
can be computed using Monte Carlo tools as mentioned already in
sec. [[#sec:theory:gas_diffusion]]. However, in practice MC tools suffer
from significant uncertainty (in particular in the form of run-by-run
RNG variation) especially at common numbers of MC samples. More
importantly though, real detectors neither have perfectly stable and
known temperatures, pressures and electromagnetic fields (for example
the field inhomogeneity of the drift field in the 7-GridPix detector
is not perfectly known), leading to important differences and
variations from theoretical numbers.

Fortunately, similar to the gas gain, the effective numbers for the
gas diffusion can be extracted from real data. The gas diffusion
constant $D_t(z)$ (introduced in sec. [[#sec:theory:gas_diffusion]])
describes the standard deviation of the transverse distance from the
initial center after drifting a distance $z$ along a homogeneous
electric field. This parameter is one of our geometric properties, in
the form of the transverse standard deviation $σ_T$ (historically
still named 'transverse RMS' though) and thus it can be used to
determine the gas diffusion coefficient. [fn:longitudinal_rms] As it
is a statistical process and $D_t$ is the standard deviation of the
population a large ensemble of events is needed.

- [ ] *MOVE DOWN*
This is available
both for calibration data as well as background data. In both cases it
is important to apply minor cuts to remove the majority of events in
which $σ_T$ contains contributions that are not due to gas diffusion
(e.g. a double X-ray event which was not separated by the cluster
finder). For calibration data the cuts should filter to single X-rays,
whereas for background events clean muon tracks are the target.


In theory one could determine it by way of a fit to the $σ_T$
distribution of all clusters in a data taking run as was done in
cite:krieger2018energy. This approach is problematic in practice due
to the large variation in possible X-ray conversion points - and
therefore drift distances - as a function of energy. Different
energies lead to different convolutions of exponential distributions
with the gas diffusion. This means a single fit does not describe the
data well in general and the upper limit is _not_ a good estimator for
the real gas diffusion coefficient (because it is due to those X-rays
converting directly behind the detector window by chance and/or
undergoing statistically large amount of diffusion).

Instead of performing an analytical convolution of the exponential
distribution and gas diffusion distributions to determine the correct
fit, a Monte Carlo approach is used here as well. The idea here is to
generate (simplified [fn:simplified_events]) fake events for a
starting gas diffusion coefficient, compare it to the real $σ_T$
distribution of the target run based on a test statistics, compute the
derivative of the test statistics as a form of loss function and
adjust the diffusion coefficient accordingly. Over a number of
iterations the distribution of the simulated $σ_T$ distribution will
converge to the real $σ_T$ distribution, if the choice of test
statistics is suitable.

One important point with regards to computing it for background data
is the realization for muons the equivalent 'conversion point' is a
uniform distribution over all distances from the detector window to
the readout as they enter typically from above. For X-rays it is just
based on the energy dependent attenuation length in the detector gas.

The test statistics chosen in practice for this purpose is the
Cramér-von-Mises criterion, defined by *CITE MISES, CRAMER*:

\[
ω² = ∫_{-∞}^∞ \left[ F_n(x) - F^*(x) \right]² \, \mathrm{d}F^*(x)
\]

where $F_n(x)$ is an empirical distribution function to test for and
$F^*(x)$ a cumulative distribution function to compare with. In case
of the two-sample case it can be computed as follows *CITE ANDERSON*:

\[
T = \frac{N M}{N + M} ω² = \frac{U}{N M (N + M)} - \frac{4 M N - 1}{6(M + N)}
\]

with $U$:

\[
U = N \sum_{i=1}^N \left( r_i - i \right)² + M \sum_{j = 1}^M \left( s_j - j \right)²
\]

In contrast to for example the Kolmogorov-Smirnov test it includes the
entire (E)CDF into the statistics instead of just the largest
deviation, which is a useful property to protect against outliers.

The iterative optimization process is effectively gradient descent,

- [ ] *REWRITE. WE ACTUALLY RETURN $σ_T$ AND NOT $D_T$!*
- [ ] *GENERALLY CLARIFY BETTER DISTINCTION BETWEEN PARAMETER $σ_T$
  AND OUR 'transverse RMS' NUMBER*  

\[
D_{T,i+1} = D_{T,i} - η \frac{∂ f(D_T)}{∂ D_T}
\]

where $f(D_T)$ is the Monte Carlo algorithm, which computes the test
statistics for a given $D_T$ and $η$ is the step size (a 'learning
rate'). The derivative is computed using finite
differences. [fn:derivatives_optimization]

Fig. [[fig:background:mlp:gas_diffusion_values]] shows the values of the
transverse gas diffusion coefficient $σ_T$ as they have been determined for all
runs. The color scale is the value of the Cramér-von-Mises test
criterion and indicates a measure of the uncertainty of the obtained
parameter. We can see that for the CDL datasets (runs above 305) the
uncertainty is larger. This is because reproducing the $σ_T$
distribution for these datasets is more problematic than for the CAST
\cefe datasets (due to more data impurity caused by X-ray backgrounds
of energies other than the target fluorescence line). Generally we can
see variation in the range from about
$\SIrange{620}{660}{μm.cm^{-1/2}}$ for the CAST data and larger
variation for the CDL data. The theoretical value we expect is about
$\SI{670}{μm.cm^{-1/2}}$, motivating the empirical deduction.

#+CAPTION: Transverse gas diffusion coefficient $σ_T$ as determined empirical based on an
#+CAPTION: iterative method attempting to reproduce the transverse RMS using simulated X-ray events
#+CAPTION: using the Cramér-von-Mises test criterion and gradient descent. All runs above run 305
#+CAPTION: are CDL calibration runs. The color scale is the CvM test score.
#+NAME: fig:background:mlp:gas_diffusion_values
#+ATTR_LATEX: :width 0.7\textwidth
[[~/phd/Figs/σT_per_run.pdf]]

Because this iterative calculation is not computationally cheap, the
resulting $D_T$ parameters for each run are cached in an HDF5 file.

- [ ] *INTRODUCE PLOT* showing the simulated vs real data after CvM optimization?

[fn:longitudinal_rms] Note that it is important to consider the
_transverse_ standard deviation and not the longitudinal one. Even for
an X-ray there will be a short track like signature during production
of the primary electrons from the initial photo-electron. This will
define the long axis of the cluster in most cases. Similarly, for
background events of cosmic muons the long axis corresponds to the
direction of travel while the transverse direction is purely due to
the diffusion process.

[fn:simplified_events] In principle the event generation is the same
algorithm as explained previously, but it does not sample from a Pólya
for the gas gain, nor compute any cluster properties. Only the
conversion point and number of target electrons based on target energy
is simulated. From these electron positions the long and short axes
are determined and $σ_T$ returned.

[fn:derivatives_optimization] Initially I tried to compute the
gradient using automatic differentiation using dual numbers for the
$D_T$ input and the MC code, but the calculation of the test
statistics is effectively independent of the input numbers due to the
calculation of the ECDF. As such the derivative information is lost. I
didn't manage to come up with a way to compute Cramér-von-Mises based
on the actual input numbers directly in a timely manner. :')

**** Cramér-von-Mises references [/]                            :noexport:

- [ ] *CITE CRAMER-VON-MISES*
  - https://doi.org/10.1080%2F03461238.1928.10416862
    Cramér, H. (1928). "On the Composition of Elementary Errors". Scandinavian Actuarial Journal. 1928 (1): 13–74
  - von Mises, R. E. (1928). Wahrscheinlichkeit, Statistik und Wahrheit. Julius Springer.
- [ ] *CITE TWO SAMPLE FORM*
  - https://doi.org/10.1214%2Faoms%2F1177704477 Anderson,
    T. W. (1962). "On the Distribution of the Two-Sample Cramer–von
    Mises Criterion" (PDF). Annals of Mathematical
    Statistics. Institute of Mathematical Statistics. 33 (3):
    1148–1159.

*** Comparison of simulated events and real data

- [ ] Show ridgeline plot of comparison of simulated and real for a
  given event
- [ ] Note that in both cases we filter to the main peak, photopeak in
  \cefe !

Fig. [[fig:background:mlp:ridgeline_simulated_vs_real_run241]] shows the
comparison of all cluster properties of simulated and real data for
one \cefe calibration run, number 241. The data is normalized to fit
into a single plot, each ridge shows a kernel density estimation of
the data. Gas gain and gas diffusion are extracted from the data as
explained in previous sections. 

All distributions outside the number of hits, energy and
total charge agree extremely well. These three are expected to not
perfectly match. Only the total charge is used as an input to the
MLP and it matches pretty well on its own. The reason is the
neighboring activation logic in the MC algorithm, which is empirically
modified such that it produces correct _geometric_ behavior at the
cost of slightly over- or underestimating the energy and number of
hits. The Pólya sampling and general neighboring logic is too
simple to properly reproduce both aspects at the same time. Other runs
show slightly worse agreement in some distributions, but overall the
agreement is good. Plots for all runs like this can be found in
*WHERE???*.

- [ ] *DECIDE WHERE TO SHOW ALL PLOTS. APPENDIX IS TOO LONG!*

#+CAPTION: Comparison of all geometric properties of simulated and real data of the
#+CAPTION: \cefe photopeak clusters of run 241 in a ridgeline plot. Each ridge shows
#+CAPTION: a KDE of the data. The gas gain and gas diffusion were
#+CAPTION: first extracted from the real run data to simulate events of the photopeak.
#+CAPTION: The two datasets agree very well with the exception of the number of hits,
#+CAPTION: energy and total charge. 
#+NAME: fig:background:mlp:ridgeline_simulated_vs_real_run241
[[~/phd/Figs/fakeEventSimulation/runComparisons/ingrid_properties_run_241_ridgeline_kde_by_run.pdf]]

*** Overview of the best performing MLP [/]

As a reference an MLP was trained (implementation [[https://github.com/Vindaar/TimepixAnalysis/blob/master/Tools/NN_playground/train_ingrid.nim][here]]) on a mix of
X-ray reference data (as signal-like training data) and a subset of
the CAST background data. The implementation was done using [[https://github.com/scinim/flambeau][Flambeu]], a
wrapper for [[https://pytorch.org/][libtorch]], using the following parameters:

- Input neurons: 14 (12 geometric, 2 non-geometric)
- Hidden layers: 2
- Neurons on hidden layer: 300
- Output neurons: 2
- Activation function: $\tanh$ or $\text{ReLU}$ (Rectified linear unit) (*ref?*)
- Gradient algorithm: Stochastic Gradient Descent (SGD) (*ref?*)
- Learning rate: \num{7e-4}
- Momentum: 0.2
- Batch size: 8192
- Training data: \num{250000} simulated X-rays and \num{250000} real
  background events *NOTE: NOT EXACTLY, NO?*, same number for
  validation
- Training epochs: \num{500000}  

- [ ] *Verify NUMBER OF USED INPUTS*

The 14 input neurons were fed with all geometric properties that do
not scale directly with the energy (no number of active pixels or
direct energy) or position on the chip (as the training data is skewed
towards the center).

This is considered a "reference" implementation, as it is essentially
the simplest ANN layout possible. As a comparison to the likelihood
method the receiver operating characteristic (ROC) curves (*REFERENCE
SOMETHING*) for the different
X-ray reference datasets is used. The signal and background data used
for each case (likelihood and MLP) is the test dataset used to test
the MLP performance after training. Fig. [[fig:roc_curves_logl_mlp]] shows
these ROC curves with the line style indicating the method and the
color corresponding to the different targets and thus different
fluorescence lines. The improvement in background rejection at a fixed signal efficiency
exceeds $\SI{10}{\percent}$ at multiple energies.

The usage of such neural networks is plainly a replacement for the
likelihood cut method used to detect the initial cluster candidates
and during classification of the septem/line veto. It does not replace
any of the vetoes, as these include information outside the center
chip, which is not available for the neural network (training and
validation becomes much more complicated if all chips were to be used,
due to the random coincidence problem mentioned in a previous
section).

- [ ] CNN as an alternative idea, add footnote and show github link to
  our work, say this has been looked into already during MSc thesis


During the training process of $\num{500 000}$ epochs every
$\num{5000}$ epochs the accuracy and loss are evaluated based on the
test sample and a model checkpoint is stored. The evolution of the
loss function for this network are shown in
fig. \ref{fig:background:mlp:training_loss}. As desired the loss
decreases during the training, but spikes somewhere between $\num{485
000}$ and $\num{490 000}$ epochs. It does recover, but does not reach
the minima found earlier. The accuracy also decreases slightly, but
not to the extent potentially expected based on the loss. Regardless,
the checkpoint after \num{485 000} epochs is used as the final MLP
model. Fig. \ref{fig:background:mlp:validation_output} shows the
output of neuron 0 (neuron 1 is a mirror image) for the test sample,
with signal like data towards 1 and background like data
towards 0. The separation is almost perfect, but small contributions
of the opposite type can be seen near the extrema. This is expected
due to the presence of real X-rays in the background dataset and
extreme statistical outliers in the X-ray dataset.

\begin{figure}[htbp]
  \centering
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{/home/basti/phd/Figs/neuralNetworks/10_05_23_sgd_tanh300_mse/loss.pdf}
    \caption{Training loss}
    \label{fig:background:mlp:training_loss}
  \end{subfigure}%
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{/home/basti/phd/Figs/neuralNetworks/10_05_23_sgd_tanh300_mse/validation_output.pdf}
    \caption{Charge spectrum}
    \label{fig:background:mlp:validation_output}
  \end{subfigure}%
  \label{fig:background:mlp:training_plots}
  \caption{\subref{fig:background:mlp:training_loss} Loss over the training progress, evaluated every \num{5000}
  epochs. A small spike in loss is seen after checkpoint \num{485000}, which is not fully recovered from. 
  \subref{fig:background:mlp:validation_output} Output of the validation data sample for neuron 0.
  Almost perfect signal and background separation.}
\end{figure}


#+begin_center
#+CAPTION: ROC curves for the comparison of the likelihood cut method (solid lines) to the
#+CAPTION: MLP predictions (dashed lines), colored by the different targets used to generate the
#+CAPTION: reference datasets. The background data used for each target corresponds to background
#+CAPTION: clusters in an energy range around the fluorescence line.
#+CAPTION: The MLP visibly outperforms the likelihood cut method in all energies. At the same
#+CAPTION: signal efficiency (x axis) a significantly higher background rejection is achieved.
#+CAPTION: *REDO THIS PLOT!!!!!!!!!!!!!!!!!!!!!!!!!!*
#+NAME: fig:roc_curves_logl_mlp
[[~/org/Figs/statusAndProgress/neuralNetworks/roc_curve_mlp_vs_likelihood_split_by_target_1000_epochs_no_energy_hits.pdf]]
#+end_center

*** Training call for best performing MLP                        :noexport:

The following is the training call that produced the MLP with the best
performance (the one used for all plots and limit calculations):

Reference: ~journal.org~ entry on <2023-05-10 Wed 14:24>.

#+begin_src sh
./train_ingrid \
    ~/CastData/data/CalibrationRuns2017_Reco.h5 \
    ~/CastData/data/CalibrationRuns2018_Reco.h5 \
    --back ~/CastData/data/DataRuns2017_Reco.h5 \
    --back ~/CastData/data/DataRuns2018_Reco.h5 \
    --modelOutpath ~/org/resources/nn_devel_mixing/10_05_23_sgd_gauss_diffusion_tanh300_mse_loss/mlp_tanh300_mse.pt \
    --plotPath ~/Sync/10_05_23_sgd_tanh300_mse/ \
    --datasets eccentricity \
    --datasets skewnessLongitudinal \
    --datasets skewnessTransverse \
    --datasets kurtosisLongitudinal \
    --datasets kurtosisTransverse \
    --datasets length \
    --datasets width \
    --datasets rmsLongitudinal \
    --datasets rmsTransverse \
    --datasets lengthDivRmsTrans \
    --datasets rotationAngle \
    --datasets fractionInTransverseRms \
    --datasets totalCharge \
    --datasets σT \
    --numHidden 300 \
    --numHidden 300 \
    --activation tanh \
    --outputActivation sigmoid \
    --lossFunction MSE \
    --optimizer SGD \
    --learningRate 7e-4 \
    --simulatedData \
    --backgroundRegion crAll \
    --nFake 250_000
#+end_src

*NOTE*: The command up there was still run with the default Nim RNG
seed for the ~shuffle~ procedure which reorders the background and
simulated data into training and validation samples!

**** Plots of the loss, accuracy and output                     :noexport:

The plots generated during the training call are also found in

[[file:Figs/neuralNetworks/10_05_23_sgd_tanh300_mse/]]

which are a direct copy of the initial plots generated from the call
above.

- [ ] We will likely want slightly nicer versions of these plots of
  course!
  The H5 file
  [[~/org/resources/nn_devel_mixing/10_05_23_sgd_gauss_diffusion_tanh300_mse_loss/mlp_desc_v2.h5]]
  contains all the required losses etc for the plots!
  The model file we use is:
  [[~/org/resources/nn_devel_mixing/10_05_23_sgd_gauss_diffusion_tanh300_mse_loss/mlp_tanh300_msecheckpoint_epoch_485000_loss_0.0055_acc_0.9933.pt]]


*** Combination of NN w/ vetoes                                  :noexport: 

- [ ] *I THINK THIS CAN BE DELETED*

[[file:~/CastData/ExternCode/TimepixAnalysis/Tools/NN_playground/predict_event.nim]]

#+begin_src sh
./predict_event ~/CastData/ExternCode/TimepixAnalysis/resources/LikelihoodFiles/lhood_2017_crGold_septemveto_lineveto_dbscan.h5 \
                ~/CastData/ExternCode/TimepixAnalysis/resources/LikelihoodFiles/lhood_2018_crGold_septemveto_lineveto_dbscan.h5 \
  --lhood --cutVal 3.2 --totalTime 3400
#+end_src

Within 0 - 8 keV:
6.413398692810459e-6 keV⁻¹•cm⁻²•s⁻¹
Within 0 - 2.8 & 4.0 - 8.0 keV:
4.133025759323338e-6 keV⁻¹•cm⁻²•s⁻¹

yields

[[~/org/Figs/statusAndProgress/neuralNetworks/background_rate_after_logl_septemveto_lineveto_and_mlp.pdf]]

*** Determination of MLP cut value
:PROPERTIES:
:CUSTOM_ID: sec:background:mlp:mlp_cut_value
:END:

Based on the output distributions of the MLP,
fig. \ref{fig:background:mlp:validation_output}, it can be used to act
as a cluster discriminator in the same way as for the $\ln\mathcal{L}$
method, following eq. [[eq:background:lnL:cut_condition]]. In practice
again the empirical distribution function of the signal-like output
data is computed and the quantile of the target software efficiency
$ε_{\text{eff}}$ is determined. Similarly to the likelihood cut method
this is done for \num{8} different energy ranges corresponding to the
CDL fluorescence lines. However, no interpolation is performed because
only the single output distribution is known. [fn:interpolation]

Due to the significant differences in gas gain between the CDL dataset
and the CAST \cefe calibration data, we do not use a single cut value
for each energy for all CAST data taking runs. Instead we use the
X-ray cluster simulation machinery to provide run specific X-ray
clusters from which to deduce cut values.

For each run we wish to apply the MLP to, we start by computing the
mean gas gain based on all gas gain intervals. Then we determine the
gas diffusion coefficient as explained in
sec. [[#sec:background:mlp:determine_gas_diffusion]]. With two of the
three required parameters for X-ray generation in hand, we then
simulate X-rays following each X-ray fluorescence line measured in the
CDL dataset, yielding 8 different simulated datasets for each
run. Based on these we compute one cut value on the MLP output
each. The so deduced cut value is applied as the MLP cut to that run
in the valid energy range.

The same approach is used for calibration runs as well as for
background runs.

[fn:interpolation] One could of course interpolate on the cut values
itself, but that is less well motivated and harder to
cross-check. Predicting a cut value from two known, neighboring
energies would likely not work very well. 

*** Verification of software efficiency using calibration data
:PROPERTIES:
:CUSTOM_ID: sec:background:mlp:effective_efficiency
:END:

As the simulated X-ray clusters certainly differ from real X-rays,
verification of the software efficiency using calibration data is
required. For all \cefe calibration runs as well as all CDL data we
produce cut values as explained in the previous section. Then we apply
these to those same runs and compute the effective efficiency as

\[
ε_{\text{effective}} = \frac{N_{\text{cut}}}{N_{\text{total}}}
\]

where $N_{\text{cut}}$ is the clusters remaining after applying the
MLP cut and $N_{\text{total}}$ is the total number of clusters after
application of the standard cleaning cuts applied for the \cefe
spectrum fit and the CDL fluorescence line fits.

The resulting efficiency is the effective efficiency the MLP
produces. A close match with the target software efficiency implies
the simulated X-ray data matches the real data well and the network
learned to identify X-rays based on physical properties (and not due
to overtraining). In the limit calculation later the mean of all these
effective efficiencies is used in place of the target software
efficiency as a realistic estimator for the signal efficiency.

Fig. [[fig:background:mlp:effective_efficiencies]] shows the effective
efficiencies obtained for all \cefe calibration and CDL runs using the
MLP checkpoint introduced earlier. The target efficiency in the plot
was $\SI{95}{\%}$ with an achieved mean effective efficiency of about
$\SI{91}{\%}$. Variation is significantly larger in the CDL runs (runs
above number 305), but fortunately towards actual larger
efficiencies. This is good because it implies using the mean of all
efficiencies - resulting in a smaller value - is conservative. The
marker symbol represents the energy of the target fluorescence
line. Note that the \cefe calibration escape peak near $\SI{3}{keV}$
is a separate data point ~"3.0"~ compared to the Silver fluorescence
line given as ~"2.98"~. That is because the two are fundamentally
different things. The escape event is a $\SI{5.9}{keV}$ photopeak
X-ray entering the detector, which ends up 'losing' about
$\SI{3}{keV}$ due to excitation of an Argon fluorescence $Kα$
X-ray. This means the absorption length is that of a $\SI{5.9}{keV}$
photon, whereas the Silver fluorescence line corresponds to a real
$\SI{2.98}{keV}$ photon and thus corresponding absorption length. As
such the geometric properties on average are different.

The standard deviation of all these effective efficiencies is one of
the systematics later used for the limit calculation.

#+CAPTION: Effective efficiencies obtained using the MLP for a target $ε = \SI{95}{\%}$
#+CAPTION: software efficiency. Runs above run 305 are CDL calibration runs. Different
#+CAPTION: symbols represent different target fluorescence lines. The \cefe escape peak
#+CAPTION: is indicated at $\SI{3}{keV}$ in contrast to the $\SI{2.98}{keV}$ Silver
#+CAPTION: line, due to different physical properties.
#+NAME: fig:background:mlp:effective_efficiencies
#+ATTR_LATEX: :width 0.7\textwidth
[[~/phd/Figs/neuralNetworks/effective_efficiency_at_0.95_mlp_mse_300tanh.pdf]]

**** Generate effective efficiency plot                         :noexport:

Before we can compute the effective efficiencies we need the cache
table. In principle we have it already for the data and calibration
runs, but we want a plot of all runs as well:
#+begin_src sh
./determineDiffusion \
    ~/CastData/data/DataRuns2017_Reco.h5 \
    ~/CastData/data/CalibrationRuns2017_Reco.h5 \
    ~/CastData/data/DataRuns2018_Reco.h5 \
    ~/CastData/data/CalibrationRuns2018_Reco.h5 \
    ~/CastData/data/CDL_2019/CDL_2019_Reco.h5
#+end_src

This currently produces the plot =~/Sync/σT_per_run.pdf=, which we
copied here:
[[~/phd/Figs/σT_per_run.pdf]]

Now we compute the effective efficiencies:
#+begin_src sh
./effective_eff_55fe \
    ~/CastData/data/CalibrationRuns2017_Reco.h5 \
    ~/CastData/data/CalibrationRuns2018_Reco.h5 \
    --model ~/org/resources/nn_devel_mixing/10_05_23_sgd_gauss_diffusion_tanh300_mse_loss/mlp_tanh300_msecheckpoint_epoch_485000_loss_0.0055_acc_0.9933.pt \
    --ε 0.95 \
    --cdlFile ~/CastData/data/CDL_2019/CDL_2019_Reco.h5 \
    --evaluateFit \
    --plotDatasets \
    --plotPath ~/Sync/10_05_23_sgd_tanh300_mse/effective_efficiencies
  #+end_src

*** Background rate using MLP 

Applying the MLP cut as explained leads to background rates as
presented in fig. [[fig:background:mlp:background_rate]], where the MLP is
compared to the $\ln\mathcal{L}$ cut both at the same software
efficiency $ε = \SI{80}{\%}$ as well as at a significantly higher
efficiency of $ε = \SI{91}{\%}$. Note the efficiencies are effective
efficiencies as explained in
sec. [[#sec:background:mlp:effective_efficiency]].

The background suppression is significantly improved in particular at
lower energies, implying other cluster properties provide better
separation at those than the three inputs used for the likelihood
cut. The mean background rates between $\SIrange{0.2}{8}{keV}$ for
each of these are:

\begin{align*}
b_{\ln\mathcal{L} @ \SI{80}{\%}} &= \SI{2.03e-05}{keV^{-1}.cm^{-2}.s^{-1}} \\
b_{\text{MLP} @ \SI{80}{\%}} &= \SI{1.15e-05}{keV^{-1}.cm^{-2}.s^{-1}} \\
b_{\text{MLP} @ \SI{91}{\%}} &= \SI{1.62e-05}{keV^{-1}.cm^{-2}.s^{-1}}
\end{align*}

#+CAPTION: Comparison of the background rate in the center region of the MLP at different
#+CAPTION: efficiencies and the standard $\ln\mathcal{L}$. The MLP improves most at
#+CAPTION: low energies and achieves comparable or better rates at higher efficiencies.
#+CAPTION: *FIX PLOT LINE*
#+NAME: fig:background:mlp:background_rate
#+ATTR_LATEX: :width 0.7\textwidth
[[~/phd/Figs/background/background_rate_gold_mlp_0.95_0.8_lnL.pdf]]

**** Generate background rate plot for MLP [/]                  :noexport:

Let's generate the background rate plot for the MLP.

We'll want to show 80% effective (so 85%) as direct comparison to LnL,
91% effective (so 95%) as reference because we use it later and
compare both to the 80% LnL.

#+begin_src sh :results drawer
plotBackgroundRate \
    /home/basti/org/resources/lhood_limits_10_05_23_mlp_sEff_0.99/lhood_c18_R2_crGold_sEff_0.85_mlp_mlp_tanh300_msecheckpoint_epoch_485000_loss_0.0055_acc_0.9933.h5 \
    /home/basti/org/resources/lhood_limits_10_05_23_mlp_sEff_0.99/lhood_c18_R3_crGold_sEff_0.85_mlp_mlp_tanh300_msecheckpoint_epoch_485000_loss_0.0055_acc_0.9933.h5 \
    /home/basti/org/resources/lhood_limits_10_05_23_mlp_sEff_0.99/lhood_c18_R2_crGold_sEff_0.95_mlp_mlp_tanh300_msecheckpoint_epoch_485000_loss_0.0055_acc_0.9933.h5 \
    /home/basti/org/resources/lhood_limits_10_05_23_mlp_sEff_0.99/lhood_c18_R3_crGold_sEff_0.95_mlp_mlp_tanh300_msecheckpoint_epoch_485000_loss_0.0055_acc_0.9933.h5 \
    ~/org/resources/lhood_limits_automation_correct_duration/likelihood_cdl2018_Run2_crGold.h5 \
    ~/org/resources/lhood_limits_automation_correct_duration/likelihood_cdl2018_Run3_crGold.h5 \
    --centerChip 3 \
    --names "MLP@0.8" --names "MLP@0.8" \
    --names "MLP@0.91" --names "MLP@0.91" \
    --names "LnL@0.8" --names "LnL@0.8" \
    --title "Background rate in center 5·5 mm², MLP at different ε" \
    --showNumClusters \
    --region crGold \
    --showTotalTime \
    --topMargin 1.5 \
    --energyDset energyFromCharge \
    --energyMin 0.2 \
    --outfile background_rate_gold_mlp_0.95_0.8_lnL.pdf \
    --outpath ~/phd/Figs/background/ \
    --quiet
#+end_src

#+RESULTS:
:results:
[INFO]:Dataset: LnL@0.8
[INFO]:	 Integrated background rate in range: 0.2 .. 12.0: 2.2566e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.2 .. 12.0: 1.9124e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: MLP@0.8
[INFO]:	 Integrated background rate in range: 0.2 .. 12.0: 1.5408e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.2 .. 12.0: 1.3057e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: MLP@0.91
[INFO]:	 Integrated background rate in range: 0.2 .. 12.0: 2.0473e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.2 .. 12.0: 1.7350e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: LnL@0.8
[INFO]:	 Integrated background rate in range: 0.5 .. 2.5: 6.2088e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.5 .. 2.5: 3.1044e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: MLP@0.8
[INFO]:	 Integrated background rate in range: 0.5 .. 2.5: 1.9348e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.5 .. 2.5: 9.6738e-06 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: MLP@0.91
[INFO]:	 Integrated background rate in range: 0.5 .. 2.5: 2.7614e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.5 .. 2.5: 1.3807e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: LnL@0.8
[INFO]:	 Integrated background rate in range: 0.5 .. 5.0: 1.1626e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.5 .. 5.0: 2.5836e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: MLP@0.8
[INFO]:	 Integrated background rate in range: 0.5 .. 5.0: 6.2968e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.5 .. 5.0: 1.3993e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: MLP@0.91
[INFO]:	 Integrated background rate in range: 0.5 .. 5.0: 8.5482e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.5 .. 5.0: 1.8996e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: LnL@0.8
[INFO]:	 Integrated background rate in range: 0.2 .. 2.5: 8.2667e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.2 .. 2.5: 3.5942e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: MLP@0.8
[INFO]:	 Integrated background rate in range: 0.2 .. 2.5: 3.1132e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.2 .. 2.5: 1.3536e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: MLP@0.91
[INFO]:	 Integrated background rate in range: 0.2 .. 2.5: 4.8017e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.2 .. 2.5: 2.0877e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: LnL@0.8
[INFO]:	 Integrated background rate in range: 4.0 .. 8.0: 2.6383e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 4.0 .. 8.0: 6.5958e-06 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: MLP@0.8
[INFO]:	 Integrated background rate in range: 4.0 .. 8.0: 1.8292e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 4.0 .. 8.0: 4.5731e-06 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: MLP@0.91
[INFO]:	 Integrated background rate in range: 4.0 .. 8.0: 2.5680e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 4.0 .. 8.0: 6.4199e-06 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: LnL@0.8
[INFO]:	 Integrated background rate in range: 0.2 .. 8.0: 1.5848e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.2 .. 8.0: 2.0317e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: MLP@0.8
[INFO]:	 Integrated background rate in range: 0.2 .. 8.0: 9.0055e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.2 .. 8.0: 1.1545e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: MLP@0.91
[INFO]:	 Integrated background rate in range: 0.2 .. 8.0: 1.2664e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.2 .. 8.0: 1.6236e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: LnL@0.8
[INFO]:	 Integrated background rate in range: 2.0 .. 8.0: 8.1788e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 2.0 .. 8.0: 1.3631e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: MLP@0.8
[INFO]:	 Integrated background rate in range: 2.0 .. 8.0: 6.1913e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 2.0 .. 8.0: 1.0319e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: MLP@0.91
[INFO]:	 Integrated background rate in range: 2.0 .. 8.0: 8.1612e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 2.0 .. 8.0: 1.3602e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:DataFrame with 7 columns and 303 rows:
     Idx       Energy         Rate    totalTime      RateErr      Dataset         yMin         yMax
  dtype:        float        float     constant        float       string        float        float
       0            0            0         3159            0      LnL@0.8            0            0
       1          0.2        2.287         3159       0.6342      LnL@0.8        1.652        2.921
       2          0.4         5.98         3159        1.026      LnL@0.8        4.955        7.006
       3          0.6        6.332         3159        1.055      LnL@0.8        5.277        7.387
       4          0.8         5.98         3159        1.026      LnL@0.8        4.955        7.006
       5            1         5.98         3159        1.026      LnL@0.8        4.955        7.006
       6          1.2        3.166         3159       0.7462      LnL@0.8         2.42        3.912
       7          1.4         2.99         3159       0.7252      LnL@0.8        2.265        3.715
       8          1.6        2.814         3159       0.7036      LnL@0.8        2.111        3.518
       9          1.8        3.166         3159       0.7462      LnL@0.8         2.42        3.912
      10            2        1.583         3159       0.5277      LnL@0.8        1.055        2.111
      11          2.2        1.759         3159       0.5562      LnL@0.8        1.203        2.315
      12          2.4       0.8794         3159       0.3933      LnL@0.8       0.4861        1.273
      13          2.6        1.583         3159       0.5277      LnL@0.8        1.055        2.111
      14          2.8        2.287         3159       0.6342      LnL@0.8        1.652        2.921
      15            3        5.101         3159       0.9472      LnL@0.8        4.154        6.048
      16          3.2        5.277         3159       0.9634      LnL@0.8        4.313         6.24
      17          3.4        4.221         3159       0.8617      LnL@0.8         3.36        5.083
      18          3.6        3.342         3159       0.7667      LnL@0.8        2.575        4.109
      19          3.8        2.111         3159       0.6093      LnL@0.8        1.501         2.72

[INFO]:INFO: storing plot in /home/basti/phd/Figs/background/background_rate_gold_mlp_0.95_0.8_lnL.pdf
[WARNING]: Printing total background time currently only supported for single datasets.
:end:

** Additional detector features as vetoes
:PROPERTIES:
:CUSTOM_ID: sec:background:additional_vetoes
:END:

Next we will cover how the additional detector features can be used as
vetoes to suppress even more background. We start by looking at the
scintillators in sec. [[#sec:background:scinti_veto]]. Then we consider
the FADC as a veto based on time evolution of the events in
sec. [[#sec:background:fadc_veto]]. Finally, we consider the outer GridPix
ring as further geometric vetoes in sec. [[#sec:background:septem_veto]]
and sec. [[#sec:background:line_veto]] in the form of the 'septem veto'
and the 'line veto'.

- [ ] *ADD INTRODUCTION TO SECTION ABOUT VETOES*
  -> Make sure to properly get a 'split' from the likelihood cut
  method here.
  Better to have a chapter / section on the different methods to
  suppress background. So use it.

*** Scintillators as vetoes
:PROPERTIES:
:CUSTOM_ID: sec:background:scinti_veto
:END:

- [ ] *ADD SECTION ABOUT EXPECTATIONS OF THE VETOS. USING THE PLOTS
  HENDRIK ALREADY SHOWED IN HIS THESIS ABOUT IONIZATION*

  

*TODO*: rewrite this paragraph in context of full chapter
  -> in particular references to GridPix1 & language that is more
  theoretical than actually referring to the setup of the Septemboard
  detector.

- [ ] *IMPORTANT*: These ideas *ARE* already explained in the detector
  chapter where the features are introduced!
  I think such explanations really should better be there, no? And
  here just explain what is done and why?
  -> take some of the explanations and language from here to
  sec. [[#sec:detector:scintillators]]!

- [ ] Show the clock cycle histograms here? Or before in a section?

As introduced in theory section [[#sec:theory:xray_fluorescence]] one
problematic source of background is X-ray fluorescence created due to
abundant muons interacting with material of - or close to - the
detector. The resulting X-rays, if detected, are indistinguishable
from those X-rays originating from an axion (or other particle of
study). And given the required background levels, cosmic induced X-ray
fluorescence plays a significant role in the datasets. Scintillators
(ideally 4π around the whole detector setup) can be very helpful to
reduce the influence of such background events by 'tagging' certain
events. For a fully encapsulated detector, any (at least) muon induced
X-ray would be preceded by a signal in one (or multiple)
scintillators. As such, if the time $t_s$ between the scintillator
trigger and the time of activity recorded with the GridPix is small
enough, the two are likely to be in real coincidence. 

In the setup used at CAST with a $\SI{42}{\cm}$ times $\SI{82}{\cm}$
scintillator paddle sitting about $\sim\SI{30}{cm}$ away from the
detector center and a $\cos²(θ)$ distribution for muons a significant
fraction of muons should be tagged. Similarly, the second scintillator
on the Septemboard detector, the small SiPM behind the readout area
should trigger precisely in those cases where a muon traverses the
detector from the front or back such that the muon track is orthogonal
to the readout plane.

The term 'non trivial trigger' in the following indicates events in
which a scintillator triggered and the number of clock cycles to the
GridPix readout was larger than \num{0} (scintillator had a trigger in
the first place) and smaller than \num{300}. The latter cut is
somewhat arbitrary, as the idea is on two things: 1. no clock cycles
of \num{4095} (indicates clock ran over) and 2. the physics involved
leading to coincidences typically takes place on time scales shorter
than $\SI{100}{clock\;cycles} = \SI{2.5}{μs}$ (see
sec. [[#sec:detector:scintillators]]). Anything above should just be a
random coincidence. \num{300} is therefore just chosen to have a
buffer to where physical clock cycles start.

During the Run-3 data taking period a total of \num{69243} non trivial
veto paddle triggers were recorded. [fn:estimation_of_tagged_muons]
The distribution of the clock cycles after which the GridPix readout
happened is shown in fig. [[fig:background:scintillator_clock_cycles]] on
the right. The narrow peak at $\SI{255}{clock;cycles}$ seems to be
some kind of artifact, potentially a firmware bug. The data was looked
at and there is nothing unusual about it (neither cluster properties
nor individual events). The source therefore remains unclear, but a
physical origin is extremely unlikely as the peak is exactly one clock
cycle wide (unrealistic for a physical process in this context) and
coincidentally exactly \num{255} (~0xFF~ in hexadecimal), hinting
towards a counting issue in the firmware. The actual distribution
looks as expected, being a more or less flat distribution
corresponding to muons traversing at different distances to the
readout. The real distribution does not start at exactly
$\SI{0}{clock;cycles}$ due to inherent processing delays and even
close tracks requiring some time to drift to the readout and
activating the FADC. Further, geometric effects play a role as very
close to the grid only perfectly parallel tracks can achieve low clock
cycles, but the further away different angles contribute to the same
times.

The SiPM recorded \num{4298} non trivial triggers in the same time,
which are shown in fig. [[fig:background:scintillator_clock_cycles]] on
the left. [fn:estimation_of_tagged_muons_sipm] Also this distribution
looks more or less as expected, showing a peak towards low clock
cycles (typical ionization and therefore similar times to accumulate
enough charge to trigger) with a tail for less and less ionizing
tracks. The same physical cut off as in the veto paddle distribution
is visible corresponding to the full $\SI{3}{cm}$ of drift time.

Both of these distributions and the physically motivated cutoffs in
clock cycles motivate a scintillator veto at clock cycles somewhere
above $\SIrange{60}{70}{clock;cycles}$. To be on the conservative side
and because random coincidences are very unlikely (on the time scales
of several clock cycles; picking a value slightly larger implies a
negligible dead time), a scintillator veto cut of
$\SI{150}{clock;cycles}$ was chosen.

- [ ] *PUT SCINTI CUT OFF INTO CONFIG.TOML*
- [ ] *CHECK THE BACKGROUND RATE USING 60, 100, 150, 300 CLOCK
  CYCLES*
  -> if everything from 100 looks the same, choose 100.

The resulting improvement of the background rate is shown in
fig. [[fig:background_rate_scinti_veto]], albeit only for the end of 2018
data as the scintillator trigger was not working correctly (as
mentioned in sec. [[#sec:cast:data_taking_woes]]). The biggest
improvements can be seen in the $\SI{3}{keV}$ and $\SI{8}{keV}$ peaks,
both of which are likely X-ray fluorescence ($\ce{Ar}_{Kα}$
$\SI{3}{keV}$ & $\ce{Cu}_{Kα}$ $\SI{8}{keV}$ and orthogonal muons
($>\SI{8}{keV}$). Arguably the improvements could be bigger, but the
efficiency of the scintillator was not ideal, resulting in some likely
muon induced X-rays to remain untagged. Lastly, the coverage of the
scintillators is leaving large angular areas without a possibility to
be tagged. 

For a future detector an almost $4π$ scintillator setup and correctly
calibrated and tested scintillators are an extremely valuable upgrade.

#+CAPTION: Clock cycle distributions of both scintillators of the end of 2018 data.
#+CAPTION: The data is filtered to all non-trivial triggers (non zero and
#+CAPTION: less than \num{300}; there are individual random coincidences in
#+CAPTION: values up to \num{4095} where all triggers whose counter overran are).
#+CAPTION: The origin of the peak at \SI{255}{clocks} in the data of the paddle is
#+CAPTION: unclear.
#+NAME: fig:background:scintillator_clock_cycles
[[~/phd/Figs/scintillators/scintillators_facet_main_2018.pdf]]

- [ ] *UPDATE PLOT OF SCINTILLATOR VETO*
#+CAPTION: Background rate based on the Run-3 CAST data achieved by the addition of a scintillator cut veto
#+CAPTION: of $\SI{10}{μs}$ for any cluster that initially passes the log likelihood
#+CAPTION: cut. The biggest improvements can be seen in the $\SI{3}{keV}$ and $\SI{8}{keV}$
#+CAPTION: peaks, both of which are likely X-ray fluorescence (Cu & Ar; both energies) and orthogonal
#+CAPTION: muons ($>\SI{8}{keV}$).
#+NAME: fig:background:background_rate_scinti_veto
[[~/phd/Figs/background/background_rate_crGold_scinti_run3.pdf]]

#+begin_comment
[INFO]:Dataset: Scinti
[INFO]:	 Integrated background rate in range: 0.0 .. 8.0: 1.5419e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 8.0: 1.9274e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Scinti
[INFO]:	 Integrated background rate in range: 2.0 .. 8.0: 7.2827e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 2.0 .. 8.0: 1.2138e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Scinti
[INFO]:	 Integrated background rate in range: 4.0 .. 8.0: 2.3606e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 4.0 .. 8.0: 5.9015e-06 keV⁻¹·cm⁻²·s⁻¹
#+end_comment

[fn:estimation_of_tagged_muons] A ball park estimate yields a coverage
of about $\SI{35}{°}$ around the zenith. Assuming $\cos²(θ)$ muons it
covers about $\SI{68}{\%}$ of muons in that axis. With a rate of
$\sim\SI{1}{cm⁻².min⁻¹}$, $\SI{1125}{h}$ of data with scintillator and
$\SI{4.2}{cm²}$ of active area in front of center GridPix roughly
\num{194000} muons are expected. About \num{70000} non trivial
triggers were recorded. Efficiency of $\sim\SI{35}{\%}$.

[fn:estimation_of_tagged_muons_sipm] Estimating the number of expected
muons for the SiPM is much more difficult, because there is little
literature on the actual muon rate at angles close to $\SI{90}{°}$. The
extended version of this thesis contains some calculations, which try
to estimate energy distribution and rate under these angles in the
detector based on numerically transporting muons from the upper
atmosphere to the surface undergoing relativistic effects, changing
atmospheric pressures and the related energy loss per distance.

**** Generate background rate plot with scintillator vetoes     :noexport:

Let's generate the plot, first for all data (effect of scinti only
visible in 2018 though!):

#+begin_src sh :results drawer
plotBackgroundRate \
 ~/phd/resources/background/autoGen/likelihood_cdl2018_Run2_crGold.h5 \
 ~/phd/resources/background/autoGen/likelihood_cdl2018_Run3_crGold.h5 \
 ~/org/resources/lhood_limits_automation_preliminary/lhood_outputs_adaptive_fadc/likelihood_cdl2018_Run2_crGold_scinti.h5 \
 ~/org/resources/lhood_limits_automation_preliminary/lhood_outputs_adaptive_fadc/likelihood_cdl2018_Run3_crGold_scinti.h5 \
 --names "No vetoes" --names "No vetoes" --names "Scinti" --names "Scinti" \
 --centerChip 3 \
 --title "Background rate from CAST data, incl. scinti veto" \
 --showNumClusters \
 --showTotalTime \
 --topMargin 1.5 \
 --energyDset energyFromCharge \
 --outfile background_rate_crGold_scinti.pdf \
 --outpath ~/phd/Figs/background/ \
 --useTeX \
 --quiet
#+end_src

#+RESULTS:
:results:
[INFO]:Dataset: No vetoes
[INFO]:	 Integrated background rate in range: 0.0 .. 12.0: 2.3355e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 12.0: 1.9462e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Scinti
[INFO]:	 Integrated background rate in range: 0.0 .. 12.0: 2.1413e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 12.0: 1.7844e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: No vetoes
[INFO]:	 Integrated background rate in range: 0.5 .. 2.5: 6.1610e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.5 .. 2.5: 3.0805e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Scinti
[INFO]:	 Integrated background rate in range: 0.5 .. 2.5: 5.9935e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.5 .. 2.5: 2.9968e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: No vetoes
[INFO]:	 Integrated background rate in range: 0.5 .. 5.0: 1.1669e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.5 .. 5.0: 2.5931e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Scinti
[INFO]:	 Integrated background rate in range: 0.5 .. 5.0: 1.0798e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.5 .. 5.0: 2.3997e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: No vetoes
[INFO]:	 Integrated background rate in range: 0.0 .. 2.5: 8.9066e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 2.5: 3.5626e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Scinti
[INFO]:	 Integrated background rate in range: 0.0 .. 2.5: 8.6722e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 2.5: 3.4689e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: No vetoes
[INFO]:	 Integrated background rate in range: 4.0 .. 8.0: 2.6285e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 4.0 .. 8.0: 6.5711e-06 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Scinti
[INFO]:	 Integrated background rate in range: 4.0 .. 8.0: 2.3606e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 4.0 .. 8.0: 5.9015e-06 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: No vetoes
[INFO]:	 Integrated background rate in range: 0.0 .. 8.0: 1.6591e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 8.0: 2.0739e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Scinti
[INFO]:	 Integrated background rate in range: 0.0 .. 8.0: 1.5419e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 8.0: 1.9274e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: No vetoes
[INFO]:	 Integrated background rate in range: 2.0 .. 8.0: 8.3039e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 2.0 .. 8.0: 1.3840e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Scinti
[INFO]:	 Integrated background rate in range: 2.0 .. 8.0: 7.2827e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 2.0 .. 8.0: 1.2138e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:DataFrame with 7 columns and 120 rows:
     Idx       Energy         Rate    totalTime      RateErr      Dataset         yMin         yMax
  dtype:        float        float     constant        float       string        float        float
       0            0        4.688         3318       0.8859    No vetoes        3.802        5.574
       1          0.2        2.511         3318       0.6484    No vetoes        1.863         3.16
       2          0.4        5.692         3318       0.9762    No vetoes        4.716        6.668
       3          0.6        6.362         3318        1.032    No vetoes         5.33        7.394
       4          0.8         5.86         3318       0.9905    No vetoes        4.869         6.85
       5            1         5.86         3318       0.9905    No vetoes        4.869         6.85
       6          1.2        3.014         3318       0.7103    No vetoes        2.303        3.724
       7          1.4        2.846         3318       0.6903    No vetoes        2.156        3.536
       8          1.6        2.846         3318       0.6903    No vetoes        2.156        3.536
       9          1.8        3.348         3318       0.7487    No vetoes          2.6        4.097
      10            2        1.507         3318       0.5023    No vetoes        1.005        2.009
      11          2.2        1.842         3318       0.5553    No vetoes        1.286        2.397
      12          2.4        1.005         3318       0.4101    No vetoes       0.5944        1.415
      13          2.6        1.674         3318       0.5294    No vetoes        1.145        2.204
      14          2.8        2.176         3318       0.6036    No vetoes        1.573         2.78
      15            3        5.525         3318       0.9617    No vetoes        4.563        6.487
      16          3.2         5.19         3318       0.9321    No vetoes        4.258        6.122
      17          3.4        4.018         3318       0.8202    No vetoes        3.198        4.838
      18          3.6        3.348         3318       0.7487    No vetoes          2.6        4.097
      19          3.8        2.511         3318       0.6484    No vetoes        1.863         3.16

[INFO]:INFO: storing plot in /home/basti/phd/Figs/background/background_rate_crGold_scinti.pdf
[WARNING]: Printing total background time currently only supported for single datasets.
shellCmd: command -v xelatex
shell 7151> /home/basti/texlive/2022/bin/x86_64-linux/xelatex
shellCmd: xelatex -output-directory /home/basti/phd/Figs/background /home/basti/phd/Figs/background/background_rate_crGold_scinti.tex
shell 7152> This is XeTeX, Version 3.141592653-2.6-0.999994 (TeX Live 2022) (preloaded format=xelatex)
shell 7152>  restricted \write18 enabled.
shell 7152> entering extended mode
shell 7152> (/home/basti/phd/Figs/background/background_rate_crGold_scinti.tex
shell 7152> LaTeX2e <2022-11-01> patch level 1
shell 7152> L3 programming layer <2022-12-17>
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/latex/standalone/standalone.cls
shell 7152> Document Class: standalone 2022/10/10 v1.3b Class to compile TeX sub-files stan
shell 7152> dalone
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/latex/tools/shellesc.sty)
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/iftex/ifluatex.sty
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/iftex/iftex.sty))
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/latex/xkeyval/xkeyval.sty
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/xkeyval/xkeyval.tex
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/xkeyval/xkvutils.tex
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/xkeyval/keyval.tex))))
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/latex/standalone/standalone.cfg)
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/latex/base/article.cls
shell 7152> Document Class: article 2022/07/02 v1.4n Standard LaTeX document class
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/latex/base/size10.clo))
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/frontendlayer/tikz.sty
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/basiclayer/pgf.sty
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/utilities/pgfrcs.sty
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgfutil-common.t
shell 7152> ex
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgfutil-common-l
shell 7152> ists.tex))
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgfutil-latex.de
shell 7152> f)
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgfrcs.code.tex
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/pgf.revision.tex)))
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/basiclayer/pgfcore.sty
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/latex/graphics/graphicx.sty
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/latex/graphics/graphics.sty
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/latex/graphics/trig.sty)
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/latex/graphics-cfg/graphics.cfg)
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/latex/graphics-def/xetex.def)))
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/systemlayer/pgfsys.sty
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/systemlayer/pgfsys.code.te
shell 7152> x
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgfkeys.code.tex
shell 7152> 
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgfkeysfiltered.
shell 7152> code.tex))
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/systemlayer/pgf.cfg)
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/systemlayer/pgfsys-xetex.d
shell 7152> ef
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/systemlayer/pgfsys-dvipdfm
shell 7152> x.def
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/systemlayer/pgfsys-common-
shell 7152> pdf.def))))
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/systemlayer/pgfsyssoftpath
shell 7152> .code.tex)
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/systemlayer/pgfsysprotocol
shell 7152> .code.tex)) (/home/basti/texlive/2022/texmf-dist/tex/latex/xcolor/xcolor.sty
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/latex/graphics-cfg/color.cfg)
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/latex/graphics/mathcolor.ltx))
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcore.code.te
shell 7152> x (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmath.code.tex
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathcalc.code.tex
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathutil.code.tex)
shell 7152> 
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathparser.code.te
shell 7152> x)
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.code
shell 7152> .tex
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.basi
shell 7152> c.code.tex)
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.trig
shell 7152> onometric.code.tex)
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.rand
shell 7152> om.code.tex)
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.comp
shell 7152> arison.code.tex)
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.base
shell 7152> .code.tex)
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.roun
shell 7152> d.code.tex)
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.misc
shell 7152> .code.tex)
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.inte
shell 7152> gerarithmetics.code.tex)))
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfloat.code.tex
shell 7152> )) (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfint.code.tex)
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepoints.c
shell 7152> ode.tex)
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepathcons
shell 7152> truct.code.tex)
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepathusag
shell 7152> e.code.tex)
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorescopes.c
shell 7152> ode.tex)
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcoregraphics
shell 7152> tate.code.tex)
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcoretransfor
shell 7152> mations.code.tex)
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorequick.co
shell 7152> de.tex)
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreobjects.
shell 7152> code.tex)
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepathproc
shell 7152> essing.code.tex)
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorearrows.c
shell 7152> ode.tex)
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreshade.co
shell 7152> de.tex)
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreimage.co
shell 7152> de.tex
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreexternal
shell 7152> .code.tex))
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorelayers.c
shell 7152> ode.tex)
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcoretranspar
shell 7152> ency.code.tex)
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepatterns
shell 7152> .code.tex)
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorerdf.code
shell 7152> .tex)))
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/modules/pgfmoduleshapes.co
shell 7152> de.tex)
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/modules/pgfmoduleplot.code
shell 7152> .tex)
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/compatibility/pgfcomp-versio
shell 7152> n-0-65.sty)
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/compatibility/pgfcomp-versio
shell 7152> n-1-18.sty))
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/utilities/pgffor.sty
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/utilities/pgfkeys.sty
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgfkeys.code.tex
shell 7152> )) (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/math/pgfmath.sty
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmath.code.tex))
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgffor.code.tex
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmath.code.tex)))
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/frontendlayer/tikz/tikz.co
shell 7152> de.tex
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/libraries/pgflibraryplotha
shell 7152> ndlers.code.tex)
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/modules/pgfmodulematrix.co
shell 7152> de.tex)
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/frontendlayer/tikz/librari
shell 7152> es/tikzlibrarytopaths.code.tex))))
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/latex/base/inputenc.sty
shell 7152> 
shell 7152> Package inputenc Warning: inputenc package ignored with utf8 based engines.
shell 7152> 
shell 7152> ) (/home/basti/texlive/2022/texmf-dist/tex/latex/geometry/geometry.sty
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/iftex/ifvtex.sty))
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/latex/unicode-math/unicode-math.sty
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/latex/l3kernel/expl3.sty
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/latex/l3backend/l3backend-xetex.def))
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/latex/unicode-math/unicode-math-xetex.
shell 7152> sty (/home/basti/texlive/2022/texmf-dist/tex/latex/l3packages/xparse/xparse.sty
shell 7152> )
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/latex/l3packages/l3keys2e/l3keys2e.sty
shell 7152> ) (/home/basti/texlive/2022/texmf-dist/tex/latex/fontspec/fontspec.sty
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/latex/fontspec/fontspec-xetex.sty
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/latex/base/fontenc.sty)
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/latex/fontspec/fontspec.cfg)))
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/latex/base/fix-cm.sty
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/latex/base/ts1enc.def))
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/latex/amsmath/amsmath.sty
shell 7152> For additional information on amsmath, use the `?' option.
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/latex/amsmath/amstext.sty
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/latex/amsmath/amsgen.sty))
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/latex/amsmath/amsbsy.sty)
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/latex/amsmath/amsopn.sty))
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/latex/unicode-math/unicode-math-table.
shell 7152> tex))) (/home/basti/texlive/2022/texmf-dist/tex/latex/siunitx/siunitx.sty
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/latex/translations/translations.sty
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/latex/etoolbox/etoolbox.sty)
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/pdftexcmds/pdftexcmds.sty
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/infwarerr/infwarerr.sty)
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/generic/ltxcmds/ltxcmds.sty)))
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/latex/tools/array.sty))
shell 7152> (/home/basti/phd/Figs/background/background_rate_crGold_scinti.aux)
shell 7152> *geometry* driver: auto-detecting
shell 7152> *geometry* detected driver: xetex
shell 7152> 
shell 7152> (/home/basti/texlive/2022/texmf-dist/tex/latex/translations/translations-basic-
shell 7152> dictionary-english.trsl) [1]
shell 7152> (/home/basti/phd/Figs/background/background_rate_crGold_scinti.aux) )
shell 7152> Output written on /home/basti/phd/Figs/background/background_rate_crGold_scinti
shell 7152> .pdf (1 page).
shell 7152> Transcript written on /home/basti/phd/Figs/background/background_rate_crGold_sc
shell 7152> inti.log.
:end:

And now only for Run-3:
#+begin_src sh :results drawer
plotBackgroundRate \
 ~/phd/resources/background/autoGen/likelihood_cdl2018_Run3_crGold.h5 \
 ~/org/resources/lhood_limits_automation_preliminary/lhood_outputs_adaptive_fadc/likelihood_cdl2018_Run3_crGold_scinti.h5 \
 --names "No vetoes" --names "Scinti" \
 --centerChip 3 \
 --title "Background rate from CAST data in Run-3, incl. scinti veto" \
 --showNumClusters \
 --showTotalTime \
 --topMargin 1.5 \
 --energyDset energyFromCharge \
 --outfile background_rate_crGold_scinti_run3.pdf \
 --outpath ~/phd/Figs/background/ \
 --useTeX \
 --quiet
#+end_src

#+RESULTS:
:results:
[INFO]:Dataset: No vetoes
[INFO]:	 Integrated background rate in range: 0.0 .. 12.0: 2.4289e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 12.0: 2.0241e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Scinti
[INFO]:	 Integrated background rate in range: 0.0 .. 12.0: 1.8525e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 12.0: 1.5438e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: No vetoes
[INFO]:	 Integrated background rate in range: 0.5 .. 2.5: 5.6605e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.5 .. 2.5: 2.8303e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Scinti
[INFO]:	 Integrated background rate in range: 0.5 .. 2.5: 5.1459e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.5 .. 2.5: 2.5730e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: No vetoes
[INFO]:	 Integrated background rate in range: 0.5 .. 5.0: 1.2196e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.5 .. 5.0: 2.7102e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Scinti
[INFO]:	 Integrated background rate in range: 0.5 .. 5.0: 9.5200e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.5 .. 5.0: 2.1156e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: No vetoes
[INFO]:	 Integrated background rate in range: 0.0 .. 2.5: 8.1821e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 2.5: 3.2728e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Scinti
[INFO]:	 Integrated background rate in range: 0.0 .. 2.5: 7.4616e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 2.5: 2.9846e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: No vetoes
[INFO]:	 Integrated background rate in range: 4.0 .. 8.0: 2.8817e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 4.0 .. 8.0: 7.2043e-06 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Scinti
[INFO]:	 Integrated background rate in range: 4.0 .. 8.0: 2.0584e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 4.0 .. 8.0: 5.1459e-06 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: No vetoes
[INFO]:	 Integrated background rate in range: 0.0 .. 8.0: 1.7033e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 8.0: 2.1291e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Scinti
[INFO]:	 Integrated background rate in range: 0.0 .. 8.0: 1.3431e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 8.0: 1.6789e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: No vetoes
[INFO]:	 Integrated background rate in range: 2.0 .. 8.0: 9.4171e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 2.0 .. 8.0: 1.5695e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Scinti
[INFO]:	 Integrated background rate in range: 2.0 .. 8.0: 6.2781e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 2.0 .. 8.0: 1.0463e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:DataFrame with 7 columns and 120 rows:
     Idx       Energy         Rate    totalTime      RateErr      Dataset         yMin         yMax
  dtype:        float        float     constant        float       string        float        float
       0            0        4.117         1080        1.455    No vetoes        2.661        5.572
       1          0.2        1.544         1080       0.8913    No vetoes       0.6525        2.435
       2          0.4         6.69         1080        1.855    No vetoes        4.834        8.545
       3          0.6        4.631         1080        1.544    No vetoes        3.088        6.175
       4          0.8        6.175         1080        1.783    No vetoes        4.393        7.958
       5            1        5.146         1080        1.627    No vetoes        3.519        6.773
       6          1.2        2.573         1080        1.151    No vetoes        1.422        3.724
       7          1.4        3.602         1080        1.361    No vetoes        2.241        4.964
       8          1.6        2.058         1080        1.029    No vetoes        1.029        3.088
       9          1.8        3.088         1080         1.26    No vetoes        1.827        4.348
      10            2        1.029         1080       0.7277    No vetoes       0.3014        1.757
      11          2.2        1.544         1080       0.8913    No vetoes       0.6525        2.435
      12          2.4        1.544         1080       0.8913    No vetoes       0.6525        2.435
      13          2.6        2.058         1080        1.029    No vetoes        1.029        3.088
      14          2.8        3.088         1080         1.26    No vetoes        1.827        4.348
      15            3        7.719         1080        1.993    No vetoes        5.726        9.712
      16          3.2        5.661         1080        1.707    No vetoes        3.954        7.367
      17          3.4        4.117         1080        1.455    No vetoes        2.661        5.572
      18          3.6        4.631         1080        1.544    No vetoes        3.088        6.175
      19          3.8        1.544         1080       0.8913    No vetoes       0.6525        2.435

[INFO]:INFO: storing plot in /home/basti/phd/Figs/background/background_rate_crGold_scinti_run3.pdf
[WARNING]: Printing total background time currently only supported for single datasets.
shellCmd: command -v xelatex
shell 3119> /home/basti/texlive/2022/bin/x86_64-linux/xelatex
shellCmd: xelatex -output-directory /home/basti/phd/Figs/background /home/basti/phd/Figs/background/background_rate_crGold_scinti_run3.tex
shell 3120> This is XeTeX, Version 3.141592653-2.6-0.999994 (TeX Live 2022) (preloaded format=xelatex)
shell 3120>  restricted \write18 enabled.
shell 3120> entering extended mode
shell 3120> (/home/basti/phd/Figs/background/background_rate_crGold_scinti_run3.tex
shell 3120> LaTeX2e <2022-11-01> patch level 1
shell 3120> L3 programming layer <2022-12-17>
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/latex/standalone/standalone.cls
shell 3120> Document Class: standalone 2022/10/10 v1.3b Class to compile TeX sub-files stan
shell 3120> dalone
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/latex/tools/shellesc.sty)
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/iftex/ifluatex.sty
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/iftex/iftex.sty))
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/latex/xkeyval/xkeyval.sty
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/xkeyval/xkeyval.tex
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/xkeyval/xkvutils.tex
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/xkeyval/keyval.tex))))
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/latex/standalone/standalone.cfg)
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/latex/base/article.cls
shell 3120> Document Class: article 2022/07/02 v1.4n Standard LaTeX document class
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/latex/base/size10.clo))
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/frontendlayer/tikz.sty
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/basiclayer/pgf.sty
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/utilities/pgfrcs.sty
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgfutil-common.t
shell 3120> ex
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgfutil-common-l
shell 3120> ists.tex))
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgfutil-latex.de
shell 3120> f)
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgfrcs.code.tex
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/pgf.revision.tex)))
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/basiclayer/pgfcore.sty
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/latex/graphics/graphicx.sty
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/latex/graphics/graphics.sty
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/latex/graphics/trig.sty)
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/latex/graphics-cfg/graphics.cfg)
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/latex/graphics-def/xetex.def)))
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/systemlayer/pgfsys.sty
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/systemlayer/pgfsys.code.te
shell 3120> x
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgfkeys.code.tex
shell 3120> 
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgfkeysfiltered.
shell 3120> code.tex))
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/systemlayer/pgf.cfg)
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/systemlayer/pgfsys-xetex.d
shell 3120> ef
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/systemlayer/pgfsys-dvipdfm
shell 3120> x.def
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/systemlayer/pgfsys-common-
shell 3120> pdf.def))))
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/systemlayer/pgfsyssoftpath
shell 3120> .code.tex)
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/systemlayer/pgfsysprotocol
shell 3120> .code.tex)) (/home/basti/texlive/2022/texmf-dist/tex/latex/xcolor/xcolor.sty
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/latex/graphics-cfg/color.cfg)
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/latex/graphics/mathcolor.ltx))
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcore.code.te
shell 3120> x (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmath.code.tex
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathcalc.code.tex
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathutil.code.tex)
shell 3120> 
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathparser.code.te
shell 3120> x)
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.code
shell 3120> .tex
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.basi
shell 3120> c.code.tex)
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.trig
shell 3120> onometric.code.tex)
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.rand
shell 3120> om.code.tex)
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.comp
shell 3120> arison.code.tex)
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.base
shell 3120> .code.tex)
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.roun
shell 3120> d.code.tex)
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.misc
shell 3120> .code.tex)
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.inte
shell 3120> gerarithmetics.code.tex)))
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfloat.code.tex
shell 3120> )) (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfint.code.tex)
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepoints.c
shell 3120> ode.tex)
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepathcons
shell 3120> truct.code.tex)
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepathusag
shell 3120> e.code.tex)
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorescopes.c
shell 3120> ode.tex)
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcoregraphics
shell 3120> tate.code.tex)
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcoretransfor
shell 3120> mations.code.tex)
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorequick.co
shell 3120> de.tex)
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreobjects.
shell 3120> code.tex)
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepathproc
shell 3120> essing.code.tex)
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorearrows.c
shell 3120> ode.tex)
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreshade.co
shell 3120> de.tex)
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreimage.co
shell 3120> de.tex
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreexternal
shell 3120> .code.tex))
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorelayers.c
shell 3120> ode.tex)
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcoretranspar
shell 3120> ency.code.tex)
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepatterns
shell 3120> .code.tex)
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorerdf.code
shell 3120> .tex)))
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/modules/pgfmoduleshapes.co
shell 3120> de.tex)
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/modules/pgfmoduleplot.code
shell 3120> .tex)
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/compatibility/pgfcomp-versio
shell 3120> n-0-65.sty)
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/compatibility/pgfcomp-versio
shell 3120> n-1-18.sty))
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/utilities/pgffor.sty
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/utilities/pgfkeys.sty
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgfkeys.code.tex
shell 3120> )) (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/math/pgfmath.sty
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmath.code.tex))
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgffor.code.tex
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmath.code.tex)))
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/frontendlayer/tikz/tikz.co
shell 3120> de.tex
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/libraries/pgflibraryplotha
shell 3120> ndlers.code.tex)
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/modules/pgfmodulematrix.co
shell 3120> de.tex)
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/frontendlayer/tikz/librari
shell 3120> es/tikzlibrarytopaths.code.tex))))
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/latex/base/inputenc.sty
shell 3120> 
shell 3120> Package inputenc Warning: inputenc package ignored with utf8 based engines.
shell 3120> 
shell 3120> ) (/home/basti/texlive/2022/texmf-dist/tex/latex/geometry/geometry.sty
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/iftex/ifvtex.sty))
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/latex/unicode-math/unicode-math.sty
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/latex/l3kernel/expl3.sty
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/latex/l3backend/l3backend-xetex.def))
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/latex/unicode-math/unicode-math-xetex.
shell 3120> sty (/home/basti/texlive/2022/texmf-dist/tex/latex/l3packages/xparse/xparse.sty
shell 3120> )
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/latex/l3packages/l3keys2e/l3keys2e.sty
shell 3120> ) (/home/basti/texlive/2022/texmf-dist/tex/latex/fontspec/fontspec.sty
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/latex/fontspec/fontspec-xetex.sty
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/latex/base/fontenc.sty)
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/latex/fontspec/fontspec.cfg)))
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/latex/base/fix-cm.sty
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/latex/base/ts1enc.def))
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/latex/amsmath/amsmath.sty
shell 3120> For additional information on amsmath, use the `?' option.
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/latex/amsmath/amstext.sty
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/latex/amsmath/amsgen.sty))
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/latex/amsmath/amsbsy.sty)
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/latex/amsmath/amsopn.sty))
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/latex/unicode-math/unicode-math-table.
shell 3120> tex))) (/home/basti/texlive/2022/texmf-dist/tex/latex/siunitx/siunitx.sty
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/latex/translations/translations.sty
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/latex/etoolbox/etoolbox.sty)
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/pdftexcmds/pdftexcmds.sty
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/infwarerr/infwarerr.sty)
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/generic/ltxcmds/ltxcmds.sty)))
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/latex/tools/array.sty))
shell 3120> No file background_rate_crGold_scinti_run3.aux.
shell 3120> *geometry* driver: auto-detecting
shell 3120> *geometry* detected driver: xetex
shell 3120> 
shell 3120> (/home/basti/texlive/2022/texmf-dist/tex/latex/translations/translations-basic-
shell 3120> dictionary-english.trsl) [1]
shell 3120> (/home/basti/phd/Figs/background/background_rate_crGold_scinti_run3.aux) )
shell 3120> Output written on /home/basti/phd/Figs/background/background_rate_crGold_scinti
shell 3120> _run3.pdf (1 page).
shell 3120> Transcript written on /home/basti/phd/Figs/background/background_rate_crGold_sc
shell 3120> inti_run3.log.
:end:



**** TODOs for the above section                                :noexport:

- [X] *INVESTIGATE THE >250 PEAK IN VETO PADDLE DATA BY CHECKING
  EVENTS THAT HAVE SUCH VALUES. DISPLAY AND PROPERTIES!*
  -> Done, see the extra section about it.

- [X] *WHY IS THE FADC POST TRIGGER NOT VISIBLE IN THE DATA?*
  -> because post trig is unrelated to when the *trigger is sent*  

**** Angle estimate covered by veto paddle                      :noexport:

Given a veto paddle size of $\SI{42}{\cm}$ times $\SI{82}{\cm}$ we can
calculate the rough arc angle covered by the paddle for muons hitting
detector material. The exact calculation of course would need to take
into account the extent of the detector, the fact that there is copper
in the vacuum tube in front of the detector etc. For now let's just
look at the center of the detector volume and see what is covered by
the paddle.

The lead shielding is $\SI{10}{cm}$ above the detector. There are
maybe a bit less than another $\SI{10}{cm}$ above the lead shielding
to the scintillator and the detector center is also a bit less than
$\SI{10}{cm}$ away from the lead shielding above ($\SI{78}{mm}$
diameter gas volume diameter, with the rest of the housing and a
couple of centimeter spacing to the lead shielding).

Assuming $\SI{30}{cm}$ from the center to the veto scintillator, the
angle to the sides is:

\[
α = \arctan\left(\frac{\SI{21}{cm}}{\SI{30}{cm}}\right) = \SI{35}{°}
\]

#+begin_src nim
import math
echo arctan(21.0/30.0).radToDeg
#+end_src

#+RESULTS:
: 34.99202019855866

so about $\SI{35}{°}$ covering to either side. This implies
$\SI{70}{°}$ coverage above the detector. In the long direction it's
even more due to the length of the paddle.

Given a roughly $\cos²(θ)$ distribution of the muon background, this
should cover

#+begin_src nim
import numericalnim, math

proc cos2(θ: float, ctx: NumContext[float, float]): float = cos(θ)*cos(θ)

proc integrate(start, stop: float): float =
  result = simpson(cos2, start, stop)
echo "Integrate from -π/2 to π/2 = ", integrate(-PI/2.0, PI/2.0), " = π/2"
echo "Integrate from -π/2 to π/2 = ", integrate(-35.0.degToRad, 35.0.degToRad)
echo "Fraction of total flux *in this dimension* = ", integrate(-35.0.degToRad, 35.0.degToRad) / (PI/2.0)
#+end_src

#+RESULTS:
| Integrate | from | -π/2  | to   | π/2 | =    | 1.570796326794897 | = |               π/2 |
| Integrate | from | -π/2  | to   | π/2 | =    | 1.080711548592458 |   |                   |
| Fraction  | of   | total | flux | *in | this |        dimension* | = | 0.688002340059947 |

So assuming a 2D case it already covers about 68% of the expected muon
flux, assuming a perfect trigger efficiency of the scintillator.

**** Estimate expected number of muons in data taking           :noexport:

The detection volume of relevance for the center chip can be roughly
said to be $\SI{4.2}{cm²}$ (1.4 cm wide chip, 3 cm height orthogonal
to sky). Assuming a muon rate of 1 cm⁻²·min⁻¹ yields for 1125 h of
background time:
#+begin_src nim
import unchained
let rate = 1.cm⁻²•min⁻¹
let time = 1125.h
let area = 4.2.cm²
echo "Expected number of muon counts = ", rate * time * area
#+end_src

#+RESULTS:
: Expected number of muon counts = 283500 UnitLess

So about \num{284500} expected muons in front of the
detector. Assuming I can trust the ~scintiInfo~ tool of an output of
(see next section):
#+begin_quote
Main triggers: 69243
FADC triggers 211683
Relevant triggers 142440
Open shutter time in s: 13.510434
Scinti1 rate: 0.3050257995269533 s^-1
Scinti2 rate: 57.21503839180888 s^-1
#+end_quote
so about 69k veto paddle triggers. That puts the efficiency at knowing
the paddle will only see about 68% of all muons
$\sim\SI{35.8}{\%}$. Not great, but also not horrible. Given that we
know our threshold was likely a bit high, this seems to be a possible
ball park estimate.

**** Generate plots of scintillator data                        :noexport:

The tool we use to plot the scintillator data is ~Tools/scintiInfo~.

It generates output into the ~out/<plot path>/~ directory:
#+begin_src sh
./scintiInfo -f ~/CastData/data/DataRuns2018_Reco.h5
#+end_src
which generates 3 plots for each scintillator containing all data,
all non trivial data and all hits > 0 and < 300.

Now let's investigate what the events of the veto paddle look like in
which we have a peak >250 clocks.

#+begin_src nim :tangle code/scintillator_data_plots.nim
import std / [sequtils, strformat]
import ingrid / [tos_helpers, ingrid_types]
import pkg / [ggplotnim, nimhdf5, datamancer, ginger]

proc allData(h5f: H5File): DataFrame =
  result = h5f.readDsets(chipDsets = some((chip: 3, dsets: TpaIngridDsetKinds.mapIt(it.toDset()))),
                         commonDsets = @["fadcReadout", "szint1ClockInt", "szint2ClockInt"])

proc plotEvents(df: DataFrame, run: int, numEvents: int, plotCount: var int,
                outpath: string,
                fadcRun: ReconstructedFadcRun) =
  let showFadc = fadcRun.eventNumber.len > 0
  for (tup, subDf) in groups(df.group_by("eventNumber")):
    if numEvents > 0 and plotCount > numEvents: break
    let dfEv = subDf.dfToSeptemEvent()
    let eventNumber = tup[0][1].toInt
    let pltSeptem = ggplot(dfEv, aes(x, y, color = "charge"), backend = bkCairo) +
      geom_point(size = 1.0) +
      xlim(0, 768) + ylim(0, 768) + scale_x_continuous() + scale_y_continuous() +
      geom_linerange(aes = aes(y = 0, xMin = 128, xMax = 640)) +
      geom_linerange(aes = aes(y = 256, xMin = 0, xMax = 768)) +
      geom_linerange(aes = aes(y = 512, xMin = 0, xMax = 768)) +
      geom_linerange(aes = aes(y = 768, xMin = 128, xMax = 640)) +
      geom_linerange(aes = aes(x = 0, yMin = 256, yMax = 512)) +
      geom_linerange(aes = aes(x = 256, yMin = 256, yMax = 512)) +
      geom_linerange(aes = aes(x = 512, yMin = 256, yMax = 512)) +
      geom_linerange(aes = aes(x = 768, yMin = 256, yMax = 512)) +
      geom_linerange(aes = aes(x = 128, yMin = 0, yMax = 256)) +
      geom_linerange(aes = aes(x = 384, yMin = 0, yMax = 256)) +
      geom_linerange(aes = aes(x = 640, yMin = 0, yMax = 256)) +
      geom_linerange(aes = aes(x = 128, yMin = 512, yMax = 768)) +
      geom_linerange(aes = aes(x = 384, yMin = 512, yMax = 768)) +
      geom_linerange(aes = aes(x = 640, yMin = 512, yMax = 768)) +
      margin(top = 1.5) +
      ggtitle(&"Septem event of event {eventNumber} and run {run}. ")

    if not showFadc:
      pltSeptem + ggsave(&"{outpath}/septemEvents/septemEvent_run_{run}_event_{eventNumber}.pdf")
    else:
      # prepare FADC plot, create canvas and place both next to one another
      let eventIdx = fadcRun.eventNumber.find(eventNumber)
      let dfFadc = toDf({ "x"         : toSeq(0 ..< 2560),
                          "data"      : fadcRun.fadcData[eventIdx, _].squeeze })
      let pltFadc = ggplot(dfFadc, aes("x", "data"), backend = bkCairo) +
        geom_line() +
        geom_point(color = "black", alpha = 0.1) +
        ggtitle(&"Fadc signal of event {eventNumber} and run {run}")
      var img = initViewport(wImg = 1200, hImg = 600, backend = bkCairo)
      img.layout(2, rows = 1)
      img.embedAsRelative(0, ggcreate(pltSeptem).view)
      img.embedAsRelative(1, ggcreate(pltFadc).view)      

      var area = img.addViewport()
      let title = &"Septemboard event and FADC signal for event {eventNumber}"
      let text = area.initText(c(0.5, 0.05, ukRelative), title, goText, taCenter, font = some(font(16.0)))
      area.addObj text
      img.children.add area
      img.draw(&"{outpath}/septemEvents/septem_fadc_run_{run}_event_{eventNumber}.pdf")
    
    inc plotCount

proc plotSzinti(h5f: H5file, df: DataFrame, cutFn: FormulaNode,
                title: string, outpath: string,
                fname: string,
                numEventPlots: int,
                plotEvents: bool,
                showFadc: bool = false
               ) =
  let toGather = df.getKeys().filterIt(it notin ["runNumber", "eventNumber"])
    
  let dfF = df.filter(cutFn)
    .filter(f{`eccentricity` < 10.0})
    .gather(toGather, "key", "value")
    
  echo dfF
  ggplot(dfF, aes("value", fill = "key")) +
    facet_wrap("key", scales = "free") + 
    geom_histogram(position = "identity", binBy = "subset") +
    legendPosition(0.90, 0.0) +
    ggtitle(title) + 
    ggsave(&"{outpath}/{fname}", width = 2000, height = 2000)

  if plotEvents:
    var plotCount = 0
    var fadcRun: ReconstructedFadcRun
    for (tup, subDf) in dfF.group_by(@["runNumber"]).groups:
      if numEventPlots > 0 and plotCount > numEventPlots: break
      let run = tup[0][1].toInt
      if showFadc:
        fadcRun = h5f.readRecoFadcRun(run)
      echo "Run ", run
      let events = subDf["eventNumber", int].toSeq1D
      let dfS = getSeptemDataFrame(h5f, run)
        .filter(f{int: `eventNumber` in events})
      echo dfS
      plotEvents(dfS, run, numEventPlots, plotCount, outpath, fadcRun)  

proc main(fname: string,
          peakAt255 = false,
          vetoPaddle = false,
          sipm = false,
          sipmXrayLike = false,
          plotEvents = true) =
  let h5f = H5open(fname, "r")
  #let fileInfo = h5f.getFileInfo()
  let df = h5f.allData()

  # first the veto paddle around the 255 peak (plot all events)
  if peakAt255:
    h5f.plotSzinti(df,
                   f{int: `szint2ClockInt` > 250 and `szint2ClockInt` < 265},
                   "Cluster properties of all events with veto paddle trigger clock cycles = 255",
                   "Figs/scintillators/peakAt255",
                   "cluster_properties_peak_at_255.pdf",
                   -1,
                   plotEvents)
  # now the veto paddle generally
  if vetoPaddle:
    h5f.plotSzinti(df,
                   f{int: `szint2ClockInt` > 0 and `szint2ClockInt` < 200},
                   "Cluster properties of all events with veto paddle > 0 && < 200",
                   "Figs/scintillators/veto_paddle/",
                   "cluster_properties_veto_paddle_less200.pdf",
                   200,
                   plotEvents)
  # finally the SiPM
  if sipm:
    h5f.plotSzinti(df,
                   f{int: `szint1ClockInt` > 0 and `szint1ClockInt` < 200},
                   "Cluster properties of all events with SiPM > 0 && < 200",
                   "Figs/scintillators/sipm/",
                   "cluster_properties_sipm_less200.pdf",                 
                   200,
                   plotEvents)
  if sipmXrayLike:
    h5f.plotSzinti(df,
                   f{float: `szint1ClockInt` > 0 and `szint1ClockInt` < 200 and
                     `energyFromCharge` > 7.0 and `energyFromCharge` < 9.0 and
                     `length` < 7.0},
                   "Cluster properties of all events with SiPM > 0 && < 200, 7 keV < energy < 9 keV, length < 7mm",
                   "Figs/scintillators/sipmXrayLike/",
                   "cluster_properties_sipm_less200_7_energy_9_length_7.pdf",                 
                   -1,
                   plotEvents,
                   showFadc = true)
    

  discard h5f.close()
    
when isMainModule:
  import cligen
  dispatch main
#+end_src

-> Conclusion from all this:
I don't see any real cause for these kinds of events. They are all
relatively busy background events of mostly standard tracks.
Maybe these events are the events that follow the events where the
FPGA has a hiccup and doesn't take the FADC trigger? Who knows or it
may be something completely different. At 255 * 25ns = 6.375μs I don't
see any physical source. More importantly are two things:
1. a physical source is *extremely unlikely* to be so narrow that it
   _always_ give 255 clock cycles.
2. 255 is 2^8 when counting from 0 (= it's ~1111 1111~), which implies
   there is a high chance it is a bug in the counting logic where
   maybe the clock stopped at 2^8 instead of 2^12 (4095) or something
   along those lines.
A physical event that always appears at 255 clock cycles is rather
unlikely! See fig. [[fig:scintillators:peak_at_255_cluster_properties]]
for the cluster properties and the
~Figs/scintillators/peakAt255/septemEvents~ directory for all the
event displays of these events.

#+CAPTION: Overview of the cluster properties of all the events with a trigger at
#+CAPTION: $\SI{255}{clock\;cycles}$. Nothing out of the ordinary for background data here.
#+NAME: fig:scintillators:peak_at_255_cluster_properties
[[~/phd/Figs/scintillators/peakAt255/cluster_properties.pdf]]



-> Conclusion regarding regular veto paddle plots and events:
The plots look pretty much like what we expect here, namely mainly 14
mm long tracks in the histogram
and looking at events the story continues like that. It's essentially
all just tracks that go through horizontally. It's still useful
though, as sometimes the statistical process of ionization leaves
blobs that could maybe be interpreted as X-rays, in particular in
corner cutting tracks.
-> As such this is also a good dataset for candidates for corner
cutting cases as we know the data is very pure "good" tracks.

#+CAPTION: Overview of the cluster properties of all the events with a trigger at
#+CAPTION: $>\SI{0}{clock\;cycles}$ and $<\SI{200}{clock;cycles}$ for the veto paddle. 
#+CAPTION: As one might expect it's dominated by full chip long tracks (peak at 14mm).
#+NAME: fig:scintillators:veto_paddle_cluster_properties
[[~/phd/Figs/scintillators/veto_paddle/cluster_properties_veto_paddle_less200.pdf]]


- [ ] *SIMILARLY DO THE SAME FOR SIPM*
  -> Extend the code above such that we also do plots for the SiPM. so
  properties & events. In addition to that add the FADC events! Read
  FADC data and place it next to the SiPM event displays. What do we
  see?
  -> In particular this is at the same time an extremely valuable
  "dataset" of events where we expect the FADC to have different
  shapes. Then when comparing the distribution of those events with
  55Fe events, do we see a difference?

-> Conclusions regarding the SiPM. The data is a bit surprising in
some ways, but explained by looking at it. There is more 14 mm like
tracks in the data than I would have assumed. But looking at the
individual events there is a common trend of tracks that are pretty
steep as to go through the SiPM (it's much bigger than one chip of
14mm after all!), but shallow enough to still cover the full chip more
or less!
Some events *do* have clusters that are *very* dense and likely
orthogonal muons though. In addition there is a large number of
extremely busy events in these plots.
As there are few events that are really X-ray like in nature, looking
at the FADC data for *most* of them is likely not very
interesting. But we should cut on the energy (7 - 9 keV) and having
triggered and a length maybe 7 mm or so. What remains is a target for
event displays including the FADC.

Also looking at the event displays with the FADC signal of those event
that are not very track like and in energies between 7 and 9 keV shows
that there are indeed quite a few whose fall and rise time is
*significantly longer* than the typical O(<100 clock cycles). This
implies there is really a 'long time' accumulation going on. Our
expectation of 1.5μs until all the track is accumulated, so this might
actually be _an_ explanation. 

#+CAPTION: Overview of the cluster properties of all the events with a trigger at
#+CAPTION: $>\SI{0}{clock\;cycles}$ and $<\SI{200}{clock;cycles}$ of the SiPM.
#+CAPTION: Surprisingly the data also contains significant amounts of 14 mm events.
#+NAME: fig:scintillators:sipm_cluster_properties
[[~/phd/Figs/scintillators/sipm/cluster_properties_sipm_less200.pdf]]

**** Muon calculations for expected SiPM rates                  :noexport:
Place our calcs here as no export?

*** FADC / ToA as veto [/]
:PROPERTIES:
:CUSTOM_ID: sec:background:fadc_veto
:END:

- [ ] *PUT THE IDEAS FROM THIS SECTION INTO AN 'OUTLOOK FOR BACKGROUND
  IMPROVEMENTS' SECTION?*
  -> This could be a useful section to extend the ideas and provide

- [ ] create an appendix for FADC veto information? Could put stuff
  like
  - rise time vs skewness
  - rise time KDE, fall time KDE
  - explanation of noise stuff?

- [ ] *REALLY MAKE SURE ALL THE POINTS MENTIONED HERE ARE ACTUALLY
  EXPLAINED PROPERLY BEFORE FOR EXAMPLE*.

- [ ] *MAKE SURE Chosen 99-th percentile AS VETO IS MENTIONED*
  -> Mention others were also tried, but improvements too small.

As previously mentioned in [[#sec:detector:fadc]] the FADC not only serves
as a trigger for the readout and reference time for the scintillator
triggers. Because of its high temporal resolution it can in principle
act as a veto of its own by providing insight into the longitudinal
cluster shape.

A cluster drifting towards the readout and finally through the grid
induces a voltage measured by the FADC. As such the length of the FADC
signal is a function of the time it takes the cluster to drift
'through' the grid. The kind of orthogonal muon events that should be
triggered by the SiPM as explained in the previous section
[[#sec:background:scinti_veto]] for example should also be detectable by
the FADC in the form of longer signal rise times than typical in an X-ray.

From gaseous detector physics theory we can estimate the typical sizes
and therefore expected signal rise times for an X-ray if we know the
gas of our detector. For the $\SI{1050}{mbar}$, $97.7/2.3\,\%$
$\ce{Ar}/\ce{iC4H10}$ mixture used with a $\SI{500}{V.cm⁻¹}$ drift
field in the Septemboard detector at CAST, the relevant parameters
are [fn:gas_properties_pyboltz]
- drift velocity $v = \SI{2.28}{cm.μs⁻¹}$
- transverse diffusion $σ_T = \SI{670}{μm.cm^{-1/2}}$
- longitudinal diffusion $σ_L = \SI{270}{μm.cm^{-1/2}}$ 
As the detector has a height of $\SI{3}{cm}$ we expect a typical X-ray
interacting close to the cathode to form a cluster of
$\sqrt{\SI{3}{cm}}·\SI{670}{μm.cm^{-1/2}} \approx \SI{1160.5}{μm}$
transverse extent and $\sqrt{\SI{3}{cm}}·\SI{270}{μm.cm^{-1/2}} \approx
\SI{467.5}{μm}$ in longitudinal size, where this corresponds to a $1
σ$ environment. To get a measure for the cluster size a rough estimate
for an upper limit is a $3 σ$ distance away from the center in each
direction. For the transverse size it leads to about $\SI{7}{mm}$ and
in longitudinal about $\SI{2.8}{mm}$. From the CAST \cefe data we see
a peak at around $\SI{6}{mm}$ of transverse cluster size along the
longer axis, which matches well with our expectation (see appendix
[[#sec:appendix:fadc_veto_empirical_cluster_length]] for the length
data). From the drift velocity and the upper bound on the longitudinal
cluster size we can therefore also compute an equivalent effective
drift time _seen by the FADC_. This comes out to be

\[
t = \SI{2.8}{mm} / \SI{22.8}{mm.μs⁻¹} = \SI{0.123}{μs}
\]

or about $\SI{123}{ns}$, equivalent to $\SI{123}{clock\;cycle}$ of the
FADC clock. Alternatively, we can compute the most likely rise time
based on the known peak of the cluster length, $\SI{6}{mm}$ and the
ratio of the transverse and longitudinal diffusion, $σ_T / σ_L \approx
2.5$ to end up at a time of $\frac{\SI{6}{mm}}{2.5} ·
\SI{22.8}{mm.μs⁻¹} = \SI{105}{ns}$.

Fig. [[fig:background:fadc_rise_time]] shows the measured rise time for
the \cefe calibration data compared to the background data. The peak
of the calibration rise time is at about $\SI{55}{ns}$. While this is
almost a factor of 2 smaller than the theoretical values mentioned
before, this is expected as those first of all are an upper bound and
secondly the "rise time" here is not the full time due to our
definition starting from $\SI{10}{\%}$ below the baseline and stopping
$\SI{2.5}{\%}$ before the peak, shortening our time
(ref. sec. [[#sec:fadc:definition_baseline_rise_fall_time]]). At the same
time we see that the background data has a much longer tail towards
higher clock cycle counts, as one expects. This implies that we can
perform a cut on the rise time and utilize it as an additional
veto. The \cefe data allows us to define a cut based on a known and
desired signal efficiency. This is done by identifying a desired
percentile on both ends of the distribution of a cleaned X-ray
dataset. Different percentiles (and associated efficiencies) will be
tested for their impact on the expected axion limit. It is important
to note that the cut values need to be determined for each of the used
FADC amplifier settings separately. This is done automatically based
on the known runs corresponding to each setting.

#+CAPTION: KDE of the rise time of the FADC signals in the \cefe and background data
#+CAPTION: of the CAST Run-3 dataset. The X-ray data is a single peak with a mean of
#+CAPTION: about $\SI{55}{ns}$ while the background distribution is extremely wide,
#+CAPTION: motivating a veto based on this data.
#+NAME: fig:background:fadc_rise_time
[[~/phd/Figs/FADC/fadc_riseTime_kde_signal_vs_background_run3.pdf]]
- [ ] *OLD PLOT*

For the signal decay time we do a simpler cut, with less of a physical
motivation. As the decay time is highly dependent on the resistance
and capacitance properties of the grid, high voltage and FADC readout
chain, we simply introduce a conservative cut in such a range as to
avoid cutting away any range that contains X-ray data. Any background
removed is just a bonus.

Finally, the FADC is only used as a veto if the individual spectrum is
not considered noisy. This is determined by 4 local dominant peaks
based on a peak finding algorithm and in addition a general skewnees
of the total signal larger than \num{-0.4}. The skewness is an
empirical cutoff as good FADC signals typically lie at larger negative
skewness values (due to them having a signal towards negative
values). See appendix [[#sec:appendix:background:fadc]] for a scatter plot of
FADC rise times and skewness values. A very low percentage of false
positives (good signals appearing 'noisy') is accepted for the
certainty of not falsely rejecting noisy FADC events.

Applying the FADC veto with a target quantile of 1 from both the lower
and upper end ($1^{\text{st}}$ and $99^{\text{th}}$ percentiles; thus
a $\SI{98}{\%}$ signal efficiency) in addition to the scintillator
veto presented in the previous section results in a background rate as
seen in fig. [[fig:background:background_rate_fadc_veto]]. The achieved
background rate of the methods already leads to a background rate of
$\SI{8.8e-6}{keV⁻¹.cm⁻².s⁻¹}$ in the energy range
$\SIrange{2}{8}{keV}$. And between $\SIrange{4}{8}{keV}$ even
$\SI{4.2e-6}{keV⁻¹.cm⁻².s⁻¹}$. The veto brings improvements across the
whole energy range, in which the FADC triggers. Interestingly, in some
cases even below that range. The events removed from the second bin
are events with a spark on the upper right GridPix, which induced an
X-ray like low energy cluster on the central chip and triggered the
FADC. In the range at around $\SI{8}{keV}$ likely candidates for
removed events are cases of orthogonal muons that did not trigger the
SiPM.  
- [ ] *MENTION SOMETHING ABOUT 3 KEV COPPER OR ORTHOGONAL MUONS?*

#+CAPTION: Comparison of pure $\ln\mathcal{L}$ cuts, additional scintillator veto and
#+CAPTION: finally also FADC veto at an efficiency of $\SI{98}{\%}$. In the range
#+CAPTION: $\SIrange{2}{8}{keV}$ a background rate of $\SI{8.8e-6}{keV⁻¹.cm⁻².s⁻¹}$ is
#+CAPTION: achieved and between $\SIrange{4}{8}{keV}$ even $\SI{4.2e-6}{keV⁻¹.cm⁻².s⁻¹}$.
#+NAME: fig:background:background_rate_fadc_veto
[[~/phd/Figs/background/background_rate_crGold_scinti_fadc.pdf]]

Generally, appendix [[#sec:appendix:background:fadc]] contains more figures of
the FADC data (rise time, fall time, ...), how the different FADC
settings affect these, what the run-to-run variation looks like and more.






- [ ] *THIS STUFF HAS ALREADY BEEN MENTIONED IN CONTEXT OF
  SCINTILLATORS!*
  -> Rewrite it! However, the scinti part does not explain it as well
  and leaves out some pieces (transverse distribution along Z)
  -> But explanation better in [[#sec:detector:scintillators]].
  -> Most of this below does not belong here. It belongs into the
  motivation, which was mentioned before. _Some_ of the stuff may be
  useful in an interpretation of the application of the FADC veto.

While the FADC serves as a very useful trigger to read out events,
decrease the likelihood of multiple independent clusters on the center
chip and serves as a reference time for triggers of the scintillators,
its time information can theoretically provide an additional veto.

Even for an almost fully 4π coverage of scintillators around the
detector setup, there is exactly one part that is never covered: the
window of the detector towards the magnet bore. If a muon traverses
through this path it is unlikely to be
shielded. [fn:shielding_magnet_other_end] In particular during solar
trackings of a helioscope experiment this should contribute a more
significant fraction of background due to the distribution of muons at
surface level following a roughly $\cos²(θ)$ distribution ($θ$ the
zenith angle).

Muons entering in such a way traverse the gas volume orthogonally to
the readout plane. The projection on the readout is thus roughly
spherical (same as an X-ray). The speed of the muon compared to the
time scale of gas diffusion & clock speed of the readout electronic
means that a muon ionizes the gas along its path effectively
instantly.

Two distinctions can be made between X-rays and such muons:
1. The effective instant ionization implies that such an cluster has a
   'duration' that is equivalent to the drift velocity times the gas
   volume height. For a typical GridPix setup of height $\SI{3}{cm}$
   with an Argon/Isobutane gas mixture, this implies times of
   $\mathcal{O}(\SI{2}{μs})$. Compared to that the duration of an
   X-ray is roughly the equivalent of the geometrical size in the
   readout plane, which for the same conditions is
   $\mathcal{O}(\SI{3}{mm})$, expressed as a time under the drift
   velocity.
   Thus, if the duration of a cluster is reasonably well known, this
   can easily separate the two types of clusters.
2. Each ionized electron undergoes diffusion along its path to the
   readout plane. For these tracks those electrons that are produced
   far away from the readout yield large diffusion, whereas those
   close to the readout very little. Assuming a constant ionization
   per distance, this implies the muon track is formed like a
   cone. The electrons arriving at the readout first are almost
   exactly on the muon path and the later ones have more and more
   diffusion. This latter effect is invisible in a setup using the
   kind of FADC as used in this detector combined with the Timepix1
   based GridPix, because the FADC records no spatial
   information. However, a detector based on GridPixes using Timepix3,
   which allows for ToA readout at the same time as ~ToT~ this effect
   may be visible and for this reason is important to keep in mind for
   the future.

For a Argon/Isobutane mixture at $\SI{1050}{mbar}$ the energy
deposition of most muons will be in the range of
$\SIrange{8}{10}{keV}$, which in any case is outside of the main
region of interest for axion searches.

Note: for a detector with an almost perfect 4π coverage, the
scintillators *behind* the detector would of course trigger for such
muons. Indeed, it is likely that using that information would already
be enough to remove such events. A discrimination based on time
information yields a more certain result than a large scintillator
might, which (even if to a small degree) does introduce random
coincidences.

- [ ] *FINISH EXPLANATION AND WHAT WE DO WITH FADC REGARDING VETO
  WHEN SEEN 2018 DATA*

- [ ] *FIX LINK IN FOOTNOTE!!*    

[fn:shielding_magnet_other_end] Of course a properly calibrated
scintillator behind the magnet should detect any muons traversing
orthogonally and a detector with good time resolution can also detect
such events in principle. Despite this, it might be a decent idea to
include some lead shielding on the opposite end of future helioscopes
to provide some shielding against muons coming from such a
direction. In particular a future experiment that tracks the Sun to much higher
altitudes will see such muon background significantly more ($\cos²(θ)$!).

[fn:gas_properties_pyboltz] The properties were calculated with
~PyBoltz~, a Cython conversion of Magboltz. The code can be found
here:
https://github.com/Vindaar/TimepixAnalysis/Tools/septemboardCastGasPyboltz
Further the numbers used here are for a temperature of $\SI{26}{°C}$,
slightly above room temperature due to the heating of the Septemboard
itself into the gas.

**** TODOs for above section [/] :noexport:

- [X] *FOUND A HUGE BUG IN THE FADC VETO CODE*
  -> The veto did not exclude anything *ABOVE* a cut... The comparison
  was wrong. In particular in that light look at the veto properly
  again. I put in some ~reasonable numbers for the cuts somewhere
  between 1-th (99-th) and 5-th (95-th) percentile of the data.

- [X] big question is still energy dependence of rise time, fall time
  -> There is not much. Lower energy events *going by escape peak
  clusters* are _slightly_ wider, but that is quite possibly just a
  side effect of having worse data.

- [ ] Investigate FADC rise and fall time now that it is fixed based
  on:
  - [ ] FADC settings in Run-2
    -> There seems to be a pretty small effect generally as the
    difference is not very big between the two datasets. However, we
    haven't actually looked at the data split by setting. 
  - [X] energy of X-rays (calibration mainly, escape & photo)
    - [ ] Look into CDL data!

- [ ] *INSERT SKEWNESS PLOT INTO APPENDIX*
  -> In ~statusAndProgress~ sec. [[#sec:fadc:noisy_events_and_fadc_veto]]

- [ ] *INSERT FADC RISE TIME EXAMPLE*

- [ ] *GET THE CORRECT NUMBERS FROM SIMULATION*
  -> In particular provide a few more words about how it compares to
  the detected lengths of the calibration clusters, which drops above
  6 significantly. Once final number easier to write this.
- [ ] *INSERT APPENDIX WITH LENGTH PLOTS*


- [ ] *FOR MY OWN CURIOSITY*:
  -> What happens if we allow application of only FADC as veto? What
  kind of background rate can we achieve then?


**** About FADC rise time from theory and reality [/]           :noexport:

The theoretical numbers of about 100 ns do not really match very well
with our experimental numbers anymore after the recent
changes. However, this is mainly due to three aspects:
1. The offsets from the baseline and to the peak shorten the rise time
   by at least (10 + 2.5) %, so our peak at ~55ns -> 61.875
   ns.
2. Realistically it is also though not taking into account that
   the bottom of the signal is likely still part of the actual
   signal. Once the trailing electrons of the cloud traverse through
   the grid, the bulk has already been deposited on the pixels and the
   discharge of the grid is underway. It's a convolution of the still
   incoming electrons and their induction and the discharge of the
   previous.
3. Our calculations use a rather random $3 σ$ size of the clusters
   (compared to the RMS). This is of course an extremely rough
   estimate for the cluster size, which mainly will be correct in the
   upper outliers.

All three aspects combined likely very well explain the difference. It
would be interesting to add to our simulation of the ~rmsTransverse~
distributions an additional step of trying to simulate the ionization
& discharge convolution to see what the actual physical signals might
look like! For that it might be enough to assume that each electron
once it passes through a grid hole "instantaneously" is amplified by
the gas gain leading to a fixed amount of induction. Would be
interesting.
- [ ] Do this!

**** Generate plot for background rate including FADC           :noexport:

Let's generate the plot:

#+begin_src sh :results drawer
plotBackgroundRate \
 ~/phd/resources/background/autoGen/likelihood_cdl2018_Run2_crGold.h5 \
 ~/phd/resources/background/autoGen/likelihood_cdl2018_Run3_crGold.h5 \
 ~/org/resources/lhood_limits_automation_preliminary/lhood_outputs_adaptive_fadc/likelihood_cdl2018_Run2_crGold_scinti.h5 \
 ~/org/resources/lhood_limits_automation_preliminary/lhood_outputs_adaptive_fadc/likelihood_cdl2018_Run3_crGold_scinti.h5 \
 ~/org/resources/lhood_limits_automation_preliminary/lhood_outputs_adaptive_fadc/likelihood_cdl2018_Run2_crGold_scinti_vetoPercentile_0.99_fadc_vetoPercentile_0.99.h5 \
 ~/org/resources/lhood_limits_automation_preliminary/lhood_outputs_adaptive_fadc/likelihood_cdl2018_Run3_crGold_scinti_vetoPercentile_0.99_fadc_vetoPercentile_0.99.h5 \
 --names "No vetoes" --names "No vetoes" \
 --names "Scinti" --names "Scinti" \
 --names "FADC" --names "FADC" \
 --centerChip 3 \
 --title "Background rate from CAST data, incl. scinti and FADC veto" \
 --showNumClusters \
 --showTotalTime \
 --topMargin 1.5 \
 --energyDset energyFromCharge \
 --outfile background_rate_crGold_scinti_fadc.pdf \
 --outpath ~/phd/Figs/background/ \
 --useTeX \
 --quiet
#+end_src

#+RESULTS:
:results:
[INFO]:Dataset: FADC
[INFO]:	 Integrated background rate in range: 0.0 .. 12.0: 1.7495e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 12.0: 1.4579e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: No vetoes
[INFO]:	 Integrated background rate in range: 0.0 .. 12.0: 2.3355e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 12.0: 1.9462e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Scinti
[INFO]:	 Integrated background rate in range: 0.0 .. 12.0: 2.1413e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 12.0: 1.7844e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: FADC
[INFO]:	 Integrated background rate in range: 0.5 .. 2.5: 4.9556e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.5 .. 2.5: 2.4778e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: No vetoes
[INFO]:	 Integrated background rate in range: 0.5 .. 2.5: 6.1610e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.5 .. 2.5: 3.0805e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Scinti
[INFO]:	 Integrated background rate in range: 0.5 .. 2.5: 5.9935e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.5 .. 2.5: 2.9968e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: FADC
[INFO]:	 Integrated background rate in range: 0.5 .. 5.0: 8.4546e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.5 .. 5.0: 1.8788e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: No vetoes
[INFO]:	 Integrated background rate in range: 0.5 .. 5.0: 1.1669e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.5 .. 5.0: 2.5931e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Scinti
[INFO]:	 Integrated background rate in range: 0.5 .. 5.0: 1.0798e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.5 .. 5.0: 2.3997e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: FADC
[INFO]:	 Integrated background rate in range: 0.0 .. 2.5: 7.2324e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 2.5: 2.8930e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: No vetoes
[INFO]:	 Integrated background rate in range: 0.0 .. 2.5: 8.9066e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 2.5: 3.5626e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Scinti
[INFO]:	 Integrated background rate in range: 0.0 .. 2.5: 8.6722e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 2.5: 3.4689e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: FADC
[INFO]:	 Integrated background rate in range: 4.0 .. 8.0: 1.6742e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 4.0 .. 8.0: 4.1854e-06 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: No vetoes
[INFO]:	 Integrated background rate in range: 4.0 .. 8.0: 2.6285e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 4.0 .. 8.0: 6.5711e-06 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Scinti
[INFO]:	 Integrated background rate in range: 4.0 .. 8.0: 2.3606e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 4.0 .. 8.0: 5.9015e-06 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: FADC
[INFO]:	 Integrated background rate in range: 0.0 .. 8.0: 1.2188e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 8.0: 1.5235e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: No vetoes
[INFO]:	 Integrated background rate in range: 0.0 .. 8.0: 1.6591e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 8.0: 2.0739e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Scinti
[INFO]:	 Integrated background rate in range: 0.0 .. 8.0: 1.5419e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 8.0: 1.9274e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: FADC
[INFO]:	 Integrated background rate in range: 2.0 .. 8.0: 5.2569e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 2.0 .. 8.0: 8.7615e-06 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: No vetoes
[INFO]:	 Integrated background rate in range: 2.0 .. 8.0: 8.3039e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 2.0 .. 8.0: 1.3840e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Scinti
[INFO]:	 Integrated background rate in range: 2.0 .. 8.0: 7.2827e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 2.0 .. 8.0: 1.2138e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:DataFrame with 7 columns and 180 rows:
     Idx       Energy         Rate    totalTime      RateErr      Dataset         yMin         yMax
  dtype:        float        float     constant        float       string        float        float
       0            0        2.679         3318       0.6697         FADC        2.009        3.348
       1          0.2        1.339         3318       0.4735         FADC       0.8658        1.813
       2          0.4        5.692         3318       0.9762         FADC        4.716        6.668
       3          0.6        6.027         3318        1.005         FADC        5.023        7.032
       4          0.8        5.692         3318       0.9762         FADC        4.716        6.668
       5            1        5.023         3318        0.917         FADC        4.106         5.94
       6          1.2        2.344         3318       0.6264         FADC        1.717         2.97
       7          1.4        2.344         3318       0.6264         FADC        1.717         2.97
       8          1.6        1.842         3318       0.5553         FADC        1.286        2.397
       9          1.8        2.511         3318       0.6484         FADC        1.863         3.16
      10            2        1.005         3318       0.4101         FADC       0.5944        1.415
      11          2.2       0.8371         3318       0.3744         FADC       0.4627        1.211
      12          2.4       0.3348         3318       0.2368         FADC      0.09807       0.5716
      13          2.6        1.172         3318       0.4429         FADC        0.729        1.615
      14          2.8       0.8371         3318       0.3744         FADC       0.4627        1.211
      15            3        4.185         3318       0.8371         FADC        3.348        5.023
      16          3.2        3.348         3318       0.7487         FADC          2.6        4.097
      17          3.4        2.679         3318       0.6697         FADC        2.009        3.348
      18          3.6        2.176         3318       0.6036         FADC        1.573         2.78
      19          3.8        1.674         3318       0.5294         FADC        1.145        2.204

[INFO]:INFO: storing plot in /home/basti/phd/Figs/background/background_rate_crGold_scinti_fadc.pdf
[WARNING]: Printing total background time currently only supported for single datasets.
shellCmd: command -v xelatex
shell 3833> /home/basti/texlive/2022/bin/x86_64-linux/xelatex
shellCmd: xelatex -output-directory /home/basti/phd/Figs/background /home/basti/phd/Figs/background/background_rate_crGold_scinti_fadc.tex
shell 3841> This is XeTeX, Version 3.141592653-2.6-0.999994 (TeX Live 2022) (preloaded format=xelatex)
shell 3841>  restricted \write18 enabled.
shell 3841> entering extended mode
shell 3841> (/home/basti/phd/Figs/background/background_rate_crGold_scinti_fadc.tex
shell 3841> LaTeX2e <2022-11-01> patch level 1
shell 3841> L3 programming layer <2022-12-17>
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/latex/standalone/standalone.cls
shell 3841> Document Class: standalone 2022/10/10 v1.3b Class to compile TeX sub-files stan
shell 3841> dalone
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/latex/tools/shellesc.sty)
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/iftex/ifluatex.sty
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/iftex/iftex.sty))
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/latex/xkeyval/xkeyval.sty
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/xkeyval/xkeyval.tex
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/xkeyval/xkvutils.tex
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/xkeyval/keyval.tex))))
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/latex/standalone/standalone.cfg)
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/latex/base/article.cls
shell 3841> Document Class: article 2022/07/02 v1.4n Standard LaTeX document class
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/latex/base/size10.clo))
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/frontendlayer/tikz.sty
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/basiclayer/pgf.sty
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/utilities/pgfrcs.sty
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgfutil-common.t
shell 3841> ex
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgfutil-common-l
shell 3841> ists.tex))
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgfutil-latex.de
shell 3841> f)
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgfrcs.code.tex
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/pgf.revision.tex)))
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/basiclayer/pgfcore.sty
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/latex/graphics/graphicx.sty
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/latex/graphics/graphics.sty
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/latex/graphics/trig.sty)
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/latex/graphics-cfg/graphics.cfg)
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/latex/graphics-def/xetex.def)))
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/systemlayer/pgfsys.sty
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/systemlayer/pgfsys.code.te
shell 3841> x
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgfkeys.code.tex
shell 3841> 
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgfkeysfiltered.
shell 3841> code.tex))
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/systemlayer/pgf.cfg)
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/systemlayer/pgfsys-xetex.d
shell 3841> ef
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/systemlayer/pgfsys-dvipdfm
shell 3841> x.def
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/systemlayer/pgfsys-common-
shell 3841> pdf.def))))
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/systemlayer/pgfsyssoftpath
shell 3841> .code.tex)
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/systemlayer/pgfsysprotocol
shell 3841> .code.tex)) (/home/basti/texlive/2022/texmf-dist/tex/latex/xcolor/xcolor.sty
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/latex/graphics-cfg/color.cfg)
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/latex/graphics/mathcolor.ltx))
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcore.code.te
shell 3841> x (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmath.code.tex
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathcalc.code.tex
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathutil.code.tex)
shell 3841> 
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathparser.code.te
shell 3841> x)
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.code
shell 3841> .tex
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.basi
shell 3841> c.code.tex)
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.trig
shell 3841> onometric.code.tex)
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.rand
shell 3841> om.code.tex)
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.comp
shell 3841> arison.code.tex)
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.base
shell 3841> .code.tex)
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.roun
shell 3841> d.code.tex)
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.misc
shell 3841> .code.tex)
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.inte
shell 3841> gerarithmetics.code.tex)))
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfloat.code.tex
shell 3841> )) (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfint.code.tex)
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepoints.c
shell 3841> ode.tex)
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepathcons
shell 3841> truct.code.tex)
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepathusag
shell 3841> e.code.tex)
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorescopes.c
shell 3841> ode.tex)
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcoregraphics
shell 3841> tate.code.tex)
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcoretransfor
shell 3841> mations.code.tex)
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorequick.co
shell 3841> de.tex)
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreobjects.
shell 3841> code.tex)
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepathproc
shell 3841> essing.code.tex)
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorearrows.c
shell 3841> ode.tex)
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreshade.co
shell 3841> de.tex)
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreimage.co
shell 3841> de.tex
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreexternal
shell 3841> .code.tex))
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorelayers.c
shell 3841> ode.tex)
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcoretranspar
shell 3841> ency.code.tex)
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepatterns
shell 3841> .code.tex)
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorerdf.code
shell 3841> .tex)))
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/modules/pgfmoduleshapes.co
shell 3841> de.tex)
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/modules/pgfmoduleplot.code
shell 3841> .tex)
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/compatibility/pgfcomp-versio
shell 3841> n-0-65.sty)
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/compatibility/pgfcomp-versio
shell 3841> n-1-18.sty))
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/utilities/pgffor.sty
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/utilities/pgfkeys.sty
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgfkeys.code.tex
shell 3841> )) (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/math/pgfmath.sty
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmath.code.tex))
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgffor.code.tex
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmath.code.tex)))
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/frontendlayer/tikz/tikz.co
shell 3841> de.tex
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/libraries/pgflibraryplotha
shell 3841> ndlers.code.tex)
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/modules/pgfmodulematrix.co
shell 3841> de.tex)
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/frontendlayer/tikz/librari
shell 3841> es/tikzlibrarytopaths.code.tex))))
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/latex/base/inputenc.sty
shell 3841> 
shell 3841> Package inputenc Warning: inputenc package ignored with utf8 based engines.
shell 3841> 
shell 3841> ) (/home/basti/texlive/2022/texmf-dist/tex/latex/geometry/geometry.sty
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/iftex/ifvtex.sty))
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/latex/unicode-math/unicode-math.sty
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/latex/l3kernel/expl3.sty
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/latex/l3backend/l3backend-xetex.def))
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/latex/unicode-math/unicode-math-xetex.
shell 3841> sty (/home/basti/texlive/2022/texmf-dist/tex/latex/l3packages/xparse/xparse.sty
shell 3841> )
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/latex/l3packages/l3keys2e/l3keys2e.sty
shell 3841> ) (/home/basti/texlive/2022/texmf-dist/tex/latex/fontspec/fontspec.sty
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/latex/fontspec/fontspec-xetex.sty
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/latex/base/fontenc.sty)
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/latex/fontspec/fontspec.cfg)))
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/latex/base/fix-cm.sty
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/latex/base/ts1enc.def))
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/latex/amsmath/amsmath.sty
shell 3841> For additional information on amsmath, use the `?' option.
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/latex/amsmath/amstext.sty
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/latex/amsmath/amsgen.sty))
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/latex/amsmath/amsbsy.sty)
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/latex/amsmath/amsopn.sty))
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/latex/unicode-math/unicode-math-table.
shell 3841> tex))) (/home/basti/texlive/2022/texmf-dist/tex/latex/siunitx/siunitx.sty
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/latex/translations/translations.sty
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/latex/etoolbox/etoolbox.sty)
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/pdftexcmds/pdftexcmds.sty
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/infwarerr/infwarerr.sty)
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/generic/ltxcmds/ltxcmds.sty)))
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/latex/tools/array.sty))
shell 3841> (/home/basti/phd/Figs/background/background_rate_crGold_scinti_fadc.aux)
shell 3841> *geometry* driver: auto-detecting
shell 3841> *geometry* detected driver: xetex
shell 3841> 
shell 3841> (/home/basti/texlive/2022/texmf-dist/tex/latex/translations/translations-basic-
shell 3841> dictionary-english.trsl) [1]
shell 3841> (/home/basti/phd/Figs/background/background_rate_crGold_scinti_fadc.aux) )
shell 3841> Output written on /home/basti/phd/Figs/background/background_rate_crGold_scinti
shell 3841> _fadc.pdf (1 page).
shell 3841> Transcript written on /home/basti/phd/Figs/background/background_rate_crGold_sc
shell 3841> inti_fadc.log.
:end:

**** Generate plots of rise/fall time for CDL data :noexport:

#+begin_src nim :results drawer :tangle /tmp/fadc_rise_fall_signal_vs_background.nim
import nimhdf5, ggplotnim
import std / [strutils, os, sequtils, sets, strformat]
import ingrid / [tos_helpers, ingrid_types]
import ingrid / calibration / [calib_fitting, calib_plotting]
import ingrid / calibration 

proc plotFallTimeRiseTime(df: DataFrame, suffix: string, riseTimeHigh: float) =
  ## Given a full run of FADC data, create the
  ## Note: it may be sensible to compute a truncated mean instead
  # local copy filtered to maximum allowed rise time
  let df = df.filter(f{`riseTime` <= riseTimeHigh})
  
  proc plotDset(dset: string) =

    let dfCalib = df.filter(f{`Type` == "⁵⁵Fe"})
    echo "============================== ", dset, " =============================="
    echo "Percentiles:"
    echo "\t 1-th: ", dfCalib[dset, float].percentile(1)
    echo "\t 5-th: ", dfCalib[dset, float].percentile(5)
    echo "\t50-th: ", dfCalib[dset, float].percentile(50)
    echo "\t mean: ", dfCalib[dset, float].mean
    echo "\t95-th: ", dfCalib[dset, float].percentile(95)
    echo "\t99-th: ", dfCalib[dset, float].percentile(99)
    
    ggplot(df, aes(dset, fill = "Type")) + 
      geom_histogram(position = "identity", bins = 100, hdKind = hdOutline, alpha = 0.7) +
      ggtitle(&"FADC signal {dset} in ⁵⁵Fe vs background data in $#" % suffix) +
      xlab(dset & " [ns]") + 
      ggsave(&"Figs/statusAndProgress/FADC/fadc_{dset}_signal_vs_background_$#.pdf" % suffix)
    ggplot(df, aes(dset, fill = "Type")) + 
      geom_density(normalize = true, alpha = 0.7, adjust = 2.0) +
      ggtitle(&"FADC signal {dset} in ⁵⁵Fe vs background data in $#" % suffix) +
      xlab(dset & " [ns]") +      
      ggsave(&"Figs/statusAndProgress/FADC/fadc_{dset}_kde_signal_vs_background_$#.pdf" % suffix)
  plotDset("fallTime")
  plotDset("riseTime")

proc read(fname, typ: string, eLow, eHigh: float): DataFrame =
  var h5f = H5open(fname, "r")
  let fileInfo = h5f.getFileInfo()
  
  var peakPos = newSeq[float]()
  result = newDataFrame()
  for run in fileInfo.runs:
    if recoBase() & $run / "fadc" notin h5f: continue # skip runs that were without FADC
    var df = h5f.readRunDsets(
      run,
      #chipDsets = some((chip: 3, dsets: @["eventNumber"])), # XXX: causes problems?? Removes some FADC data
                                                             # but not due to events!
      fadcDsets = @["eventNumber",
                    "baseline",
                    "riseStart",
                    "riseTime",                  
                    "fallStop",
                    "fallTime",
                    "minvals",
                    "argMinval"]                 
    )
    # in calibration case filter to 
    if typ == "⁵⁵Fe":
      let xrayRefCuts = getXrayCleaningCuts()
      let cut = xrayRefCuts["Mn-Cr-12kV"]
      let grp = h5f[(recoBase() & $run / "chip_3").grp_str]
      let passIdx = cutOnProperties(
        h5f,
        grp,
        crSilver, # try cutting to silver
        (toDset(igRmsTransverse), cut.minRms, cut.maxRms),
        (toDset(igRmsTransverse), 0.0, cut.maxEccentricity),        
        (toDset(igLength), 0.0, cut.maxLength),
        (toDset(igHits), cut.minPix, Inf),
        (toDset(igEnergyFromCharge), eLow, eHigh)
      )
      let dfChip = h5f.readRunDsets(run, chipDsets = some((chip: 3, dsets: @["eventNumber"])))
      let allEvNums = dfChip["eventNumber", int]
      let evNums = passIdx.mapIt(allEvNums[it]).toSet
      df = df.filter(f{int: `eventNumber` in evNums})
    df["runNumber"] = run
    result.add df
  result["Type"] = typ
  echo result

proc main(back, calib: string, year: int,
          energyLow = 0.0, energyHigh = Inf,
          riseTimeHigh = Inf
         ) =
  var df = newDataFrame()
  df.add read(back, "Background", energyLow, energyHigh)
  df.add read(calib, "⁵⁵Fe", energyLow, energyHigh)
  
  let is2017 = year == 2017
  let is2018 = year == 2018
  if not is2017 and not is2018:
    raise newException(IOError, "The input file is neither clearly a 2017 nor 2018 calibration file!")
  
  let yearToRun = if is2017: 2 else: 3
  let suffix = "Run-$#" % $yearToRun
  plotFallTimeRiseTime(df, suffix, riseTimeHigh) 
      
when isMainModule:
  import cligen
  dispatch main
#+end_src

#+begin_src sh
plotFadc \
    -c ~/CastData/data/CalibrationRuns2018_Reco.h5 \
    -b ~/CastData/data/DataRuns2018_Reco.h5 \
    --year 2018 \
    --outpath ~/phd/Figs/FADC/ \
    --riseTimeHigh 500
#+end_src

#+RESULTS:
| INFO:                          | Run         | /reconstruction/run_239/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_241/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_243/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_245/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_247/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_249/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_251/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_253/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_255/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_257/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_259/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_260/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_262/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_264/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_266/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_269/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_271/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_273/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_275/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_277/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_280/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_282/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_284/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_286/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_288/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_290/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_292/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_294/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_296/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_300/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_302/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_304/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| DataFrame                      | with        | 11                             | columns   | and        | 123934   | rows:    |               |            |           |          |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| Idx                            | eventNumber | baseline                       | riseStart | riseTime   | fallStop | fallTime | noisy         | argMinval  | runNumber | Settings |        Type |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| dtype:                         | int         | float                          | int       | int        | int      | int      | int           | int        | float     | constant |    constant |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| 0                              | 2           | 0.008423                       | 603       | 41         | 1139     | 472      | 0             | 657        | 239       | Setting  |           3 | ⁵⁵Fe |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| 1                              | 4           | 0.01014                        | 604       | 54         | 1194     | 508      | 0             | 673        | 239       | Setting  |           3 | ⁵⁵Fe |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| 2                              | 8           | 0.006491                       | 616       | 66         | 1167     | 460      | 0             | 695        | 239       | Setting  |           3 | ⁵⁵Fe |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| 3                              | 9           | 0.005923                       | 580       | 52         | 1159     | 499      | 0             | 651        | 239       | Setting  |           3 | ⁵⁵Fe |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| 4                              | 15          | 0.007497                       | 568       | 55         | 1078     | 430      | 0             | 639        | 239       | Setting  |           3 | ⁵⁵Fe |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| 5                              | 16          | 0.007565                       | 566       | 56         | 1184     | 536      | 0             | 637        | 239       | Setting  |           3 | ⁵⁵Fe |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| 6                              | 18          | 0.009988                       | 593       | 55         | 1182     | 508      | 0             | 662        | 239       | Setting  |           3 | ⁵⁵Fe |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| 7                              | 19          | 0.007538                       | 596       | 51         | 1157     | 484      | 0             | 658        | 239       | Setting  |           3 | ⁵⁵Fe |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| 8                              | 20          | 0.009162                       | 609       | 53         | 1199     | 506      | 0             | 672        | 239       | Setting  |           3 | ⁵⁵Fe |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| 9                              | 22          | 0.00629                        | 609       | 57         | 1139     | 449      | 0             | 677        | 239       | Setting  |           3 | ⁵⁵Fe |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| 10                             | 29          | 0.007168                       | 598       | 45         | 1108     | 443      | 0             | 654        | 239       | Setting  |           3 | ⁵⁵Fe |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| 11                             | 33          | 0.007921                       | 610       | 47         | 1134     | 453      | 0             | 668        | 239       | Setting  |           3 | ⁵⁵Fe |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| 12                             | 34          | 0.01071                        | 576       | 53         | 1171     | 510      | 0             | 645        | 239       | Setting  |           3 | ⁵⁵Fe |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| 13                             | 37          | 0.007671                       | 540       | 63         | 1149     | 520      | 0             | 616        | 239       | Setting  |           3 | ⁵⁵Fe |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| 14                             | 39          | 0.009933                       | 593       | 59         | 1164     | 487      | 0             | 665        | 239       | Setting  |           3 | ⁵⁵Fe |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| 15                             | 41          | 0.01002                        | 609       | 46         | 1154     | 476      | 0             | 669        | 239       | Setting  |           3 | ⁵⁵Fe |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| 16                             | 45          | 0.007921                       | 600       | 54         | 1157     | 480      | 0             | 667        | 239       | Setting  |           3 | ⁵⁵Fe |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| 17                             | 49          | 0.006646                       | 602       | 56         | 1149     | 465      | 0             | 673        | 239       | Setting  |           3 | ⁵⁵Fe |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| 18                             | 50          | 0.008408                       | 578       | 52         | 1152     | 492      | 0             | 641        | 239       | Setting  |           3 | ⁵⁵Fe |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| 19                             | 51          | 0.008542                       | 609       | 44         | 1153     | 472      | 0             | 660        | 239       | Setting  |           3 | ⁵⁵Fe |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
|                                |             |                                |           |            |          |          |               |            |           |          |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_240/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_242/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_244/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_246/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_248/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_250/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_254/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_256/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_258/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_261/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_263/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_265/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_267/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_268/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_270/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_272/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_274/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_276/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_278/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_279/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_281/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_283/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_285/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_287/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_289/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_291/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_293/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_295/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_297/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_301/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_303/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | Run         | /reconstruction/run_306/fadc   | does      | not        | have     | any      | data          | for        | dataset   | minvals  |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| DataFrame                      | with        | 11                             | columns   | and        | 211718   | rows:    |               |            |           |          |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| Idx                            | eventNumber | baseline                       | riseStart | riseTime   | fallStop | fallTime | noisy         | argMinval  | runNumber | Settings |        Type |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| dtype:                         | int         | float                          | int       | int        | int      | int      | int           | int        | float     | constant |    constant |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| 0                              | 4           | 0.002716                       | 679       | 93         | 1238     | 432      | 0             | 790        | 240       | Setting  |           3 | back |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| 1                              | 7           | 0.005544                       | 542       | 66         | 1355     | 709      | 0             | 619        | 240       | Setting  |           3 | back |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| 2                              | 21          | -0.001553                      | 560       | 193        | 1316     | 537      | 0             | 762        | 240       | Setting  |           3 | back |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| 3                              | 47          | 0.004027                       | 635       | 61         | 1178     | 456      | 0             | 709        | 240       | Setting  |           3 | back |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| 4                              | 61          | 0.0001456                      | 531       | 75         | 1140     | 503      | 0             | 622        | 240       | Setting  |           3 | back |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| 5                              | 71          | 0.006051                       | 386       | 134        | 1012     | 441      | 0             | 545        | 240       | Setting  |           3 | back |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| 6                              | 75          | -0.002058                      | 571       | 218        | 1173     | 362      | 0             | 801        | 240       | Setting  |           3 | back |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| 7                              | 76          | 0.008234                       | 467       | 184        | 1198     | 487      | 0             | 696        | 240       | Setting  |           3 | back |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| 8                              | 81          | 0.003344                       | 718       | 90         | 1243     | 405      | 0             | 828        | 240       | Setting  |           3 | back |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| 9                              | 84          | 0.002498                       | 673       | 132        | 1239     | 404      | 0             | 822        | 240       | Setting  |           3 | back |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| 10                             | 89          | -0.003767                      | 635       | 157        | 1430     | 604      | 0             | 801        | 240       | Setting  |           3 | back |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| 11                             | 95          | 0.8999                         | 18        | 2          | 982      | 208      | 0             | 169        | 240       | Setting  |           3 | back |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| 12                             | 100         | 0.0007305                      | 643       | 449        | 1545     | 416      | 0             | 1102       | 240       | Setting  |           3 | back |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| 13                             | 106         | -0.004829                      | 673       | 120        | 1170     | 349      | 0             | 806        | 240       | Setting  |           3 | back |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| 14                             | 111         | -0.03493                       | 123       | 10         | 708      | 294      | 0             | 142        | 240       | Setting  |           3 | back |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| 15                             | 121         | 0.0002351                      | 522       | 139        | 1267     | 573      | 0             | 679        | 240       | Setting  |           3 | back |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| 16                             | 144         | 0.004718                       | 742       | 67         | 1279     | 447      | 0             | 824        | 240       | Setting  |           3 | back |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| 17                             | 146         | 0.0001567                      | 718       | 103        | 1646     | 806      | 1             | 832        | 240       | Setting  |           3 | back |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| 18                             | 150         | 0.009029                       | 306       | 72         | 893      | 488      | 0             | 389        | 240       | Setting  |           3 | back |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| 19                             | 159         | -0.001712                      | 541       | 45         | 1090     | 482      | 0             | 596        | 240       | Setting  |           3 | back |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
|                                |             |                                |           |            |          |          |               |            |           |          |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| ============================== | fallTime    | ============================== |           |            |          |          |               |            |           |          |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| Type:                          | Type        | (kind:                         | VString,  | str:       | back     |          |               |            |           |          |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| Percentiles:                   |             |                                |           |            |          |          |               |            |           |          |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
|                                | 1-th:       | 17.0                           |           |            |          |          |               |            |           |          |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
|                                | 5-th:       | 309.0                          |           |            |          |          |               |            |           |          |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
|                                | 50-th:      | 458.0                          |           |            |          |          |               |            |           |          |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
|                                | mean:       | 447.5550088992432              |           |            |          |          |               |            |           |          |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
|                                | 80-th:      | 493.0                          |           |            |          |          |               |            |           |          |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
|                                | 95-th:      | 560.0                          |           |            |          |          |               |            |           |          |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
|                                | 99-th:      | 747.0                          |           |            |          |          |               |            |           |          |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| ============================== | fallTime    | ============================== |           |            |          |          |               |            |           |          |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| Type:                          | Type        | (kind:                         | VString,  | str:       | ⁵⁵Fe     |          |               |            |           |          |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| Percentiles:                   |             |                                |           |            |          |          |               |            |           |          |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
|                                | 1-th:       | 367.0                          |           |            |          |          |               |            |           |          |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
|                                | 5-th:       | 415.0                          |           |            |          |          |               |            |           |          |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
|                                | 50-th:      | 468.0                          |           |            |          |          |               |            |           |          |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
|                                | mean:       | 465.2696155956492              |           |            |          |          |               |            |           |          |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
|                                | 80-th:      | 488.0                          |           |            |          |          |               |            |           |          |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
|                                | 95-th:      | 505.0                          |           |            |          |          |               |            |           |          |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
|                                | 99-th:      | 522.0                          |           |            |          |          |               |            |           |          |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | The         | integer                        | column    | `fallTime` | has      | been     | automatically | determined | to        | be       | continuous. | To   | overwrite | this | behavior | add | a | `+ | scale_x/y_discrete()` | call | to | the | plotting | chain. | Choose | `x` | or | `y` | depending | on | which | axis | this | column | refers | to. | Or | apply | a | `factor` | to | the | column | name | in | the | `aes` | call, | i.e. | `aes(..., | fallTime | ...)`. |
| INFO:                          | The         | integer                        | column    | `fallTime` | has      | been     | automatically | determined | to        | be       | continuous. | To   | overwrite | this | behavior | add | a | `+ | scale_x/y_discrete()` | call | to | the | plotting | chain. | Choose | `x` | or | `y` | depending | on | which | axis | this | column | refers | to. | Or | apply | a | `factor` | to | the | column | name | in | the | `aes` | call, | i.e. | `aes(..., | fallTime | ...)`. |
| INFO:                          | The         | integer                        | column    | `fallTime` | has      | been     | automatically | determined | to        | be       | continuous. | To   | overwrite | this | behavior | add | a | `+ | scale_x/y_discrete()` | call | to | the | plotting | chain. | Choose | `x` | or | `y` | depending | on | which | axis | this | column | refers | to. | Or | apply | a | `factor` | to | the | column | name | in | the | `aes` | call, | i.e. | `aes(..., | fallTime | ...)`. |
| INFO:                          | The         | integer                        | column    | `fallTime` | has      | been     | automatically | determined | to        | be       | continuous. | To   | overwrite | this | behavior | add | a | `+ | scale_x/y_discrete()` | call | to | the | plotting | chain. | Choose | `x` | or | `y` | depending | on | which | axis | this | column | refers | to. | Or | apply | a | `factor` | to | the | column | name | in | the | `aes` | call, | i.e. | `aes(..., | fallTime | ...)`. |
| INFO:                          | The         | integer                        | column    | `fallTime` | has      | been     | automatically | determined | to        | be       | continuous. | To   | overwrite | this | behavior | add | a | `+ | scale_x/y_discrete()` | call | to | the | plotting | chain. | Choose | `x` | or | `y` | depending | on | which | axis | this | column | refers | to. | Or | apply | a | `factor` | to | the | column | name | in | the | `aes` | call, | i.e. | `aes(..., | fallTime | ...)`. |
| INFO:                          | The         | integer                        | column    | `fallTime` | has      | been     | automatically | determined | to        | be       | continuous. | To   | overwrite | this | behavior | add | a | `+ | scale_x/y_discrete()` | call | to | the | plotting | chain. | Choose | `x` | or | `y` | depending | on | which | axis | this | column | refers | to. | Or | apply | a | `factor` | to | the | column | name | in | the | `aes` | call, | i.e. | `aes(..., | fallTime | ...)`. |
| ============================== | riseTime    | ============================== |           |            |          |          |               |            |           |          |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| Type:                          | Type        | (kind:                         | VString,  | str:       | back     |          |               |            |           |          |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| Percentiles:                   |             |                                |           |            |          |          |               |            |           |          |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
|                                | 1-th:       | 12.0                           |           |            |          |          |               |            |           |          |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
|                                | 5-th:       | 47.0                           |           |            |          |          |               |            |           |          |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
|                                | 50-th:      | 99.0                           |           |            |          |          |               |            |           |          |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
|                                | mean:       | 131.5238977238196              |           |            |          |          |               |            |           |          |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
|                                | 80-th:      | 191.0                          |           |            |          |          |               |            |           |          |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
|                                | 95-th:      | 339.0                          |           |            |          |          |               |            |           |          |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
|                                | 99-th:      | 450.0                          |           |            |          |          |               |            |           |          |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| ============================== | riseTime    | ============================== |           |            |          |          |               |            |           |          |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| Type:                          | Type        | (kind:                         | VString,  | str:       | ⁵⁵Fe     |          |               |            |           |          |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| Percentiles:                   |             |                                |           |            |          |          |               |            |           |          |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
|                                | 1-th:       | 44.0                           |           |            |          |          |               |            |           |          |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
|                                | 5-th:       | 46.0                           |           |            |          |          |               |            |           |          |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
|                                | 50-th:      | 55.0                           |           |            |          |          |               |            |           |          |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
|                                | mean:       | 55.4189313494497               |           |            |          |          |               |            |           |          |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
|                                | 80-th:      | 59.0                           |           |            |          |          |               |            |           |          |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
|                                | 95-th:      | 64.0                           |           |            |          |          |               |            |           |          |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
|                                | 99-th:      | 71.0                           |           |            |          |          |               |            |           |          |             |      |           |      |          |     |   |    |                       |      |    |     |          |        |        |     |    |     |           |    |       |      |      |        |        |     |    |       |   |          |    |     |        |      |    |     |       |       |      |           |          |        |
| INFO:                          | The         | integer                        | column    | `riseTime` | has      | been     | automatically | determined | to        | be       | continuous. | To   | overwrite | this | behavior | add | a | `+ | scale_x/y_discrete()` | call | to | the | plotting | chain. | Choose | `x` | or | `y` | depending | on | which | axis | this | column | refers | to. | Or | apply | a | `factor` | to | the | column | name | in | the | `aes` | call, | i.e. | `aes(..., | riseTime | ...)`. |
| INFO:                          | The         | integer                        | column    | `riseTime` | has      | been     | automatically | determined | to        | be       | continuous. | To   | overwrite | this | behavior | add | a | `+ | scale_x/y_discrete()` | call | to | the | plotting | chain. | Choose | `x` | or | `y` | depending | on | which | axis | this | column | refers | to. | Or | apply | a | `factor` | to | the | column | name | in | the | `aes` | call, | i.e. | `aes(..., | riseTime | ...)`. |
| INFO:                          | The         | integer                        | column    | `riseTime` | has      | been     | automatically | determined | to        | be       | continuous. | To   | overwrite | this | behavior | add | a | `+ | scale_x/y_discrete()` | call | to | the | plotting | chain. | Choose | `x` | or | `y` | depending | on | which | axis | this | column | refers | to. | Or | apply | a | `factor` | to | the | column | name | in | the | `aes` | call, | i.e. | `aes(..., | riseTime | ...)`. |
| INFO:                          | The         | integer                        | column    | `riseTime` | has      | been     | automatically | determined | to        | be       | continuous. | To   | overwrite | this | behavior | add | a | `+ | scale_x/y_discrete()` | call | to | the | plotting | chain. | Choose | `x` | or | `y` | depending | on | which | axis | this | column | refers | to. | Or | apply | a | `factor` | to | the | column | name | in | the | `aes` | call, | i.e. | `aes(..., | riseTime | ...)`. |
| INFO:                          | The         | integer                        | column    | `riseTime` | has      | been     | automatically | determined | to        | be       | continuous. | To   | overwrite | this | behavior | add | a | `+ | scale_x/y_discrete()` | call | to | the | plotting | chain. | Choose | `x` | or | `y` | depending | on | which | axis | this | column | refers | to. | Or | apply | a | `factor` | to | the | column | name | in | the | `aes` | call, | i.e. | `aes(..., | riseTime | ...)`. |
| INFO:                          | The         | integer                        | column    | `riseTime` | has      | been     | automatically | determined | to        | be       | continuous. | To   | overwrite | this | behavior | add | a | `+ | scale_x/y_discrete()` | call | to | the | plotting | chain. | Choose | `x` | or | `y` | depending | on | which | axis | this | column | refers | to. | Or | apply | a | `factor` | to | the | column | name | in | the | `aes` | call, | i.e. | `aes(..., | riseTime | ...)`. |


**** Generate plots comparing rise/fall time of X-rays and background :noexport:

- [ ] *MOVE THE CODE CURRENTLY IN STATUS TO TPA!*

Use our FADC plotting tool to generate the plots for the rise and fall
time with a custom upper end
#+begin_src sh
cd /tmp
ntangle ~/org/Doc/StatusAndProgress.org && nim c -d:danger /t/fadc_rise_fall_signal_vs_background
./fadc_rise_fall_signal_vs_background \
    -c ~/CastData/data/CalibrationRuns2017_Reco.h5 \
    -b ~/CastData/data/DataRuns2017_Reco.h5 \
    --year 2017 \
    --riseTimeHigh 600
./fadc_rise_fall_signal_vs_background \
    -c ~/CastData/data/CalibrationRuns2018_Reco.h5 \
    -b ~/CastData/data/DataRuns2018_Reco.h5 \
    --year 2018 \
    --riseTimeHigh 600
#+end_src

- [ ] *RE GENERATE PLOT FOR 2018 WHEN SEPTEM+LINE VETO APPLICATION DONE!*


- [X] fixed the issue causing the flat offset / background

This is somewhat of a continuation of
sec. [[#sec:reco:fadc_rise_fall_plots]].

- [ ] *REVISIT THIS WITH 2018 DATA*

#+begin_src nim :results drawer :tangle code/fadc_rise_fall_signal_vs_background.nim
import nimhdf5, ggplotnim
import std / [strutils, os, sequtils, sets, strformat]
import ingrid / [tos_helpers, ingrid_types]
import ingrid / calibration / [calib_fitting, calib_plotting]
import ingrid / calibration 

proc stripPrefix(s, p: string): string =
  result = s
  result.removePrefix(p)

proc plotFallTimeRiseTime(df: DataFrame, suffix: string) =
  ## Given a full run of FADC data, create the
  ## Note: it may be sensible to compute a truncated mean instead
  proc plotDset(dset: string) =

    let dfCalib = df.filter(f{`Type` == "calib"})
    echo "============================== ", dset, " =============================="
    echo "Percentiles:"
    echo "\t 1-th: ", dfCalib[dset, float].percentile(1)
    echo "\t 5-th: ", dfCalib[dset, float].percentile(5)
    echo "\t95-th: ", dfCalib[dset, float].percentile(95)
    echo "\t99-th: ", dfCalib[dset, float].percentile(99)    
    
    ggplot(df, aes(dset, fill = "Type")) + 
      geom_histogram(position = "identity", bins = 100, hdKind = hdOutline, alpha = 0.7) +
      ggtitle(&"Comparison of FADC signal {dset} in ⁵⁵Fe vs background data in $#" % suffix) +
      ggsave(&"Figs/statusAndProgress/FADC/fadc_{dset}_signal_vs_background_$#.pdf" % suffix)
    ggplot(df, aes(dset, fill = "Type")) + 
      geom_density(normalize = true, alpha = 0.7, adjust = 2.0) +
      ggtitle(&"Comparison of FADC signal {dset} in ⁵⁵Fe vs background data in $#" % suffix) +
      ggsave(&"Figs/statusAndProgress/FADC/fadc_{dset}_kde_signal_vs_background_$#.pdf" % suffix)
  plotDset("fallTime")
  plotDset("riseTime")

  when false:
    let dfG = df.group_by("runNumber").summarize(f{float: "riseTime" << truncMean(col("riseTime").toSeq1D, 0.05)},
                                                 f{float: "fallTime" << truncMean(col("fallTime").toSeq1D, 0.05)})
    ggplot(dfG, aes(runNumber, riseTime, color = fallTime)) + 
      geom_point() +
      ggtitle("Comparison of FADC signal rise times in ⁵⁵Fe data for all runs in $#" % suffix) +
      ggsave("Figs/FADC/fadc_mean_riseTime_$#.pdf" % suffix)
    ggplot(dfG, aes(runNumber, fallTime, color = riseTime)) + 
      geom_point() +
      ggtitle("Comparison of FADC signal fall times in ⁵⁵Fe data for all runsin $#" % suffix) +
      ggsave("Figs/FADC/fadc_mean_fallTime_$#.pdf" % suffix)

proc read(fname, typ: string): DataFrame =
  var h5f = H5open(fname, "r")
  let fileInfo = h5f.getFileInfo()
  
  var peakPos = newSeq[float]()
  result = newDataFrame()
  for run in fileInfo.runs:
    if recoBase() & $run / "fadc" notin h5f: continue # skip runs that were without FADC
    var df = h5f.readRunDsets(
      run,
      #chipDsets = some((chip: 3, dsets: @["eventNumber"])), # XXX: causes problems?? Removes some FADC data
                                                             # but not due to events!
      fadcDsets = @["eventNumber",
                    "baseline",
                    "riseStart",
                    "riseTime",                  
                    "fallStop",
                    "fallTime",
                    "minvals",
                    "argMinval"]                 
    )
    # in calibration case filter to 
    if typ == "calib":
      let xrayRefCuts = getXrayCleaningCuts()
      let cut = xrayRefCuts["Mn-Cr-12kV"]
      let grp = h5f[(recoBase() & $run / "chip_3").grp_str]
      let passIdx = cutOnProperties(
        h5f,
        grp,
        crSilver, # try cutting to silver
        (toDset(igRmsTransverse), cut.minRms, cut.maxRms),
        (toDset(igRmsTransverse), 0.0, cut.maxEccentricity),        
        (toDset(igLength), 0.0, cut.maxLength),
        (toDset(igHits), cut.minPix, Inf),
        #(toDset(igEnergyFromCharge), 2.5, 3.5)
      )
      let dfChip = h5f.readRunDsets(run, chipDsets = some((chip: 3, dsets: @["eventNumber"])))
      let allEvNums = dfChip["eventNumber", int]
      let evNums = passIdx.mapIt(allEvNums[it]).toSet
      df = df.filter(f{int: `eventNumber` in evNums})
    df["runNumber"] = run
    result.add df
  result["Type"] = typ
  echo result

proc main(back, calib: string, year: int) =
  var df = newDataFrame()
  df.add read(back, "back")
  df.add read(calib, "calib")
  
  let is2017 = year == 2017
  let is2018 = year == 2018
  if not is2017 and not is2018:
    raise newException(IOError, "The input file is neither clearly a 2017 nor 2018 calibration file!")
  
  let yearToRun = if is2017: 2 else: 3
  let suffix = "run$#" % $yearToRun
  plotFallTimeRiseTime(df, suffix) 
      
when isMainModule:
  import cligen
  dispatch main
#+end_src

- *EXPLANATION FOR FLAT BACKGROUND IN RISE / FALL TIME:*
  The "dead" register causes our fall / rise time calculation to
  break! This leads to a 'background' of homogeneous rise / fall times
  -> THIS NEEDS TO BE FIXED FIRST!!

**** Calculate expected cluster sizes and rise times [/]        :noexport:

- [ ] Once happy with the text from ~statusAndProgress~, move all the
  related parts, concluding in the explicit code using the gas
  property CSV to compute the expected values explicitly.

  

*** Outer GridPix as veto - 'septem veto'
:PROPERTIES:
:CUSTOM_ID: sec:background:septem_veto
:END:

The final hardware feature that is used to improve the background
rate is the outer ring of GridPixes. The size of large clusters is a
significant fraction of a single GridPix. This means that depending on
the cluster center position, parts of the cluster may very well be
outside of the chip. While the most important area of the chip is the
center area (due to the X-ray optics focusing the axion induced
X-rays), misalignment and the extended nature of the 'axion image'
mean that a significant portion of the chip should be as low in
background as possible.

Fig. [[fig:background_clusters_no_septem_veto]] shows the cluster centers
based on the background data taken at CAST in 2017 and 2018 remaining
after the likelihood cut. Evidently, the cluster density is
significantly lower in the center area than towards the edges and in
particular the corners. The closer the cluster center is to the edges,
the higher the chance that parts of it are cut off. In particular,
cutting of from a track like cluster most likely *reduces* the length
and thus makes the cluster *more* spherical.
- [ ] *REFER TO THE IMAGE OF CLUSTER CENTERS FIRST INTRODUCED IN LNL STUFF*

#+begin_center
#+CAPTION: Each point represents the cluster center of a single cluster that passes
#+CAPTION: the likelihood cut. The color scale in addition represents whether multiple
#+CAPTION: cluster centers were on the same pixel. The data is the full 2017/18 background
#+CAPTION: data. It is very evident that the gold region (marked as a square) has the
#+CAPTION: lowest background. From there towards the edges and in particular the corners
#+CAPTION: the background increases significantly. This is mostly a geometric effect,
#+CAPTION: for which an example can be seen in fig. [[fig:example_clusters_septem_veto]].
#+CAPTION: *TODO ADD THE GOLD REGION OUTLINE*
#+NAME: fig:background_clusters_no_septem_veto
#+end_center

Normally the individual chips are treated as separate in the analysis
chain. The 'septem veto' is the name for an additional veto step,
which can be optionally applied to the center chip. With it, each
cluster that is considered signal-like based on the likelihood cut,
will be analyzed in a second step. The full event is reconstructed
again from the beginning, but this time as an event covering *all 7*
chips. This allows the cluster finding algorithm to detect clusters
beyond the center chip boundaries. After finding all clusters, the
normal cluster reconstruction to compute all properties is done
again. Finally, for each cluster in the event the likelihood value is
computed and compared with the likelihood cut. If now all clusters
whose center is on the central chip are considered background-like,
the event is vetoed, because the initial signal-like cluster turned
out to be part of a larger cluster. 

There is a slight complication here. The layout of the septemboard
includes spacing, which is required to mount the chips. This spacing,
in addition to dropping efficiency towards the edges of a GridPix mean
significant information is lost. When reconstructing the full
'septemboard events' this spacing would break the cluster finding
algorithms, as the spacing might extend the distance over the cutoff
criterion. For this reason the cluster finding algorithm is actually
performed on a 'tight layout' where the spacing has been reduced to
zero. For the calculation of the geometric properties of all found
clusters however, the found clusters are transformed into the real
septemboard coordinates including spacing. [fn:spacing_impact_rms]


An example that shows two clusters on the center chip, one of which
was initially interpreted as a signal like event before being vetoed
by the 'septem veto', is shown in
fig. [[fig:example_clusters_septem_veto]]. The colors indicate the
clusters each pixel belongs to according to the cluster finding
algorithm. As the chips are treated separately initially, there are
two clusters found on the center chip. The purple and cyan cluster
"portions" of the center chip. The purple part passes the likelihood
cut (X-rays at such low energies are much less spherical on average;
same diffusion, but less electrons) initially, which triggers the
'septem veto'. A full 7 GridPix event reconstruction shows the
additional parts of the two clusters. The purple cluster is finally
rejected.

It is a good example, as it shows a cluster that is still relatively
close to the center, and yet still 'connects' to another chip.

- [ ] *POSSIBLY REPLACE THIS EVENT, CERTAINLY REPLACE PLOT*
  -> Show an event that uses real septem layout!
#+begin_center
#+CAPTION: An example event showing all 7 GridPix of the CAST GridPix1 detector.
#+CAPTION: The outlines are the boundaries of each chip (without the real spacing between
#+CAPTION: them) and the color of each point indicates the cluster which it is part of
#+CAPTION: according to the cluster finder.
#+CAPTION: Initially the purple cluster (center chip portion) passes the log likelihood cut
#+CAPTION: (i.e. is signal like), but is vetoed by the 'septem veto'
#+CAPTION: as there are more pixels outside the center chip that are part of this cluster.
#+CAPTION: The cyan cluster in the bottom left of the center chip is in addition a good example of
#+CAPTION: how in particular cutting a track in the corners leads to a much more spherical
#+CAPTION: cluster.
#+NAME: fig:example_clusters_septem_veto
[[~/org/Figs/statusAndProgress/exampleEvents/background_septem_vetoed.pdf]]
#+end_center

The background rate with the septem veto is shown in
fig. [[fig:background_rate_septem_veto]], where we see that most of the
improvement is in the lower energy range $< \SI{2}{keV}$. This is the
most important region for the solar axion flux for the axion-electron
coupling. Looking at the mean background rate in intervals of
interest, between $\SIrange{2}{8}{keV}$ it is
\SI{7.3385e-06}{keV⁻¹.cm⁻².s⁻¹} and $\SIrange{4}{8}{keV} =
\SI{3.5158e-06}{keV⁻¹.cm⁻².s⁻¹}$. And even in the full range down to
$\SI{0}{keV}$ the mean is in the $10⁻⁶$ range. 

#+begin_center
#+CAPTION: Background rate achieved based on the 'septem veto' (in addition to the
#+CAPTION: scintillator cut & FADC veto) for the full 2017/18 dataset within the center
#+CAPTION: $\SI[parse-numbers=false]{5 \times 5}{mm²}$. Significant improvement in the
#+CAPTION: $< \SI{2}{keV}$ range, which is most important for axion-electron coupling solar
#+CAPTION: flux.
#+CAPTION: The background rate between $\SIrange{2}{8}{keV}$ is \SI{7.3385e-06}{keV⁻¹.cm⁻².s⁻¹} and
#+CAPTION: $\SIrange{4}{8}{keV} = \SI{3.5158e-06}{keV⁻¹.cm⁻².s⁻¹}$. And even in the full range
#+CAPTION: down to $\SI{0}{keV}$ the mean is in the $10⁻⁶$ range.
#+NAME: fig:background:background_rate_septem_veto
[[~/phd/Figs/background/background_rate_crGold_scinti_fadc_septem.pdf]]
#+end_center

[fn:spacing_impact_rms] In principle this has a small impact on the
calculation of the RMS of the cluster, if there is now a large gap
between two parts of the 'same' cluster. However, as we don't really
rely on the properties too much it is of no real concern. Technically
we still calculate the $\ln\mathcal{L}$ of each cluster, but any
cluster passing the $\ln\mathcal{L}$ cut before being added to a
larger cluster almost certainly won't pass it afterwards. The slight
bias in the RMS won't change that.

**** Generate plot for background rate including septem veto    :noexport:

Let's generate the plot:

#+begin_src sh :results drawer
plotBackgroundRate \
 ~/phd/resources/background/autoGen/likelihood_cdl2018_Run2_crGold.h5 \
 ~/phd/resources/background/autoGen/likelihood_cdl2018_Run3_crGold.h5 \
 ~/org/resources/lhood_limits_automation_preliminary/lhood_outputs_adaptive_fadc/likelihood_cdl2018_Run2_crGold_scinti.h5 \
 ~/org/resources/lhood_limits_automation_preliminary/lhood_outputs_adaptive_fadc/likelihood_cdl2018_Run3_crGold_scinti.h5 \
 ~/org/resources/lhood_limits_automation_preliminary/lhood_outputs_adaptive_fadc/likelihood_cdl2018_Run2_crGold_scinti_vetoPercentile_0.99_fadc_vetoPercentile_0.99.h5 \
 ~/org/resources/lhood_limits_automation_preliminary/lhood_outputs_adaptive_fadc/likelihood_cdl2018_Run3_crGold_scinti_vetoPercentile_0.99_fadc_vetoPercentile_0.99.h5 \
 ~/org/resources/lhood_limits_automation_preliminary/lhood_outputs_adaptive_fadc/likelihood_cdl2018_Run2_crGold_scinti_vetoPercentile_0.99_fadc_vetoPercentile_0.99_septem_vetoPercentile_0.99.h5 \
 ~/org/resources/lhood_limits_automation_preliminary/lhood_outputs_adaptive_fadc/likelihood_cdl2018_Run3_crGold_scinti_vetoPercentile_0.99_fadc_vetoPercentile_0.99_septem_vetoPercentile_0.99.h5 \
 --names "No vetoes" --names "No vetoes" \
 --names "Scinti" --names "Scinti" \
 --names "FADC" --names "FADC" \
 --names "Septem" --names "Septem" \
 --centerChip 3 \
 --title "Background rate from CAST data, incl. scinti, FADC, septem veto" \
 --showNumClusters \
 --showTotalTime \
 --topMargin 1.5 \
 --energyDset energyFromCharge \
 --outfile background_rate_crGold_scinti_fadc_septem.pdf \
 --outpath ~/phd/Figs/background/ \
 --useTeX \
 --quiet
#+end_src

#+RESULTS:
:results:
[INFO]:Dataset: FADC
[INFO]:	 Integrated background rate in range: 0.0 .. 12.0: 1.7495e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 12.0: 1.4579e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: No vetoes
[INFO]:	 Integrated background rate in range: 0.0 .. 12.0: 2.3355e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 12.0: 1.9462e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Scinti
[INFO]:	 Integrated background rate in range: 0.0 .. 12.0: 2.1413e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 12.0: 1.7844e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Septem
[INFO]:	 Integrated background rate in range: 0.0 .. 12.0: 1.1602e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 12.0: 9.6684e-06 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: FADC
[INFO]:	 Integrated background rate in range: 0.5 .. 2.5: 4.9556e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.5 .. 2.5: 2.4778e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: No vetoes
[INFO]:	 Integrated background rate in range: 0.5 .. 2.5: 6.1610e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.5 .. 2.5: 3.0805e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Scinti
[INFO]:	 Integrated background rate in range: 0.5 .. 2.5: 5.9935e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.5 .. 2.5: 2.9968e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Septem
[INFO]:	 Integrated background rate in range: 0.5 .. 2.5: 2.1095e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.5 .. 2.5: 1.0547e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: FADC
[INFO]:	 Integrated background rate in range: 0.5 .. 5.0: 8.4546e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.5 .. 5.0: 1.8788e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: No vetoes
[INFO]:	 Integrated background rate in range: 0.5 .. 5.0: 1.1669e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.5 .. 5.0: 2.5931e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Scinti
[INFO]:	 Integrated background rate in range: 0.5 .. 5.0: 1.0798e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.5 .. 5.0: 2.3997e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Septem
[INFO]:	 Integrated background rate in range: 0.5 .. 5.0: 5.0058e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.5 .. 5.0: 1.1124e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: FADC
[INFO]:	 Integrated background rate in range: 0.0 .. 2.5: 7.2324e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 2.5: 2.8930e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: No vetoes
[INFO]:	 Integrated background rate in range: 0.0 .. 2.5: 8.9066e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 2.5: 3.5626e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Scinti
[INFO]:	 Integrated background rate in range: 0.0 .. 2.5: 8.6722e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 2.5: 3.4689e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Septem
[INFO]:	 Integrated background rate in range: 0.0 .. 2.5: 3.2814e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 2.5: 1.3126e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: FADC
[INFO]:	 Integrated background rate in range: 4.0 .. 8.0: 1.6742e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 4.0 .. 8.0: 4.1854e-06 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: No vetoes
[INFO]:	 Integrated background rate in range: 4.0 .. 8.0: 2.6285e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 4.0 .. 8.0: 6.5711e-06 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Scinti
[INFO]:	 Integrated background rate in range: 4.0 .. 8.0: 2.3606e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 4.0 .. 8.0: 5.9015e-06 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Septem
[INFO]:	 Integrated background rate in range: 4.0 .. 8.0: 1.4063e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 4.0 .. 8.0: 3.5158e-06 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: FADC
[INFO]:	 Integrated background rate in range: 0.0 .. 8.0: 1.2188e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 8.0: 1.5235e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: No vetoes
[INFO]:	 Integrated background rate in range: 0.0 .. 8.0: 1.6591e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 8.0: 2.0739e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Scinti
[INFO]:	 Integrated background rate in range: 0.0 .. 8.0: 1.5419e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 8.0: 1.9274e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Septem
[INFO]:	 Integrated background rate in range: 0.0 .. 8.0: 7.4668e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 8.0: 9.3335e-06 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: FADC
[INFO]:	 Integrated background rate in range: 2.0 .. 8.0: 5.2569e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 2.0 .. 8.0: 8.7615e-06 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: No vetoes
[INFO]:	 Integrated background rate in range: 2.0 .. 8.0: 8.3039e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 2.0 .. 8.0: 1.3840e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Scinti
[INFO]:	 Integrated background rate in range: 2.0 .. 8.0: 7.2827e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 2.0 .. 8.0: 1.2138e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Septem
[INFO]:	 Integrated background rate in range: 2.0 .. 8.0: 4.4031e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 2.0 .. 8.0: 7.3385e-06 keV⁻¹·cm⁻²·s⁻¹
[INFO]:DataFrame with 7 columns and 240 rows:
     Idx       Energy         Rate    totalTime      RateErr      Dataset         yMin         yMax
  dtype:        float        float     constant        float       string        float        float
       0            0        2.679         3318       0.6697         FADC        2.009        3.348
       1          0.2        1.339         3318       0.4735         FADC       0.8658        1.813
       2          0.4        5.692         3318       0.9762         FADC        4.716        6.668
       3          0.6        6.027         3318        1.005         FADC        5.023        7.032
       4          0.8        5.692         3318       0.9762         FADC        4.716        6.668
       5            1        5.023         3318        0.917         FADC        4.106         5.94
       6          1.2        2.344         3318       0.6264         FADC        1.717         2.97
       7          1.4        2.344         3318       0.6264         FADC        1.717         2.97
       8          1.6        1.842         3318       0.5553         FADC        1.286        2.397
       9          1.8        2.511         3318       0.6484         FADC        1.863         3.16
      10            2        1.005         3318       0.4101         FADC       0.5944        1.415
      11          2.2       0.8371         3318       0.3744         FADC       0.4627        1.211
      12          2.4       0.3348         3318       0.2368         FADC      0.09807       0.5716
      13          2.6        1.172         3318       0.4429         FADC        0.729        1.615
      14          2.8       0.8371         3318       0.3744         FADC       0.4627        1.211
      15            3        4.185         3318       0.8371         FADC        3.348        5.023
      16          3.2        3.348         3318       0.7487         FADC          2.6        4.097
      17          3.4        2.679         3318       0.6697         FADC        2.009        3.348
      18          3.6        2.176         3318       0.6036         FADC        1.573         2.78
      19          3.8        1.674         3318       0.5294         FADC        1.145        2.204

[INFO]:INFO: storing plot in /home/basti/phd/Figs/background/background_rate_crGold_scinti_fadc_septem.pdf
[WARNING]: Printing total background time currently only supported for single datasets.
shellCmd: command -v xelatex
shell 8008> /home/basti/texlive/2022/bin/x86_64-linux/xelatex
shellCmd: xelatex -output-directory /home/basti/phd/Figs/background /home/basti/phd/Figs/background/background_rate_crGold_scinti_fadc_septem.tex
shell 8009> This is XeTeX, Version 3.141592653-2.6-0.999994 (TeX Live 2022) (preloaded format=xelatex)
shell 8009>  restricted \write18 enabled.
shell 8009> entering extended mode
shell 8009> (/home/basti/phd/Figs/background/background_rate_crGold_scinti_fadc_septem.tex
shell 8009> LaTeX2e <2022-11-01> patch level 1
shell 8009> L3 programming layer <2022-12-17>
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/latex/standalone/standalone.cls
shell 8009> Document Class: standalone 2022/10/10 v1.3b Class to compile TeX sub-files stan
shell 8009> dalone
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/latex/tools/shellesc.sty)
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/iftex/ifluatex.sty
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/iftex/iftex.sty))
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/latex/xkeyval/xkeyval.sty
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/xkeyval/xkeyval.tex
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/xkeyval/xkvutils.tex
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/xkeyval/keyval.tex))))
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/latex/standalone/standalone.cfg)
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/latex/base/article.cls
shell 8009> Document Class: article 2022/07/02 v1.4n Standard LaTeX document class
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/latex/base/size10.clo))
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/frontendlayer/tikz.sty
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/basiclayer/pgf.sty
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/utilities/pgfrcs.sty
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgfutil-common.t
shell 8009> ex
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgfutil-common-l
shell 8009> ists.tex))
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgfutil-latex.de
shell 8009> f)
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgfrcs.code.tex
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/pgf.revision.tex)))
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/basiclayer/pgfcore.sty
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/latex/graphics/graphicx.sty
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/latex/graphics/graphics.sty
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/latex/graphics/trig.sty)
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/latex/graphics-cfg/graphics.cfg)
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/latex/graphics-def/xetex.def)))
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/systemlayer/pgfsys.sty
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/systemlayer/pgfsys.code.te
shell 8009> x
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgfkeys.code.tex
shell 8009> 
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgfkeysfiltered.
shell 8009> code.tex))
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/systemlayer/pgf.cfg)
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/systemlayer/pgfsys-xetex.d
shell 8009> ef
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/systemlayer/pgfsys-dvipdfm
shell 8009> x.def
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/systemlayer/pgfsys-common-
shell 8009> pdf.def))))
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/systemlayer/pgfsyssoftpath
shell 8009> .code.tex)
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/systemlayer/pgfsysprotocol
shell 8009> .code.tex)) (/home/basti/texlive/2022/texmf-dist/tex/latex/xcolor/xcolor.sty
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/latex/graphics-cfg/color.cfg)
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/latex/graphics/mathcolor.ltx))
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcore.code.te
shell 8009> x (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmath.code.tex
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathcalc.code.tex
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathutil.code.tex)
shell 8009> 
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathparser.code.te
shell 8009> x)
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.code
shell 8009> .tex
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.basi
shell 8009> c.code.tex)
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.trig
shell 8009> onometric.code.tex)
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.rand
shell 8009> om.code.tex)
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.comp
shell 8009> arison.code.tex)
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.base
shell 8009> .code.tex)
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.roun
shell 8009> d.code.tex)
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.misc
shell 8009> .code.tex)
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.inte
shell 8009> gerarithmetics.code.tex)))
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfloat.code.tex
shell 8009> )) (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfint.code.tex)
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepoints.c
shell 8009> ode.tex)
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepathcons
shell 8009> truct.code.tex)
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepathusag
shell 8009> e.code.tex)
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorescopes.c
shell 8009> ode.tex)
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcoregraphics
shell 8009> tate.code.tex)
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcoretransfor
shell 8009> mations.code.tex)
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorequick.co
shell 8009> de.tex)
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreobjects.
shell 8009> code.tex)
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepathproc
shell 8009> essing.code.tex)
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorearrows.c
shell 8009> ode.tex)
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreshade.co
shell 8009> de.tex)
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreimage.co
shell 8009> de.tex
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreexternal
shell 8009> .code.tex))
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorelayers.c
shell 8009> ode.tex)
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcoretranspar
shell 8009> ency.code.tex)
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepatterns
shell 8009> .code.tex)
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorerdf.code
shell 8009> .tex)))
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/modules/pgfmoduleshapes.co
shell 8009> de.tex)
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/modules/pgfmoduleplot.code
shell 8009> .tex)
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/compatibility/pgfcomp-versio
shell 8009> n-0-65.sty)
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/compatibility/pgfcomp-versio
shell 8009> n-1-18.sty))
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/utilities/pgffor.sty
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/utilities/pgfkeys.sty
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgfkeys.code.tex
shell 8009> )) (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/math/pgfmath.sty
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmath.code.tex))
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgffor.code.tex
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmath.code.tex)))
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/frontendlayer/tikz/tikz.co
shell 8009> de.tex
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/libraries/pgflibraryplotha
shell 8009> ndlers.code.tex)
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/modules/pgfmodulematrix.co
shell 8009> de.tex)
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/frontendlayer/tikz/librari
shell 8009> es/tikzlibrarytopaths.code.tex))))
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/latex/base/inputenc.sty
shell 8009> 
shell 8009> Package inputenc Warning: inputenc package ignored with utf8 based engines.
shell 8009> 
shell 8009> ) (/home/basti/texlive/2022/texmf-dist/tex/latex/geometry/geometry.sty
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/iftex/ifvtex.sty))
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/latex/unicode-math/unicode-math.sty
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/latex/l3kernel/expl3.sty
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/latex/l3backend/l3backend-xetex.def))
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/latex/unicode-math/unicode-math-xetex.
shell 8009> sty (/home/basti/texlive/2022/texmf-dist/tex/latex/l3packages/xparse/xparse.sty
shell 8009> )
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/latex/l3packages/l3keys2e/l3keys2e.sty
shell 8009> ) (/home/basti/texlive/2022/texmf-dist/tex/latex/fontspec/fontspec.sty
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/latex/fontspec/fontspec-xetex.sty
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/latex/base/fontenc.sty)
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/latex/fontspec/fontspec.cfg)))
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/latex/base/fix-cm.sty
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/latex/base/ts1enc.def))
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/latex/amsmath/amsmath.sty
shell 8009> For additional information on amsmath, use the `?' option.
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/latex/amsmath/amstext.sty
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/latex/amsmath/amsgen.sty))
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/latex/amsmath/amsbsy.sty)
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/latex/amsmath/amsopn.sty))
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/latex/unicode-math/unicode-math-table.
shell 8009> tex))) (/home/basti/texlive/2022/texmf-dist/tex/latex/siunitx/siunitx.sty
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/latex/translations/translations.sty
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/latex/etoolbox/etoolbox.sty)
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/pdftexcmds/pdftexcmds.sty
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/infwarerr/infwarerr.sty)
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/generic/ltxcmds/ltxcmds.sty)))
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/latex/tools/array.sty))
shell 8009> (/home/basti/phd/Figs/background/background_rate_crGold_scinti_fadc_septem.aux)
shell 8009> *geometry* driver: auto-detecting
shell 8009> *geometry* detected driver: xetex
shell 8009> 
shell 8009> (/home/basti/texlive/2022/texmf-dist/tex/latex/translations/translations-basic-
shell 8009> dictionary-english.trsl) [1]
shell 8009> (/home/basti/phd/Figs/background/background_rate_crGold_scinti_fadc_septem.aux)
shell 8009>  )
shell 8009> Output written on /home/basti/phd/Figs/background/background_rate_crGold_scinti
shell 8009> _fadc_septem.pdf (1 page).
shell 8009> Transcript written on /home/basti/phd/Figs/background/background_rate_crGold_sc
shell 8009> inti_fadc_septem.log.
:end:

*** 'Line veto' 
:PROPERTIES:
:CUSTOM_ID: sec:background:line_veto
:END:

There is one more further optional step, dubbed the 'line veto', which
checks whether there are clusters whose long axis "points at" the
cluster that passed the log likelihood cut. The idea being that there
is a high chance that such clusters are correlated, especially because
ionization is an inherently statistical process. It starts from the
clusters identified in the previous septem veto. Only clusters whose
eccentricity is larger than some cutoff $ε_{\text{cut}}$ are
considered. The value of this cutoff impacts the efficiency but also
the expected random coincidence rate of the veto.

An example of an
event being vetoed by the 'line veto' is shown in
fig. [[fig:background:example_clusters_line_veto]].

#+begin_center
#+CAPTION: Example event, which highlights the use case of the 'line veto'. The
#+CAPTION: blue cluster in the upper chip is eccentric and its long axis 'points'
#+CAPTION: towards the red center cluster (which initially passes the $\ln\mathcal{L}$ cut).
#+CAPTION: The black circle is a measure for the radius of the center cluster. If the
#+CAPTION: line of the eccentric cluster cuts the circle, the cluster is vetoed.
#+NAME: fig:background:example_clusters_line_veto
[[~/org/Figs/statusAndProgress/exampleEvents/background_event_vetoed_by_lineveto.pdf]]
#+end_center

- [ ] *EXTEND THIS ON THE NOTION OF THE ECCENTRICITY LINE CUTOFF!*

- [ ] *MOVE MOST OF THIS DOWN TO sec. [[#sec:background:all_vetoes_combined]]*


Applying this veto to the full center chip improves the amount of
background significantly over the background seen in
fig. [[fig:background_clusters_no_septem_veto]]. The equivalent plot using
the septem veto is seen in fig. [[fig:background:background_clusters_septem_line_veto]].

#+begin_center
#+CAPTION: The equivalent figure to fig. [[fig:background_clusters_no_septem_veto]] but including
#+CAPTION: the 'septem veto'. The main improvement happens towards the corners. In total the
#+CAPTION: number of background clusters drops by a factor of 4, from $\sim\num{43000}$ to
#+CAPTION: $\sim\num{9600}$.
#+CAPTION: *TODO THIS INCLUDES THE LINE VETO. COMPARISON SHOULD BE SHOWN LATER!!!!*
#           -> Namely in section about combined background rate.
#+NAME: fig:background:background_clusters_septem_line_veto
[[~/phd/Figs/backgroundClusters/background_cluster_centers_all_vetoes.pdf]]
#+end_center

In total all vetoes together have achieved a background reduction of a
factor of about 10 over the regular $\ln\mathcal{L}$ method. But also
the center most region (a square of the center \SI{25}{mm²}) sees
an improvement, especially at low
energies. Fig. [[fig:background:background_suppression_all_vetoes]]
highlights the massive improvement over the pure $\ln\mathcal{L}$
method when compared to
fig. \ref{fig:background:background_suppression_tiles_no_vetoes_2017_18}. In
the corners the improvements reach up to a factor of 30 (top left),
but even in the center improvements of about a factor of 2 are achieved.

- [ ] *SHOULD THIS BE SIDE BY SIDE WITH OTHER PLOT?*
  -> I think this should be in the "fun combine all vetoes" subsection!
#+CAPTION: Local background suppression using all vetoes in the 2017/18 CAST
#+CAPTION: data. Compared to fig. \ref{fig:background:background_suppression_tiles_no_vetoes_2017_18} massive
#+CAPTION: improvements, up to a factor of 30 are visible. Even the center region
#+CAPTION: improves by a factor of 2.
#+NAME: fig:background:background_suppression_all_vetoes
[[~/phd/Figs/backgroundClusters/background_suppression_tile_map_all_vetoes.pdf]]

The achieved background rate goes well into the $10⁻⁶$ range with this
veto over the whole energy range, as the influence is largest at low
energies. The rate comes out to $\SI{7.3245e-06}{keV⁻¹.cm⁻².s⁻¹}$ over
the whole range up to $\SI{8}{keV}$ with
$\SI{6.7804e-06}{keV⁻¹.cm⁻².s⁻¹}$ starting from $\SI{2}{keV}$ and
$\SI{3.0972e-06}{keV⁻¹.cm⁻².s⁻¹}$ if starting from
$\SI{4}{keV}$. Fig. [[fig:background:background_rate_line_veto]] shows the
background rate.

#+CAPTION: Background rate of all CAST 2017/18 data in the center
#+CAPTION: $\SI[parse-numbers=false]{5 \times 5}{mm²}$ using all vetoes including the
#+CAPTION: line veto. Improvements are seen especially at low energies. The mean
#+CAPTION: background rate in the full range below $\SI{8}{keV}$ is
#+CAPTION: $\SI{7.3245e-06}{keV⁻¹.cm⁻².s⁻¹}$.
#+NAME: fig:background:background_rate_line_veto
[[~/phd/Figs/background/background_rate_crGold_scinti_fadc_septem_line.pdf]]



**** Possible eccentricity cutoff [/]
:PROPERTIES:
:CUSTOM_ID: sec:background:line_veto:eccentricity_cutoff
:END:

- [ ] *DISCUSS THE ECCENTRICITY CUTOFF IN LITTLE MORE DETAIL, SHOW THE
  EFFICIENCIES AS DESCRIBED IN SUBSECTION BELOW SOMEWHERE!*


Fig. [[fig:background:fraction_passing_line_veto_ecc_cut]] shows the
behavior of the line veto efficiency for real data and fake
bootstrapped data depending on what eccentricity a cluster needs to
participate as an active cluster in the line veto. The ideal choice is
somewhere in the middle.
- [ ] *DECIDE ON A FINAL VALUE WE USE AND GIVE ARGUMENTS!*
  -> Also see ratio plot! But to decide need to think through what we
  actually care about most! Direct ratio may not be optimal.

#+CAPTION: Fraction of events in Run-3 data (green), which pass (i.e. not rejected) the line
#+CAPTION: veto depending on the eccentricity cut used, which decides how eccentric a
#+CAPTION: cluster needs to be in order to be used for the veto. The purple points are
#+CAPTION: using fake bootstrapped data from real clusters passing the $\ln\mathcal{L}$ cut
#+CAPTION: together with real outer GridPix data from *other* events. The fraction of events
#+CAPTION: being vetoed in the latter is a measure for the random coincidence rate.
#+CAPTION: (See ratio plot to see that 1.4 - 1.5 is likely best?)
#+NAME: fig:background:fraction_passing_line_veto_ecc_cut
[[~/phd/Figs/background/estimateSeptemVetoRandomCoinc/fraction_passing_line_veto_ecc_cut.pdf]]


**** TODOs for section above                                    :noexport:

- [X] Not really a TODO, but interestingly the # of background
  clusters on the whole chip with all vetoes increased from 9500
  (ref. IAXO TDR plots) to ~10400 or so. But the number in the gold
  region dropped significantly from ~500 to ~350! The changes are many
  that were made in the mean time. All aspects of the method underwent
  changes and fixes, so this is not really a surprise.
-> *UPDATE*: <2023-03-10 Fri 09:41> Ohhh, I think I understand what's
  going on and why we have more total clusters...
  It's the fact that in this case we're running without the tracking
  information in the file, ergo we use all of the data and therefore
  have more clusters in total!

**** Generate plot for background rate with all vetoes          :noexport:

Let's generate the plot:

#+begin_src sh :results drawer
plotBackgroundRate \
 ~/phd/resources/background/autoGen/likelihood_cdl2018_Run2_crGold.h5 \
 ~/phd/resources/background/autoGen/likelihood_cdl2018_Run3_crGold.h5 \
 ~/org/resources/lhood_limits_automation_preliminary/lhood_outputs_adaptive_fadc/likelihood_cdl2018_Run2_crGold_scinti.h5 \
 ~/org/resources/lhood_limits_automation_preliminary/lhood_outputs_adaptive_fadc/likelihood_cdl2018_Run3_crGold_scinti.h5 \
 ~/org/resources/lhood_limits_automation_preliminary/lhood_outputs_adaptive_fadc/likelihood_cdl2018_Run2_crGold_scinti_vetoPercentile_0.99_fadc_vetoPercentile_0.99.h5 \
 ~/org/resources/lhood_limits_automation_preliminary/lhood_outputs_adaptive_fadc/likelihood_cdl2018_Run3_crGold_scinti_vetoPercentile_0.99_fadc_vetoPercentile_0.99.h5 \
 ~/org/resources/lhood_limits_automation_preliminary/lhood_outputs_adaptive_fadc/likelihood_cdl2018_Run2_crGold_scinti_vetoPercentile_0.99_fadc_vetoPercentile_0.99_septem_vetoPercentile_0.99.h5 \
 ~/org/resources/lhood_limits_automation_preliminary/lhood_outputs_adaptive_fadc/likelihood_cdl2018_Run3_crGold_scinti_vetoPercentile_0.99_fadc_vetoPercentile_0.99_septem_vetoPercentile_0.99.h5 \
 ~/org/resources/lhood_limits_automation_preliminary/lhood_outputs_adaptive_fadc/likelihood_cdl2018_Run2_crGold_scinti_vetoPercentile_0.99_fadc_vetoPercentile_0.99_septem_vetoPercentile_0.99_line_vetoPercentile_0.99.h5 \
 ~/org/resources/lhood_limits_automation_preliminary/lhood_outputs_adaptive_fadc/likelihood_cdl2018_Run3_crGold_scinti_vetoPercentile_0.99_fadc_vetoPercentile_0.99_septem_vetoPercentile_0.99_line_vetoPercentile_0.99.h5 \
 --names "No vetoes" --names "No vetoes" \
 --names "Scinti" --names "Scinti" \
 --names "FADC" --names "FADC" \
 --names "Septem" --names "Septem" \
 --names "Line" --names "Line" \
 --centerChip 3 \
 --title "Background rate from CAST data, incl. scinti, FADC, septem, line veto" \
 --showNumClusters \
 --showTotalTime \
 --topMargin 1.5 \
 --energyDset energyFromCharge \
 --outfile background_rate_crGold_scinti_fadc_septem_line.pdf \
 --outpath ~/phd/Figs/background/ \
 --useTeX \
 --quiet
#+end_src

#+RESULTS:
:results:
[INFO]:Dataset: FADC
[INFO]:	 Integrated background rate in range: 0.0 .. 12.0: 1.7495e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 12.0: 1.4579e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Line
[INFO]:	 Integrated background rate in range: 0.0 .. 12.0: 9.8944e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 12.0: 8.2453e-06 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: No vetoes
[INFO]:	 Integrated background rate in range: 0.0 .. 12.0: 2.3355e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 12.0: 1.9462e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Scinti
[INFO]:	 Integrated background rate in range: 0.0 .. 12.0: 2.1413e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 12.0: 1.7844e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Septem
[INFO]:	 Integrated background rate in range: 0.0 .. 12.0: 1.1602e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 12.0: 9.6684e-06 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: FADC
[INFO]:	 Integrated background rate in range: 0.5 .. 2.5: 4.9556e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.5 .. 2.5: 2.4778e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Line
[INFO]:	 Integrated background rate in range: 0.5 .. 2.5: 1.3059e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.5 .. 2.5: 6.5293e-06 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: No vetoes
[INFO]:	 Integrated background rate in range: 0.5 .. 2.5: 6.1610e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.5 .. 2.5: 3.0805e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Scinti
[INFO]:	 Integrated background rate in range: 0.5 .. 2.5: 5.9935e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.5 .. 2.5: 2.9968e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Septem
[INFO]:	 Integrated background rate in range: 0.5 .. 2.5: 2.1095e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.5 .. 2.5: 1.0547e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: FADC
[INFO]:	 Integrated background rate in range: 0.5 .. 5.0: 8.4546e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.5 .. 5.0: 1.8788e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Line
[INFO]:	 Integrated background rate in range: 0.5 .. 5.0: 4.0348e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.5 .. 5.0: 8.9661e-06 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: No vetoes
[INFO]:	 Integrated background rate in range: 0.5 .. 5.0: 1.1669e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.5 .. 5.0: 2.5931e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Scinti
[INFO]:	 Integrated background rate in range: 0.5 .. 5.0: 1.0798e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.5 .. 5.0: 2.3997e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Septem
[INFO]:	 Integrated background rate in range: 0.5 .. 5.0: 5.0058e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.5 .. 5.0: 1.1124e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: FADC
[INFO]:	 Integrated background rate in range: 0.0 .. 2.5: 7.2324e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 2.5: 2.8930e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Line
[INFO]:	 Integrated background rate in range: 0.0 .. 2.5: 2.0090e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 2.5: 8.0360e-06 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: No vetoes
[INFO]:	 Integrated background rate in range: 0.0 .. 2.5: 8.9066e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 2.5: 3.5626e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Scinti
[INFO]:	 Integrated background rate in range: 0.0 .. 2.5: 8.6722e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 2.5: 3.4689e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Septem
[INFO]:	 Integrated background rate in range: 0.0 .. 2.5: 3.2814e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 2.5: 1.3126e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: FADC
[INFO]:	 Integrated background rate in range: 4.0 .. 8.0: 1.6742e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 4.0 .. 8.0: 4.1854e-06 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Line
[INFO]:	 Integrated background rate in range: 4.0 .. 8.0: 1.2389e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 4.0 .. 8.0: 3.0972e-06 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: No vetoes
[INFO]:	 Integrated background rate in range: 4.0 .. 8.0: 2.6285e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 4.0 .. 8.0: 6.5711e-06 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Scinti
[INFO]:	 Integrated background rate in range: 4.0 .. 8.0: 2.3606e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 4.0 .. 8.0: 5.9015e-06 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Septem
[INFO]:	 Integrated background rate in range: 4.0 .. 8.0: 1.4063e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 4.0 .. 8.0: 3.5158e-06 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: FADC
[INFO]:	 Integrated background rate in range: 0.0 .. 8.0: 1.2188e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 8.0: 1.5235e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Line
[INFO]:	 Integrated background rate in range: 0.0 .. 8.0: 5.8596e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 8.0: 7.3245e-06 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: No vetoes
[INFO]:	 Integrated background rate in range: 0.0 .. 8.0: 1.6591e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 8.0: 2.0739e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Scinti
[INFO]:	 Integrated background rate in range: 0.0 .. 8.0: 1.5419e-04 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 8.0: 1.9274e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Septem
[INFO]:	 Integrated background rate in range: 0.0 .. 8.0: 7.4668e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 0.0 .. 8.0: 9.3335e-06 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: FADC
[INFO]:	 Integrated background rate in range: 2.0 .. 8.0: 5.2569e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 2.0 .. 8.0: 8.7615e-06 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Line
[INFO]:	 Integrated background rate in range: 2.0 .. 8.0: 4.0682e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 2.0 .. 8.0: 6.7804e-06 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: No vetoes
[INFO]:	 Integrated background rate in range: 2.0 .. 8.0: 8.3039e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 2.0 .. 8.0: 1.3840e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Scinti
[INFO]:	 Integrated background rate in range: 2.0 .. 8.0: 7.2827e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 2.0 .. 8.0: 1.2138e-05 keV⁻¹·cm⁻²·s⁻¹
[INFO]:Dataset: Septem
[INFO]:	 Integrated background rate in range: 2.0 .. 8.0: 4.4031e-05 cm⁻²·s⁻¹
[INFO]:	 Integrated background rate/keV in range: 2.0 .. 8.0: 7.3385e-06 keV⁻¹·cm⁻²·s⁻¹
[INFO]:DataFrame with 7 columns and 300 rows:
     Idx       Energy         Rate    totalTime      RateErr      Dataset         yMin         yMax
  dtype:        float        float     constant        float       string        float        float
       0            0        2.679         3318       0.6697         FADC        2.009        3.348
       1          0.2        1.339         3318       0.4735         FADC       0.8658        1.813
       2          0.4        5.692         3318       0.9762         FADC        4.716        6.668
       3          0.6        6.027         3318        1.005         FADC        5.023        7.032
       4          0.8        5.692         3318       0.9762         FADC        4.716        6.668
       5            1        5.023         3318        0.917         FADC        4.106         5.94
       6          1.2        2.344         3318       0.6264         FADC        1.717         2.97
       7          1.4        2.344         3318       0.6264         FADC        1.717         2.97
       8          1.6        1.842         3318       0.5553         FADC        1.286        2.397
       9          1.8        2.511         3318       0.6484         FADC        1.863         3.16
      10            2        1.005         3318       0.4101         FADC       0.5944        1.415
      11          2.2       0.8371         3318       0.3744         FADC       0.4627        1.211
      12          2.4       0.3348         3318       0.2368         FADC      0.09807       0.5716
      13          2.6        1.172         3318       0.4429         FADC        0.729        1.615
      14          2.8       0.8371         3318       0.3744         FADC       0.4627        1.211
      15            3        4.185         3318       0.8371         FADC        3.348        5.023
      16          3.2        3.348         3318       0.7487         FADC          2.6        4.097
      17          3.4        2.679         3318       0.6697         FADC        2.009        3.348
      18          3.6        2.176         3318       0.6036         FADC        1.573         2.78
      19          3.8        1.674         3318       0.5294         FADC        1.145        2.204

[INFO]:INFO: storing plot in /home/basti/phd/Figs/background/background_rate_crGold_scinti_fadc_septem_line.pdf
[WARNING]: Printing total background time currently only supported for single datasets.
shellCmd: command -v xelatex
shell 5658> /home/basti/texlive/2022/bin/x86_64-linux/xelatex
shellCmd: xelatex -output-directory /home/basti/phd/Figs/background /home/basti/phd/Figs/background/background_rate_crGold_scinti_fadc_septem_line.tex
shell 5659> This is XeTeX, Version 3.141592653-2.6-0.999994 (TeX Live 2022) (preloaded format=xelatex)
shell 5659>  restricted \write18 enabled.
shell 5659> entering extended mode
shell 5659> 
shell 5659> (/home/basti/phd/Figs/background/background_rate_crGold_scinti_fadc_septem_line
shell 5659> .tex
shell 5659> LaTeX2e <2022-11-01> patch level 1
shell 5659> L3 programming layer <2022-12-17>
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/latex/standalone/standalone.cls
shell 5659> Document Class: standalone 2022/10/10 v1.3b Class to compile TeX sub-files stan
shell 5659> dalone
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/latex/tools/shellesc.sty)
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/iftex/ifluatex.sty
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/iftex/iftex.sty))
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/latex/xkeyval/xkeyval.sty
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/xkeyval/xkeyval.tex
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/xkeyval/xkvutils.tex
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/xkeyval/keyval.tex))))
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/latex/standalone/standalone.cfg)
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/latex/base/article.cls
shell 5659> Document Class: article 2022/07/02 v1.4n Standard LaTeX document class
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/latex/base/size10.clo))
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/frontendlayer/tikz.sty
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/basiclayer/pgf.sty
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/utilities/pgfrcs.sty
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgfutil-common.t
shell 5659> ex
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgfutil-common-l
shell 5659> ists.tex))
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgfutil-latex.de
shell 5659> f)
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgfrcs.code.tex
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/pgf.revision.tex)))
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/basiclayer/pgfcore.sty
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/latex/graphics/graphicx.sty
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/latex/graphics/graphics.sty
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/latex/graphics/trig.sty)
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/latex/graphics-cfg/graphics.cfg)
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/latex/graphics-def/xetex.def)))
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/systemlayer/pgfsys.sty
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/systemlayer/pgfsys.code.te
shell 5659> x
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgfkeys.code.tex
shell 5659> 
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgfkeysfiltered.
shell 5659> code.tex))
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/systemlayer/pgf.cfg)
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/systemlayer/pgfsys-xetex.d
shell 5659> ef
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/systemlayer/pgfsys-dvipdfm
shell 5659> x.def
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/systemlayer/pgfsys-common-
shell 5659> pdf.def))))
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/systemlayer/pgfsyssoftpath
shell 5659> .code.tex)
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/systemlayer/pgfsysprotocol
shell 5659> .code.tex)) (/home/basti/texlive/2022/texmf-dist/tex/latex/xcolor/xcolor.sty
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/latex/graphics-cfg/color.cfg)
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/latex/graphics/mathcolor.ltx))
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcore.code.te
shell 5659> x (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmath.code.tex
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathcalc.code.tex
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathutil.code.tex)
shell 5659> 
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathparser.code.te
shell 5659> x)
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.code
shell 5659> .tex
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.basi
shell 5659> c.code.tex)
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.trig
shell 5659> onometric.code.tex)
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.rand
shell 5659> om.code.tex)
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.comp
shell 5659> arison.code.tex)
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.base
shell 5659> .code.tex)
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.roun
shell 5659> d.code.tex)
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.misc
shell 5659> .code.tex)
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.inte
shell 5659> gerarithmetics.code.tex)))
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfloat.code.tex
shell 5659> )) (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfint.code.tex)
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepoints.c
shell 5659> ode.tex)
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepathcons
shell 5659> truct.code.tex)
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepathusag
shell 5659> e.code.tex)
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorescopes.c
shell 5659> ode.tex)
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcoregraphics
shell 5659> tate.code.tex)
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcoretransfor
shell 5659> mations.code.tex)
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorequick.co
shell 5659> de.tex)
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreobjects.
shell 5659> code.tex)
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepathproc
shell 5659> essing.code.tex)
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorearrows.c
shell 5659> ode.tex)
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreshade.co
shell 5659> de.tex)
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreimage.co
shell 5659> de.tex
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreexternal
shell 5659> .code.tex))
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorelayers.c
shell 5659> ode.tex)
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcoretranspar
shell 5659> ency.code.tex)
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepatterns
shell 5659> .code.tex)
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorerdf.code
shell 5659> .tex)))
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/modules/pgfmoduleshapes.co
shell 5659> de.tex)
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/modules/pgfmoduleplot.code
shell 5659> .tex)
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/compatibility/pgfcomp-versio
shell 5659> n-0-65.sty)
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/compatibility/pgfcomp-versio
shell 5659> n-1-18.sty))
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/utilities/pgffor.sty
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/utilities/pgfkeys.sty
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgfkeys.code.tex
shell 5659> )) (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/math/pgfmath.sty
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmath.code.tex))
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgffor.code.tex
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmath.code.tex)))
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/frontendlayer/tikz/tikz.co
shell 5659> de.tex
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/libraries/pgflibraryplotha
shell 5659> ndlers.code.tex)
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/modules/pgfmodulematrix.co
shell 5659> de.tex)
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/frontendlayer/tikz/librari
shell 5659> es/tikzlibrarytopaths.code.tex))))
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/latex/base/inputenc.sty
shell 5659> 
shell 5659> Package inputenc Warning: inputenc package ignored with utf8 based engines.
shell 5659> 
shell 5659> ) (/home/basti/texlive/2022/texmf-dist/tex/latex/geometry/geometry.sty
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/iftex/ifvtex.sty))
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/latex/unicode-math/unicode-math.sty
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/latex/l3kernel/expl3.sty
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/latex/l3backend/l3backend-xetex.def))
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/latex/unicode-math/unicode-math-xetex.
shell 5659> sty (/home/basti/texlive/2022/texmf-dist/tex/latex/l3packages/xparse/xparse.sty
shell 5659> )
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/latex/l3packages/l3keys2e/l3keys2e.sty
shell 5659> ) (/home/basti/texlive/2022/texmf-dist/tex/latex/fontspec/fontspec.sty
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/latex/fontspec/fontspec-xetex.sty
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/latex/base/fontenc.sty)
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/latex/fontspec/fontspec.cfg)))
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/latex/base/fix-cm.sty
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/latex/base/ts1enc.def))
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/latex/amsmath/amsmath.sty
shell 5659> For additional information on amsmath, use the `?' option.
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/latex/amsmath/amstext.sty
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/latex/amsmath/amsgen.sty))
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/latex/amsmath/amsbsy.sty)
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/latex/amsmath/amsopn.sty))
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/latex/unicode-math/unicode-math-table.
shell 5659> tex))) (/home/basti/texlive/2022/texmf-dist/tex/latex/siunitx/siunitx.sty
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/latex/translations/translations.sty
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/latex/etoolbox/etoolbox.sty)
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/pdftexcmds/pdftexcmds.sty
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/infwarerr/infwarerr.sty)
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/generic/ltxcmds/ltxcmds.sty)))
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/latex/tools/array.sty))
shell 5659> No file background_rate_crGold_scinti_fadc_septem_line.aux.
shell 5659> *geometry* driver: auto-detecting
shell 5659> *geometry* detected driver: xetex
shell 5659> 
shell 5659> (/home/basti/texlive/2022/texmf-dist/tex/latex/translations/translations-basic-
shell 5659> dictionary-english.trsl) [1]
shell 5659> (/home/basti/phd/Figs/background/background_rate_crGold_scinti_fadc_septem_line
shell 5659> .aux) )
shell 5659> Output written on /home/basti/phd/Figs/background/background_rate_crGold_scinti
shell 5659> _fadc_septem_line.pdf (1 page).
shell 5659> Transcript written on /home/basti/phd/Figs/background/background_rate_crGold_sc
shell 5659> inti_fadc_septem_line.log.
:end:

**** Generate plot of background clusters left over with all vetoes :noexport:

#+begin_src sh :results drawer
plotBackgroundClusters \
    ~/org/resources/lhood_limits_automation_preliminary/lhood_outputs_adaptive_fadc/likelihood_cdl2018_Run2_crAll_scinti_vetoPercentile_0.99_fadc_vetoPercentile_0.99_septem_vetoPercentile_0.99_line_vetoPercentile_0.99.h5 \
    ~/org/resources/lhood_limits_automation_preliminary/lhood_outputs_adaptive_fadc/likelihood_cdl2018_Run3_crAll_scinti_vetoPercentile_0.99_fadc_vetoPercentile_0.99_septem_vetoPercentile_0.99_line_vetoPercentile_0.99.h5 \
    --zMax 5 \
    --title "X-ray like clusters of CAST data after all vetoes" \
    --outpath ~/phd/Figs/backgroundClusters/ \
    --filterNoisyPixels \
    --filterEnergy 12.0 \
    --suffix "_all_vetoes"
#+end_src

#+RESULTS:
:results:
@["/home/basti/org/resources/lhood_limits_automation_preliminary/lhood_outputs_adaptive_fadc/likelihood_cdl2018_Run2_crAll_scinti_vetoPercentile_0.99_fadc_vetoPercentile_0.99_septem_vetoPercentile_0.99_line_vetoPercentile_0.99.h5", "/home/basti/org/resources/lhood_limits_automation_preliminary/lhood_outputs_adaptive_fadc/likelihood_cdl2018_Run3_crAll_scinti_vetoPercentile_0.99_fadc_vetoPercentile_0.99_septem_vetoPercentile_0.99_line_vetoPercentile_0.99.h5"]
[INFO]: Saving plot to /home/basti/phd/Figs/backgroundClusters//background_cluster_centers_all_vetoes.pdf
INFO: The integer column `x` has been automatically determined to be continuous. To overwrite this behavior add a `+ scale_x/y_discrete()` call to the plotting chain. Choose `x` or `y` depending on which axis this column refers to. Or apply a `factor` to the column name in the `aes` call, i.e. `aes(..., factor("x"), ...)`.
INFO: The integer column `y` has been automatically determined to be continuous. To overwrite this behavior add a `+ scale_x/y_discrete()` call to the plotting chain. Choose `x` or `y` depending on which axis this column refers to. Or apply a `factor` to the column name in the `aes` call, i.e. `aes(..., factor("y"), ...)`.
DataFrame with 4 columns and 49 rows:
     Idx           xI           yI           cI           sI
  dtype:          int          int          int        float
       0            0            0          486        68.27
       1            0           36          176        188.5
       2            0           73           62        535.2
       3            0          109           45        737.3
       4            0          146          175        189.6
       5            0          182          216        153.6
       6            0          219          174        190.7
       7           36            0          655        50.66
       8           36           36          470         70.6
       9           36           73          123        269.8
      10           36          109           96        345.6
      11           36          146          119        278.8
      12           36          182          358        92.68
      13           36          219          289        114.8
      14           73            0          297        111.7
      15           73           36          152        218.3
      16           73           73           46        721.3
      17           73          109           52        638.1
      18           73          146           47          706
      19           73          182          240        138.3

shellCmd: command -v xelatex
shell 7659> /home/basti/texlive/2022/bin/x86_64-linux/xelatex
shellCmd: xelatex -output-directory /home/basti/phd/Figs/backgroundClusters /home/basti/phd/Figs/backgroundClusters/background_suppression_tile_map.tex
shell 7660> This is XeTeX, Version 3.141592653-2.6-0.999994 (TeX Live 2022) (preloaded format=xelatex)
shell 7660>  restricted \write18 enabled.
shell 7660> entering extended mode
shell 7660> (/home/basti/phd/Figs/backgroundClusters/background_suppression_tile_map.tex
shell 7660> LaTeX2e <2022-11-01> patch level 1
shell 7660> L3 programming layer <2022-12-17>
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/latex/standalone/standalone.cls
shell 7660> Document Class: standalone 2022/10/10 v1.3b Class to compile TeX sub-files stan
shell 7660> dalone
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/latex/tools/shellesc.sty)
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/iftex/ifluatex.sty
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/iftex/iftex.sty))
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/latex/xkeyval/xkeyval.sty
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/xkeyval/xkeyval.tex
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/xkeyval/xkvutils.tex
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/xkeyval/keyval.tex))))
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/latex/standalone/standalone.cfg)
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/latex/base/article.cls
shell 7660> Document Class: article 2022/07/02 v1.4n Standard LaTeX document class
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/latex/base/size10.clo))
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/frontendlayer/tikz.sty
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/basiclayer/pgf.sty
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/utilities/pgfrcs.sty
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgfutil-common.t
shell 7660> ex
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgfutil-common-l
shell 7660> ists.tex))
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgfutil-latex.de
shell 7660> f)
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgfrcs.code.tex
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/pgf.revision.tex)))
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/basiclayer/pgfcore.sty
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/latex/graphics/graphicx.sty
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/latex/graphics/graphics.sty
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/latex/graphics/trig.sty)
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/latex/graphics-cfg/graphics.cfg)
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/latex/graphics-def/xetex.def)))
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/systemlayer/pgfsys.sty
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/systemlayer/pgfsys.code.te
shell 7660> x
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgfkeys.code.tex
shell 7660> 
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgfkeysfiltered.
shell 7660> code.tex))
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/systemlayer/pgf.cfg)
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/systemlayer/pgfsys-xetex.d
shell 7660> ef
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/systemlayer/pgfsys-dvipdfm
shell 7660> x.def
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/systemlayer/pgfsys-common-
shell 7660> pdf.def))))
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/systemlayer/pgfsyssoftpath
shell 7660> .code.tex)
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/systemlayer/pgfsysprotocol
shell 7660> .code.tex)) (/home/basti/texlive/2022/texmf-dist/tex/latex/xcolor/xcolor.sty
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/latex/graphics-cfg/color.cfg)
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/latex/graphics/mathcolor.ltx))
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcore.code.te
shell 7660> x (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmath.code.tex
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathcalc.code.tex
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathutil.code.tex)
shell 7660> 
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathparser.code.te
shell 7660> x)
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.code
shell 7660> .tex
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.basi
shell 7660> c.code.tex)
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.trig
shell 7660> onometric.code.tex)
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.rand
shell 7660> om.code.tex)
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.comp
shell 7660> arison.code.tex)
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.base
shell 7660> .code.tex)
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.roun
shell 7660> d.code.tex)
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.misc
shell 7660> .code.tex)
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.inte
shell 7660> gerarithmetics.code.tex)))
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmathfloat.code.tex
shell 7660> )) (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfint.code.tex)
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepoints.c
shell 7660> ode.tex)
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepathcons
shell 7660> truct.code.tex)
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepathusag
shell 7660> e.code.tex)
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorescopes.c
shell 7660> ode.tex)
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcoregraphics
shell 7660> tate.code.tex)
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcoretransfor
shell 7660> mations.code.tex)
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorequick.co
shell 7660> de.tex)
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreobjects.
shell 7660> code.tex)
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepathproc
shell 7660> essing.code.tex)
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorearrows.c
shell 7660> ode.tex)
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreshade.co
shell 7660> de.tex)
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreimage.co
shell 7660> de.tex
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreexternal
shell 7660> .code.tex))
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorelayers.c
shell 7660> ode.tex)
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcoretranspar
shell 7660> ency.code.tex)
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepatterns
shell 7660> .code.tex)
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/basiclayer/pgfcorerdf.code
shell 7660> .tex)))
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/modules/pgfmoduleshapes.co
shell 7660> de.tex)
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/modules/pgfmoduleplot.code
shell 7660> .tex)
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/compatibility/pgfcomp-versio
shell 7660> n-0-65.sty)
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/compatibility/pgfcomp-versio
shell 7660> n-1-18.sty))
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/utilities/pgffor.sty
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/utilities/pgfkeys.sty
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgfkeys.code.tex
shell 7660> )) (/home/basti/texlive/2022/texmf-dist/tex/latex/pgf/math/pgfmath.sty
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmath.code.tex))
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/utilities/pgffor.code.tex
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/math/pgfmath.code.tex)))
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/frontendlayer/tikz/tikz.co
shell 7660> de.tex
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/libraries/pgflibraryplotha
shell 7660> ndlers.code.tex)
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/modules/pgfmodulematrix.co
shell 7660> de.tex)
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pgf/frontendlayer/tikz/librari
shell 7660> es/tikzlibrarytopaths.code.tex))))
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/latex/base/inputenc.sty
shell 7660> 
shell 7660> Package inputenc Warning: inputenc package ignored with utf8 based engines.
shell 7660> 
shell 7660> ) (/home/basti/texlive/2022/texmf-dist/tex/latex/geometry/geometry.sty
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/iftex/ifvtex.sty))
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/latex/unicode-math/unicode-math.sty
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/latex/l3kernel/expl3.sty
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/latex/l3backend/l3backend-xetex.def))
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/latex/unicode-math/unicode-math-xetex.
shell 7660> sty (/home/basti/texlive/2022/texmf-dist/tex/latex/l3packages/xparse/xparse.sty
shell 7660> )
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/latex/l3packages/l3keys2e/l3keys2e.sty
shell 7660> ) (/home/basti/texlive/2022/texmf-dist/tex/latex/fontspec/fontspec.sty
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/latex/fontspec/fontspec-xetex.sty
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/latex/base/fontenc.sty)
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/latex/fontspec/fontspec.cfg)))
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/latex/base/fix-cm.sty
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/latex/base/ts1enc.def))
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/latex/amsmath/amsmath.sty
shell 7660> For additional information on amsmath, use the `?' option.
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/latex/amsmath/amstext.sty
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/latex/amsmath/amsgen.sty))
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/latex/amsmath/amsbsy.sty)
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/latex/amsmath/amsopn.sty))
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/latex/unicode-math/unicode-math-table.
shell 7660> tex))) (/home/basti/texlive/2022/texmf-dist/tex/latex/siunitx/siunitx.sty
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/latex/translations/translations.sty
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/latex/etoolbox/etoolbox.sty)
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/pdftexcmds/pdftexcmds.sty
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/infwarerr/infwarerr.sty)
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/generic/ltxcmds/ltxcmds.sty)))
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/latex/tools/array.sty))
shell 7660> (/home/basti/phd/Figs/backgroundClusters/background_suppression_tile_map.aux)
shell 7660> *geometry* driver: auto-detecting
shell 7660> *geometry* detected driver: xetex
shell 7660> 
shell 7660> (/home/basti/texlive/2022/texmf-dist/tex/latex/translations/translations-basic-
shell 7660> dictionary-english.trsl)
shell 7660> Missing character: There is no ⁶ (U+2076) in font [lmroman17-regular]:mapping=t
shell 7660> ex-text;!
shell 7660> [1]
shell 7660> (/home/basti/phd/Figs/backgroundClusters/background_suppression_tile_map.aux) )
shell 7660> Output written on /home/basti/phd/Figs/backgroundClusters/background_suppressio
shell 7660> n_tile_map.pdf (1 page).
shell 7660> Transcript written on /home/basti/phd/Figs/backgroundClusters/background_suppre
shell 7660> ssion_tile_map.log.
:end:


*** Hough transformation as a cluster finding helper             :noexport:

At some point we considered whether a Hough transformation could be a
useful tool in the application of the outer GridPix ring as a
veto. The notes about this are here for completeness, as it showcases
an interesting idea leading to a dead end. To avoid others investing
time in it, we include it. Or rather, if you have plenty of experience
with Hough transformations and know what to make of this, then feel
free and use it as a starting inspiration!

- [X] Add our notes about attempts to use hough transformations.

**** Hough transformation for cluster finding                    :noexport:

I started a Hough trafo playground in
[[file:~/CastData/ExternCode/TimepixAnalysis/Tools/houghTrafoPlayground/houghTrafoPlayground.nim]].

Reading up on Hough transformations is a bit confusing, but what we
are doing for the moment:
- compute connecting lines between *each* point pair in a septem event
  (so for N hits, that's N² lines)
- for each line compute the slope and intersect

From this information we can look at different things:
1. the plots of all lines. Very messy, but gives an idea if the lines
   are correct.
2. a histogram of all found slopes
3. a histogram of all found intersects
4. a scatter plot of slopes vs. intersects

The "algorithm" to compute the Hough transformation is pretty dumb at
the moment:
#+begin_src nim
var xs = newSeqOfCap[int](x.len * x.len)
var ys = newSeqOfCap[int](x.len * x.len)
var ids = newSeqOfCap[string](x.len * x.len)
var slopes = newSeqOfCap[float](x.len * x.len)
var intersects = newSeqOfCap[float](x.len * x.len)
echo x
for i in 0 ..< x.len:
  for j in 0 ..< x.len:
    if i != j:                   # don't look at same point
      xs.add x[j]
      ys.add y[j]
      xs.add x[i]
      ys.add y[i]
      ids.add $i & "/" & $j
      ids.add $i & "/" & $j
      if xs[^1] - xs[^2] > 0:    # if same x, slope is inf
        let slope = (ys[^1] - ys[^2]).float / (xs[^1] - xs[^2]).float
        slopes.add slope
        # make sure both points yield same intercept
        doAssert abs(
          (y[j].float - slope * x[j].float) - (y[i].float - slope * x[i].float)
        ) < 1e-4 
        intersects.add (y[j].float - slope * x[j].float)
#+end_src

Let's look at a couple of examples:

***** Example 0 =septemEvent_run_272_event_95288.csv=

#+begin_center
#+CAPTION: The corresponding septem event.
#+NAME: fig_hough_septem_example_0
[[~/org/Figs/statusAndProgress/houghTrafo/plot_septem_run_272_event_95288.pdf]]
#+end_center

#+begin_center
#+CAPTION: All connecting lines between the pixel pairs.
#+NAME: fig_hough_lines_example_0
[[~/org/Figs/statusAndProgress/houghTrafo/lines_run_272_event_95288.png]]
#+end_center

#+begin_center
#+CAPTION: Histogram of all slopes
#+NAME: fig_hough_histo_slopes_example_0
[[~/org/Figs/statusAndProgress/houghTrafo/histo_slopes_run_272_event_95288.pdf]]
#+end_center

#+begin_center
#+CAPTION: Histogram of all intersects
#+NAME: fig_hough_histo_intersects_example_0
[[~/org/Figs/statusAndProgress/houghTrafo/histo_intersects_run_272_event_95288.pdf]]
#+end_center

#+begin_center
#+CAPTION: Scatter plot of slopes vs intersects.
#+NAME: fig_hough_slope_vs_intersects_example_0
[[~/org/Figs/statusAndProgress/houghTrafo/slope_vs_intersects_run_272_event_95288.png]]
#+end_center

***** Example 1 =septemEvent_run_265_event_1662.csv=

#+begin_center
#+CAPTION: The corresponding septem event.
#+NAME: fig_hough_septem_example_1
[[~/org/Figs/statusAndProgress/houghTrafo/plot_septem_run_265_event_1662.pdf]]
#+end_center

#+begin_center
#+CAPTION: All connecting lines between the pixel pairs.
#+NAME: fig_hough_lines_example_1
[[~/org/Figs/statusAndProgress/houghTrafo/lines_run_265_event_1662.png]]
#+end_center

#+begin_center
#+CAPTION: Histogram of all slopes
#+NAME: fig_hough_histo_slopes_example_1
[[~/org/Figs/statusAndProgress/houghTrafo/histo_slopes_run_265_event_1662.pdf]]
#+end_center

#+begin_center
#+CAPTION: Histogram of all intersects
#+NAME: fig_hough_histo_intersects_example_1
[[~/org/Figs/statusAndProgress/houghTrafo/histo_intersects_run_265_event_1662.pdf]]
#+end_center

#+begin_center
#+CAPTION: Scatter plot of slopes vs intersects.
#+NAME: fig_hough_slope_vs_intersects_example_1
[[~/org/Figs/statusAndProgress/houghTrafo/slope_vs_intersects_run_265_event_1662.png]]
#+end_center

***** Example 2 =septemEvent_run_261_event_809.csv=

#+begin_center
#+CAPTION: The corresponding septem event.
#+NAME: fig_hough_septem_example_2
[[~/org/Figs/statusAndProgress/houghTrafo/plot_septem_run_261_event_809.pdf]]
#+end_center

#+begin_center
#+CAPTION: All connecting lines between the pixel pairs.
#+NAME: fig_hough_lines_example_2
[[~/org/Figs/statusAndProgress/houghTrafo/lines_run_261_event_809.png]]
#+end_center

#+begin_center
#+CAPTION: Histogram of all slopes
#+NAME: fig_hough_histo_slopes_example_2
[[~/org/Figs/statusAndProgress/houghTrafo/histo_slopes_run_261_event_809.pdf]]
#+end_center

#+begin_center
#+CAPTION: Histogram of all intersects
#+NAME: fig_hough_histo_intersects_example_2
[[~/org/Figs/statusAndProgress/houghTrafo/histo_intersects_run_261_event_809.pdf]]
#+end_center

#+begin_center
#+CAPTION: Scatter plot of slopes vs intersects.
#+NAME: fig_hough_slope_vs_intersects_example_2
[[~/org/Figs/statusAndProgress/houghTrafo/slope_vs_intersects_run_261_event_809.png]]
#+end_center

***** Example 3 =septemEvent_run_291_event_31480.csv=

#+begin_center
#+CAPTION: The corresponding septem event.
#+NAME: fig_hough_septem_example_3
[[~/org/Figs/statusAndProgress/houghTrafo/plot_septem_run_291_event_31480.pdf]]
#+end_center

#+begin_center
#+CAPTION: All connecting lines between the pixel pairs.
#+NAME: fig_hough_lines_example_3
[[~/org/Figs/statusAndProgress/houghTrafo/lines_run_291_event_31480.png]]
#+end_center

#+begin_center
#+CAPTION: Histogram of all slopes
#+NAME: fig_hough_histo_slopes_example_3
[[~/org/Figs/statusAndProgress/houghTrafo/histo_slopes_run_291_event_31480.pdf]]
#+end_center

#+begin_center
#+CAPTION: Histogram of all intersects
#+NAME: fig_hough_histo_intersects_example_3
[[~/org/Figs/statusAndProgress/houghTrafo/histo_intersects_run_291_event_31480.pdf]]
#+end_center

#+begin_center
#+CAPTION: Scatter plot of slopes vs intersects.
#+NAME: fig_hough_slope_vs_intersects_example_3
[[~/org/Figs/statusAndProgress/houghTrafo/slope_vs_intersects_run_291_event_31480.png]]
#+end_center

***** Example 4 =septemEvent_run_306_event_4340.csv=

#+begin_center
#+CAPTION: The corresponding septem event.
#+NAME: fig_hough_septem_example_4
[[~/org/Figs/statusAndProgress/houghTrafo/plot_septem_run_306_event_4340.pdf]]
#+end_center

#+begin_center
#+CAPTION: All connecting lines between the pixel pairs.
#+NAME: fig_hough_lines_example_4
[[~/org/Figs/statusAndProgress/houghTrafo/lines_run_306_event_4340.png]]
#+end_center

#+begin_center
#+CAPTION: Histogram of all slopes
#+NAME: fig_hough_histo_slopes_example_4
[[~/org/Figs/statusAndProgress/houghTrafo/histo_slopes_run_306_event_4340.pdf]]
#+end_center

#+begin_center
#+CAPTION: Histogram of all intersects
#+NAME: fig_hough_histo_intersects_example_4
[[~/org/Figs/statusAndProgress/houghTrafo/histo_intersects_run_306_event_4340.pdf]]
#+end_center

#+begin_center
#+CAPTION: Scatter plot of slopes vs intersects.
#+NAME: fig_hough_slope_vs_intersects_example_4
[[~/org/Figs/statusAndProgress/houghTrafo/slope_vs_intersects_run_306_event_4340.png]]
#+end_center

***** Conclusion

The hough transformation produces too much data that is too hard to
interpret in the context of our goal. It doesn't actually help us a
lot here, so we'll drop the pursuit of that.



*** Estimating the random coincidence rate of the septem & line veto [/]
:PROPERTIES:
:CUSTOM_ID: sec:background:estimate_veto_efficiency
:END:


- [ ] *NEED to explain that eccentricity line veto cutoff is not used,
  but tested. Also NEED to obviously give the numbers for both setups.*


- [ ] *NAME THE ABSOLUTE EFFICIENCIES OF EACH SETUP*

- [ ] *IMPORTANT:*
  The random coincidence we calculate here changes not only the dead
  time for the tracking time, but also for the background rate! As
  such we need to regulate both!

- [ ] *REWRITE THIS!*
  -> Important parts are that background rates are only interesting if
  one understands the associated efficiencies. So need to explain
  that. This part should become :noexport:, but a shortened simpler
  version of this should remain.


One potential issue with the septem and line veto is that the shutter
times we ran with at CAST are very long ($> \SI{2}{s}$), but only the
center chip is triggered by the FADC. This means that the outer chips
can record cluster data that is not correlated to what the center chip
sees. When applying one of these two vetoes the chance for random
coincidence might be non negligible.

In order to estimate this we can create fake events from real clusters
on the center chip and clusters for the outer chips using different
events. This way we bootstrap a larger number of events than otherwise
available and knowing that the geometric data cannot be
correlated. Any vetoing in these cases therefore *must* be a random
coincidence.

As the ~likelihood~ tool already uses effectively an index to map the
cluster indices for each chip to their respective event number, we've
implemented this there (~--estimateRandomCoinc~) by rewriting the
index.

It is a good idea to also run it together with the ~--plotseptem~
option to actually see some events and verify with your own eyes that
the events are actually "correct" (i.e. not the original ones). You
will note that there are many events that "clearly" look as if the
bootstrapping is not working correctly, because they look way too much
as if they are "obviously correlated". To give yourself a better sense
that this is indeed just coincidence, you can run the tool with the
~--estFixedEvents~ option, which bootstraps events using a fixed
cluster in the center for each run. Checking out the event displays of
those is a convincing affair that unfortunately random coincidences
are even convincing to our own eyes.

#+begin_src sh
likelihood -f ~/CastData/data/DataRuns2018_Reco.h5 \
           --h5out /tmp/lhood_2018_crAll_80eff_septem_fake.h5 \
           --region crAll --cdlYear 2018 \
           --cdlFile ~/CastData/data/CDL_2019/calibration-cdl-2018.h5 \
           --septemveto --estimateRandomCoinc
#+end_src

which writes the file ~/tmp/septem_fake_veto.txt~, which for this case
is found
[[file:~/org/resources/septem_veto_random_coincidences/septem_veto_before_after_fake_events_septem_old.txt]]
(note: updated numbers from latest state of code is the same file
without ~_old~ suffix)

Mean value of and fraction (from script in next section):
File: /home/basti/org/resources/septem_veto_random_coincidences/septem_veto_before_after_fake_events_septem.txt
	Mean output = 1674.705882352941
	Fraction of events left = 0.8373529411764704

From this file the method seems to remove typically a bit more than
300 out of 2000 bootstrapped fake events. This seems to imply a random
coincidence rate of about 17% (or effectively a reduction of further
17% in efficiency / 17% increase in dead time).

Of course this does not even include the line veto, which will drop it
further. Before we combine both of them, let's run it with the line
veto _alone_:
#+begin_src sh
likelihood -f ~/CastData/data/DataRuns2018_Reco.h5 \
           --h5out /tmp/lhood_2018_crAll_80eff_line_fake.h5 \
           --region crAll --cdlYear 2018 \
           --cdlFile ~/CastData/data/CDL_2019/calibration-cdl-2018.h5 \
           --lineveto --estimateRandomCoinc
#+end_src
this results in:
[[~/org/resources/septem_veto_random_coincidences/septem_veto_before_after_fake_events_line.txt]]

Mean value of:
File: /home/basti/org/resources/septem_veto_random_coincidences/septem_veto_before_after_fake_events_line.txt
	Mean output = 1708.382352941177
	Fraction of events left = 0.8541911764705882

And finally both together:
#+begin_src sh
likelihood -f ~/CastData/data/DataRuns2018_Reco.h5 \
           --h5out /tmp/lhood_2018_crAll_80eff_septem_line_fake.h5 \
           --region crAll --cdlYear 2018 \
           --cdlFile ~/CastData/data/CDL_2019/calibration-cdl-2018.h5 \
           --septemveto --lineveto --estimateRandomCoinc
#+end_src

which generated the following output:
[[file:~/org/resources/septem_veto_random_coincidences/septem_veto_before_after_fake_events_septem_line.txt]]

Mean value of: 
File: /home/basti/org/resources/septem_veto_random_coincidences/septem_veto_before_after_fake_events_septem_line.txt
	Mean output = 1573.676470588235
	Fraction of events left = 0.7868382352941178

This comes out to a fraction of 78.68% of the events left after
running the vetoes on our bootstrapped fake events. Combining it with
a software efficiency of ε = 80% the total combined efficiency then
would be $ε_\text{total} = 0.8 · 0.7868 = 0.629$, so about 63%.


Finally now let's prepare some event displays for the case of using
different center clusters and using the same ones. We run the
~likelihood~ tool with the ~--plotSeptem~ option and stop the program
after we have enough plots.
In this context note the energy cut range for the ~--plotseptem~
option (by default set to 5 keV), adjustable by the
~PLOT_SEPTEM_E_CUTOFF~ environment variable.

First with different center clusters:
#+begin_src sh
PLOT_SEPTEM_E_CUTOFF=10.0 likelihood \
                          -f ~/CastData/data/DataRuns2018_Reco.h5 \
                          --h5out /tmp/dummy.h5 \
                          --region crAll --cdlYear 2018 \
                          --cdlFile ~/CastData/data/CDL_2019/calibration-cdl-2018.h5 \
                          --septemveto --lineveto --estimateRandomCoinc --plotseptem
#+end_src
which are wrapped up using ~pdfunite~ and stored in:
[[file:Figs/background/estimateSeptemVetoRandomCoinc/fake_events_septem_line_veto_all_outer_events.pdf]]

and now with fixed clusters:
#+begin_src sh
PLOT_SEPTEM_E_CUTOFF=10.0 likelihood \
                          -f ~/CastData/data/DataRuns2018_Reco.h5 \
                          --h5out /tmp/dummy.h5 \
                          --region crAll --cdlYear 2018 \
                          --cdlFile ~/CastData/data/CDL_2019/calibration-cdl-2018.h5 \
                          --septemveto --lineveto --estimateRandomCoinc --estFixedEvent --plotseptem
#+end_src
(Note that the cluster that is chosen can be set using
~SEPTEM_FAKE_FIXED_CLUSTER~ to a different index, by default it just
uses ~5~).
These events are here:
[[file:Figs/background/estimateSeptemVetoRandomCoinc/fake_events_fixed_cluster_septem_line_veto_all_outer_events.pdf]]


Combining different options of the line veto and the eccentricity cut
for the line veto, as well as applying both the septem and the line
veto for real data as well as fake bootstrapped data we can make an
informed decision about the settings to use. At the same time get an
understanding for the real dead time we
introduce. Fig. [[fig:background:fraction_passing_line_veto_ecc_cut]]
shows precisely such data. We can see that the fraction that passes
the veto setups (y axis) drops the further we go towards a low
eccentricity cut (x axis). For the real data (~Real~ suffix in the
legend) the drop is _faster_ than for fake boostrapped data (~Fake~ suffix
in the legend) however, which means that we can use the lowest
eccentricity cut as we like (effectively disabling the cut at
$ε_\text{cut} = 1.0$). The exact choice between the purple / green
pair (line veto including all clusters, even the one containing the
original cluster) and the turquoise / blue pair (septem veto + line
veto with only those clusters that do not contain the original; those
are covered by the septem veto) is not entirely clear. Both will be
investigated for their effect on the expected limit. The important
point is that the fake data allows us to estimate the random
coincidence rate, which needs to be treated as an additional dead time
during background _and_ solar tracking time. A lower background may or
may not be beneficial, compared to a higher dead time.

#+CAPTION: Fraction of events in Run-3 data (green), which pass (i.e. not rejected) the line
#+CAPTION: veto depending on the eccentricity cut used, which decides how eccentric a
#+CAPTION: cluster needs to be in order to be used for the veto. The purple points are
#+CAPTION: using fake bootstrapped data from real clusters passing the $\ln\mathcal{L}$ cut
#+CAPTION: together with real outer GridPix data from *other* events. The fraction of events
#+CAPTION: being vetoed in the latter is a measure for the random coincidence rate. 
#+NAME: fig:background:fraction_passing_line_veto_ecc_cut
[[~/phd/Figs/background/estimateSeptemVetoRandomCoinc/fraction_passing_line_veto_ecc_cut.pdf]]


**** TODO Rewrite the whole estimation to a proper program [/]

*IMPORTANT*

That program should call ~likelihood~ alone, and ~likelihood~ needs to
be rewritten such that it outputs the septem random coincidence (or
real removal) into the H5 output file. Maybe just add a type that
stores the information which we serialize.
With the serialized info about the veto settings we can then
reconstruct in code what is what.

Or possibly better if the output is written to a separate file such
that we don't store all the cluster data.

Anyhow, then rewrite the code snippet in the section below that prints
the information about the random coincidence rates and creates the
plot.



**** Run a whole bunch more cases

The below is running now <2023-02-10 Fri 01:43>.
Still running as of <2023-02-10 Fri 11:55>, damn this is slow.
- [X] *INVESTIGATE PERFORMANCE AFTER IT'S DONE*

#+begin_src nim :tangle code/analyze_line_veto_different_ecc_cutoff.nim
import shell, strutils, os

#let vals = @[1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0]
#let vals = @[1.0, 1.1]
let vals = @[1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0]
#let vetoes = @["--lineveto", "--lineveto --estimateRandomCoinc"]
let vetoes = @["--septemveto --lineveto", "--septemveto --lineveto --estimateRandomCoinc"]

## XXX: ADD CODE DIFFERENTIATING SEPTEM + LINE & LINE ONLY IN NAMES AS WELL!
#const lineVeto = "lvRegular"
const lineVeto = "lvRegularNoHLC"
let cmd = """
LINE_VETO_KIND=$# \
    ECC_LINE_VETO_CUT=$# \
    USE_REAL_LAYOUT=true \
    likelihood -f ~/CastData/data/DataRuns2018_Reco.h5 \
    --h5out /t/lhood_2018_crAll_80eff_septem_line_ecc_cutoff_$#_$#_real_layout$#.h5 \
    --region crAll --cdlYear 2018 \
    --cdlFile ~/CastData/data/CDL_2019/calibration-cdl-2018.h5 $#
"""
proc toName(veto: string): string = (if "estimateRandomCoinc" in veto: "_fake_events" else: "")
for val in vals:
  for veto in vetoes:
    let final = cmd % [ lineVeto, $val, $val, lineVeto, toName(veto),
                        $veto ]
    let (res, err) = shellVerbose:
      one:
        cd /tmp
        ($final)
    writeFile("/tmp/logL_output_septem_line_ecc_cutoff_$#_$#_real_layout$#.txt" % [$val, lineVeto, toName(veto)], res)
    let outpath = "/home/basti/org/resources/septem_veto_random_coincidences/autoGen/"
    let outfile = "septem_veto_before_after_septem_line_ecc_cutoff_$#_$#_real_layout$#.txt" % [$val, lineVeto, toName(veto)]
    copyFile("/tmp/septem_veto_before_after.txt", outpath / outfile)
    removeFile("/tmp/septem_veto_before_after.txt") # remove file to not append more and more to file
#+end_src

It has finally finished some time before <2023-02-10 Fri 20:02>. Holy
moly how slow.

We will keep the generated ~lhood_*~ and ~logL_output_*~ files in
[[file:~/org/resources/septem_veto_random_coincidences/autoGen/]] together
with the ~septem_veto_befor_after_*~ files.

See the code in one of the next sections for the 'analysis' of this
dataset.

- [X] *RERUN THE ABOVE AFTER LINE VETO BUGFIX & PERF IMPROVEMENTS*

**** Number of events removed in real usage

- [ ] *MAYBE EXTEND CODE SNIPPET ABOVE TO ALLOW CHOOSING BETWEEN ε_cut
  ANALYSIS AND REAL FRACTIONS*
  

As a reference let's quickly run the code also for the normal use case
where we don't do any bootstrapping:

#+begin_src sh
likelihood \
  -f ~/CastData/data/DataRuns2018_Reco.h5 \
  --h5out /tmp/dummy_real.h5 \
  --region crAll --cdlYear 2018 \
  --cdlFile ~/CastData/data/CDL_2019/calibration-cdl-2018.h5 \
  --septemveto
#+end_src

which results in
[[file:~/org/resources/septem_veto_random_coincidences/septem_veto_before_after_only_septem.txt]]

Next the line veto alone:
#+begin_src sh
likelihood \
  -f ~/CastData/data/DataRuns2018_Reco.h5 \
  --h5out /tmp/dummy_real.h5 \
  --region crAll --cdlYear 2018 \
  --cdlFile ~/CastData/data/CDL_2019/calibration-cdl-2018.h5 \
  --lineveto
#+end_src

which results in:
[[file:~/org/resources/septem_veto_random_coincidences/septem_veto_before_after_only_line.txt]]

And finally both together:
#+begin_src sh
likelihood \
  -f ~/CastData/data/DataRuns2018_Reco.h5 \
  --h5out /tmp/dummy_real_2.h5 \
  --region crAll --cdlYear 2018 \
  --cdlFile ~/CastData/data/CDL_2019/calibration-cdl-2018.h5 \
  --septemveto --lineveto 
#+end_src

and this finally yields:

[[file:~/org/resources/septem_veto_random_coincidences/septem_veto_before_after_septem_line.txt]]


And further for reference let's compute the fake rate when only using the
septem veto (as we have no eccentricity dependence, hence a single
value):
#+begin_src sh
likelihood -f ~/CastData/data/DataRuns2018_Reco.h5 \
  --h5out /tmp/lhood_2018_crAll_80eff_septem_real_layout.h5 \
  --region crAll \
  --cdlYear 2018 \
  --cdlFile ~/CastData/data/CDL_2019/calibration-cdl-2018.h5 \
  --septemveto \
  --estimateRandomCoinc
#+end_src


Run the line veto with new features:
- real septemboard layout
- eccentricity cut off for tracks participating (ecc > 1.6)
#+begin_src sh
LINE_VETO_KIND=lvRegularNoHLC \
    ECC_LINE_VETO_CUT=1.6 \
    USE_REAL_LAYOUT=true \
    likelihood -f ~/CastData/data/DataRuns2018_Reco.h5 \
    --h5out /tmp/lhood_2018_crAll_80eff_line_ecc_cutof_1.6_real_layout.h5 \
    --region crAll \
    --cdlYear 2018 \
    --cdlFile ~/CastData/data/CDL_2019/calibration-cdl-2018.h5 \
    --lineveto
#+end_src

- [ ] *WE SHOULD REALLY LOOK INTO RUNNING THE LINE VETO ONLY USING
  DIFFERENT ε CUTOFFS!*
  -> Then compare the real application with the fake bootstrap
  application and see if there is a sweet spot in terms of S/N.

Let's calculate the fraction in all cases:
#+begin_src nim :results drawer
import strutils
let files = @["/home/basti/org/resources/septem_veto_random_coincidences/septem_veto_before_after_only_septem.txt",
              "/home/basti/org/resources/septem_veto_random_coincidences/septem_veto_before_after_only_line.txt",              
              "/home/basti/org/resources/septem_veto_random_coincidences/septem_veto_before_after_septem_line.txt",
              "/home/basti/org/resources/septem_veto_random_coincidences/septem_veto_before_after_fake_events_septem.txt",
              "/home/basti/org/resources/septem_veto_random_coincidences/septem_veto_before_after_fake_events_line.txt",              
              "/home/basti/org/resources/septem_veto_random_coincidences/septem_veto_before_after_fake_events_septem_line.txt"]
proc parseFile(fname: string): float =
  var lines = fname.readFile.strip.splitLines()
  var line = 0
  var numRuns = 0
  var outputs = 0
  # if file has more than 68 lines, remove everything before, as that means
  # those were from a previous run
  if lines.len > 68:
    lines = lines[^68 .. ^1]
    doAssert lines.len == 68
  while line < lines.len:
    if lines[line].len == 0: break
    # parse input
    # `Septem events before: 1069 (L,F) = (false, false)`
    let input = lines[line].split(':')[1].strip.split()[0].parseInt
    # parse output
    # `Septem events after fake cut: 137`
    inc line
    let output = lines[line].split(':')[1].strip.parseInt
    result += output.float / input.float
    outputs += output
    inc numRuns
    inc line
  echo "\tMean output = ", outputs.float / numRuns.float
  result = result / numRuns.float

# first the predefined files:  
for f in files:
  echo "File: ", f
  echo "\tFraction of events left = ", parseFile(f)

# now all files in our eccentricity cut run directory
const path = "/home/basti/org/resources/septem_veto_random_coincidences/autoGen/"
import std / [os, parseutils, strutils]
import ggplotnim
proc parseEccentricityCutoff(f: string): float =
  let str = "ecc_cutoff_"
  let startIdx = find(f, str) + str.len
  var res = ""
  let stopIdx = parseUntil(f, res, until = "_", start = startIdx)
  echo res
  result = parseFloat(res)

proc determineType(f: string): string =
  ## I'm sorry for this. :) 
  if "only_line_ecc" in f:
    result.add "Line"
  elif "septem_line_ecc" in f:
    result.add "SeptemLine"
  else:
    doAssert false, "What? " & $f
  if "lvRegularNoHLC" in f:
    result.add "lvRegularNoHLC"
  elif "lvRegular" in f:
    result.add "lvRegular"
  else: # also lvRegularNoHLC, could use else above, but clearer this way. Files 
    result.add "lvRegularNoHLC" # without veto kind are older, therefore no HLC
  if "_fake_events.txt" in f:
    result.add "Fake"
  else:
    result.add "Real"

var df = newDataFrame()
# walk all files and determine the type
for f in walkFiles(path / "septem_veto_before_after*.txt"):
  echo "File: ", f
  let frac = parseFile(f)
  let eccCut = parseEccentricityCutoff(f)
  let typ = determineType(f)
  echo "\tFraction of events left = ", frac
  df.add toDf({"Type" : typ, "ε_cut" : eccCut, "FractionPass" : frac})

df.writeCsv("/home/basti/org/resources/septem_line_random_coincidences_ecc_cut.csv", precision = 8)  
ggplot(df, aes("ε_cut", "FractionPass", color = "Type")) +
  geom_point() +
  ggtitle("Fraction of events passing line veto based on ε cutoff") +
  margin(right = 9) + 
  ggsave("Figs/background/estimateSeptemVetoRandomCoinc/fraction_passing_line_veto_ecc_cut.pdf",
         width = 800, height = 480)
  #ggsave("/tmp/fraction_passing_line_veto_ecc_cut.pdf", width = 800, height = 480)

## XXX: we probably don't need the following plot for the real data, as the eccentricity
## cut does not cause anything to get worse at lower values. Real improvement better than
## fake coincidence rate.
#df = df.spread("Type", "FractionPass").mutate(f{float: "Ratio" ~ `Real` / `Fake`})
#ggplot(df, aes("ε_cut", "Ratio")) +
#  geom_point() +
#  ggtitle("Ratio of fraction of events passing line veto real/fake based on ε cutoff") + 
#  #ggsave("Figs/background/estimateSeptemVetoRandomCoinc/ratio_real_fake_fraction_passing_line_veto_ecc_cut.pdf")
#  ggsave("/tmp/ratio_real_fake_fraction_passing_line_veto_ecc_cut.pdf")
#+end_src

#+RESULTS:
:results:
File: /home/basti/org/resources/septem_veto_random_coincidences/septem_veto_before_after_only_septem.txt
	Mean output = 129.6176470588235
	Fraction of events left = 0.1482137110344671
File: /home/basti/org/resources/septem_veto_random_coincidences/septem_veto_before_after_only_line.txt
	Mean output = 279.9117647058824
	Fraction of events left = 0.3226213387764036
File: /home/basti/org/resources/septem_veto_random_coincidences/septem_veto_before_after_septem_line.txt
	Mean output = 86.79411764705883
	Fraction of events left = 0.09919758241761836
File: /home/basti/org/resources/septem_veto_random_coincidences/septem_veto_before_after_fake_events_septem.txt
	Mean output = 1568.205882352941
	Fraction of events left = 0.7841029411764704
File: /home/basti/org/resources/septem_veto_random_coincidences/septem_veto_before_after_fake_events_line.txt
	Mean output = 1708.382352941177
	Fraction of events left = 0.8541911764705882
File: /home/basti/org/resources/septem_veto_random_coincidences/septem_veto_before_after_fake_events_septem_line.txt
	Mean output = 1573.676470588235
	Fraction of events left = 0.7868382352941178
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_only_line_ecc_cutoff_1.0_lvRegular_real_layout.txt
	Mean output = 193.3235294117647
1.0
	Fraction of events left = 0.2213710741366144
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_only_line_ecc_cutoff_1.0_lvRegular_real_layout_fake_events.txt
	Mean output = 1720.35294117647
1.0
	Fraction of events left = 0.8601764705882353
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_only_line_ecc_cutoff_1.0_real_layout.txt
	Mean output = 610.3529411764706
1.0
	Fraction of events left = 0.690861976619262
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_only_line_ecc_cutoff_1.0_real_layout_fake_events.txt
	Mean output = 1834.0
1.0
	Fraction of events left = 0.9170000000000001
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_only_line_ecc_cutoff_1.1_lvRegular_real_layout.txt
	Mean output = 232.0294117647059
1.1
	Fraction of events left = 0.2658086845496733
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_only_line_ecc_cutoff_1.1_lvRegular_real_layout_fake_events.txt
	Mean output = 1740.617647058823
1.1
	Fraction of events left = 0.8703088235294119
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_only_line_ecc_cutoff_1.1_real_layout.txt
	Mean output = 625.7058823529412
1.1
	Fraction of events left = 0.7079137296253094
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_only_line_ecc_cutoff_1.1_real_layout_fake_events.txt
	Mean output = 1848.705882352941
1.1
	Fraction of events left = 0.9243529411764705
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_only_line_ecc_cutoff_1.2_lvRegular_real_layout.txt
	Mean output = 267.1176470588235
1.2
	Fraction of events left = 0.3053849927113212
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_only_line_ecc_cutoff_1.2_lvRegular_real_layout_fake_events.txt
	Mean output = 1758.85294117647
1.2
	Fraction of events left = 0.8794264705882355
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_only_line_ecc_cutoff_1.2_real_layout.txt
	Mean output = 639.1176470588235
1.2
	Fraction of events left = 0.7226167303002865
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_only_line_ecc_cutoff_1.2_real_layout_fake_events.txt
	Mean output = 1861.35294117647
1.2
	Fraction of events left = 0.9306764705882352
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_only_line_ecc_cutoff_1.3_lvRegular_real_layout.txt
	Mean output = 301.6764705882353
1.3
	Fraction of events left = 0.3454157240905325
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_only_line_ecc_cutoff_1.3_lvRegular_real_layout_fake_events.txt
	Mean output = 1774.705882352941
1.3
	Fraction of events left = 0.8873529411764708
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_only_line_ecc_cutoff_1.3_real_layout.txt
	Mean output = 653.2941176470588
1.3
	Fraction of events left = 0.7387264181345766
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_only_line_ecc_cutoff_1.3_real_layout_fake_events.txt
	Mean output = 1872.294117647059
1.3
	Fraction of events left = 0.936147058823529
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_only_line_ecc_cutoff_1.4_lvRegular_real_layout.txt
	Mean output = 334.2058823529412
1.4
	Fraction of events left = 0.3819518596290207
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_only_line_ecc_cutoff_1.4_lvRegular_real_layout_fake_events.txt
	Mean output = 1789.264705882353
1.4
	Fraction of events left = 0.8946323529411766
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_only_line_ecc_cutoff_1.4_real_layout.txt
	Mean output = 666.0588235294117
1.4
	Fraction of events left = 0.7536070159704531
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_only_line_ecc_cutoff_1.4_real_layout_fake_events.txt
	Mean output = 1881.705882352941
1.4
	Fraction of events left = 0.9408529411764706
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_only_line_ecc_cutoff_1.5_lvRegular_real_layout.txt
	Mean output = 364.1764705882353
1.5
	Fraction of events left = 0.4153775763953914
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_only_line_ecc_cutoff_1.5_lvRegular_real_layout_fake_events.txt
	Mean output = 1802.088235294118
1.5
	Fraction of events left = 0.9010441176470585
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_only_line_ecc_cutoff_1.5_real_layout.txt
	Mean output = 677.5294117647059
1.5
	Fraction of events left = 0.7674234785251015
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_only_line_ecc_cutoff_1.5_real_layout_fake_events.txt
	Mean output = 1890.5
1.5
	Fraction of events left = 0.9452499999999998
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_only_line_ecc_cutoff_1.6_lvRegular_real_layout.txt
	Mean output = 392.3235294117647
1.6
	Fraction of events left = 0.4465736613326556
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_only_line_ecc_cutoff_1.6_lvRegular_real_layout_fake_events.txt
	Mean output = 1815.176470588235
1.6
	Fraction of events left = 0.9075882352941179
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_only_line_ecc_cutoff_1.6_real_layout.txt
	Mean output = 688.5
1.6
	Fraction of events left = 0.7796355368668799
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_only_line_ecc_cutoff_1.6_real_layout_fake_events.txt
	Mean output = 1898.676470588235
1.6
	Fraction of events left = 0.9493382352941175
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_only_line_ecc_cutoff_1.7_lvRegular_real_layout.txt
	Mean output = 417.6470588235294
1.7
	Fraction of events left = 0.4752128436833915
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_only_line_ecc_cutoff_1.7_lvRegular_real_layout_fake_events.txt
	Mean output = 1827.294117647059
1.7
	Fraction of events left = 0.9136470588235294
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_only_line_ecc_cutoff_1.7_real_layout.txt
	Mean output = 698.7058823529412
1.7
	Fraction of events left = 0.7910478112082485
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_only_line_ecc_cutoff_1.7_real_layout_fake_events.txt
	Mean output = 1905.764705882353
1.7
	Fraction of events left = 0.9528823529411765
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_only_line_ecc_cutoff_1.8_lvRegular_real_layout.txt
	Mean output = 441.5882352941176
1.8
	Fraction of events left = 0.5016402435934247
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_only_line_ecc_cutoff_1.8_lvRegular_real_layout_fake_events.txt
	Mean output = 1838.558823529412
1.8
	Fraction of events left = 0.9192794117647062
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_only_line_ecc_cutoff_1.8_real_layout.txt
	Mean output = 708.9705882352941
1.8
	Fraction of events left = 0.8019786834892618
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_only_line_ecc_cutoff_1.8_real_layout_fake_events.txt
	Mean output = 1912.382352941177
1.8
	Fraction of events left = 0.956191176470588
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_only_line_ecc_cutoff_1.9_lvRegular_real_layout.txt
	Mean output = 464.5294117647059
1.9
	Fraction of events left = 0.5285049560731815
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_only_line_ecc_cutoff_1.9_lvRegular_real_layout_fake_events.txt
	Mean output = 1848.5
1.9
	Fraction of events left = 0.9242499999999999
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_only_line_ecc_cutoff_1.9_real_layout.txt
	Mean output = 718.7352941176471
1.9
	Fraction of events left = 0.813399253104142
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_only_line_ecc_cutoff_1.9_real_layout_fake_events.txt
	Mean output = 1917.411764705882
1.9
	Fraction of events left = 0.9587058823529411
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_only_line_ecc_cutoff_2.0_lvRegular_real_layout.txt
	Mean output = 487.0294117647059
2.0
	Fraction of events left = 0.5530554088044699
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_only_line_ecc_cutoff_2.0_lvRegular_real_layout_fake_events.txt
	Mean output = 1857.176470588235
2.0
	Fraction of events left = 0.9285882352941174
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_only_line_ecc_cutoff_2.0_real_layout.txt
	Mean output = 727.7352941176471
2.0
	Fraction of events left = 0.8236163260425843
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_only_line_ecc_cutoff_2.0_real_layout_fake_events.txt
	Mean output = 1922.911764705882
2.0
	Fraction of events left = 0.961455882352941
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_septem_line_ecc_cutoff_1.0_lvRegularNoHLC_real_layout.txt
	Mean output = 121.5
1.0
	Fraction of events left = 0.1372058585160641
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_septem_line_ecc_cutoff_1.0_lvRegularNoHLC_real_layout_fake_events.txt
	Mean output = 1465.029411764706
1.0
	Fraction of events left = 0.732514705882353
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_septem_line_ecc_cutoff_1.1_lvRegularNoHLC_real_layout.txt
	Mean output = 126.8529411764706
1.1
	Fraction of events left = 0.1432055704027816
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_septem_line_ecc_cutoff_1.1_lvRegularNoHLC_real_layout_fake_events.txt
	Mean output = 1474.617647058823
1.1
	Fraction of events left = 0.7373088235294116
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_septem_line_ecc_cutoff_1.2_lvRegularNoHLC_real_layout.txt
	Mean output = 132.5588235294118
1.2
	Fraction of events left = 0.1493895345390256
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_septem_line_ecc_cutoff_1.2_lvRegularNoHLC_real_layout_fake_events.txt
	Mean output = 1483.029411764706
1.2
	Fraction of events left = 0.7415147058823529
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_septem_line_ecc_cutoff_1.3_lvRegularNoHLC_real_layout.txt
	Mean output = 137.4705882352941
1.3
	Fraction of events left = 0.1551740781879195
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_septem_line_ecc_cutoff_1.3_lvRegularNoHLC_real_layout_fake_events.txt
	Mean output = 1489.85294117647
1.3
	Fraction of events left = 0.7449264705882355
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_septem_line_ecc_cutoff_1.4_lvRegularNoHLC_real_layout.txt
	Mean output = 142.1176470588235
1.4
	Fraction of events left = 0.1605457614353923
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_septem_line_ecc_cutoff_1.4_lvRegularNoHLC_real_layout_fake_events.txt
	Mean output = 1496.323529411765
1.4
	Fraction of events left = 0.7481617647058825
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_septem_line_ecc_cutoff_1.5_lvRegularNoHLC_real_layout.txt
	Mean output = 146.2058823529412
1.5
	Fraction of events left = 0.1649403056130558
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_septem_line_ecc_cutoff_1.5_lvRegularNoHLC_real_layout_fake_events.txt
	Mean output = 1502.470588235294
1.5
	Fraction of events left = 0.751235294117647
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_septem_line_ecc_cutoff_1.6_lvRegularNoHLC_real_layout.txt
	Mean output = 150.2058823529412
1.6
	Fraction of events left = 0.1691456138087639
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_septem_line_ecc_cutoff_1.6_lvRegularNoHLC_real_layout_fake_events.txt
	Mean output = 1507.970588235294
1.6
	Fraction of events left = 0.7539852941176468
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_septem_line_ecc_cutoff_1.7_lvRegularNoHLC_real_layout.txt
	Mean output = 153.8529411764706
1.7
	Fraction of events left = 0.173365478568938
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_septem_line_ecc_cutoff_1.7_lvRegularNoHLC_real_layout_fake_events.txt
	Mean output = 1512.705882352941
1.7
	Fraction of events left = 0.7563529411764706
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_septem_line_ecc_cutoff_1.8_lvRegularNoHLC_real_layout.txt
	Mean output = 157.0
1.8
	Fraction of events left = 0.1769774598234377
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_septem_line_ecc_cutoff_1.8_lvRegularNoHLC_real_layout_fake_events.txt
	Mean output = 1517.058823529412
1.8
	Fraction of events left = 0.7585294117647059
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_septem_line_ecc_cutoff_1.9_lvRegularNoHLC_real_layout.txt
	Mean output = 160.5
1.9
	Fraction of events left = 0.1813640335371248
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_septem_line_ecc_cutoff_1.9_lvRegularNoHLC_real_layout_fake_events.txt
	Mean output = 1520.323529411765
1.9
	Fraction of events left = 0.7601617647058823
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_septem_line_ecc_cutoff_2.0_lvRegularNoHLC_real_layout.txt
	Mean output = 163.2647058823529
2.0
	Fraction of events left = 0.1846827506979342
File: /home/basti/org/resources/septem_veto_random_coincidences/autoGen/septem_veto_before_after_septem_line_ecc_cutoff_2.0_lvRegularNoHLC_real_layout_fake_events.txt
	Mean output = 1523.470588235294
2.0
	Fraction of events left = 0.761735294117647
:end:

(about the first set of files)
So about 14.8% in the only septem case and 9.9% in the septem + line
veto case.


- [ ] *MOVE BELOW TO PROPER THESIS PART!*
(about the ε cut)
#+CAPTION: Fraction of events in Run-3 data (green), which pass (i.e. not rejected) the line
#+CAPTION: veto depending on the eccentricity cut used, which decides how eccentric a
#+CAPTION: cluster needs to be in order to be used for the veto. The purple points are
#+CAPTION: using fake bootstrapped data from real clusters passing the $\ln\mathcal{L}$ cut
#+CAPTION: together with real outer GridPix data from *other* events. The fraction of events
#+CAPTION: being vetoed in the latter is a measure for the random coincidence rate. 
#+NAME: fig:background:fraction_passing_line_veto_ecc_cut
[[~/phd/Figs/background/estimateSeptemVetoRandomCoinc/fraction_passing_line_veto_ecc_cut.pdf]]

#+CAPTION: Ratio of the events passing in the real line veto application to the fake data application
#+CAPTION: for different $ε_{\text{cut}}$ cutoff values. The optimum seems to be in the range of
#+CAPTION: 1.4 to 1.5
[[~/phd/Figs/background/estimateSeptemVetoRandomCoinc/ratio_real_fake_fraction_passing_line_veto_ecc_cut.pdf]]

***** Investigate significantly lower fake event fraction passing 
*UPDATE*: <2023-02-13 Mon 16:50>

The numbers visible in the plot are *MUCH LOWER* than what we had
previously after implementing the line veto alone!!

Let's run with the equivalent of the old parameters:
#+begin_src sh
LINE_VETO_KIND=lvRegular \
    ECC_LINE_VETO_CUT=1.0 \
    USE_REAL_LAYOUT=false \
    likelihood -f ~/CastData/data/DataRuns2018_Reco.h5 \
    --h5out /t/lhood_2018_crAll_80eff_line_ecc_cutof_1.0_tight_layout_lvRegular.h5 \
    --region crAll --cdlYear 2018 \
    --cdlFile ~/CastData/data/CDL_2019/calibration-cdl-2018.h5 --lineveto --estimateRandomCoinc
#+end_src


-> As it turns out this was a bug in our logic that decides which
cluster is of interest to the line veto. We accidentally always deemed
it interesting, if the original cluster was on its own... Fixed now.

*** On the line veto without septem veto

When dealing with the line veto without the septem veto there are
multiple questions that come up of course.

First of all, what is the cluster that you're actually targeting with
our 'line'? The original cluster (OC) that passed lnL or a
hypothetical larger cluster that was found during the septem event
reconstruction (HLC).

Assuming the former, the next question is whether we want to allow an
HLC to veto our OC? In a naive implementation this is precisely what's
happening, because in the regular use case of septem veto + line veto,
the line veto would never have any effect anymore, as an HLC would
almost certainly be vetoed by the septem veto! But without the septem
veto, this decision is fully up to the line veto and the question
becomes relevant.
(we will implement a switch, maybe based on an environment variable or
flag)

In the latter case the tricky part is mainly just identifying the
correct cluster which to test for in order to find its
center. However, this needs to be implemented to avoid the HLC in the
above mentioned case. With it done, we then have 3 different ways to
do the line veto:

1. 'regular' line veto. *Every* cluster checks the line to the center
   cluster. Without septem veto this includes HLC checking OC.
2. 'regular without HLC' line veto: Lines check the OC, but the HLC is
   explicitly *not* considered.
3. 'checking the HLC' line veto: In this case *all* clusters check the
   center of the HLC.


Thoughts on LvCheckHLC:
- The radii around the new HLC become so large that in practice this
  won't be a very good idea I think!
- The ~lineVetoRejected~ part of the title seems to be "true" in too
  many cases. What's going on here? See:
  [[file:~/org/Figs/statusAndProgress/estimateSeptemVetoRandomCoinc/septem_events_only_line_veto_check_hlc_not_final.pdf]]
  for example "2882 and run 297" on page 31. Like huh?
  My first guess is that the distance calculation is off somehow?
  Similar page 33 and probably many more. Even worse is page 34: "event 30 and run 297"!
  -> Yeah, as it turns out the problem was just that our
  ~inRegionOfInterest~ check had become outdated due to our change of

- [ ] Select example events for each of the 'line veto kinds' to
  demonstrate their differences.

OC: Original Cluster (passing lnL cut on center chip)
HCL: Hypothetical Large Cluster (new cluster that OC is part of after
septemboard reco)
Regular:  
[[~/org/Figs/statusAndProgress/estimateSeptemVetoRandomCoinc/example_event_line_veto_regular.pdf]]
is an example event in which we see the "regular" line veto without
using the septem veto. Things to note:
- the black circle shows the 'radius' of the OC, *not* the HLC
- the OC is actually part of a HLC
- because of this and because the HLC is a nice track, the event is
  *vetoed*, not by the green track, but by the HLC itself!
This wouldn't be a problem if we also used the septem veto, as this
event would already be removed due to the septem veto!
(More plots: [[file:~/org/Figs/statusAndProgress/estimateSeptemVetoRandomCoinc/septem_events_only_line_veto_regular_fixed_check.pdf]])

Regular no HLC:
[[~/org/Figs/statusAndProgress/estimateSeptemVetoRandomCoinc/example_event_line_veto_regular_noHLC.pdf]]
The reference cluster to check for is still the regular OC with the
same radius. And again the OC is part of an HLC. However, in contrast
to the 'regular' case, this event is not vetoed. The green and purple
clusters simply don't point at the black circle and the HLC itself is
*not considered* here. This defines the 'regular no HLC' veto.
[[~/org/Figs/statusAndProgress/estimateSeptemVetoRandomCoinc/example_event_line_veto_regular_noHLC_close_hit.pdf]]
is just an example of an event that proves the method works & a nice
example of a cluster _barely_ hitting the radius of the OC.
On the other hand though this is also a good example for why we should
have an eccentricity cut on those clusters that we use to check for
lines! The green cluster in this second event is not even remotely
eccentric enough and indeed is actually part of the orange track!
(More plots: [[file:~/org/Figs/statusAndProgress/estimateSeptemVetoRandomCoinc/septem_events_only_line_veto_regular_noHCL_fixed_check.pdf]])

Check HLC cluster:
[[~/org/Figs/statusAndProgress/estimateSeptemVetoRandomCoinc/example_event_line_veto_check_hlc_is_problematic.pdf]]
Is an example event where we can see how ridiculous the "check HLC"
veto kind can become. There is a very large cluster that the OC is
actually part of (in red). But because of that the radius is *SO
LARGE* that it even encapsulates a whole other cluster (that
technically should ideally be part of the 'lower' of the tracks!).
For this reason I don't think this method is particularly useful. In
other events of course it looks more reasonable, but still. There
probably isn't a good way to make this work reliably. In any case
though, for events that are significant in size, they would almost
certainly never pass any lnL cuts anyhow.
(More plots: [[file:~/org/Figs/statusAndProgress/estimateSeptemVetoRandomCoinc/septem_events_only_line_veto_check_hlc_fixed_check.pdf]])



The following is a broken event. THe purple cluster is not used for
line veto. Why?
/t/problem_event_12435_run_297.pdf


- [X] Implement a cutoff for the eccentricity that a cluster must have
  in order to partake in the line veto. Currently this can only be set
  via an environment variable (~ECC_LINE_VETO_CUT~). A good value is
  around the 1.4 - 1.6 range I think (anything that rules out most
  X-ray like clusters!)
  
- [X] *UNRELATED TO HERE, BUT STILL IMPORTANT*
  [[~/org/Figs/statusAndProgress/exampleEvents/example_line_veto_needs_chip_spacing.pdf]]
  is an example event that shows we need to introduce the correct chip
  spacing *for the line veto*. For the septem veto it's not very
  important, because the distance is way more important than the angle
  of how things match up. But for the line veto it's essential, as can
  be seen in that example (note that it uses ~lvRegularNoHLC~ and no
  septem veto, i.e. that's why the veto is false, despite the purple HLC of
  course "hitting" the original cluster)
  -> This has been implemented now. Activated (for now) via an
  environment variable ~USE_REAL_LAYOUT~.
  An example event for the spacing & the eccentricity cutoff is:
  [[file:~/org/Figs/statusAndProgress/exampleEvents/example_event_with_line_spacing_and_ecc_cutoff.pdf]]
  which was generated using:
  #+begin_src sh
LINE_VETO_KIND=lvRegularNoHLC \
    ECC_LINE_VETO_CUT=1.6 \
    USE_REAL_LAYOUT=true \
    likelihood -f ~/CastData/data/DataRuns2018_Reco.h5 \
    --h5out /tmp/lhood_2018_crAll_80eff_line.h5 \
    --region crAll --cdlYear 2018 \
    --cdlFile ~/CastData/data/CDL_2019/calibration-cdl-2018.h5 \
    --lineveto --plotseptem
  #+end_src
  and then just extract it from the ~/plots/septemEvents~
  directory. Note the definition of the environment variables like this!
  
**** Outdated: Estimation using subset of outer ring events

The text here was written when we were still bootstrapping events only
from the subset of *event numbers* that actually have a cluster that
passes lnL on the center chip. This subset is of course biased even on
the outer chip. Assuming that center clusters often come with activity
on the outer chips, means there are less events representing those
cases where there isn't even any activity in the center. This
over represents activity on the outer chip.


#+begin_src sh
likelihood -f ~/CastData/data/DataRuns2018_Reco.h5 \
           --h5out /tmp/lhood_2018_crAll_80eff_septem_fake.h5 \
           --region crAll --cdlYear 2018 \
           --cdlFile ~/CastData/data/CDL_2019/calibration-cdl-2018.h5 \
           --septemveto --estimateRandomCoinc
#+end_src

which writes the file ~/tmp/septem_fake_veto.txt~, which for this case
is found
[[file:~/org/resources/septem_veto_random_coincidences/estimates_septem_veto_random_coincidences.txt]]

Mean value of: 1522.61764706.

From this file the method seems to remove typically a bit less than
500 out of 2000 bootstrapped fake events. This seems to imply a random
coincidence rate of almost 25% (or effectively a reduction of further
25% in efficiency / 25% increase in dead time). Pretty scary stuff.

Of course this does not even include the line veto, which will drop it
further. Let's run that:
#+begin_src sh
likelihood -f ~/CastData/data/DataRuns2018_Reco.h5 \
           --h5out /tmp/lhood_2018_crAll_80eff_septem_line_fake.h5 \
           --region crAll --cdlYear 2018 \
           --cdlFile ~/CastData/data/CDL_2019/calibration-cdl-2018.h5 \
           --septemveto --lineveto --estimateRandomCoinc
#+end_src

which generated the following output:
[[file:~/org/resources/septem_veto_random_coincidences/estimates_septem_line_veto_random_coincidences.txt]]

Mean value of: 1373.70588235.

This comes out to a fraction of 68.68% of the events left after
running the vetoes on our bootstrapped fake events. Combining it with
a software efficiency of ε = 80% the total combined efficiency then
would be $ε_\text{total} = 0.8 · 0.6868 = 0.5494$, so about 55%.


*** Veto setups of interest [/]


- [ ] Write a section about a) the motivation behind looking at
  different setups to begin with and b) show the kind of setups we
  will be looking at later in the limit part.


As a reference the random coincidence rates of:
- pure septem veto = 0.7841029411764704
- pure line veto   = 0.8601764705882353
- septem + line veto = 0.732514705882353

Let's compute the total efficiencies of all the setups we look at.


|      limit | name                                                                                                                                                                                                                                                                                                                                |
|------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 8.9258e-23 |                                                                                                          |
| 8.8516e-23 | _scinti                                                                                                  |
| 8.5385e-23 | _scinti_vetoPercentile_0.95_fadc_vetoPercentile_0.95                                                     |
| 7.2889e-23 | _scinti_vetoPercentile_0.95_fadc_vetoPercentile_0.95_line_vetoPercentile_0.95                            |
|  7.538e-23 | _scinti_vetoPercentile_0.95_fadc_vetoPercentile_0.95_septem_vetoPercentile_0.95                          |
| 7.2365e-23 | _scinti_vetoPercentile_0.95_fadc_vetoPercentile_0.95_septem_vetoPercentile_0.95_line_vetoPercentile_0.95 |
| 8.6007e-23 | _scinti_vetoPercentile_0.99_fadc_vetoPercentile_0.99                                                     |
| 7.3555e-23 | _scinti_vetoPercentile_0.99_fadc_vetoPercentile_0.99_line_vetoPercentile_0.99                            |
| 7.5671e-23 | _scinti_vetoPercentile_0.99_fadc_vetoPercentile_0.99_septem_vetoPercentile_0.99                          |
| 7.3249e-23 | _scinti_vetoPercentile_0.99_fadc_vetoPercentile_0.99_septem_vetoPercentile_0.99_line_vetoPercentile_0.99 |
| 8.4108e-23 | _scinti_vetoPercentile_0.9_fadc_vetoPercentile_0.9                                                       |
| 7.2315e-23 | _scinti_vetoPercentile_0.9_fadc_vetoPercentile_0.9_line_vetoPercentile_0.9                               |
| 7.4109e-23 | _scinti_vetoPercentile_0.9_fadc_vetoPercentile_0.9_septem_vetoPercentile_0.9                             |
| 7.1508e-23 | _scinti_vetoPercentile_0.9_fadc_vetoPercentile_0.9_septem_vetoPercentile_0.9_line_vetoPercentile_0.9     |



| $ε_{\ln\mathcal{L}, \text{eff}}$ | Scinti | FADC | $ε_{\text{FADC, eff}}$ | Septem | Line | Efficiency | Expected limit (nmc=1000) |
|----------------------------------+--------+------+------------------------+--------+------+------------+---------------------------|
|                              0.8 | x      | x    |                      - | x      | x    |        0.8 |                8.9258e-23 |
|                              0.8 | o      | x    |                      - | x      | x    |        0.8 |                8.8516e-23 |
|----------------------------------+--------+------+------------------------+--------+------+------------+---------------------------|
|                              0.8 | o      | o    |                   0.98 | x      | x    |      0.784 |                8.6007e-23 |
|                              0.8 | o      | o    |                   0.90 | x      | x    |       0.72 |                8.5385e-23 |
|                              0.8 | o      | o    |                   0.80 | x      | x    |       0.64 |                8.4108e-23 |
|----------------------------------+--------+------+------------------------+--------+------+------------+---------------------------|
|                              0.8 | o      | o    |                   0.98 | o      | x    |       0.61 |                7.5671e-23 |
|                              0.8 | o      | o    |                   0.90 | o      | x    |       0.56 |                 7.538e-23 |
|                              0.8 | o      | o    |                   0.80 | o      | x    |       0.50 |                7.4109e-23 |
|----------------------------------+--------+------+------------------------+--------+------+------------+---------------------------|
|                              0.8 | o      | o    |                   0.98 | x      | o    |       0.67 |                7.3555e-23 |
|                              0.8 | o      | o    |                   0.90 | x      | o    |       0.62 |                7.2889e-23 |
|                              0.8 | o      | o    |                   0.80 | x      | o    |       0.55 |                7.2315e-23 |
|----------------------------------+--------+------+------------------------+--------+------+------------+---------------------------|
|                              0.8 | o      | o    |                   0.98 | o      | o    |       0.57 |                7.3249e-23 |
|                              0.8 | o      | o    |                   0.90 | o      | o    |       0.52 |                7.2365e-23 |
|                              0.8 | o      | o    |                   0.80 | o      | o    |       0.47 |                7.1508e-23 |

- [ ] *ADD THE VERSIONS WE NOW LOOK AT*
  - [ ] without FADC
  - [ ] different lnL cut (70% & 90%)

- [ ] Maybe introduce a couple of versions with lower lnL software efficiency?

- [ ] Introduce all setups we care about for the limit calculation,
  final decision will be done based on which yields the best
  _expected_ limit.

- [ ] Have a table with the setups, their background rates (full
  range) and their efficiencies!

*** Final background rate using all vetoes [/]
:PROPERTIES:
:CUSTOM_ID: sec:background:all_vetoes_combined
:END:


- [ ] *ADD MLP in addition!*

All the vetoes discussed above yield a very good improvement of the
background rate, shown in fig. [[fig:background_rate_all_vetoes]], which
contains the comparisons of all vetoes discussed above. Each veto
builds on the previous ones.

The background rate between $\SIrange{0}{8}{keV}$ ends up at
$<\SI{1.1e-5}{keV⁻¹ cm⁻² s⁻¹}$. While the vetoes allow for a good
reduction over the initial background rate (in particular over the
whole chip, as seen in fig. [[fig:background_clusters_septem_veto]], the
limitations of the achieved background needs to be discussed in the
context of a GridPix3 based detector with 7 GridPix.

First of all the main features visible in the background rate are of
course the argon fluorescence line at around $\SI{3}{keV}$ and the
copper fluorescence lines at $\sim\SIrange{8}{9}{keV}$. There are two
main ways to excite these lines:
1. via cosmics
2. via radioactive impurities of the detector material
These also split into two groups:
1. the inducing particle induces these *within* the sensitive area of
   the readout (only possible for the excitation of the argon line or
   the copper of the anode)
2. the inducing particle induces these *outside* the sensitive area of
   the readout
   
In the first case the GridPix1 detector runs into a severe limitation
due to its readout. The readout of the Timepix1 is shutter based. The
combination of 7 such GridPix1 leads to a significantly long readout
time, which is why a shutter length of about $\SI{2.4}{s}$ was chosen
for the data taking at CAST. As we only had an FADC to trigger and
thus close the shutter prematurely for the center chip, the long time
scales mean the chance of random coincidences of events on the outer
chips is significant. For example a muon that traverses over the outer
chips, but neither fulfills the septem or line veto, there is no way
to be certain whether the cluster seen on the center chip is
correlated or not. While an aggressive "no activity on outer chips
allowed" veto is possible, it severely increases the dead time due to
the long shutter times. This particular case is perfectly resolved by
the usage of the GridPix3, as that version not only
allows for a data driven readout resolving the problem of long
shutter times, but also allows for *simultaneous* readout of ~ToT~ and
ToA. So with a GridPix3 detector such random coincidences are reduced
to 'real' random coincidences, which are of the time scale of the
physical processes we are interested in.

The second case will remain an issue even with a GridPix3 based
detector. *However*, passive mitigation of this is possible by
using a different gas mixture (e.g. xenon based) to avoid the
argon fluorescence line altogether.

*TODO*: once merged, replace/add radiopure talk with reference to
corresponding section
Further, the GridPix3 detector will be built using radiopure
materials. This should significantly reduce the amount of induced
fluorescence to those parts that are cosmic induced. And finally the
cosmic induced events will also be further reduced by usage of a fully
covering scintillator veto system.

All of these combinations should lead to a significant improvement of
the background rate. 

#+begin_center
#+CAPTION: Background rate in the center $\SI[parse-numbers=false]{5 \times 5}{mm²}$ region using
#+CAPTION: the full 2017/18 GridPix1 dataset from CAST. Each successive veto, applied in the order
#+CAPTION: in which they are explained above is shown cumulatively. The 'line veto' contains all
#+CAPTION: discussed vetoes. It yields a background rate in the region between $\SIrange{0}{8}{keV}$ of
#+CAPTION: $<\SI{1.1e-5}{keV⁻¹ cm⁻² s⁻¹}$.
#+NAME: fig:background_rate_all_vetoes
[[~/org/Figs/statusAndProgress/IAXO_TDR/background_rate_2017_2018_scinti_veto_septem_veto_line_veto.pdf]]
#+end_center

- [ ] *INCLUDE MLP*


*** Example using GridPix3 data

Maybe separate, maybe merged into the other sections.


** Understanding background rate

- [ ] this is already partially handled in the summary section of
  background with all vetoes.
  
3 keV is easy to understand.

8-9 keV is more difficult. 

** Muon calculations                                              :noexport:

Probably not going to make it into final thesis? We'll see, take from
StatusAndProgress. Maybe shortened version will make it.

- [ ] *INSERT THESE!* They are now referenced in a foot note in the
  scintillator section!
  
- [ ] *IF NOT ALREADY DONE, ESTIMATE THE EXPECTED RATE OF MUONS UNDER
  THOSE ANGLES NUMERICALLY*
  -> so that we have a total number of muons we should have detected
  e.g. orthogonally in the CAST data, e.g. to compare with SiPM
  trigger rates etc.

* Limit calculation                                                :Analysis:
:PROPERTIES:
:CUSTOM_ID: sec:limit
:END:
#+LATEX: \minitoc

In this chapter we will introduce a generic limit calculation method,
which can be used to compute limits on different axion or ALP coupling
constants.

We will first present the limit method by introducing the likelihood
function we use to compute a limit in
sec. [[#sec:limit:limit_method]]. 

Then we will extend it in
sec. [[#sec:limit:systematics]] to include systematic uncertainties. Up
next we discuss the definition of expected limits and how they are
computed.

With the 

- [ ] Need to rethink how to structure it first...


- [ ] *TALK ABOUT ASIMOV DATASET IN CONTEXT OF EXPECTED LIMITS* and
  how it likely won't help us, as we cannot 'compute' the Asimov
  dataset.
  The closest we could do is to take the exact numbers of candidates
  predicted by the ~expCounts~ we use to sample background events
  from. Problem: what is 0.1 candidates? Well, we could take our
  *derivation of the unbinned approach literally* and simply take the
  power to $c_i = 0.1$ or similar!
  -> That might _actually_ work.

#+begin_comment
If we decide to present different limits, we should have one section
(maybe in theory) where we present our methodology and then compute
the limits based on that method for each of the different cases:
- chameleon
- axion photon
- axion electron

Using background rate & methods to determine it.

*Need* to show the log L phase space according to Igor. Well, that
seems useful anyway.

*TODO* somewhere explain what an expected vs. a real limit is.
#+end_comment

** Limit method (from the paper)                                  :noexport:

To compute a limit on the axion-electron coupling constant we use a
Bayesian approach based on finding the $95^{\text{th}}$ percentile of
the marginal posterior likelihood. Our initial likelihood function is
derived from a ratio of two Poisson distributions, the signal plus
background hypothesis over the pure background hypothesis:

\[
\mathcal{L} = \prod_i \frac{P_{\text{pois}}(c_i; s_i + b_i)}{P_{\text{pois}}(c_i; b_i)}
\]

which runs over all channels $i$ and $c_i$ are the number of
candidates in each bin, $s_i, b_i$ the signal and background,
respectively. The signal $s_i$ is the expected amount of signal in bin
$i$ based on the solar axion flux and all detection efficiencies
included. The background is given by a background model constructed
from the entire background dataset at CAST during non-tracking
times. This likelihood is taken to the unbinned likelihood by choosing bins in
time such that each bin only contains either 0 or 1 candidates.

The likelihood function simplifies to

\[
\mathcal{L} = e^{-s_{\text{tot}}} \prod_i \left(1 + \frac{s_i}{b_i}\right)
\]

in this case. Here $s_{\text{tot}}$ is the total expected signal over
the entire signal sensitive data taking period - a total number of
expected axion induced X-rays recorded by our detector.

Further, systematics are taken into account by multiplying with one
normal distribution for each nuisance parameter, which is normalized
such that $\mathcal{N}(θ = 0, σ) = 1$. The signal and background
parameters are scaled by $θ$, such that a positive $θ$ increases the
parameter and a negative decreases it. At the same time the normal
distribution acts as a penalty term. To compute the limit the explicit
$θ$ dependencies must be removed, which is done by marginalization,
i.e. integrating them out

\[
\mathcal{L}_{M} = \iiiint_{-∞}^∞ \exp(-s'_{\text{tot}}) \cdot \prod_i \left(1 +
\frac{s_i''}{b_i'}\right) \cdot
\exp\left[-\frac{θ_b²}{2 σ_b²} - \frac{θ_s²}{2 σ_s²} -
\frac{θ_x²}{2 σ_{xy}²} - \frac{θ_y²}{2 σ_{xy}²} \right]
\, \mathrm{d}\,θ_b \mathrm{d}\,θ_s \mathrm{d}\,θ_x \mathrm{d}\,θ_y
\]

# \begin{align*}
# \mathcal{L}_M &= \iiiint_{-∞}^∞ \left(\prod_i \frac{P_{\text{pois}}(n_i; s_i'' + b_i')}{P_{\text{pois}}(n_i; b_i')}\right) \cdot \mathcal{N}(θ_s, σ_s)
# \cdot \mathcal{N}(θ_b, σ_b) \cdot \mathcal{N}(θ_x, σ_x) \cdot \mathcal{N}(θ_y, σ_y) \\
# \mathcal{L}'(g, θ_s, θ_b, θ_x, θ_y) &= e^{s'_\text{tot}} \prod_i (1 + \frac{s_i''}{b_i'}) ·
#   \exp\left[-\frac{1}{2} \left(\frac{θ_s}{σ_s}\right)² 
#     -\frac{1}{2} \left(\frac{θ_b}{σ_b}\right)² 
#     -\frac{1}{2} \left(\frac{θ_x}{σ_x}\right)² 
#     -\frac{1}{2} \left(\frac{θ_y}{σ_y}\right)² \right]
# \end{align*}

with $a' = a ( 1 + θ_a )$ and $a'' = a ( 1 + θ_a ) ( 1 + θ_x ) ( 1 +
θ_y )$. This keeps the possibility of variance due to systematics
including the penalization embedded in the marginal likelihood, but
restores a single variable likelihood function with a well defined
single value for the $95^{\text{th}}$ quantile.

The limit $g'_{ae}$ is then defined by

\[
0.95 = \frac{∫_{-∞}^{g_{ae}'} \mathcal{L}(g_{ae}) π(g_{ae}) \, \mathrm{d}g_{ae}}{∫_{-∞}^∞ \mathcal{L}(g_{ae}) π(g_{ae}) \, \mathrm{d}g_{ae}}
\]

which is computed from an empirical cumulative distribution function. 

Evaluation of a four-fold integral where the integrand is expensive to
evaluate computationally due to the curse of dimensionality. As such
the Metropolis-Hastings Markov Chain Monte Carlo algorithm is used to
build evaluate the integrand efficiently only in those regions of the
parameter space where the integrand contributes to the integral.

** Limit method - introduction
:PROPERTIES:
:CUSTOM_ID: sec:limit:method_introduction
:END:

- Context and terminology :: An experiment tries to detect a new
  phenomenon of the kind where you expect very little signal compared
  to background sources. We have a dataset in which the experiment is
  _sensitive_ to the phenomenon, another dataset in which it is _not
  sensitive_ and finally a theoretical model of our _expected signal_.
  
  Any data entry (after cleaning) in the sensitive dataset is a
  _candidate_. Each candidate is drawn from a distribution of the
  present background plus the expected signal contribution (c = s +
  b). Any entry in the non sensitive dataset is _background_ only.

- Goal :: compute the value of a parameter (coupling constant) such
  that there is 95% confidence that the combined hypothesis of signal
  and background sources are compatible with the background only
  hypothesis.

- Condition :: Our experiment should be such that the data in some
  "channels" of our choice can be modeled by a Poisson distribution
  
  \[
  P_{\text{Pois}}(k; λ) = \frac{λ^k e^{-λ}}{k!}.
  \]
  
  Each such channel with an expected mean of $λ$ counts has
  probability $P_{\text{Pois}}(k; λ)$ to measure $k$ counts. Because
  the Poisson distribution (as written here) is a probability density
  function, multiple different channels can be combined to a
  "likelihood" for an experiment outcome by taking the product of each
  channel's Poisson probability
  
  \[
  \mathcal{L}(λ) = \prod_i P_{i, \text{Pois}}(k; λ) = \prod_i \frac{λ_i^{k_i} e^{-λ_i}}{k_i!}
  \]
  
  i.e. given a set of $k_i$ recorded counts for all different channels
  $i$ with expected means $λ_i$ the "likelihood" gives us the literal
  likelihood to record exactly that experimental outcome. Note that
  the parameter of the likelihood function is the mean $λ$ and not the
  recorded data $k$! The likelihood function describes the likelihood
  for a *fixed set of data* (our real measured counts) for different
  parameters (our signal & background models - where background model
  is constant as well).

  In addition the method described below is valid under the assumption
  that our experiment did not have a statistically significant
  detection in the signal sensitive dataset compared to the background
  dataset! 

# - [ ] Why do we use the likelihood ratio that we use?

** Limit method - likelihood function
:PROPERTIES:
:CUSTOM_ID: sec:limit:method_likelihood
:END:

The likelihood function as described in the 
previous section is not helpful to compute a limit for the usage
with different datasets as described before. For that case we want
to have some kind of a "test statistic" that relates the sensitive
dataset with its seen candidates to the background dataset. For
practical purposes we prefer to define such a statistic which is
monotonically increasing in the number of candidates (see T. Junk's
1999 paper for details or read
sec. [[#sec:math_behind_cls_plus_b]]). There are different choices
possible, but the one we use is:
\[
\mathcal{L}(s, b) = \prod_i \frac{P_{\text{pois}}(c_i; s_i + b_i)}{P_{\text{pois}}(c_i; b_i)}
\]
so the ratio of the signal plus background over the pure background
hypothesis. The number $c_i$ is the real number of measured
*candidates*. So the numerator gives the probability to measure $c_i$
counts in each channel $i$ given the signal plus background
hypothesis. On the other hand the denominator measures the probability
to measure $c_i$ counts in each channel $i$ assuming only the
background hypothesis.
  
#+begin_center
For each channel $i$ the ratio of probabilities itself is not strictly
speaking a probability density function, because the integral

\[
\int_{-∞}^{∞}Q\, \mathrm{d}x = N \neq 1
\]

where $N$ can be interpreted as a hypothetical number of total number
of counts measured in the experiment. A PDF requires this integral to
be 1.

As a result the full construct $\mathcal{L}$ of the product of these ratios is
technically not a likelihood function either. It is usually referred
to as an "extended likelihood function".

For all practical purposes though we will continue to treat is as a
likelihood function and call it $L$ as usual.
#+end_center

Note the important fact that $\mathcal{L}$ really is only a function of our
signal hypothesis $s$ and our background model $b$. Each experimental
outcome *has its own* $\mathcal{L}$. This is precisely why the likelihood
function describes everything about an experimental outcome (at least
if the signal and background models are reasonably understood) and
thus different experiments can be combined by combining them in
"likelihood space" (multiplying their $\mathcal{L}$ or adding $\ln \mathcal{L}$ values)
to get a combined likelihood to compute a limit for.

- Deriving a practical version of $\mathcal{L}$ :: 
  The version of $\mathcal{L}$ presented above is still quite impractical to use
  and the ratio of exponentials of the Poisson distributions can be
  simplified significantly:
  \begin{align*}
    \mathcal{L} &= \prod_i \frac{P(c_i, s_i + b_i)}{P(c_i, b_i)} =
        \prod_i \frac{ \frac{(s_i + b_i)^{n_i}}{n_i!} e^{-(s_i + b_i)} }{ \frac{b_i^{n_i}}{n_i!} e^{-b_i}} \\
      &= \prod_i \frac{e^{-s_i} (s_i + b_i)^{c_i}}{b_i^{c_i}} =
        e^{-s_\text{tot}} \prod_i \frac{(s_i + b_i)^{c_i}}{b_i^{c_i}} \\
      &= e^{-s_\text{tot}} \prod_i \left(1 + \frac{s_i}{b_i} \right)^{c_i}         
  \end{align*}
  This really is the heart of computing a limit with a number of
  $s_{\text{tot}}$ expected events from the signal hypothesis
  (depending on the parameter to be studied, the coupling constant),
  $c_i$ measured counts in each channel and $s_i$ expected signal
  events and $b_i$ expected background events in that channel.

  As mentioned previously though the choice of what a channel is, is
  completely up to us! One such choice might be binning the candidates
  in energy. However, there is one choice that is particularly simple
  and is often referred to as the "unbinned likelihood". Namely, we
  create channels in _time_ such that each "time bin" is so short as
  to either have 0 entries (most channels) or 1 entry. This means we
  have a large number of channels, but because of our definition of
  $\mathcal{L}$ this does not matter. All channels with 0 candidates do not
  contribute to $\mathcal{L}$ (they are $(1 + \frac{s_i}{b_i})^0 = 1$). As a
  result our expression of $\mathcal{L}$ simplifies further to:

  \[
  \mathcal{L} = e^{-s_\text{tot}} \prod_i \left(1 + \frac{s_i}{b_i}\right)
  \]

  where $i$ is now all channels where a candidate is contained ($c_i =
  1$).

** Limit method - computing $\mathcal{L}$
:PROPERTIES:
:CUSTOM_ID: sec:limit:method_computing_L
:END:
  
Our simplified version of $\mathcal{L}$ using very short time bins now allows
to explicitly compute the likelihood for a set of parameters. Let's
now look at each of the constituents $s_{\text{tot}}$, $s_i$ and
$b_i$ and discuss how they are computed. We will focus on the
explicit case of an X-ray detector behind a telescope at CAST.

Here it is important to note that the signal hypothesis depends on
the coupling constant we wish to compute a limit for, we will just
call it $g$ in the remainder (it may be $g_{aγ}$ or $g_{ae}$ or any
other coupling constant). And this is the actual parameter of $\mathcal{L}$,
but more on that in the next section on how a limit is actually
computed later.

First of all the signal contribution in each channel $s_i$. It is
effectively a number of counts that one would expect within the time
window of the channel $i$. While this seems tricky given that we
have not explicitly defined such a window we can:
- either assume our time interval to be infinitesimally small and
  give a _signal rate_ (i.e. per second)
- or make use of the neat property that our expression only contains
  *the ratio* of $s_i$ and $b_i$. What this means is that we can
  choose our units ourselves, _as long as we use the same units for
  $s_i$ as for $b_i$_!
We will use the second case and scale each candidate's signal and
background contribution to the total tracking time (signal sensitive
dataset length).
Each parameter with a subscript $i$ is the corresponding value that
the candidate has we are currently looking at (e.g. $E_i$ is the energy
of the recorded candidate $i$ used to compute the expected signal).
#+NAME: eq:limit_method_signal_si
\begin{equation}
s_i(g) = f(g, E_i) · A · t · P_{a \rightarrow γ}(g_{aγ}) · ε(E_i) · r(x_i, y_i)
\end{equation}
where:
- $f(g, E_i)$ is the axion flux at energy $E_i$ in units of
  $\si{keV^{-1}.cm^{-2}.s^{-1}}$ as a function of $g$.
- $A$ is the area of the magnet bore in $\si{cm²}$
- $t$ is the tracking time in $\si{s}$
- $P_{a \rightarrow γ}$ is the conversion probability of the axion
  converting into a photon computed via
  \[
  P_{a \rightarrow γ}(g_{aγ}) = \left( \frac{g_{aγ} B L}{2} \right)²
  \]
  written in *natural units* (meaning if we wish to use the equation
  as written here we need to convert $B = \SI{9}{T}$ and $L =
  \SI{9.26}{m}$ into values expressed in powers of electronvolt
  $\si{eV}$). 
- $ε(E_i)$ is the combined detection efficiency, i.e. the
  combination of X-ray telescope effective area, the transparency of
  the detector window and the absorption probability of an X-ray in
  the gas.
- $r(x_i, y_i)$ is the expected amount of flux from the solar axion
  flux after it is focused by the X-ray telescope in the readout
  plane of the detector at the candidate's position $(x_i, y_i)$
  (this requires a raytracing model). It should be expressed as a
  fractional value in units of $\si{cm^{-2}}$.
As a result the units of $s_i$ are then given in
$\si{keV^{-1}.cm^{-2}}$ with the tracking time integrated out.
If one computes a limit for $g_{aγ}$ then $f$ and $P$ both depend on
the coupling of interest. In case of e.g. an axion-electron coupling
limit $g_{ae}$ the conversion probability can be treated as constant
(with a fixed $g_{aγ}$).

Secondly the background hypothesis $b_i$ for each channel. Its
value depends on whether we assume a constant background model, an
energy dependent one or even an energy plus position dependent
model. In either case the main point is to evaluate that background
model at the (potentially) position $(x_i, y_i)$ of the candidate and
energy $E_i$ of the candidate. The value should then be scaled to
the same units of as $s_i$, namely
$\si{keV^{-1}.cm^{-2}}$. Depending on how the model is defined this
might just be a multiplication by the total tracking time in seconds.

The final piece is the total signal $s_{\text{tot}}$, corresponding
to the total number of counts expected from our signal hypothesis
for the given dataset. This is nothing else as the integration of
$s_i$ over the entire energy range and detection area. However,
because $s_i$ implies the signal for candidate $i$, we write $s(E,
x, y)$  to mean the equivalent signal as if we had a candidate at
$(E, x, y)$
\[
s_{\text{tot}} = ∫_0^{E_{\text{max}}} ∫_A s(E, x, y) \mathrm{d}E \mathrm{d}x \mathrm{d}y
\]
where $A$ simply implies integrating the full area in which $(x, y)$
is defined (the axion flux is bounded within a region much smaller
than the active detection area and hence all contributions outside
are 0).

** Limit method - computing a limit
:PROPERTIES:
:CUSTOM_ID: sec:limit:method_computing_a_limit
:END:
  
With the above we are now able to evaluate $\mathcal{L}$ for a set of
candidates ${c_i(E_i, x_i, y_i)}$. As mentioned before it is
important to realize that $\mathcal{L}$ is a function of the coupling constant
$g$, $\mathcal{L}(g)$ with all other parameters effectively constant in the
context of "one experiment".

With this in mind the "limit" is defined as the 95-th percentile of
$\mathcal{L}(g)$ *within the physical region of $g$* (the region $g < 0$ is
explicitly ignored, as a coupling constant cannot be negative! This
can be "rigorously" justified in Bayesian statistics by saying the
prior $π(g)$ is 0 for $g < 0$.).

So we can define it implicitly as:
#+NAME: eq:limit_method:limit_def
\begin{equation}
0.95 = \frac{∫_0^{g'} \mathcal{L}(g) \mathrm{d}g}{∫_0^∞ \mathcal{L}(g) \mathrm{d}g}
\end{equation}

In practice the integral cannot be evaluated until
infinity. Fortunately, our choice of $\mathcal{L}$ in the first place means
that the function converges to $0$ quickly for large values of
$g$. Therefore, we only need to compute values to a "large enough"
value of $g$ to get the shape of $\mathcal{L}(g)$. From there we can use any
numerical approach (via an empirical CDF for example) to determine
the coupling constant $g'$ that corresponds to the 95-th percentile
of $\mathcal{L}(g)$.

In an intuitive sense the limit means the following:
$\SI{95}{\percent}$ of all coupling constants that reproduce the
data we measured - given our signal and background hypotheses - are
smaller than $g'$.

Fig. [[fig:limit_method:example_limit_95th_perc]] shows the likelihood
as a function of $g_{ae}²$. The blue area is the lower \SI{95}{\%} of the
parameter space and the red area is everything below. Therefore, the
limit in this particular set of toy candidates is at the
intersection of the two colors.

#+CAPTION: Example likelihood as a function of $g_{ae}²$ for a set of toy candidates.
#+CAPTION: Blue is the lower 95-th percentile of the integral over the likelihood
#+CAPTION: function and red the upper 5-th. The limit is at the intersection.
#+NAME: fig:limit_method:example_limit_95th_perc
[[~/org/Figs/statusAndProgress/limitCalculation/mcmc_histo_example_limit_determination.pdf]]

- [ ] *REPLACE THIS PLOT* by an analytical likelihood function!
  -> Use the binned example limit code to produce it.

** Limit method - toy candidate sets and expected limits
:PROPERTIES:
:CUSTOM_ID: sec:limit:method_expected_limit
:END:

Assuming a constant background over some chip area with only an
energy dependence, the background hypothesis can be used to draw toy
candidates that can be used in place for the real candidates to
compute limits.

In this situation the background hypothesis can be modeled as
follows:

\[
B = \{ P_{\text{Pois}}(k; λ = b_i) \: | \: \text{for all energy bins } E_i \},
\]

that is the background is the set of all energy bins $E_i$, where
each bin content is described by a Poisson distribution with a mean
and expectation value of $λ = b_i$ counts.

To compute a set of toy candidates then, we simply iterate over all
energy bins and draw a number from each Poisson distribution. This
is the number of candidates in that bin for the toy. Given that we
assumed a constant background over the chip area, we finally need to
draw the $(x_i, y_i)$ positions for each toy candidate from a
uniform distribution.

These sets of toy candidates can be used to compute an "expected
limit". The term expected limit is usually understood to mean the
median of sets of representative toy candidates. If $L_{t_i}$ is
the limit of the toy candidate set $t_i$, the expected limit
$\langle L \rangle$ is
defined as

\[
\langle L \rangle = \mathrm{median}( \{ L_{t_i} \} )
\]

If the number of toy candidate sets is large enough the expected
limit should prove accurate. The real limit will then be below or
above with $\SI{50}{\%}$ chance each.

** Limit method - extending $\mathcal{L}$ for systematics
:PROPERTIES:
:CUSTOM_ID: sec:limit:method_systematics
:END:

The aforementioned likelihood ratio assumes perfect knowledge of the
inputs for the signal and background hypotheses. In practice neither
of these is known perfectly though. Each input typically has
associated a small systematic uncertainty (e.g. the width of the
detector window is only known up to N nanometers, the pressure in
the chamber only up to M milli bar, magnet length only up to A
centimeter etc.). These all affect the "real" numbers one should
actually calculate with. So how does one treat these uncertainties?

The basic starting point is realizing that the values we use are our
"best guess" of the real value. _Usually_ it is a reasonable
approximation that the real value will likely be within some
standard deviation around that best guess, following a normal
distribution. Further, it is a good idea to identify all systematic
uncertainties and classify them by which aspect of $s_i$ or $b_i$
they affect (amount of signal or background or the position { in
some other type of likelihood function possibly other } ). Another
reasonable assumption is to combine different uncertainties of the
same type by the square root of their squared sum, i.e. computing
the euclidean radius N dimensions (for N uncertainties of the same
type).

For example assuming we had these systematics (expressed as
relative numbers from the best guess):
- signal uncertainties:
  - magnet length: \SI{0.2}{\%}
  - magnet bore diameter: \SI{2.3}{\%}
  - window thickness: \SI{0.6}{\%}
- position uncertainty (of where the axion image is projected):
  - detector alignment: \SI{5}{\%}
- background uncertainty:
  - A: \SI{0.5}{\%} (whatever it may be, all real ones of mine are
    very specific)

From here we compute 3 combined systematics:
- $σ_s = \sqrt{ 0.2² + 2.3² + 0.6²} = \SI{2.38}{\%}$
- $σ_p = \SI{5}{\%}$
- $σ_b = \SI{0.5}{\%}$

The previous explanation and assumptions already tells us everything
about how to encode these uncertainties into the limit
calculation. For a value corresponding to our "best guess" we want
to recover the likelihood function $\mathcal{L}$ from before. The further we
get away from our "best guess" the more the likelihood function
should be "penalized", i.e. the actual likelihood of that parameter
given our data should be *lower*. We will see in a minute what is
meant by "being at the 'best guess'" or "away from it".

We encode this by multiplying the initial likelihood $\mathcal{L}$ with
additional normal distributions, one for each uncertainty (4 in
total in our case, signal, background, and two position
uncertainties). Each adds an additional parameter, a "nuisance parameter".

To illustrate the details, let's look at the case of adding a single
nuisance parameter. In particular we'll look at the nuisance
parameter for the signal as it requires more care.

The idea is to express our uncertainty of a parameter - in this case
the signal - by introducing an additional parameter $s_i'$. In
contrast to $s_i$ it describes a possible _other_ value of $s_i$ due
to our systematic uncertainty. For simplicity we rewrite our
likelihood $\mathcal{L}$ as $\mathcal{L}'(s_i, s_i', b_i)$:

\[
\mathcal{L}' = e^{-s'_\text{tot}} \prod_i (1 + \frac{s_i'}{b_i}) · \exp\left[-\frac{1}{2} \left(\frac{s_i' - s_i}{σ_s'}\right)² \right]
\]

where $s_i'$ takes the place of the $s_i$. The added gaussian then
provides a penalty for any deviation from $s_i$. The standard
deviation of the gaussian $σ_s'$ is the actual systematic
uncertainty on our parameter $s_i$ in units of $s_i$ (so not in
percent as we showed examples further up, but as an effective number
of counts { or whatever unit $s_i$ is expressed in } ).

This form of adding a secondary parameter $s_i'$ of the same units
as $s_i$ is not the most practical, but maybe provides the best
explanation as to how the name 'penalty term' arises for the added
gaussian. If $s_i = s_i'$ then the exponential term is $1$ meaning
the likelihood remains unchanged. For any other value the
exponential is $< 1$ _decreasing_ the likelihood $\mathcal{L}'$.

By a change of variables we can replace the "unitful" parameter
$s_i'$ by a unitless number $ϑ_s$. We would like the exponential to
be $\exp(-ϑ_s²/(2 σ_s²))$ to only express deviation from our
best guess $s_i$. $ϑ_s = 0$ means no deviation and $|ϑ_s| = 1$
implies $s_i = -s_i'$. Note that the standard deviation of this is
now $σ_s$ and *not* $σ_s'$ as seen in the expression above. This
$σ_s$ corresponds to our systematic uncertainty on the signal as a
percentage.

To arrive at this expression:

\begin{align*}
\frac{s_i' - s_i}{σ_s'} &= \frac{ϑ_s}{σ_s} \\
\Rightarrow s_i' &= \frac{σ_s'}{σ_s} ϑ_s + s_i \\
\text{with } s_i &= \frac{σ_s'}{σ_s} \\
s_i' &= s_i + s_i ϑ_s \\
\Rightarrow s_i' &= s_i (1 + ϑ_s) \\
\end{align*}

where we made use of the fact that the two standard deviations are
related by the signal $s_i$ (which can be seen by defining $ϑ_s$ as
the normalized difference $ϑ_s = \frac{s'_i - s_i}{s_i}$).

This results in the following final (single nuisance parameter)
likelihood $\mathcal{L}'$:

\[
\mathcal{L}' = e^{-s'_\text{tot}} \prod_i (1 + \frac{s_i'}{b_i}) · \exp\left[-\frac{1}{2} \left(\frac{ϑ_s}{σ_s}\right)² \right]
\]

where $s_i' = s_i (1 + ϑ_s)$ and similarly $s_{\text{tot}}' =
s_{\text{tot}} ( 1 + ϑ_s )$ (the latter just follows because $1 +
ϑ_s$ is a constant under the different channels $i$, see the
appendix below).

The same approach is used to encode the background systematic
uncertainty. The position uncertainty is generally handled the same,
but the $x$ and $y$ coordinates are treated separately.

As shown in eq. [[eq:limit_method_signal_si]] the signal $s_i$ actually
depends on the positions $(x_i, y_i)$ of each candidate via the
raytracing image $r$.

With this we can introduce the nuisance parameters by replacing $r$
by an $r'$ such that \[ r' ↦ r(x_i - x'_i, y_i - y'_i) \] which
effectively moves the center position by $(x'_i, y'_i)$. In addition
we need to add penalty terms for each of these introduced
parameters:

\[
\mathcal{L}' = \exp[-s] \cdot \prod_i \left(1 + \frac{s'_i}{b_i}\right) \cdot
  \exp\left[-\left(\frac{x_i - x'_i}{\sqrt{2}σ} \right)² \right] \cdot \exp\left[-\left(\frac{x_i - x'_i}{\sqrt{2}σ} \right)² \right]
\]

where $s'_i$ is now the modification from above using $r'$ instead
of $r$. Now we perform the same substitution as we do for $θ_b$ and
$θ_s$  to arrive at:

\[
\mathcal{L}' = \exp[-s] \cdot \prod_i \left(1 + \frac{s'_i}{b_i}\right) \cdot
  \exp\left[-\left(\frac{θ_x}{\sqrt{2}σ_x} \right)² \right] \cdot \exp\left[-\left(\frac{θ_y}{\sqrt{2}σ_y} \right)² \right]
\]

The substitution for $r'$ means the following for the parameters:
\[
r' = r\left(x (1 + θ_x), y (1 + θ_y)\right)
\]
where essentially a deviation of $|θ| = 1$ means we move the spot
to the edge of the chip.

Putting all these four nuisance parameters together we have

#+NAME: eq:limit_method:likelihood_function_def
\begin{align}
\mathcal{L}' &= \left(\prod_i \frac{P_{\text{pois}}(n_i; s_i + b_i)}{P_{\text{pois}}(n_i; b_i)}\right) \cdot \mathcal{N}(θ_s, σ_s)
\cdot \mathcal{N}(θ_b, σ_b) \cdot \mathcal{N}(θ_x, σ_x) \cdot \mathcal{N}(θ_y, σ_y) \\
\mathcal{L}'(g, ϑ_s, ϑ_b, ϑ_x, ϑ_y) &= e^{s'_\text{tot}} \prod_i (1 + \frac{s_i''}{b_i'}) ·
  \exp\left[-\frac{1}{2} \left(\frac{ϑ_s}{σ_s}\right)² 
    -\frac{1}{2} \left(\frac{ϑ_b}{σ_b}\right)² 
    -\frac{1}{2} \left(\frac{ϑ_x}{σ_x}\right)² 
    -\frac{1}{2} \left(\frac{ϑ_y}{σ_y}\right)² \right]
\end{align}

where here the doubly primed $s_i''$ refers to modification for the
signal nuisance parameter _as well as_ for the position uncertainty
via $r'$.

An example of the impact of the nuisance parameters on the likelihood
space as well as on the parameters ($s, b, x, y$) is shown in
fig. \ref{fig:limit:method_systematics:example}. First,
fig. \ref{fig:limit:method_systematics:example_theta_0.6} shows how
the axion image moves when $θ_{x,y}$ change, in this example $θ_{x,y}
= 0.6$ moves the image center to the bottom left ($θ_{x,y} = 1$ would
move the center into the corner). Note that the window strongback is
not tied to the axion image, but remains
fixed. Fig. \ref{fig:limit:method_systematics:example_sigma_0.05} and
\ref{fig:limit:method_systematics:example_sigma_0.25} show the impact
of the nuisance parameters on the likelihood space. The larger the
standard deviation $σ_{x,y}$ is, the more of the $θ_{x,y}$ space
contributes meaningfully to $\mathcal{L}_M$. In the former example - a
realistic uncertainty - only small regions around the center are
allowed to contribute. Regions further outside receive too large of a
penalty. However, at large uncertainties significant regions of the
parameter space play an important role. Given that each point on the
figures \ref{fig:limit:method_systematics:example_sigma_0.05} and
\ref{fig:limit:method_systematics:example_sigma_0.25} describe one
axion image like \ref{fig:limit:method_systematics:example_theta_0.6},
brighter regions imply positions where the axion image is moved to
parts that provide a larger $s/b$ in the center portion of the axion
image, while still only having a small enough penalty.

\begin{figure}[htbp]
  \centering
  \begin{subfigure}{0.33\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{~/org/Figs/statusAndProgress/limitSanityChecks/axion_image_limit_calc_theta_0_6.pdf}
    \caption{Axion image at $θ_{x,y} = 0.6$}
    \label{fig:limit:method_systematics:example_theta_0.6}
  \end{subfigure}%
  \begin{subfigure}{0.33\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{~/org/Figs/statusAndProgress/limitSanityChecks/likelihood_sigma_0.05_manyθx_θy.pdf}
    \caption{$σ_{x,y} = 0.05$}
    \label{fig:limit:method_systematics:example_sigma_0.05}
  \end{subfigure}%
  \begin{subfigure}{0.33\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{~/org/Figs/statusAndProgress/limitSanityChecks/likelihood_sigma_0.25_manyθx_θy.pdf}
    \caption{$σ_{x,y} = 0.25$}
    \label{fig:limit:method_systematics:example_sigma_0.25}
  \end{subfigure}%
  \label{fig:limit:method_systematics:example}
  \caption{\subref{fig:limit:method_systematics:example_theta_0.6} Impact of the position nuisance parameter on the
  axion image. A value of $θ_{x,y} = 0.6$ is shown, moving the center of the image to the bottom left corner.
  \subref{fig:limit:method_systematics:example_sigma_0.05} shows the impact on the likelihood itself for varying
  $θ_{x,y}$ values given a standard deviation of $σ_{x,y} = 0.05$. Small variations of the position still yield
  contributions to $\mathcal{L}_M$.
  \subref{fig:limit:method_systematics:example_sigma_0.25} the same for $σ_{x,y} = 0.25$. At this value large regions
  of the $θ_{x,y}$ parameter space contribute to $\mathcal{L}_M$, generally regions of larger $s/b$.
  }
\end{figure}

*** TODOs for this section [1/1] :noexport:

- [X] *INSERT SUB FIG OF THE EXAMPLE FROM THE TALK. θ_x, θ_y*!!

- [ ] *POTENTIALLY* think about moving the example images to a section
  further down. Reasoning being that at this point we have not shown
  or talked about the axion image at all here. This being mostly
  theoretical after all.

** Limit method - evaluating $\mathcal{L}$ with nuisance parameters
:PROPERTIES:
:CUSTOM_ID: sec:limit:method_mcmc
:END:

The likelihood function we started with $\mathcal{L}$ was only a function of
the coupling constant $g$ we want to compute a limit for. With the
inclusion of the four nuisance parameters however, $\mathcal{L}'$ is now a
function of 5 parameters, $\mathcal{L}'(g, ϑ_s, ϑ_b, ϑ_x, ϑ_y)$. Following our
definition of a limit via a fixed percentile of the integral over
the coupling constant, eq. [[eq:limit_method:limit_def]], leads to
problem for $\mathcal{L}'$. If anything one could define a contour describing
the 95-th percentile of the "integral volume", but this would lead
to infinitely many values of $g$ that describe said contour.

As a result to still define a sane limit value, the concept of the
_likelihood][marginal likelihood]] function $\mathcal{L}'_M$ is introduced. The idea is to
integrate out the nuisance parameters

\[
\mathcal{L}'_M(g) = \iiiint_{-∞}^∞ \mathcal{L}'(g, ϑ_s, ϑ_b, ϑ_x, ϑ_y)\,\mathrm{d}ϑ_s\mathrm{d}ϑ_b\mathrm{d}ϑ_x\mathrm{d}ϑ_y
\]

Depending on the exact definition of $\mathcal{L}'$ in use these integrals
may be analytically computable. In many cases however they are not
and numerical techniques to evaluate the integral must be utilized.

Aside from the technical aspects about how to evaluate $\mathcal{L}'_M(g)$ at
a specific $g$, the limit calculation continues exactly as for the
case without nuisance parameters once $\mathcal{L}'_M(g)$ is defined as such.

- Practical calculation of $\mathcal{L}'_M(g)$ in our case ::

  In case of our explicit likelihood function
  eq. [[eq:limit_method:likelihood_function_def]] there is already one
  particular case that makes the marginal likelihood not analytically
  integrable because the $b_i' = b_i(1 + ϑ_b)$ term introduces a
  singularity for $ϑ_b = -1$. For practical purposes this is not too
  relevant, as values approaching $ϑ_b = -1$ would imply having zero
  background and within a reasonable systematic uncertainty the
  penalty term makes contributions in this limit so small such that
  this area does not physically contribute to the integral.

  Using standard numerical integration routines (simpson, adaptive
  Gauss-Kronecker etc.) are all too expensive to compute such a
  four-fold integration under the context of computing many limits for
  an expected limit. For this reason Monte Carlo approaches are used,
  in particular the Metropolis-Hastings (MH) Markov Chain Monte Carlo
  (MCMC) is used. The basic idea of general Monte Carlo integration
  routines is to evaluate the function at random points and computing
  the integral based on the function evaluation at these points (by
  scaling the evaluations correctly). Unless the function is very
  'spiky' in the integration space, Monte Carlo approaches provide
  good accuracy at a fraction of the computational effort as normal
  numerical algorithms even in higher dimensions. However, we can do
  better than relying on _fully_ random points in the integration
  space. The Metropolis-Hastings algorithm tries to evaluate the
  function more often in those points where the contributions are
  large. The basic idea is the following:

  - [ ] REWRITE THIS
  
  Pick a random point in the integration space as a starting
  point $p_0$. Next, pick another random point $p_1$ within the vicinity of
  $p_0$. If the function evaluates to a larger value at $p_1$ accept
  it as the new current position. If it is smaller, accept it with a
  probability of $1 - \frac{f(p_i)}{f(p_{i-1})}$ (i.e. if the new
  value is close to the old one we accept it with a high probability
  and if the new one is much lower accept it rarely). This guarantees 
  to pick values inching closer to the most contributing areas of the
  integral in the integration space, while still allowing to get out
  of local maxima due to the random acceptance of "worse"
  positions. However, this also implies that regions of constant
  $\mathcal{L}$ (regions where the values are close to 0, but also
  generally 'flat' regions) produce a pure random walk from the
  algorithm, because $\frac{f(p_{i-1})}{f(p_i)} \approx 1$ in those
  regions. This needs to be taken into account.

  If a new point is accepted and becomes the current position, the
  "chain" of points is extended (hence "Markov Chain"). By creating a
  chain of reasonable length the integration space is taken into
  account well enough. Because the initial point is completely random
  (up to some possible prior) the first $N$ links of the chain are in
  a region of low interest (and depending on the interpretation of the
  chain "wrong"). For that reason one defines a cutoff $N_b$ of the
  first elements that are thrown away as "burn-in" before using the
  chain to evaluate the integral or parameters.

  In addition it can be valuable to not only start a single Markov
  Chain from one random point, but instead start _multiple_ chains
  from different points in the integration space. This increases the
  chance to cover different regions of interest even in the presence
  of multiple peaks separated too far away to likely "jump over" via
  the probabilistic acceptance.

  Furthermore, outside of using Metropolis-Hastings we still have to
  make sure the evaluation of $\mathcal{L}'(g, ϑ_s, ϑ_b, ϑ_x, ϑ_y)$ is fast. We
  will discuss this in the next section about the evaluation of
  $\mathcal{L}'$.

  In particular: Metropolis-Hastings (Markov Chain Monte Carlo) to
    evaluate integration space
  1. let $\vec{x}$ be a random vector in the integration space and
    $f(\vec{x})$ the function to evaluate
  2. pick new point $\vec{x}'$ in vicinity of $\vec{x}$
  3. sample from random uniform in $[0, 1]$: $u$
  4. accept $\vec{x}'$ if $u < \frac{f(\vec{x}')}{f(\vec{x})}$, add
    $\vec{x}'$ to chain and iterate (if $f(\vec{x}') > f(\vec{x})$ every
     new link accepted!)
  5. long enough chain samples integration space well
  6. throw away first N elements as "burn in"
  7. generate multiple chains to be less dependent on starting position
  
  $⇒$ $\mathcal{L}_m$ computed by histogram of sampled $g_{ae}²$ values

  - [ ] Check sagemath calculations for x and y systematics

- [ ] *DO NOT HAVE* a section *"about the evaluation of L'"*
    
** Limit method - practicalities for our real case                :noexport:    
- Evaluate $\mathcal{L}'$ in our case ::

  - [ ] background position dependent
    - [ ] use k-d tree to store background cluster information of (x,
      y, E) per cluster. Interpolation using custom metric with
      gaussian weighting in (x, y) but constant weight in E
    - [ ] towards corners need to correct for loss of area
    #+begin_src nim
  template computeBackground(): untyped {.dirty.} =
    let px = c.pos.x.toIdx
    let py = c.pos.y.toIdx
    interp.kd.query_ball_point([px.float, py.float, c.energy.float].toTensor,
                             radius = interp.radius,
                             metric = CustomMetric)
      .compValue()
      .correctEdgeCutoff(interp.radius, px, py) # this should be correct
      .normalizeValue(interp.radius, interp.energyRange, interp.backgroundTime)
      .toIntegrated(interp.trackingTime)
    #+end_src
    - [ ] background values cached, to avoid recomputing values if
      same candidate is asked for

  - [ ] Signal
    - [ ] detection efficiency, window (w/o strongback) + gas +
      telescope efficiency (energy dependent)
    - [ ] axion flux, rescale by g_ae²
    - [ ] conversion prob
    - [ ] raytracing result (telescope focusing) + window strongback

  - [ ] candidate sampling
    - [ ] handled using a grid of NxNxM volumes (x, y, E)
    - [ ] sample in each volume & assign uniform positions in volume
  
** Note about likelihood integral                                 :noexport:

The likelihood is a product of probability density functions. However,
it is important to note that the likelihood is a function of the
*parameter* and not the data. As such integrating over all parameters
does not necessarily equate to 1!

** $s'$ is equivalent to $s_i'$ ?                                 :noexport:

\begin{align*}
s &= Σ_i s_i \\
s_i' &= s_i (1 + θ_s) \\
s' &= Σ_i s_i' \\
   &= Σ_i s_i (1 + θ_s) \\
   &\text{as }(1 + θ_s)\text{ is constant} \\
   &= (1 + θ_s) Σ_i s_i \\
   &= (1 + θ_s) s \\
s' &= s (1 + θ_s) \\
\end{align*}
so indeed, this is perfectly valid.

** Derivation of short form of $\mathcal{L}$ [/]                  :noexport:

- [ ] *WRITE THE NON LOG FORM*

This uses the logarithm form, but the non log form is even easier actually.

\begin{align*}
\ln \mathcal{\mathcal{L}} &= \ln \prod_i \frac{ \frac{(s_i + b_i)^{n_i}}{n_i!} e^{-(s_i + b_i)}  }{ \frac{b_i^{n_i}}{n_i!} e^{-b_i} } \\
  &= \sum_i \ln \frac{ \frac{(s_i + b_i)^{n_i}}{n_i!} e^{-(s_i + b_i)}  }{ \frac{b_i^{n_i}}{n_i!} e^{-b_i} } \\
  &= \sum_i \ln \frac{(s_i + b_i)^{n_i}}{n_i!} e^{-(s_i + b_i)}  - \ln \frac{b_i^{n_i}}{n_i!} e^{-b_i}  \\
  &= \sum_i n_i \ln (s_i + b_i) - \ln n_i! - (s_i + b_i) - (n_i \ln b_i - \ln n_i! -b_i)  \\
  &= \sum_i n_i \ln (s_i + b_i) - (s_i + b_i) - n_i \ln b_i + b_i  \\
  &= \sum_i n_i \ln (s_i + b_i) - (s_i + b_i - b_i) - n_i \ln b_i  \\
  &= \sum_i n_i \ln \left(\frac{s_i + b_i}{b_i}\right) - s_i  \\
  &= -s_{\text{tot}} + \sum_i n_i \ln \left(\frac{s_i + b_i}{b_i} \right) \\
  &\text{or alternatively} \\
  &= -s_{\text{tot}} + \sum_i n_i \ln \left(1 + \frac{s_i}{b_i} \right) \\
\end{align*}

** Implementing a basic limit calculation method [/]              :noexport:

The following are two examples for a basic limit calculation in
code. This is to showcase the basic idea without getting lost in too
many details. The real code we use for the limit is found here *INSERT
ME*

- [ ] *INSERT REF*

Simplest implementation:
- single channel
- no detection efficiencies etc., just a flux that scales with $g²$
- constant background (due to single channel)
- no telescope, i.e. area for signal flux is the same as for
  background (due to no focusing)  
#+begin_src nim :results drawer :exports both :tangle /tmp/simple_limit_example.nim
import unchained, math
## Assumptions:
const totalTime = 100.0.h # 100 of "tracking time"
const totalArea = 10.cm² # assume 10 cm² area (magnet bore and chip! This case has no telescope)
defUnit(cm⁻²•s⁻¹)

proc flux(g: float): cm⁻²•s⁻¹ =
  ## Dummy flux. Just the coupling constant squared · 1e-6
  result = 1e-6 * (g*g).cm⁻²•s⁻¹

proc totalFlux(g: float): float =
  ## Flux integrated to total time and area
  result = flux(g) * totalTime.to(Second) * totalArea

## Assume signal and background in counts of the single channel!
## (Yes, `signal` is the same as `totalFlux` in this case)
proc signal(g: float): float = flux(g) * totalTime * totalArea ## Signal only depends on coupling in this simple model
proc background(): float = 1e-6.cm⁻²•s⁻¹ * totalTime * totalArea ## Single channel, i.e. constant background
  
proc likelihood(g: float, cs: int): float = ## `cs` = number of candidates in the single channel
  result = exp(-totalFlux(g)) # `e^{-s_tot}`
  result *= pow(1 + signal(g) / background(), cs.float)

proc poisson(k: int, λ: float): float = λ^k * exp(-λ) / (fac(k))

echo "Background counts = ", background(), ". Probabilty to measure 4 counts given background: ", poisson(4, background())
echo "equal to signal counts at g = 1: ", signal(1.0)
echo "Likelihood at g = 1 for 4 candidates = ", likelihood(1.0, 4)

## Let's plot it from 0 to 3 assuming 4 candidates
import ggplotnim
let xs = linspace(0.0, 3.0, 100)
let ys = xs.map_inline(likelihood(x, 4))

ggplot(toDf(xs, ys), aes("xs", "ys")) +
  geom_line() +
  ggsave("/tmp/simple_likelihood.pdf")

## Compute limit, CDF@95%
import algorithm
let yCumSum = ys.cumSum()                    # cumulative sum
let yMax = yCumSum.max                       # maximum of the cumulative sum
let yCdf = yCumSum.map_inline(x / yMax)      # normalize to get (empirical) CDF
let limitIdx = yCdf.toSeq1D.lowerBound(0.95) # limit at 95% of the CDF
echo "Limit at : ", xs[limitIdx]
#+end_src

#+RESULTS:
:results:
Background counts = 3.6. Probabilty to measure 4 counts given background: 0.1912223391751322
equal to signal counts at g = 1: 3.6
Likelihood at g = 1 for 4 candidates = 0.4371795591566811
Limit at : 1.12121212121212
:end:

More realistic implementation, above plus:
- real solar axion flux
- TODO: (detection efficiency) (could just use fixed efficiency)
- X-ray telescope without usage of local flux information
- multiple channels in energy
#+begin_src nim :results drawer :exports both :tangle /tmp/energy_bins_limit_example.nim
import unchained, math, seqmath, sequtils, algorithm
## Assumptions:
const totalTime = 100.0.h # 100 of "tracking time"
const areaBore = π * (2.15 * 2.15).cm²
const chipArea = 5.mm * 5.mm # assume all flux is focused into an area of 5x5 mm²
                             # on the detector. Relevant area for background!
defUnit(GeV⁻¹)
defUnit(cm⁻²•s⁻¹)
defUnit(keV⁻¹)
defUnit(keV⁻¹•cm⁻²•s⁻¹)

## Constants defining the channels and background info
const
  Energies =   @[0.5,    1.5,    2.5,    3.5,    4.5,    5.5,     6.5,    7.5,  8.5,    9.5].mapIt(it.keV)
  Background = @[0.5e-5, 2.5e-5, 4.5e-5, 4.0e-5, 1.0e-5, 0.75e-5, 0.8e-5, 3e-5, 3.5e-5, 2.0e-5]
    .mapIt(it.keV⁻¹•cm⁻²•s⁻¹) # convert to a rate
  ## A possible set of candidates from `Background · chipArea · totalTime · 1 keV`
  ## (1e-5 · 5x5mm² · 100h = 0.9 counts)
  Candidates = @[0,      2,      7,     3,      1,      0,       1,      4,    3,      2]

proc solarAxionFlux(ω: keV, g_aγ: GeV⁻¹): keV⁻¹•cm⁻²•s⁻¹ =
  # axion flux produced by the Primakoff effect in solar core
  # in units of keV⁻¹•m⁻²•yr⁻¹
  let flux = 2.0 * 1e18.keV⁻¹•m⁻²•yr⁻¹ * (g_aγ / 1e-12.GeV⁻¹)^2 * pow(ω / 1.keV, 2.450) * exp(-0.829 * ω / 1.keV)
  # convert flux to correct units
  result = flux.to(keV⁻¹•cm⁻²•s⁻¹)

func conversionProbability(g_aγ: GeV⁻¹): UnitLess =
  ## the conversion probability in the CAST magnet (depends on g_aγ)
  ## simplified vacuum conversion prob. for small masses
  let B = 9.0.T
  let L = 9.26.m
  result = pow( (g_aγ * B.toNaturalUnit * L.toNaturalUnit / 2.0), 2.0 )

from numericalnim import simpson # simpson numerical integration routine  
proc totalFlux(g_aγ: GeV⁻¹): float =
  ## Flux integrated to total time, energy and area
  # 1. integrate the solar flux
  ## NOTE: in practice this integration must not be done in this proc! Only perform once!
  let xs = linspace(0.0, 10.0, 100)
  let fl = xs.mapIt(solarAxionFlux(it.keV, g_aγ))
  let integral = simpson(fl.mapIt(it.float), # convert units to float for compatibility
                         xs).cm⁻²•s⁻¹ # convert back to units (integrated out `keV⁻¹`!)
  # 2. compute final flux by "integrating" out the time and area
  result = integral * totalTime * areaBore * conversionProbability(g_aγ)

## NOTE: only important that signal and background have the same units!  
proc signal(E: keV, g_aγ: GeV⁻¹): keV⁻¹ =
  ## Returns the axion flux based on `g` and energy `E`
  result = solarAxionFlux(E, g_aγ) * totalTime.to(Second) * areaBore * conversionProbability(g_aγ)

proc background(E: keV): keV⁻¹ =
  ## Compute an interpolation of energies and background
  ## NOTE: For simplicity we only evaluate at the channel energies anyway. In practice
  ## one likely wants interpolation to handle all energies in the allowed range correctly!
  let idx = Energies.lowerBound(E) # get idx of this energy
  ## Note: area of interest is the region on the chip, in which the signal is focused!
  ## This also allows us to see that the "closer" we cut to the expected axion signal on the
  ## detector, the less background we have compared to the *fixed* signal flux!
  result = (Background[idx] * totalTime * chipArea).to(keV⁻¹)
  
proc likelihood(g_aγ: GeV⁻¹, energies: seq[keV], cs: seq[int]): float =
  ## `energies` = energies corresponding to each channel
  ## `cs` = each element is number of counts in that energy channel
  result = exp(-totalFlux(g_aγ)) # `e^{-s_tot}`
  for i in 0 ..< cs.len:
    let c = cs[i]       # number of candidates in this channel
    let E = energies[i] # energy of this channel
    let s = signal(E, g_aγ)
    let b = background(E)
    result *= pow(1 + signal(E, g_aγ) / background(E), c.float)

## Let's plot it from 0 to 3 assuming 4 candidates
import ggplotnim
# define coupling constants
let xs = logspace(-13, -10, 300).mapIt(it.GeV⁻¹) # logspace 1e-13 GeV⁻¹ to 1e-8 GeV⁻¹
let ys = xs.mapIt(likelihood(it, Energies, Candidates))

let df = toDf({"xs" : xs.mapIt(it.float), ys})
ggplot(df, aes("xs", "ys")) +
  geom_line() +
  ggsave("/tmp/energy_bins_likelihood.pdf")

## Compute limit, CDF@95%
import algorithm
# limit needs non logspace x & y data! (at least if computed in this simple way)
let xLin = linspace(0.0, 1e-10, 1000).mapIt(it.GeV⁻¹)
let yLin = xLin.mapIt(likelihood(it, Energies, Candidates))
let yCumSum = yLin.cumSum()          # cumulative sum
let yMax = yCumSum.max               # maximum of the cumulative sum
let yCdf = yCumSum.mapIt(it / yMax)  # normalize to get (empirical) CDF
let limitIdx = yCdf.lowerBound(0.95) # limit at 95% of the CDF
echo "Limit at : ", xLin[limitIdx]
# Code outputs:
# Limit at : 6.44645e-11 GeV⁻¹
#+end_src

#+RESULTS:
:results:
Limit at : 6.44645e-11 GeV⁻¹
:end:



** Generic limit calculation method                               :noexport:
:PROPERTIES:
:CUSTOM_ID: sec:limit:limit_method
:END:

- [ ] *REPHRASE ALL THIS AS TO FIRST DERIVE THE ORIGIN OF THE NATURE
  VERSION, THEN PRESENT OUR EXTENSION* 
 
We will now present a limit calculation method that is based on the
limit presented in cite:cast_nature, but extended to provide a fully
generic limit calculation method that requires no restriction to
specific regions of interest.

The likelihood function used in cite:cast_nature is
#+NAME: eq:nature_likelihood_function
\begin{align}
\ln \mathcal{L} = -R_T + \sum_i^n \ln R(E_i, d_i, \vec{x}_i) 
\end{align}
where $R_T$ is the total expected signal and $R$ the sum of signal and
background contributions. The details will be explained further down.

First we will derive the Bayesian method and discuss the individual contributions.

#+begin_comment
Describe log L and χ² distribution and how to compute limit.

Unphysicality, fix by rescaling, χ² min + 4 thing
*UPDATE*: good that we now understand how this actually works,
i.e. integrate the posterior probability (likelihood * prior / normalization)
#+end_comment

The maths of the likelihood expression we use, is mostly straight
forward.

A likelihood function is purely defined as the product of the
individual probabilities for each 'channel' in our measurement. That
way the likelihood gives us the total probability to get this exact
measurement outcome out of all possible outcomes, as a function of the
coupling constant (in our case).

If we start from a likelihood ratio \footnote{Likelihood ratio simply
means taking a ratio of two different likelihood functions.} of the
signal + background hypothesis over the pure background hypothesis, in
the binned case we can derive formula [[#eq:nature_likelihood_function]]
from first principles. The number of measured counts in each bin is
simply a Poisson distribution:

#+begin_comment
Clarify what a "bin" refers to and what "channels" are. 
#+end_comment

\[
P_{\text{Pois}}(k; λ) = \frac{λ^k e^{-λ}}{k!} 
\]
for each bin an expected number of counts $λ$ (the mean) then means a probability given $P$
for a "measured" number of counts $k$. Combining multiple "channels"
is then simply the product of these individual channels, giving us the
likelihood for one experiment:

\[
\mathcal{L} = \prod_i P_{i, \text{Pois}}(k; λ) = \prod_i \frac{λ_i^{k_i} e^{-λ_i}}{k_i!}
\]

Applying this to the previously mentioned likelihood ratio $\mathcal{L}$ gives
us:

\[
\mathcal{L} = \frac{\prod_i P_{i, \text{Pois}}(n_i; s_i + b_i)}{\prod_iP_{i,\text{Pois}}(n_i; b_i)} =
 \prod_i \frac{ \frac{(s_i + b_i)^{n_i}}{n_i!} e^{-(s_i + b_i)} }{ \frac{b_i^{n_i}}{n_i!} e^{-b_i}}
\]

where $n_i$ is simply the number of measured candidates within the
signal sensitive region in each bin $i$. Typically, each bin might be a
bin in energy, but it can be any kind of "bin" as long as each bin
corresponds to something that follows a Poisson distribution. We will
make use of this fact in a bit.

This can be interpreted as an extended likelihood function, as the
ratio of two Poisson distributions does not satisfy the normalization
condition:

\[
\int_{-∞}^{∞}P\, \mathrm{d}x = 1
\]

anymore. Instead we have:

\[
\int_{-∞}^{∞}Q\, \mathrm{d}x = N
\]

where $N$ can be interpreted as a hypothetical number of total number
of counts measured in the experiment; the starting point of the
definition of the extended maximum likelihood estimation.

From here we derive the logarithm of the expression to get the
numerically more stable $\ln \mathcal{L}$ expression:

#+NAME: eq:likelihood_1_plus_s_over_b_form
\begin{align*}
\ln \mathcal{\mathcal{L}} &= \ln \prod_i \frac{ \frac{(s_i + b_i)^{n_i}}{n_i!} e^{-(s_i + b_i)}  }{ \frac{b_i^{n_i}}{n_i!} e^{-b_i} } \\
  &= \sum_i \ln \frac{ \frac{(s_i + b_i)^{n_i}}{n_i!} e^{-(s_i + b_i)}  }{ \frac{b_i^{n_i}}{n_i!} e^{-b_i} } \\
  &= \sum_i \ln \frac{(s_i + b_i)^{n_i}}{n_i!} e^{-(s_i + b_i)}  - \ln \frac{b_i^{n_i}}{n_i!} e^{-b_i}  \\
  &= \sum_i n_i \ln (s_i + b_i) - \ln n_i! - (s_i + b_i) - (n_i \ln b_i - \ln n_i! -b_i)  \\
  &= \sum_i n_i \ln (s_i + b_i) - (s_i + b_i) - n_i \ln b_i + b_i  \\
  &= \sum_i n_i \ln (s_i + b_i) - (s_i + b_i - b_i) - n_i \ln b_i  \\
  &= \sum_i n_i \ln \left(\frac{s_i + b_i}{b_i}\right) - s_i  \\
  &= -s_{\text{tot}} + \sum_i n_i \ln \left(\frac{s_i + b_i}{b_i} \right) \\
  &\text{or alternatively} \\
  &= -s_{\text{tot}} + \sum_i n_i \ln \left(1 + \frac{s_i}{b_i} \right) \\
\end{align*}

here $s_{\text{tot}}$ represents the total number of "signal" like
counts expected (in our case total number of expected photons due to
axion conversion).

The last two lines show us that all that is really important is that
the normalization of $s_i$ and $b_i$ are the same. The absolute
normalization (e.g. if it's $\si{keV⁻¹}$ or absolute counts etc.) plays no
role as that will be a constant multiplier. That constant can always
be neglected in the total likelihood (a constant only moves the logL
curve up / down, but does not change the location of its maximum!).

Coming back to the "channels" / the binning: if we choose our bins
such that they are bins in time, so small that each bin either
contains 0 or 1 count, we reduce this to our desired unbinned log
likelihood, if we start from the second to last line above and drop
the constant $-\ln b_i$ term:

\[
\ln \mathcal{L} = -R_{\text{tot}} + \sum_{\text{candidates}} \ln(s_i + b_i)
\]
where the sum now runs over each candidate instead of the abstract
"channels".


- [ ] *CLARIFY THE MEANING OF EACH TERM AND WHAT IT CORRESPONDS TO
  BETTER!*
  
With an understanding of where the formula comes from, we can more
safely make statements about what the shape of the logL curve is going
to look like. For that it is important to realize that:
- $-R_{\text{tot}}$ depends on $g_{ae²}$, $-R_{\text{tot}}(g_{ae²})$
- $s_i$ depends on $g_{ae²}$, $\vec{x}$ and the cluster energy $E$,
  $s_i(g_{ae²}, \vec{x}, E)$
- $b_i$ only depends on the cluster energy $E$
- [ ] *IN OUR APPLICATION $b_i$ DOES DEPEND ON POSITION AS WELL!*
  
This means $b_i$ for all intents and purposes is constant under a
change of the coupling constant for a fixed set of candidate
clusters. For a scan over $\ln\mathcal{L}$ this is precisely given.

Now let's consider what each part's contribution will look like as a
$\ln\mathcal{L}$ curve:

1. $-R_{\text{tot}}$: the total number of counts depends on the axion
   flux, the tracking time and the total detection efficiencies. The
   latter two are simply constants when integrating over the region of
   interest in energy, namely \SIrange{0}{10}{\keV}. The axion flux
   scales as:
   \[
   f(g') = f(g) \frac{g^{\prime 2}}{g²}
   \]
   i.e. a squared rescaling of the flux. Given that we scan the logL
   space in $g_{ae²}$, this means the total flux (the integral of $f$
   over all energies) just scales linearly with $g_{ae²}$.
   Due to the minus sign in front, the result is plainly a line with
   negative slope going through the origin at $g_{ae²} = 0$.
2. $b_i$: the background hypothesis is a function only depending on
   the energy of each cluster (and implicitly on the relevant area and
   tracking time). For a fixed set of candidates during a $g_{ae²}$
   scan, its contribution is plainly constant.
3. $s_i$: this is the complicated one. It not only depends on
   $g_{ae²}$, but also the energy $E$ and more importantly the
   position of the cluster center $\vec{x}$. The latter is used for
   the effective flux at each position on the chip that is expected
   from axion conversion after focusing of the X-ray telescope.
   In principle the cluster center positions are constant for a single
   scan of $g_{ae²}$. This means effectively the signal $s_i$ behaves
   exactly like $R_{\text{tot}}$ under $g_{ae²}$. The resulting
   behavior is thus also linear, except with a positive slope.
   The big difference between $s_i$ and $R_{\text{tot}}$ however is
   that $R_{\text{tot}}$ is integrated over all energies, whereas
   $s_i$ is only evaluated at specific energies (in keV⁻¹).

#+begin_comment
Rephrase this whole part & remove the confusion, as it's now fully
understood how to approach it.
#+end_comment

Combining these three facts we expect to find some maximum somewhere,
where the $R_{\text{tot}}$ term and the $s_i$ term cancel each
other. The $b_i$ term only contributes an offset (which however
depends on the candidates, which is why it cannot be ignored!).

This brings us to a particular problem:
What happens if the candidates are all located outside of the axion
sensitive region? In that case their contribution will be zero, due to
the position dependency of $\vec{x}$. This leads to a pure
$R_{\text{tot}}$ negative slope on top of a constant background $b_i$.
In this case the logL curve does *not* have a maximum! And in cases
where arbitrarily little contribution is had, there *will* be a
maximum somewhere, but it will be very far into the unphysical range,
at which point the 1σ width (based on logL_max - 0.5) yield a width
that leads to a physical limit at 0 (due to the gaussian CDF being
essentially 1 many σ away from the center).

- [ ] *THIS IS NOT A PROBLEM ANYMORE*
Given our statistics of $O(30)$ candidates in our tracking time, this
presents a problem. For toy experiments there is a high chance of
getting precisely that problem. A large number of candidates (70 - 90%
maybe) will be outside the sensitive region, resulting in no good way
to determine the limit.

**** Further expl: Bayes integral *AFTER MAIL FROM IGOR ON <2021-10-03 Sun>*:

- [ ] *THIS SHOULD BE REPHRASED (how it relates to final version /
  application of nature version) AND BECOME NOEXPORT*
  -> This is very useful knowledge, because otherwise one gets as
  confused as I did!


Essentially saying that we simply integrate over and demand:

0.95 = ∫_-∞^∞ L(g_ae²) Π(g_ae²) / L_0 d(g_ae²)

where L is the likelihood function (*not* the ln L!), Π is the prior
that is used to exclude the unphysical region of the likelihood phase
space, i.e. it is:

Π(g_ae²) = { 0 if g_ae² < 0, 1 if g_ae² >= 0 }

And L_0 is simply a normalization constant to make sure the integral
is normalized to 1.

Thus, the integral reduces to the physical range:

0.95 = ∫_0^∞ L(g_ae²) / L_0 d(g_ae²)

where the 0.95 is, due to normalization, simply the requirement of a
95% confidence limit.

With this out of the way I implemented this into the limit calculation
code as the =lkBayesScan= limit.

** Likelihood ingredients in detail [/]
:PROPERTIES:
:CUSTOM_ID: sec:limit:limit_method
:END:

- [ ] *All The text here below is irrelevant now*
  -> Must become an introduction of sort for the ingredients.
- [ ] *RENAME THE CUSTOM ID OF THIS SECTION*  

The limit calculation method is based on the approach presented in
cite:cast_nature, with modifications to better suit the GridPix
detector and make the method more generic (under exchange of the
model to be studied).

#+begin_comment
Extend this by the derivation for the marginal likelihood that shows
how one gets to the shown equation from what's shown in the previous
section.

-> We derive this in ~statusAndProgress~, incl Sagemath stuff. The
sagemath must become a :noexport: section.

It's important we highlight how one adds additional terms to the
original likelihood to end up at the marginal likelihood.
#+end_comment

We will now go through the ingredients for the limit method one by
one. The final likelihood including nuisance parameters we evaluate is
(see section [[#sec:limit:limit_method]] for the derivation):

\[
\mathcal{L}_{SBM} = ∫_{-∞}^∞∫_{-∞}^∞∫_{-∞}^∞∫_{-∞}^∞ \exp(-s'_{\text{tot}}) \cdot \prod_i \left(1 +
\frac{s_i''}{b_i'}\right) \cdot
\exp\left[-\frac{θ_b²}{2 σ_b²} - \frac{θ_s²}{2 σ_s²} -
\frac{θ_x²}{2 σ_x²} - \frac{θ_y²}{2 σ_y²} \right]
\, \mathrm{d}\,θ_b \mathrm{d}\,θ_s \mathrm{d}\,θ_x \mathrm{d}\,θ_y.
\]
where primed symbols refer to the base symbol with a modification due
to the value of the corresponding nuisance parameter, i.e.
$x' = x(1 + θ_x)$. The double primed $s_i''$ not only includes $θ_s$,
but also the position dependent nuisance parameters $θ_x$ and $θ_y$
(see again the previous section).

The inputs required to compute a likelihood value are (with the
relevant parameters):
- a set of candidate clusters (either from the real solar tracking or
  randomly sampled ones) with cluster centers at $(x_i, y_i)$ and
  associated energies $E_i$ (over which the product $i$ runs).
- the solar axion flux produced the model to be analyzed, as a
  function of energy (depending on $g_{ae}$ and $g_{aγ}$, but the
  $g_{aγ}$ contribution can be ignored for certain choices of $g_{ae}$
  and $g_{aγ}$.
- the conversion probability of axions in a magnetic field (depending
  on $g_{aγ}$.
- the efficiency of the X-ray optics as a function of energy $E$.
- the transmission probability of X-rays through the detector window
  as a function of $E$ and the entrance position $(x, y)$.
- the absorption probability of X-rays in the used Argon gas as a
  function of energy $E$.
- the average depth $\langle d \rangle$ at which the X-rays produce a photo-electron in
  the Argon gas for the expected flux of converted solar axions (after
  propagation through the X-ray optics and detector window).
- the resulting flux of axion induced X-rays as a function of the
  position $(x, y)$ on the detector, depending on $g_{ae}$.
- the expected background rate at any position $(x, y)$ in the
  detector and any energy $E$.

From here we will go through each of these contributions to explain
how each is obtained and what they look like, specific to our limit
calculation and the CAST data taking with the Septemboard detector.

  \begin{equation}
  s_i(g) = f(g, E_i) · A · t · P_{a \rightarrow γ}(g_{aγ}) · ε(E_i) · r(x_i, y_i)
  \end{equation}

- [ ] Should we here essentially just refer back to the "theory like"
  sections before where each of these ingredients is already
  introduced? At least for things like Argon absorption / detection
  efficiency etc. those will certainly be presented before. That means
  by showing them here again, we essentially show the same thing
  again.
  For things like the background interpolation that I would just
  introduce "somewhere here".

  One option would be: for all where it _makes sense_, have a big
  "inputs plot" (maybe a facet, or a =ggmulti= plot) of all inputs we
  have? (*UPDATE*: <2022-08-13 Sat 15:48> see sanity checks for
  likelihood code)
  Then again, here we mainly present the *technique* still. So for
  example the final candidates wouldn't show up here. But ok, it's
  *only* the candidates that actually changes.

*** Magnet bore and solar tracking time - $A$, $t$

Starting with the simplest inputs to the signal, the magnet bore area
and the solar tracking time. The CAST magnet has a bore diameter of
$d_{\text{bore}} = \SI{43}{mm}$, as introduced in
sec. [[#sec:helioscopes:cast]]. The relevant area for the solar axion flux
is the entire magnet bore, because the X-ray telescope covers the full
area. As such, $A$ is a constant of:

\[
A = π (\SI{21.5}{mm})² = \SI{1452.2}{mm²}.
\]

The time of interest is the total solar tracking duration, in which
the detector was sensitive (i.e. removing the dead time due to
readout). As given in the CAST data taking overview, the amount of
active solar tracking time is

\[
t = \SI{161.05}{h}.
\]


*** Solar axion flux - $f(g, E_i)$ [0/3]
:PROPERTIES:
:CUSTOM_ID: sec:limit:ingredients:solar_axion_flux
:END:

The solar axion flux for the model to be studied must be known both as
a function of energy $E$ and also as a differential flux as a function
of the solar radius to produce the expected axion image via raytracing
(see sec. [[#sec:appendix:raytracing]]). 

-> This is only for the axion *IMAGE*:
This means we require knowledge about the production as shown in the
heatmap of fig. [[fig:limit:axion_production_heatmap]]. (Move this to
theory?)

- [ ] generate heatmap of production as function of energy and solar
  radius
- [ ] or show here just the signal?
- [ ] the idea being here that we highlight what one needs to replace
  in order to compute a limit of something else possibly (which then
  allows us later to have a section "axion-photon" or "chameleon"
  where we just present the inputs "see sec. blub, here's the
  production heatmap" kind of deal

The solar axion flux is based on the calculations by J. Redondo
cite:Redondo_2013. An implementation following it is found in
*RAYTRACING_CODE*. [fn:axion_flux_raytracing] The code includes the
calculation of the Primakoff production as well as the ABC
processes. For the latter the Opacity Project (OPAC) *CITE ME* are
used, following cite:Redondo_2013.

As the input to the limit calculation for the signal $s_i$ the solar
axion flux $f(g, E_i)$ is needed as a differential flux. For a
specific set of coupling constants, $g_{ae} = \num{1e-13}$ and $g_{aγ}
= \SI{1e-12}{GeV⁻¹}$, the flux is shown in
fig. [[fig:limit:ingredients:flux]]. The data in the plot is computed
using a Sun ⇔ Earth distance of $d_{S⇔E} = \SI{0.989}{AU}$ due to the
times of the year in which solar trackings were taken at
CAST. Fig. [[fig:limit:ingredients:distance_sun_earth]] shows the distance
between Sun and Earth during the entire data taking period, with the
solar trackings marked in purple. The data for the distance is
obtained using the JPL Horizons API.

- [ ] *JPL HORIZONS API CITATION*
  -> I started the citation in ~references.org~, but it's not finished
  yet!!!!! There's probably an official citation that one should use
  for it!

- [X] *Should we show the Earth⇔Sun distance here or in data summary
  of CAST data taking?*
  -> For now here. Might be moved at some point.

- [ ] *ADD REFERENCE TO AXIONELECTRONLIMIT*.
  -> *Rename the repository!* Has nothing to do with a *LIMIT*

\begin{figure}[htbp]
  \centering
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{~/org/Figs/statusAndProgress/differential_flux_sun_earth_distance/differential_solar_axion_fluxg_ae_1e-13_g_ag_1e-12_g_aN_1e-15_0.989AU.pdf}
    \caption{Solar axion flux}
    \label{fig:limit:ingredients:solar_axion_flux}
  \end{subfigure}%
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{~/org/Figs/statusAndProgress/systematics/sun_earth_distance_cast_solar_tracking.pdf}
    \caption{Distance Sun-Earth}
    \label{fig:limit:ingredients:distance_sun_earth}
  \end{subfigure}%
  \label{fig:limit:ingredients:flux_distance}
  \caption{\subref{fig:limit:ingredients:solar_axion_flux} Differential solar axion flux assuming a
  distance to the Sun of $\SI{0.989}{AU}$ based on \subref{fig:limit:ingredients:distance_sun_earth}}.
\end{figure}

[fn:axion_flux_raytracing] The code was mainly developed by Johanna
von Oy under my supervision. I only contributed minor feature additions
and refactoring, as well as performance related modifications.

**** Generate solar axion flux plot and distance Sun-Earth      :noexport:

Differential flux:
-> ~readOpacityFiles~

Distance Sun Earth:
-> ~horizonsAPI~ plus code in ~journal.org~

*** Conversion probability - $P_{aγ}(g_{aγ})$

The conversion probability of the arriving axions is simply a constant
factor, depending on $g_{aγ}$, see section
[[#sec:theory:axion_interactions]] for the derivation from the general
formula.

The simplified expression for coherent conversion in a constant
magnetic field [fn:inhomogeneous_fields] is

\[
P(g_{aγ}, B, L) = \left(\frac{g_{aγ} \cdot B \cdot L}{2}\right)^2
\]
where the relevant numbers for the CAST magnet are:

\begin{align*}
B &= \SI{8.8}{T} &↦ B_{\text{natural}} &= \SI{1719.1}{eV^2} \\
L &= \SI{9.26}{m} &↦ L_{\text{natural}} &= \SI{4.69272e7}{eV^{-1}}. 
\end{align*}

The magnetic field is taken from the CAST slow control log files and
matches the values used in the paper of CAST CAPP
cite:cast_capp_nature (in contrast to some older papers which assumed
$\SI{9}{T}$, based on when the magnet was still intended to be run at
above $\SI{13000}{A}$).

Assuming a fixed axion-photon coupling of $g_{aγ} =
\SI{1e-12}{GeV^{-1}}$ the conversion probability comes out to:
\begin{align*}
P(g_{aγ}, B, L) &= \left(\frac{g_{aγ} \cdot B \cdot L}{2}\right)^2 \\
               &= \left(\frac{\SI{1e-12}{GeV^{-1}} \cdot \SI{1719.1}{eV^2} \cdot \SI{4.693e7}{eV^{-1}}}{2}\right)^2 \\
               &= \num{1.627e-21}
\end{align*}

[fn:inhomogeneous_fields] Note that in a perfect analysis one would
compute the conversion in a realistic magnetic field, as the field
strength is not perfectly homogeneous. That would require a very
precise field map of the magnet. In addition the calculations for
axion conversions in inhomogeneous magnetic fields is significantly
more complicated. As far as I understand it requires essentially a
"path integral like" approach of all possible paths through the
magnet, where each path sees different, varying field strengths. Due
to the small size of the LHC dipole prototype magnet and general
stringent requirements for homogeneity this is not done for this
analysis. However, likely for future (Baby)IAXO analyses this will be
necessary.

**** TODO for this section [/]                                  :noexport:

- [ ] *FIX CITATION OF CAST CAPP PAPER*

- [X] *Introduce the full expression*
  -> In theory
- [X] *Show that it simplifies to sinc(x) case in vacuum*
  -> In theory
- [X] *Simplify for small masses sin(x) -> x*
  -> In theory
- [X] *Show probability in SI units!*
  -> In theory
- [ ] *MENTION 8.8 T from CAPP & CAST SLOW CONTROL LOGS*

**** Computing conversion factors and comparing natural to SI eq. :noexport:

The conversion factors from Tesla and meter to natural units are as follows:
#+begin_src nim :results raw
import unchained
echo "Conversion factor Tesla: ", 1.T.toNaturalUnit()
echo "Conversion factor Meter: ", 1.m.toNaturalUnit()
#+end_src

#+RESULTS:
Conversion factor Tesla: 195.353 ElectronVolt²
Conversion factor Meter: 5.06773e+06 ElectronVolt⁻¹

*TODO*: Move this out of the thesis and just show the numbers in text?
Keep the "derivation / computation" for the "full" version (:noexport:
?).

As such, the resulting conversion probability ends up as:

#+begin_src nim :results raw
import unchained, math
echo "8.8 T  = ", 8.8.T.toNaturalUnit()
echo "9.26 m = ", 9.26.m.toNaturalUnit()
echo "P      = ", pow( 1e-12.GeV⁻¹ * 8.8.T.toNaturalUnit() * 9.26.m.toNaturalUnit() / 2.0, 2.0)
#+end_src

#+RESULTS:
8.8 T  = 1719.1 eV²
9.26 m = 4.69272e+07 eV⁻¹
P      = 1.627022264358953e-21

\begin{align*}
P(g_{aγ}, B, L) &= \left(\frac{g_{aγ} \cdot B \cdot L}{2}\right)^2 \\
               &= \left(\frac{\SI{1e-12}{\per GeV} \cdot \SI{1719.1}{eV^2} \cdot \SI{4.693e7}{eV}}{2}\right)^2 \\
               &= \num{1.627e-21}
\end{align*}

Note that this is of the same (inverse) order of magnitude as the flux
of solar axions ($\sim10^{21}$ in some sensible unit of time), meaning
the experiment expects $\mathcal{O}(1)$ counts, which is sensible.

#+begin_src nim
import unchained, math
echo "8.8 T  = ", 8.8.T.toNaturalUnit()
echo "9.26 m = ", 9.26.m.toNaturalUnit()
echo "P(natural) = ", pow( 1e-12.GeV⁻¹ * 8.8.T.toNaturalUnit() * 9.26.m.toNaturalUnit() / 2.0, 2.0)
echo "P(SI)      = ", ε0 * (hp / (2*π)) * (c^3) * (1e-12.GeV⁻¹ * 8.8.T * 9.26.m / 2.0)^2
#+end_src

#+RESULTS:
|        8.8 | T | =                     | 1719.1 eV²       |
|       9.26 | m | =                     | 4.69272e+07 eV⁻¹ |
| P(natural) | = | 1.627022264358953e-21 |                  |
|      P(SI) | = | 1.62702e-21 UnitLess  |                  |

As we can see, both approaches yield the same numbers, meaning the
additional conversion factors are correct.

*** Detection efficiency - $ε(E_i)$ [/]
:PROPERTIES:
:CUSTOM_ID: sec:limit:ingredients:detection_eff
:END:

- [ ] *Don't forget this includes things like the veto efficiencies as
  well! Strongback only implicitly!*

The detection efficiency $ε(E_i)$ includes multiple aspects of the
full setup. It can be further decomposed into 

\[
ε(E_i) = ε_{\text{telescope}}(E_i) · ε_{\text{window}}(E_i) · ε_{\text{gas}}(E_i) · ε_{\text{software eff.}} · ε_{\text{veto eff.}}
\]

where the first three parts are energy dependent and the latter two
constants.

- telescope effective area (i.e. reflectivity under different angles
  etc)
- detector window transmission
- gas absorption probability
- software efficiency (setup dependent)
- veto efficiency (setup dependent)

**** TODO Telescope efficiency - $ε_{\text{telescope}}(E_i)$ [/]

- [ ] *UPDATE THE USED EFFECTIVE AREA!!!*
  See journal [[#sec:journal:2023_07_13]]!!

- [ ] Merge with window transmission and argon gas, as detection efficiency

- [X] Refer back to section that describes the LLNL telescope!

- [ ] Think about having a proper introduction to effective area in
  the theory introduction?
  -> Then here we can just give a quick reiteration and refer to the
  plot of the combined efficiency?

The X-ray telescope further has a direct impact not only on the shape
of the axion signal on the readout, but also the total number of
X-rays transmitted. The effective transmission of an X-ray telescope
is significantly lower than in the optical range. This is typically
quoted using the term "effective area". In section
[[#sec:helioscopes:cast:xray_optics]] the effective area of the two X-ray
optics used at CAST is shown. 

The term effective area refers to the equivalent area a perfect X-ray
telescope would cover. As such, the real efficiency $ε_{\text{tel}}$ can be computed by
the ratio of the effective area $A_{\text{eff}}$ and the total area of the optic $A_{\text{tel}}$.

\[
ε_{\text{tel}}(E) = \frac{A_{\text{eff}}(E)}{A_{\text{tel}}}
\]

where the effective area $A_{\text{eff}}$ depends on the
energy. [fn:eff_area_reflectivity] In case of CAST the relevant total
area is not actually the cross-sectional area of the optic itself, but
rather the exposed area due to the diameter of the magnet
coldbore. With a coldbore diameter of $d_{\text{cb}} = \SI{43}{mm}$
the effective area can be converted to $ε_{\text{tel}}$.

The resulting effective area is shown in
fig. [[fig:limit:limit_method:combined_detection_eff]] in the next section
together with the window transmission and gas absorption.

[fn:eff_area_reflectivity] Note that $ε_{\text{tel}}$ here is the average effective
efficiency of the full telescope and *not* the reflectivity of a
single shell. As a Wolter I optic requires two reflections
$ε_{\text{tel}}$ is equivalent to the reflectivity squared
$R²$. Individual reflectivities of shells are further complicated by
the fact that different shells receive parallel light under different
angles, which means the reflectivity varies between shells. Therefore,
this is a measure for the average efficiency.


**** Window transmission and Argon gas absorption - $ε_{\text{window}}(E_i), ε_{\text{gas}}(E_i)$

The detector entrance window is the next point affecting the possible
signal to be detected. The windows, as explained in section
[[#sec:detector:sin_window]] are made from $\SI{300}{nm}$ thick silicon
nitride with a \SI{20}{nm} thick Aluminium coating. Its transmission
is very good down to about $\SI{1}{keV} below which it also starts to
degrade rapidly.

While the window also has 4 \SI{500}{μm} thick strongbacks which in
total occlude about $\SI{22.2}{\%} of the center region, these are
_not_ taken into account into the combined detection
efficiency. Instead they are handled together with the axion image
$r(x_i, y_i)$ in sec. [[#sec:limit:ingredients:raytracing]]. 

- [ ] plot of the effective area
- [ ] compute window transmission with =xrayAttenuation=
  -> Needs to be done in theory already! Redo that plot that still
  uses Henke data!
  -> Already *is* an xrayAttenuation plot! But it only shows Si3N4 and
  Argon, no aluminum or isobutane.

**** Software efficiency and veto efficiency $ε_{\text{software eff.}} · ε_{\text{veto eff.}}$

The software efficiency $ε_{\text{software eff.}}$ of course depends
on the specific setting which is used. Its value will range from
somewhere between \SIrange{80}{97}{\%}.  The veto efficiencies in
principle can also vary significantly depending on the choice of
parameters (e.g. whether the 'line veto' uses an eccentricity cutoff
or not), but as explained in
sec. [[#sec:background:estimate_veto_efficiency]] the septem and line
vetoes are just considered as either yes or no. The FADC veto has also
been fixed to a $1^{\text{st}}$ to $99^{\text{th}}$ percentile cut on
the signal rise time, see sec. [[#sec:background:fadc_veto]]. 

As such the relevant veto efficiencies are:

\begin{align*}
ε_{\text{FADC}} &= \SI{98}{\%} \\
ε_{\text{septem}} &= \SI{78.41}{\%} \\
ε_{\text{line}} &= \SI{86.02}{\%} \\
ε_{\text{septem+line}} &= \SI{73.25}{\%} 
\end{align*}

where the last one corresponds to using both the septem and the line
veto at the same time. Considering for example the case of using both
these vetoes together with a software efficiency of $\SI{80}{\%}$ we
see that the combined efficiency is already only about
$\SI{58.6}{\%}$, which is an extreme loss in sensitivity. 

**** Combined detection efficiency

The previous sections cover aspects which affect the detection
efficiency of the detector and thus impact the amount of signal
available. Combined they yield a detection efficiency as shown in
fig. [[fig:limit:limit_method:combined_detection_eff]]. As can be seen,
the combined detection efficiency maxes out at about 40% around 1.5
keV (*CHECK NUMBERS*) without taking into account the software and
veto efficiencies.

*TODO*: Also include the version that is split up into the individual
pieces somewhere!

#+ATTR_LATEX: :width 0.8\textwidth
#+CAPTION: The combined detection efficiency of the detector, taking into account the
#+CAPTION: telescope efficiency via the effective area, the window absorption probability
#+CAPTION: and the absorption probability in the detector gas.
#+CAPTION: 
#+CAPTION: *TODO:* REPLACE THE PLOT! ONLY DET EFF
#+NAME: fig:limit:limit_method:combined_detection_eff
[[~/phd/Figs/limit/combined_detection_efficiency.pdf]]

# The old plot we had here, from the sanity checks
# ~/org/Figs/statusAndProgress/limitSanityChecks/sanity_detection_eff.pdf

***** Generate plot of detection efficiency                    :noexport:

- [ ] Well, do we need the ingredients separately? Not really right?

We need the effective area (ideally we would compute it! but of course
currently we cannot reproduce it :( ).

So just read the extended LLNL file.

- [X] Need densities of Aluminium, ...
  -> 2.7 g•cm⁻³
- [X] Need to update xrayAttenuation to create the plot!
  -> Done.
- [ ] NEED TO update numericalnim for interpolation!
- [ ] NEED TO update seqmath for linspace fixes
  
#+begin_src nim :tangle /home/basti/phd/code/combined_detection_efficiency.nim
import std / strutils
import xrayAttenuation, ggplotnim

proc initCASTGasMixture*(): GasMixture =
  ## Returns the absorption length for the given energy in keV for CAST
  ## gas conditions:
  ## - Argon / Isobutane 97.7 / 2.3 %
  ## - 20°C ( for this difference in temperature barely matters)
  # define Argon
  let arC = compound((Ar, 1)) # need Argon gas as a Compound
  let isobutane = compound((C, 4), (H, 10))
  # define the gas mixture
  result = initGasMixture(293.K, 1050.mbar, [(arC, 0.977), (isobutane, 0.023)])


# generate a compound of silicon and nitrogen with correct number of atoms
let Si₃N₄ = compound((Si, 3), (N, 4))
# And the aluminium coating of 20nm.
let Al = Aluminium.init(2.7.g•cm⁻³)
# instaniate the CAST gas mixture
let gas = initCASTGasMixture()

echo Si₃N₄.ρ
echo Al.ρ
echo gas.ρ

# define energies in which to compute the transmission
# (we don't start at 0, as at 0 energy the parameters are not well defined)
let energies = linspace(0.0, 10.0, 1000)
let AlS = r"$\SI{20}{nm}\,\ce{Al}$"
let SiNS = r"$\SI{300}{nm}\,\ce{Si3 N4}$"
let GasS = r"$\SI{3}{cm}\,\ce{Ar}/\ce{Iso}$"
var df = toDf({"E" : energies})
  .mutate(f{float: GasS ~ transmission(gas, 3.cm, `E`.keV).float },
          f{float: GasS ~ 1.0 - idx(GasS)},
          f{float: AlS ~ transmission(Al, Al.ρ, 20.nm, `E`.keV).float },
          f{float: SiNS ~ transmission(Si₃N₄, Si₃N₄.ρ, 300.nm, `E`.keV).float })
let dfLLNL = readCsv("/home/basti/org/resources/llnl_xray_telescope_cast_effective_area_extended.csv")
  .mutate(f{"Efficiency" ~ (idx("EffectiveArea[cm²]").cm² / (2.15.cm * 2.15.cm * π)).float})
import numericalnim, sequtils
let interp = newLinear1D(dfLLNL["Energy[keV]", float].toSeq1D,
                         dfLLNL["Efficiency", float].toSeq1D)
df["LLNL"] = energies.mapIt(interp.eval(it))
df = df.mutate(f{"Combined" ~ idx(GasS) * idx(AlS) * idx(SiNS) * `LLNL`})
df = df.gather([GasS, AlS, SiNS, "LLNL", "Combined"], "Type", "Efficiency")
df = df.dropNaN()

echo df
#proc compTrans[T: AnyCompound](el: T, ρ: g•cm⁻³, length: Meter): DataFrame =
#  result = toDf({ "Energy [keV]" : energies })
#    .mutate(f{float: "μ" ~ el.attenuationCoefficient(idx("Energy [keV]").keV).float},
#            f{float: "Trans" ~ transmission(`μ`.cm²•g⁻¹, ρ, length).float},
#            f{"Compound" <- el.name})
#var df = newDataFrame()
## compute transmission for Si₃N₄ (known density and desired length)
#df.add Si₃N₄.compTrans(3.44.g•cm⁻³, 300.nm.to(Meter))
## and for argon 
#df.add ar.compTrans(ρ_Ar, 3.cm.to(Meter))
## create a plot for the transmissions
#echo df
#let dS = pretty(300.nm, 3, short = true)
#let dA = pretty(3.cm, 1, short = true)
#let si = r"$\mathrm{Si}₃\mathrm{N}₄$"
ggplot(df, aes("E", "Efficiency", color = "Type")) +
  geom_line() +
  xlab("Energy [keV]") + ylab("Efficiency") +
  ggtitle("Combined detection efficiency and constituents") + 
  ggsave("/home/basti/phd/Figs/limit/combined_detection_efficiency.pdf",
         width = 800, height = 600,
         useTex = true, standalone = true)
#+end_src

#+RESULTS:
  
*** Average absorption depth of X-rays

- [ ] FOcal spot in center of chamber
- [ ] UPdated calc monte carlo for median pos  

In order to compute a realistic axion image based on raytracing the
plane at which to compute the image needs to be known, as the focal
spot size changes significantly depending on the distance to the focal
point of the X-ray optics.  The beamline behind the telescope is
designed such that the focal spot is in the center of the gas volume,
meaning $\SI{1.5}{cm}$ behind our window.

This is of particular importance for a gaseous detector, as the
raytracing only makes sense up to the generation of a photoelectron,
after which the produced primary electrons undergo
diffusion. Therefore, one needs to compute the average absorption
depth of X-rays in the relevant energy ranges for the used gas mixture
of the detector. This is easiest done based on a Monte Carlo
simulation taking into account the incoming X-ray flux distribution
(given the solar axion flux we consider) $f(E)$, the telescope
effective area $ε_{\text{LLNL}}$ and window transmission, $ε_{\ce{Si3.N4}}, ε_{\ce{Al}}$,

\[
I(E) = f(E) · ε_{\text{LLNL}} · ε_{\ce{Si3 N4}} · ε_{\ce{Al}}.
\]

$I(E)$ yields the correct energy distribution of expected signal
X-rays. For each sampled X-ray we can then draw a conversion point
based on the attenuation length and the Beer-Lambert law introduced in
sec. [[sec:theory:xray_matter_gas]], eq. [[eq:theory:beer_lambert_law]] for
its energy. Computing the median of all conversion points is then an
estimator for the point at which to compute the axion image. For more
details see the extended version of this thesis.

- [ ] *EXTEND WITH REFERENCES TO THEORY CHAPTER ABOUT ABSORPTION,
  ATTENUATION LENGTH, ETC*

Performing this calculation leads to a median conversion point of
$\langle d \rangle = \SI{0.2928}{cm}$ behind the detector window, with a standard deviation
of $\SI{0.4247}{cm}$ due to a long tail from higher energy X-rays. It
may be worthwhile to perform this calculation for distinct energies to
then compute different axion images for different energies with each
their own effective 'depth' behind the window. 

- [X] *INSERT COMPUTATION*
  - [ ] *REFERENCE IT PROPERLY*

**** Average distance X-rays travel in Argon at CAST conditions :noexport:
:PROPERTIES:
:CUSTOM_ID: sec:average_depth_xrays_argon
:END:

In order to be able to compute the correct distance to use in the
raytracer for the position of the axion image, we need a good
understanding of where the average X-ray will convert in the gas.

By combining the expected axion flux (or rather that folded with the
telescope and window transmission to get the correct energy
distribution) with the absorption length of X-rays at different
energies we can compute a weighted mean of all X-rays and come up with
a single number.

For that reason we wrote [[https://github.com/SciNim/xrayAttenuation][xrayAttenuation]].

Let's give it a try.

***** Analytical approach
:PROPERTIES:
:CUSTOM_ID: sec:axion_conversion_point:analytical
:END:

#+begin_src nim :tangle /tmp/entrance_distance_xrays_cast.nim
import xrayAttenuation, ggplotnim, unchained

# 1. read the file containing efficiencies
var effDf = readCsv("/home/basti/org/resources/combined_detector_efficiencies.csv")
  .mutate(f{"NoGasEff" ~ idx("300nm SiN") * idx("20nm Al") * `LLNL`})

# 2. compute the absorption length for Argon
let ar = Argon.init()
let ρ_Ar = density(1050.mbar.to(Pascal), 293.K, ar.molarMass)
effDf = effDf
  .filter(f{idx("Energy [keV]") > 0.05})
  .mutate(f{float: "l_abs" ~ absorptionLength(ar, ρ_Ar, idx("Energy [keV]").keV).float})
# compute the weighted mean of the effective flux behind the window with the
# absorption length, i.e.
# `<x> = Σ_i (ω_i x_i) / Σ_i ω_i`
let weightedMean = (effDf["NoGasEff", float] *. effDf["l_abs", float]).sum() /
  effDf["NoGasEff", float].sum()
echo "Weighted mean of distance: ", weightedMean.Meter.to(cm)
# for reference the effective flux:
ggplot(effDf, aes("Energy [keV]", "NoGasEff")) +
  geom_line() +
  ggsave("/tmp/combined_efficiency_no_gas.pdf")
ggplot(effDf, aes("Energy [keV]", "l_abs")) +
  geom_line() +
  ggsave("/tmp/absorption_length_argon_cast.pdf")
#+end_src

#+RESULTS:
| /home/basti/CastData/ExternCode/xrayAttenuation/resources/nist_mass_attenuation/data_element_Argon_Z_18.csv |      |    |           |            |
| /home/basti/CastData/ExternCode/xrayAttenuation/resources/henke_form_factors/ar.nff                         |      |    |           |            |
| Weighted                                                                                                    | mean | of | distance: | 1.22082 cm |

This means the "effective" position of the axion image should be
0.0122 m or 1.22 cm in the detector. This is (fortunately) relatively
close to the 1.5 cm (center of the detector) that we used so far.

- [X] Is the above even correct? The absorption length describes the
  distance at which only 1/e particles are left. That means at that
  distance (1 - 1/e) have disappeared. To get a number don't we need
  to do a monte carlo (or some kind of integral) of the average?
  -> Well, the mean of an exponential distribution is 1/λ (if defined
  as $\exp(-λx)$!), from that point of view I think the above is
  perfectly adequate!
  Note however that the _median_ of the distribution is $\frac{\ln
  2}{λ}$! When looking at the distribution of our transverse RMS
  values for example the peak corresponds to something that is _closer 
  to the median_ (but is not exactly the median either; the peak is
  the 'mode' of the distribution).
  Arguably more interesting is the cutoff we see in the data as that
  corresponds to the largest _possible_ diffusion (but again that is
  being folded with the statistics of getting a larger RMS! :/ )

  *UPDATE*: <2023-06-14 Wed 17:24>
  See the section below for the numerical approach. As it turns out
  the above unfortunately is not correct for 3 important reasons (2 of
  which we were aware of):
  1. It does not include the axion spectrum, it changes the location of
     the mean slightly.
  2. It implicitly assumes all X-rays of all energies will be
     detected. This implies an infinitely long detector and not our
     detector limited by a length of 3 cm!
     This skews the actual mean to *lower values*, because the mean of
     those that _are_ detected are at smaller values.
  3. Point 2 implies not only that some X-rays won't be detected, but
     effectively it gives a _higher weight_ to energies that are
     absorbed with certainty compared to those that sometimes are not
     absorbed! This _further_ reduces the mean.
     This can be interpreted as reducing the input flux by the
     percentage of the absorption probability for each energy. 
     In this sense the above needs to be multiplied by the absorption
     probability to be more correct! Yet this still does not make it
     completely right, as that just assumes the fraction of photons of
     a given energy are reduced, but not that all detected ones have
     lengths consistent with a 3cm long volume!
  4. (minor) does not include isobutane.

A (shortened and) improved version of the above (but still not quite correct!):
#+begin_src nim :tangle /tmp/entrance_distance_xrays_cast_incl_gas.nim
import xrayAttenuation, ggplotnim, unchained
# 1. read the file containing efficiencies
var effDf = readCsv("/home/basti/org/resources/combined_detector_efficiencies.csv")
  .mutate(f{"NoGasEff" ~ idx("300nm SiN") * idx("20nm Al") * `LLNL` * idx("30mm Ar Abs.")})
# 2. compute the absorption length for Argon
let ar = Argon.init()
let ρ_Ar = density(1050.mbar.to(Pascal), 293.K, ar.molarMass)
effDf = effDf.filter(f{idx("Energy [keV]") > 0.05})
  .mutate(f{float: "l_abs" ~ absorptionLength(ar, ρ_Ar, idx("Energy [keV]").keV).float})
let weightedMean = (effDf["NoGasEff", float] *. effDf["l_abs", float]).sum() /
  effDf["NoGasEff", float].sum()
echo "Weighted mean of distance: ", weightedMean.Meter.to(cm)
#+end_src

#+RESULTS:
| /home/basti/CastData/ExternCode/xrayAttenuation/resources/nist_mass_attenuation/data_element_Argon_Z_18.csv |      |    |           |            |
| /home/basti/CastData/ExternCode/xrayAttenuation/resources/henke_form_factors/ar.nff                         |      |    |           |            |
| Weighted                                                                                                    | mean | of | distance: | 1.06606 cm |

We could further multiply in the axion flux of course, but as this
cannot be fully correct in this way, we'll do it numerically. We would
have to calculate the real mean of the exponential distribution for
each energy based on the truncated exponential
distribution. Effectively we have a bonded exponential between 0 and 3
cm, whose mean is of course going to differ from the parameter $λ$.

***** Numerical approach  
  
Let's write a version of the above code that computes the result by
sampling from the exponential distribution for the conversion point.

What we need:
- our sampling logic
- sampling from exponential distribution depending on energy
- the axion flux

Let's start by importing the modules we need:  
#+begin_src nim :tangle /tmp/sample_axion_xrays_conversion_points.nim
import helpers / sampling_helper # sampling distributions
import unchained                 # sane units
import ggplotnim                 # see something!
import xrayAttenuation           # window efficiencies
import math, sequtils
#+end_src
where the ~sampling_helpers~ is a small module to sample from a
procedure or a sequence.

In addition let's define some helpers:
#+begin_src nim :tangle /tmp/sample_axion_xrays_conversion_points.nim
from os import `/`
const ResourcePath = "/home/basti/org/resources"
const OutputPath = "/home/basti/org/Figs/statusAndProgress/axion_conversion_point_sampling/"
#+end_src

Now let's read the LLNL telescope efficiency as well as the axion flux
model. Note that we may wish to calculate the absorption points not
only for a specific axion flux model, but potentially any other kind
of signal. We'll build in functionality to disable different
contributions.

#+begin_src nim :tangle /tmp/sample_axion_xrays_conversion_points.nim
let dfAx = readCsv(ResourcePath / "solar_axion_flux_differential_g_ae_1e-13_g_ag_1e-12_g_aN_1e-15.csv")
  .filter(f{`type` == "Total flux"})
let dfLLNL = readCsv(ResourcePath / "llnl_xray_telescope_cast_effective_area_parallel_light_DTU_thesis.csv")
  .mutate(f{"Efficiency" ~ idx("EffectiveArea[cm²]") / (PI * 2.15 * 2.15)})
#+end_src
Note: to get the differential axion flux use ~readOpacityFile~ from
https://github.com/jovoy/AxionElectronLimit. It generates the CSV
file.

Next up we need to define the material properties of the detector
window in order to compute its transmission.
#+begin_src nim :tangle /tmp/sample_axion_xrays_conversion_points.nim
let Si₃N₄ = compound((Si, 3), (N, 4)) # actual window
const ρSiN = 3.44.g•cm⁻³
const lSiN = 300.nm                  # window thickness
let Al = Aluminium.init()            # aluminium coating
const ρAl = 2.7.g•cm⁻³
const lAl = 20.nm                    # coating thickness
#+end_src
With these numbers we can compute the transmission at an arbitrary
energy. In order to compute the correct inputs for the calculation we
now have everything. We wish to compute the following, the intensity
$I(E)$ is the flux that enters the detector 

\[
I(E) = f(E) · ε_{\text{LLNL}} · ε_{\ce{Si3.N4}} · ε_{\ce{Al}}
\]

where $f(E)$ is the solar axion flux and the $ε_i$ are the
efficiencies associated with the telescope and transmission of the
window. The idea is to sample from this intensity distribution to get
a realistic set of X-rays as they would be experienced in the
experiment. One technical aspect still to be done is an interpolation
of the axion flux and LLNL telescope efficiency to evaluate the data
at an arbitrary energy as to define a function that yields $I(E)$.

#+begin_quote
_Important note_: We fully neglect here the conversion probability and
area of the magnet bore. These (as well as a potential time component)
are purely constants and do not affect the *shape* of the distribution
$I(E)$. We want to sample from it to get the correct weighting of the
different energies, but do not care about absolute numbers. So
differential fluxes are fine.
#+end_quote

The idea is to define the interpolators and then create a procedure
that captures the previously defined properties and interpolators.

#+begin_src nim :tangle /tmp/sample_axion_xrays_conversion_points.nim
from numericalnim import newLinear1D, eval
let axInterp = newLinear1D(dfAx["Energy", float].toSeq1D, dfAx["diffFlux", float].toSeq1D)
let llnlInterp = newLinear1D(dfLLNL["Energy[keV]", float].toSeq1D, dfLLNL["Efficiency", float].toSeq1D)
#+end_src

With the interpolators defined let's write the implementation for
$I(E)$:

#+begin_src nim :tangle /tmp/sample_axion_xrays_conversion_points.nim
proc I(E: keV): float =
  ## Compute the intensity of the axion flux after telescope & window eff.
  ##
  ## Axion flux and LLNL efficiency can be disabled by compiling with
  ## `-d:noAxionFlux` and `-d:noLLNL`, respectively.
  result = transmission(Si₃N₄, ρSiN, lSiN, E) * transmission(Al, ρAl, lAl, E)
  when not defined(noAxionFlux):
    result *= axInterp.eval(E.float)
  when not defined(noLLNL):
    result *= llnlInterp.eval(E.float)
  
#+end_src

Let's test it and see what we get for e.g. $\SI{1}{keV}$:
#+begin_src nim :tangle /tmp/sample_axion_xrays_conversion_points.nim
echo I(1.keV)
#+end_src
yields $1.249e20$. Not the most insightful, but it seems to
work. Let's plot it:
#+begin_src nim :tangle /tmp/sample_axion_xrays_conversion_points.nim
let energies = linspace(0.01, 10.0, 1000).mapIt(it.keV)
let Is = energies.mapIt(I(it))
block PlotI:
  let df = toDf({ "E [keV]" : energies.mapIt(it.float),
                  "I" : Is })
  ggplot(df, aes("E [keV]", "I")) +
    geom_line() +
    ggtitle("Intensity entering the detector gas") + 
    ggsave(OutputPath / "intensity_axion_conversion_point_simulation.pdf")
#+end_src
shown in fig. [[fig:axion_converison_point:intensity]]. It looks exactly
as we would expect.

#+CAPTION: Intensity that enters the detector taking into account LLNL telescope and window
#+CAPTION: efficiencies as well as the solar axion flux
#+NAME: fig:axion_converison_point:intensity
[[~/org/Figs/statusAndProgress/axion_conversion_point_sampling/intensity_axion_conversion_point_simulation.pdf]]

Now we define the sampler for the intensity distribution $I(E)$, which
returns an energy weighted by $I(E)$:
#+begin_src nim :tangle /tmp/sample_axion_xrays_conversion_points.nim
let Isampler = sampler(
  (proc(x: float): float = I(x.keV)), # wrap `I(E)` to take `float`
  0.01, 10.0, num = 1000 # use 1000 points for EDF & sample in 0.01 to 10 keV
)
#+end_src
and define a random number generator:
#+begin_src nim :tangle /tmp/sample_axion_xrays_conversion_points.nim
import random
var rnd = initRand(0x42)
#+end_src

First we will sample 100,000 energies from the distribution to
see if we recover the intensity plot from before. 

#+begin_src nim :tangle /tmp/sample_axion_xrays_conversion_points.nim
block ISampled:
  const nmc = 100_000
  let df = toDf( {"E [keV]" : toSeq(0 ..< nmc).mapIt(rnd.sample(Isampler)) })
  ggplot(df, aes("E [keV]")) +
    geom_histogram(bins = 200, hdKind = hdOutline) +
    ggtitle("Energies sampled from I(E)") +
    ggsave(OutputPath / "energies_intensity_sampled.pdf")
#+end_src

This yields
fig. [[fig:axion_conversion_point:energies_sampled_intensity]], which
clearly shows the sampling works as intended.

#+CAPTION: Energies sampled from the distribution $I(E)$ using 100k samples.
#+CAPTION: The shape is nicely reproduced, here plotted using a histogram of
#+CAPTION: 200 bins.
#+NAME: fig:axion_conversion_point:energies_sampled_intensity
[[~/org/Figs/statusAndProgress/axion_conversion_point_sampling/energies_intensity_sampled.pdf]]

The final piece now is to use the same sampling logic to generate
energies according to $I(E)$, which correspond to X-rays of said
energy entering the detector. For each of these energies then sample
from the Beer-Lambert law

\[
I(z) = I_0 \exp\left[ - \frac{z}{l_{\text{abs}} } \right]
\]
where $I_0$ is some initial intensity and $l_\text{abs}$ the
absorption length. The absorption length is computed from the gas
mixture properties of the gas used at CAST, namely Argon/Isobutane
97.7/2.3 at $\SI{1050}{mbar}$. It is the inverse of the attenuation
coefficient $μ_M$
 
\[
l_{\text{abs}} = \frac{1}{μ_M}
\]

where the attenuation coefficient is computed via

\[
μ_m = \frac{N_A}{M * σ_A}
\]

with $N_A$ Avogadro's constant, $M$ the molar mass of the compound and
$σ_A$ the atomic absorption cross section. The latter again is defined
by

\[
σ_A = 2 r_e λ f₂
\]

with $r_e$ the classical electron radius, $λ$ the wavelength of the
X-ray and $f₂$ the second scattering factor. Scattering factors are
tabulated for different elements, for example by [[https://www.nist.gov/pml/x-ray-form-factor-attenuation-and-scattering-tables][NIST]] and
[[https://henke.lbl.gov/optical_constants][Henke]]. For a further discussion of this see the README and
implementation of [[https://github.com/SciNim/xrayAttenuation][~xrayAttenuation~]].

We will now go ahead and define the CAST gas mixture:
#+begin_src nim :tangle /tmp/sample_axion_xrays_conversion_points.nim
proc initCASTGasMixture(): GasMixture =
  ## Returns the absorption length for the given energy in keV for CAST
  ## gas conditions:
  ## - Argon / Isobutane 97.7 / 2.3 %
  ## - 20°C ( for this difference in temperature barely matters)
  let arC = compound((Ar, 1)) # need Argon gas as a Compound
  let isobutane = compound((C, 4), (H, 10))
  # define the gas mixture
  result = initGasMixture(293.K, 1050.mbar, [(arC, 0.977), (isobutane, 0.023)])
let gm = initCASTGasMixture()  
#+end_src

To sample from the Beer-Lambert law with a given absorption length we
also define a helper that returns a sampler for the target energy
using the definition of a normalized exponential distribution

\[
f_e(x, λ) = \frac{1}{λ} \exp \left[ -\frac{x}{λ} \right]
\]

#+begin_quote
The sampling of the conversion point is the crucial aspect of
this. Naively we might want to sample between the detector volume from
0 to $\SI{3}{cm}$. However, this skews our result. Our calculation
depends on the energy distribution of the incoming X-rays. If the
absorption length is long enough the probability of reaching the
readout plane and thus not being detected is significant. Restricting
the sampler to $\SI{3}{cm}$ would pretend that independent of
absorption length we would _always_ convert within the volume, giving
too large a weight to the energies that should sometimes not be detected!
#+end_quote

Let's define the sampler now. It takes the gas mixture and the target
energy. A constant ~SampleTo~ is defined to adjust the position to
which we sample at compile time (to play around with different numbers).
#+begin_src nim :tangle /tmp/sample_axion_xrays_conversion_points.nim
proc generateSampler(gm: GasMixture, targetEnergy: keV): Sampler =
  ## Generate the exponential distribution to sample from based on the
  ## given absorption length
  # `xrayAttenuation` `absorptionLength` returns number in meter!
  let λ = absorptionLength(gm, targetEnergy).to(cm)
  let fnSample = (proc(x: float): float =
                    result = expFn(x, λ.float) # expFn = 1/λ · exp(-x/λ)
  )
  const SampleTo {.intdefine.} = 20 ## `SampleTo` can be set via `-d:SampleTo=<int>`
  let num = (SampleTo.float / 3.0 * 1000).round.int # number of points to sample at
  result = sampler(fnSample, 0.0, SampleTo, num = num)
#+end_src
Note that this is inefficient, because we generate a new sampler from
which we only sample a single point, namely the conversion point of that
X-ray. If one intended to perform a more complex calculation or wanted
to sample orders of magnitude more X-rays, one should either
restructure the code (i.e. sample from known energies and then reorder
based on the weight defined by $I(E)$ or cache the samplers and
pre-bin the energies.

For reference let's compute the absorption length as a function of
energy for the CAST gas mixture:
#+begin_src nim :tangle /tmp/sample_axion_xrays_conversion_points.nim
block GasAbs:
  let df = toDf({ "E [keV]" : linspace(0.03, 10.0, 1000),
                  "l_abs [cm]" : linspace(0.03, 10.0, 1000).mapIt(absorptionLength(gm, it.keV).m.to(cm).float) })
  ggplot(df, aes("E [keV]", "l_abs [cm]")) +
    geom_line() +
    ggtitle("Absorption length of X-rays in CAST gas mixture: " & $gm) +
    margin(top = 1.5) + 
    ggsave(OutputPath / "cast_gas_absorption_length.pdf")
#+end_src
which yields fig. [[fig:axion_conversion_point:absorption_length]]

#+CAPTION: Absorption length in the CAST gas mixture as a function of X-ray energy.
#+NAME: fig:axion_conversion_point:absorption_length
[[~/org/Figs/statusAndProgress/axion_conversion_point_sampling/cast_gas_absorption_length.pdf]]

So, finally: let's write the MC sampling!
#+begin_src nim :tangle /tmp/sample_axion_xrays_conversion_points.nim
const nmc = 500_000 # start with 100k samples
var Es = newSeqOfCap[keV](nmc)
var zs = newSeqOfCap[cm](nmc)
while zs.len < nmc:
  # 1. sample an energy according to `I(E)`
  let E = rnd.sample(Isampler).keV
  # 2. get the sampler for this energy
  let distSampler = generateSampler(gm, E) 
  # 3. sample from it
  var z = Inf.cm
  when defined(Equiv3cmSampling): ## To get the same result as directly sampling
                                  ## only up to 3 cm use the following code
    while z > 3.0.cm:
      z = rnd.sample(distSampler).cm 
  elif defined(UnboundedVolume): ## This branch pretends the detection volume
                                 ## is unbounded if we sample within 20cm
    z = rnd.sample(distSampler).cm 
  else: ## This branch is the physically correct one. If an X-ray reaches the
        ## readout plane it is _not_ recorded, but it was still part of the
        ## incoming flux!
    z = rnd.sample(distSampler).cm
    if z > 3.0.cm: continue # just drop this X-ray
  zs.add z
  Es.add E
#+end_src

Great, now we have sampled the conversion points according to the
correct intensity. We can now ask for statistics or create different
plots (e.g. conversion point by energies etc.).

#+begin_src nim :tangle /tmp/sample_axion_xrays_conversion_points.nim
import stats, seqmath # mean, variance and percentile
let zsF = zs.mapIt(it.float) # for math
echo "Mean conversion position = ", zsF.mean().cm
echo "Median conversion position = ", zsF.percentile(50).cm
echo "Variance of conversion position = ", zsF.variance().cm
#+end_src

This prints the following:
#+begin_src nim
Mean conversion position = 0.556813 cm
Median conversion position = 0.292802 cm
Variance of conversion position = 0.424726 cm
#+end_src

As we can see (unfortunately) our initial assumption of a mean
distance of $\SI{1.22}{cm}$ are quite of the mark. The more realistic
number is only $\SI{0.56}{cm}$. And if we were to use the median it's
only $\SI{0.29}{cm}$.

Let's plot the conversion points of all sampled (and recorded!) X-rays
as well as what their distribution against energy looks like.
#+begin_src nim :tangle /tmp/sample_axion_xrays_conversion_points.nim
let dfZ = toDf({ "E [keV]" : Es.mapIt(it.float),
                 "z [cm]"  : zs.mapIt(it.float) })
ggplot(dfZ, aes("z [cm]")) +
  geom_histogram(bins = 200, hdKind = hdOutline) +
  ggtitle("Conversion points of all sampled X-rays according to I(E)") +
  ggsave(OutputPath / "sampled_axion_conversion_points.pdf")
ggplot(dfZ, aes("E [keV]", "z [cm]")) +
  geom_point(size = 1.0, alpha = 0.2) + 
  ggtitle("Conversion points of all sampled X-rays according to I(E) against their energy") +
  ggsave(OutputPath / "sampled_axion_conversion_points_vs_energy.png",
        width = 1200, height = 800)  
#+end_src

The former is shown in
fig. [[fig:axion_conversion_point:sampled_axion_conversion_points]]. The
overlapping exponential distribution is obvious, as one would expect.
The same data is shown in
fig. [[fig:axion_conversion_point:sampled_axion_conversion_points_by_energy]],
but in this case not as a histogram, but by their energy as a scatter
plot. We can clearly see the impact of the absorption length on the
conversion points for each energy!

#+CAPTION: Distribution of the conversion points of all sampled X-rays for which
#+CAPTION: conversion in the detector took place as sampled from $I(E)$.
#+NAME: fig:axion_conversion_point:sampled_axion_conversion_points
[[~/org/Figs/statusAndProgress/axion_conversion_point_sampling/sampled_axion_conversion_points.pdf]]

#+CAPTION: Distribution of the conversion points of all sampled X-rays for which
#+CAPTION: conversion in the detector took place as sampled from $I(E)$ as a scatter
#+CAPTION: plot against the energy for each X-ray.
#+NAME: fig:axion_conversion_point:sampled_axion_conversion_points_by_energy
[[~/org/Figs/statusAndProgress/axion_conversion_point_sampling/sampled_axion_conversion_points_vs_energy.png]]


***** Compiling and running the code

The code above is written in literate programming style. To compile
and run it we use ~ntangle~ to extract it from the Org file:
#+begin_src nim
ntangle <file>
#+end_src
which generates [[file:/tmp/sample_axion_xrays_conversion_points.nim]].

Compiling and running it can be done via:
#+begin_src nim
nim r -d:danger /tmp/sample_axion_xrays_conversion_points.nim
#+end_src
which compiles and runs it as an optimized build.

We have the following compilation flags to compute different cases:
- ~-d:noLLNL~: do not include the LLNL efficiency into the input
  intensity
- ~-d:noAxionFlux~: do not include the axion flux into the input
  intensity
- ~-d:SampleTo=<int>~: change to where we sample the position (only to
  3cm for example)
- ~-d:UnboundedVolume~: if used together with the default ~SampleTo~
  (or any large value) will effectively compute the case of an
  unbounded detection volume (i.e. every X-ray recorded with 100%
  certainty).
- ~-d:Equiv3cmSampling~: Running this with the default ~SampleTo~ (or
  any large value) will effectively change the sampling to a maximum
  \SI{3}{cm} sampling. This can be used as a good crossheck to verify
  the sampling behavior is independent of the sampling range.

Configurations of note:
#+begin_src nim
nim r -d:danger -d:noAxionFlux /tmp/sample_axion_xrays_conversion_points.nim
#+end_src
$⇒$ realistic case for a flat input spectrum
Yields:
#+begin_src nim
Mean conversion position = 0.712102 cm
Median conversion position = 0.445233 cm
Variance of conversion position = 0.528094 cm
#+end_src

#+begin_src nim
nim r -d:danger -d:noAxionFlux -d:UnboundedVolume /tmp/sample_axion_xrays_conversion_points.nim
#+end_src
$⇒$ the closest analogue to the analytical calculation from section
[[#sec:axion_conversion_point:analytical]] (outside of including isobutane
here)
Yields:
#+begin_src nim
Mean conversion position = 1.25789 cm
Median conversion position = 0.560379 cm
Variance of conversion position = 3.63818 cm
#+end_src

#+begin_src nim
nim r -d:danger /tmp/sample_axion_xrays_conversion_points.nim
#+end_src
$⇒$ the case we most care about and of which the numbers are mentioned
in the text above. 




*** Raytracing axion image
:PROPERTIES:
:CUSTOM_ID: sec:limit:ingredients:raytracing
:END:

The axion image is computed based on a raytracing Monte Carlo
simulation. It is part of the same code base as the code producing the
differential solar axion flux, as mentioned in
sec. [[#sec:limit:ingredients:solar_axion_flux]]. For an introduction to
the raytracing technique and details about the LLNL telescope mirror
shells, see appendix [[#sec:appendix:raytracing]].

Fig. [[fig:limit:ingredients:axion_image]] shows the image, computed for
a Sun-Earth distance of $\SI{0.989}{AU}$ and a distance of
$\SI{0.2928}{cm}$ behind the detector window. So it is
$\SI{1.2072}{cm}$ _in front_ of the focal point.

Instead of using the raytracing image to fully characterize the axion
flux including efficiency losses, we _only_ use it to define the
spatial distribution. This means we rescale the full axion flux
distribution - before taking the window strongback into account - such
that it represents the fractional X-ray flux per square
centimeter. That way, when we multiply it with the rest of the
expression in the signal calculation eq. [[eq:limit_method_signal_si]],
the result is the expected number of counts at the given position and
energy per $\si{cm²}$, because the axion flux is already given as an
expected number of counts.

#+CAPTION: Axion image as computed using raytracing for the AGSS09 solar model
#+CAPTION: and under the assumption that the axion-electron coupling constant
#+CAPTION: $g_{ae}$ (*GIVE NUMBER*) dominates over the axion-photon coupling $g_{aγ}$.
#+CAPTION: The diagonal lines with missing flux are the detector window strongbacks.
#+NAME: fig:limit:ingredients:axion_image
#+ATTR_LATEX: :width 0.8\textwidth
[[~/org/Figs/statusAndProgress/axionImages/axion_image_2018_1487_93_0.989AU_with_window.pdf]]

*** Background [0/6]

The background must be evaluated at the position and energy of each
cluster candidate. As the background is not constant in energy or
position on the chip, we need a continuous description in those
dimensions of the background rate.

In order to obtain such a thing, we start from all X-ray like clusters
remaining after background rejection, see
fig. [[fig:limit:background_clusters]] where each point is a cluster
center with the color indicating its energy (similar to the candidates
plot further up).
- [ ] *CLUSTER PLOT HERE AGAIN?*

For the background we construct a background interpolation based on
all clusters found in the background dataset. This is done by defining
$b_i$ as a function of candidate position $x_i, y_i$ and energy $E_i$ using

\[
b_i(x_i, y_i, E_i) = \frac{I(x_i, y_i, E_i)}{W(x_i, y_i, E_i)}
\]

where $I$ is an intensity defined over clusters within a range $R$ and
a normalization weight $W$. From here on we will drop the candidate
suffix $i$. The intensity is given by

\[
I(x, y, E) = \sum_{b ∈ \{ \mathcal{D}(\vec{x}_b, \vec{x}) \leq R \}}\mathcal{M}(\vec{x}_b, E_b)
  = \sum_{b ∈ \{ \mathcal{D}(\vec{x}_b, \vec{x}) \leq R \} } \exp \left[ -\frac{1}{2} \mathcal{D}² / σ² \right] \text{ for clarity w/o arguments},
\]

where we introduce $\mathcal{M}$ to refer to the measure we use and
$\mathcal{D}$ to our metric:

\begin{equation*}
\mathcal{D}( (\vec{x}_1, E_1), (\vec{x}_2, E_2)) =
  \begin{cases}
    (\vec{x}_1 - \vec{x}_2)² \text{ if } |E_1 - E_2| \leq R \\
    ∞ \text{ if } (\vec{x}_1 - \vec{x}_2)² > R² \\
    ∞ \text{ if } |E_1 - E_2| > R
  \end{cases}
\end{equation*}

with $\vec{x} = \vektor{x \\ y}$.  Note first of all that this
effectively describes a cylinder. Any point inside $| \vec{x}_1 -
\vec{x}_2 | \leq R$ simply yields a euclidean distance, as long as
their energy is smaller than $R$. Secondly, this requires rescaling
the energy as a common number $R$, but in practice the implementation
of this custom metric simply compares energies directly, with the
'height' in energy of the cylinder expressed as $ΔE$.

The commonly used value for the radius $R$ in the x-y plane are
$R = \SI{40}{pixel}$ and in energy $ΔE = ± \SI{0.3}{keV}$. The
standard deviation of the normal distribution for the weighting in the
measure $σ$ is set to $\frac{R}{3}$.  The basic idea of the measure is
simply to provide the highest weight to those clusters close to the
point we evaluate and approach 0 at the edge of $R$ to avoid
discontinuities in the resulting interpolation.

Finally, the normalization weight $W$ is required to convert the sum
of $I$ into a background rate. It is the 'volume' of our measure
within the boundaries set by our metric $\mathcal{D}$:

\begin{align*}
W(x', y', E') &= t_B ∫_{E' - E_c}^{E' + E_c} ∫_{\mathcal{D}(\vec{x'}, \vec{x}) \leq R} \mathcal{M}(x', y') \, \mathrm{d}x\, \mathrm{d}y\, \mathrm{d} E \\
  &= t_B ∫_{E' - E_c}^{E' + E_c} ∫_{\mathcal{D}(\vec{x'}, \vec{x}) \leq R} \exp\left[ -\frac{1}{2} \mathcal{D}² / σ² \right] \, \mathrm{d}x \, \mathrm{d}y \, \mathrm{d} E \\
  &= t_B ∫_{E' - E_c}^{E' + E_c} ∫_0^R ∫_0^{2π} r \exp\left[ -\frac{1}{2} \frac{\mathcal{D}² }{σ²} \right] \, \mathrm{d}r\, \mathrm{d}φ\, \mathrm{d} E \\
  &= t_B ∫_{E' - E_c}^{E' + E_c} -2 π \left( σ²  \exp\left[ -\frac{1}{2} \frac{R²}{σ^2} \right] - σ² \right) \, \mathrm{d} E \\
  &= -4 π t_B E_c \left( σ² \exp\left[ -\frac{1}{2} \frac{R²}{σ^2} \right] - σ² \right), \\
\end{align*}

where we made use of the fact that within the region of interest
$\mathcal{D}'$ is effectively just a radius $r$ around the point we
evaluate. $t_B$ is the total active background data taking time. If
our measure was $\mathcal{M} = 1$ -- meaning we would count the
clusters in $\mathcal{D}(\vec{x}, \vec{x}') \leq R$ -- the
normalization $W$ would simply be the volume of the cylinder.

- [X] *SHOW RESULT OF INTEGRAL, INCLUDE SAGEMATH CALC*
  - [ ] *VERIFY IT IS ACTUALLY CORRECT LIKE THIS RIGHT NOW, i.e. the
    math notation*

This yields a smooth and continuous interpolation of the background
over the entire chip. However, towards the edges of the chip it
underestimates the background rate, because once part of the cylinder
is not contained within the chip, fewer clusters contribute. For that
reason we correct for the chip edges by upscaling the value within the
chip by the missing area.

Keep in mind the area of a [[https://en.wikipedia.org/wiki/Circular_segment][circle segment]]:
\[
A = r² / 2 · (ϑ - \sin(ϑ))
\]
where $r$ is the radius of the circle and ϑ the angle that cuts off
the circle.

In the general case we need to know the area of a circle that is cut
off from 2 sides. By subtracting the corresponding areas of circle
segments for each of the lines that cut something off, we remove too
much. So we need to add back:
- another circle segment, of the angle between the two angles given by
  the twice counted area
- the area of the triangle with the two sides given by $R - r'$ in
  length, where $r'$ is the distance that is cut off from the circle.

In combination the area remaining for a circle cut off from two
(orthogonal, fortunately) lines is:

\[
E = F - A - B + C + D
\]
where:
- $F$: the total area of the circle
- $A$: the area of the first circle segment
- $B$: the area of the second circle segment
- $C$: the area of the triangle built by the two line cutoffs:
  \[
  C = \frac{r' r''}{2} 
  \]
  with $r'$ as defined above for cutoff A and $r''$ for cutoff B.
- $D$: the area of the circle segment given by the angles between the
  two cutoff lines touching the circle edge:
  \[
  D = r² / 2 · (α - \sin(α))
  \]
  where $α$ is:
  \[
  α = π/2 - ϑ_1 - ϑ_2
  \]
  where $ϑ_{1,2}$ are the related to the angles $ϑ$ needed to compute
  each circle segment, by:
  \[
  ϑ' = (π - ϑ) / 2
  \]
  denoted as $ϑ'$ here.

Fig. \ref{fig:limit:background_interpolation} shows an example of the background
interpolation centered at $\SI{3}{keV}$, with all clusters within a
radius of $\num{40}$ pixels and in an energy range from
$\SIrange{2.7}{3.3}{keV}$. Fig. \ref{fig:limit:interpolation_clusters}
shows the initial step of the interpolation, with all colored points
inside the circle being clusters that are contained in $\mathcal{D}
\leq R$. Their color represents the weight based on the measure
$\mathcal{M}$. After normalization and calculation for each point on
the chip, we get the interpolation shown in
fig. \ref{fig:limit:background_interpolation_example}.

Implementation wise, as the lookup of the closest neighbors in general
is an $N²$ operation for $N$ clusters, all clusters are stored in a
$k$-d tree, for fast querying of clusters close to the point to be
evaluated. Furthermore, because the likelihood $\mathcal{L}$ is
evaluated many times for a given set of candidates to compute a limit,
we perform caching of the background interpolation values for each
candidate. That way we only compute the interpolation once for each
candidate.

- [ ] *REFERNECE k-d TREE*

\begin{figure}[htbp]
  \centering
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{~/org/Figs/statusAndProgress/limitSanityChecks/interpolation_clusters_E_3.0_keV_x_110_y_80.pdf}
    \caption{Intensity at a point}
    \label{fig:limit:interpolation_clusters}
  \end{subfigure}%
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{~/org/Figs/statusAndProgress/limitSanityChecks/normalized_interpolation_at_3.0keV_ymax_5e-05.pdf}
    \caption{Interpolation}
    \label{fig:limit:background_interpolation_example}
  \end{subfigure}%
  \label{fig:limit:background_interpolation}
  \caption{\subref{fig:limit:interpolation_clusters} Calculation of intensity $I$ at the center of the red circle.
  Black crosses indicate \underline{all} background clusters. The red circle indicates cutoff $R$ in the x-y plane. Only
  clusters with colored dots inside the circle are within $\SIrange{2.70}{3.3}{keV}$. Their color is the weight based
  on the gaussian metric $\mathcal{D}$. 
  \subref{fig:limit:background_interpolation_example} Example of the resulting background interpolation at $\SI{3}{keV}$ computed
  over the entire chip. A smooth, correctly normalized interpolation is obtained.
  }
\end{figure}

**** Notes about this section [2/9]                             :noexport:
Figures:
1. [X] background clusters
   -> They are part of the background rate section
   -> Partially in plot below.
   Note though that we don't have the exact set of clusters that are
   used in e.g. the 95% case etc.
2. [X] (optional) plot showing "selection" of clusters based on
   =queryBallPoint=? I.e. show all clusters in grey, radius & then in
   color those that are in the radius? -> Done
3. [-] background "interpolation" based purely on query ball point of raw
   data, at a slice of energy. -> not important enough
4. in a facet with 3 show same after normalization?
5. [-] show effect of area cutoff correction. Also results in a "final"
   background rate interpolated.
   -> Also not that relevant.
6. [X] interpolation at one slice


- [ ] About edge correction:
  - [ ] Maybe make a quick schematic of Inkscape showing the different
  areas A, B, C, D, E, F?
  - [ ] Maybe take most of the description out of the regular thesis?


Highlight:
So what? This allows us to evaluate the background rate correctly on
the full chip! Generic ALPs can be studied this way, as we don't have
to manually define regions on the chip with specific backgrounds etc.!
One of the fundamental points about making this whole procedure generic.

- [ ] explain using k-d tree to efficiently look up "neighbors" at any
  point (x, y, E). In particular explain how energy works. Not taken
  into account in distance aside from whether inside. So gaussian
  weighting only in x/y.
  
- [X] use number & distance to these points to compute a weighted
  number of elements in desired "radius"
  -> Explained
- [ ] Explain that energy is rescaled according to a the "radius" of
  the metric
- [X] potentially rescale number based on area cut off due to edges of
  the chip. how does this work.
  -> Just need to rephrase it.
- [X] renormalize from an effective "number" of clusters to a
  rate. . how does rescaling work.
  -> Done

- [ ] explain how the integration works etc, copy from sections
  [[sec:correct_inter_cutoff]] and [[sec:limit:gaussian_weight_normalization]]
  in status.
  -> Both sections copied now below.

- [X] *INSERT CROSSES FOR CLUSTERS IN THE ENERGY RANGE!*
  -> Done as a separate plot!

- [ ] *TODO*: check the integration of the gaussian weight again. Is
  that really correct??!! Ahh, it might be correct. What we try to do
  is not to compute anything related to the actual neighbors found in
  the radius, but rather to get the "equivalent area" of the weighted
  data!


- [ ] *IMPORTANT:*
  Our knowledge of the random coincidence rate of the septem & line
  veto implies that the *time* used to calculate the tracking time and
  background time must be modified by that factor. So not only a dead
  time in the *tracking* part, but also for the *background* part!
  -> No, I don't think this is true. Our background rate is valid as
  is. This is the same as the thought of whether a background rate
  should be corrected for by the signal efficiency. This is never
  done. 
  

**** STARTED Homogeneous background for energies > 2 keV [/]    :noexport:
:PROPERTIES:
:CUSTOM_ID: sec:correct_inter_cutoff
:END:


- [ ] finish the final explanation part!!
- [ ] Remove nowadays understood aspects of below! 


The issue with energies larger than 2 keV and performing the
interpolation of all events larger than 2 in a reasonably large
radius, has one specific problem.

The plot in fig. [[fig:background_interpolation_larger_2keV]] shows what
the interpolation for > 2 keV looks like for a radius of 80 pixels.

It is very evident that the background *appears* higher in the center
area than in the edges / corners of the chip.

The reason for this is pretty obvious once one thinks about it
deeper. Namely, an event with a significant energy that went through
decent amounts of diffusion, cannot have its cluster center (given
that it's X-ray like here) actually close to the edge / corner of the
detector. On average its center will be half the diffusion radius
*away* from the edges. If we then interpolate based on the cluster
center information, we end up at a typical boundary problem, i.e. they
are underrepresented.

#+begin_center
#+CAPTION: Background interpolation for 2017/18 X-ray like data for all clusters above \SI{2}{keV}
#+CAPTION: using a radius of 80 pixels. It is evident that the background in the center appears higher
#+CAPTION: than at the edges, despite expecting either the opposite or constant background. Reason is
#+CAPTION: cutoff at edges, so no contributions can come from outside + diffusion causing cluster 
#+CAPTION: centers to always be a distance away from the edges.
#+NAME: fig:background_interpolation_larger_2keV
[[~/org/Figs/statusAndProgress/background_interpolation/background_interp_larger_2keV_radius_80.pdf]]
#+end_center

Now, what is a good solution for this problem?

In principle we can just say "background is constant over the chip at
this energy above 2 keV" and neglect the whole interpolation here,
i.e. set it constant.

If we wish to keep an interpolation around, we will have to modify the
data that we use to create the actual 2D interpolator.

Of course the same issue is present in the < 2 keV dataset to an
extent. The question there is: does it matter? Essentially, the
statement about having *less* background there is factually true. But
only to the extent of diffusion putting the centers away from the
edges, *not* from just picking up nothing from the area within the
search radius that lies outside the chip (where thus no data can be
found).

Ideally, we correct for this by scaling all points that contain data
outside the chip by the fraction of area that is within the radius
divided by the total area. That way we pretend that there is an
'equal' amount of background found in this area in the full radius
around the point.

How?

Trigonometry for that isn't fully trivial, but also not super hard.

Keep in mind the area of a [[https://en.wikipedia.org/wiki/Circular_segment][circle segment]]:
\[
A = r² / 2 * (ϑ - sin(ϑ))
\]
where $r$ is the radius of the circle and ϑ the angle that cuts off
the circle.

However, in the general case we need to know the area of a circle that
is cut off from 2 sides. By subtracting the corresponding areas of
circle segments for each of the lines that cut something off, we
remove too much. So we need to add back:
- another circle segment, of the angle between the two angles given by
  the twice counted area
- the area of the triangle with the two sides given by $R - r'$ in
  length, where $r'$ is the distance that is cut off from the circle.

In combination the area remaining for a circle cut off from two
(orthogonal, fortunately) lines is:

\[
E = F - A - B + C + D
\]
where:
- $F$: the total area of the circle
- $A$: the area of the first circle segment
- $B$: the area of the second circle segment
- $C$: the area of the triangle built by the two line cutoffs:
  \[
  C = \frac{r' r''}{2} 
  \]
  with $r'$ as defined above for cutoff A and $r''$ for cutoff B.
- $D$: the area of the circle segment given by the angles between the
  two cutoff lines touching the circle edge:
  \[
  D = r² / 2 * (α - sin(α))
  \]
  where $α$ is:
  \[
  α = π/2 - ϑ_1 - ϑ_2
  \]
  where $ϑ_{1,2}$ are the related to the angles $ϑ$ needed to compute
  each circle segment, by:
  \[
  ϑ' = (π - ϑ) / 2
  \]
  denoted as $ϑ'$ here.

Implemented this as a prototype in:
[[file:~/org/Misc/circle_segments.nim]]
*UPDATE*: <2023-03-01 Wed 18:02> which now also lives in TPA in the
~NimUtil/helpers~ directory!

Next step: incorporate this into the interpolation to re-weight the
interpolation near the corners.

**** Normalization of gaussian weighted k-d tree background interpolation :noexport:
:PROPERTIES:
:CUSTOM_ID: sec:limit:gaussian_weight_normalization
:END:


The background interpolation described above includes multiple steps
required to finalize it.

As mentioned, we start by building a k-d tree on the data using a
custom metric:

#+begin_src nim
proc distance(metric: typedesc[CustomMetric], v, w: Tensor[float]): float =
  doAssert v.squeeze.rank == 1
  doAssert w.squeeze.rank == 1
  let xyDist = pow(abs(v[0] - w[0]), 2.0) + pow(abs(v[1] - w[1]), 2.0)
  let zDist = pow(abs(v[2] - w[2]), 2.0)
  if zDist <= Radius * Radius:
    result = xyDist
  else:
    result = zDist
  #if xyDist > zDist:
  #  result = xyDist
  #elif xyDist < zDist and zDist <= Radius:
  #  result = xyDist
  #else:
  #  result = zDist
#+end_src
or in pure math:

Let $R$ be a cutoff value.
\begin{equation}
\mathcal{D}( (\vec{x}_1, E_1), (\vec{x}_2, E_2)) =
  \begin{cases}
    (\vec{x}_1 - \vec{x}_2)² \text{ if } |E_1 - E_2| \leq R \\
    |E_1 - E_2|
  \end{cases}
\end{equation}
where we make sure to scale the energies such that a value for the
radius in Euclidean space of the x / y geometry covers the same range
as it does in energy.

This creates essentially a cylinder. In words it means we use the
distance in x and y as the actual distance, unless the distance in
energy is larger than the allowed cutoff, in which case we return it.

This simply assures that:
- if two clusters are close in energy, but further in Euclidean
  distance than the allowed cutoff, they will be removed later
- if two clusters are too far away in energy they will be removed,
  despite possibly being close in x/y
- otherwise the distance in energy is *irrelevant*.

The next step is to compute the actual background value associated with each $(x,
y, E)$ point.

In the most naive approach (as presented in the first few plots in the
section above), we can associate to each point the number of clusters
found within a certain radius (including or excluding the energy
dimension).

For obvious reasons treating each point independent of the distance
as a single count (pure nearest neighbor) is problematic, as the
distance matters of course.
Thus, our choice is a weighted nearest neighbor. Indeed, we weigh the
distance by normal distribution centered around the location at which
we want to compute the background.

So, in code our total weight for an individual point is:
#+begin_src nim
template compValue(tup: untyped, byCount = false, energyConst = false): untyped =
  if byCount:
    tup.idx.size.float # for the pure nearest neighbor case
  else:
    # weigh by distance using gaussian of radius being 3 sigma
    let dists = tup[0]
    var val = 0.0
    for d in items(dists):
      # default, gaussian an energy
      val += smath.gauss(d, mean = 0.0, sigma = radius / 3.0)
    val
#+end_src
where =tup= contains the distances to all neighbors found within the
desired radius.

In math this means we first modify our distance measure $\mathcal{D}$
from above to:
\begin{equation}
\mathcal{D'}( (\vec{x}_1, E_1), (\vec{x}_2, E_2)) =
  \begin{cases}
    (\vec{x}_1 - \vec{x}_2)² \text{ if } |E_1 - E_2| \leq R \\
    0 \text{ if } (\vec{x}_1 - \vec{x}_2)² > R² \\
    0 \text{ if } |E_1 - E_2| > R
  \end{cases}
\end{equation}
to incorporate the nearest neighbor properties of dropping everything
outside of the radius either in x/y or in (scaled) energy.

\begin{align*}
I(\vec{x}_e, E_e) &= Σ_i \exp \left[ -\frac{1}{2} \left( \mathcal{D'}((\vec{x}_e, E_e), (\vec{x}_i, E_i)) \right)² / σ² \right] \\
I(\vec{x}_e, E_e) &= Σ_i \exp \left[ -\frac{1}{2} \mathcal{D'}² / σ² \right] \text{ for clarity w/o arguments}\\
I(\vec{x}_e, E_e) &= Σ_i \mathcal{M}(\vec{x}_i, E_i) \\
\text{where we introduce }&\mathcal{M}\text{ to refer to the measure we use.}
\end{align*}
where =i= runs over all clusters ($\mathcal{D'}$ takes care of only
allowing points in the radius to contribute) and the subscript =e= stands
for the evaluation point. $σ$ is the sigma of the (non-normalized!)
Gaussian distribution for the weights, which is set to $σ =
\frac{R}{3}$.

This gives us a valid interpolated value for each possible value of
position and energy pairs. However, these are still not normalized,
nor corrected for the cutoff of the radius once it's not fully "on"
the chip anymore. The normalization is done via the area of circle
segments, as described in the previous section
[[#sec:correct_inter_cutoff]].

The normalization will be described next.
For the case of unweighted points (taking every cluster in the
'cylinder'), it would simply be done by dividing by the:
- background data taking time
- energy range of interest
- *volume of the cylinder*

But for a weighted distance measure $\mathcal{D'}$, we need to perform the
integration over the measure (which we do implicitly for the
non-weighted case by taking the area! Each point simply contributes
with 1, resulting in the area of the circle).

The necessary integration over the energy can be reduced to simply
dividing by the energy range (the 'cylinder height' part if one will),
as everything is constant in the energy direction, i.e. no weighting
in that axis.

Let's look at what happens in the trivial case for an understanding of
what we are actually doing when normalizing by area of a non-weighted thing.

The measure in the unweighted case is thus:
\[
\mathcal{M}(x, y) = 1
\]

Now, we need to integrate this measure over the region of interest
around a point (i.e from a point x over the full radius that  we
consider):

\begin{align*}
W &= \int_{x² + y² < R²} \mathcal{M}(x', y')\, \mathrm{d}x \mathrm{d}y \\
 &= \int_{x² + y² < R²} 1\, \mathrm{d}x \mathrm{d}y \\
 &= \int_0^R \int_0^{2 π} r\, \mathrm{d}r \mathrm{d}φ \\
 &= \int_0^{2 π} \frac{1}{2} R² \, \mathrm{d}φ \\
 &= 2 π\frac{1}{2} R² \\
 &= π R²
\end{align*}
where the additional $r$ after transformation from cartesian
coordinates to polar coordinates is from the Jacobi determinant (ref:
https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant#Example_2:_polar-Cartesian_transformation
as a reminder). For this reason it is important that we start our
assumption in cartesian coordinates, as otherwise we miss out that
crucial factor!
Unexpectedly, the result is simply the area of a circle with radius
$R$, as we intuitively assumed to be for a trivial measure.

For our actual measure we use:
\[
\mathcal{M}(\vec{x}_i, E_i) = \exp \left[ - \frac{1}{2} \mathcal{D'}²((\vec{x}_e, E_e), (\vec{x}_i, E_i)) / σ² \right]
\]
the procedure follows in the exact same fashion (we leave out the
arguments to $\mathcal{D}$ in the further part:
\begin{align*}
W &= \int_{x² + y² < R²} \mathcal{M}(x', y')\, \mathrm{d}x \mathrm{d}y \\
 &= \int_{x² + y² < R²} \exp \left[ - \frac{1}{2} \mathcal{D'}² / σ² \right] \, \mathrm{d}x \mathrm{d}y \\
 &= \int_0^R \int_0^{2 π} r \exp \left[ - \frac{1}{2} \mathcal{D'}² / σ² \right]\, \mathrm{d}r \mathrm{d}φ 
\end{align*}
which can be integrated using standard procedures or just using
SageMath, ...:

#+begin_src sage :eval no-export
sage: r = var('r') # for radial variable we integrate over
sage: σ = var('σ') # for constant sigma
sage: φ = var('φ') # for angle variable we integrate over
sage: R = var('R') # for the radius to which we integrate
sage: assume(R > 0) # required for sensible integration
sage: f = exp(-r ** 2 / (sqrt(2) * σ) ** 2) * r 
sage: result = integrate(integrate(f, r, 0, R), φ, 0, 2 * pi)
sage: result
-2*pi*(σ^2*e^(-1/2*R^2/σ^2) - σ^2)
sage: result(R = 100, σ = 33.33333).n()
6903.76027055093
#+end_src

The final normalization in code:
#+begin_src nim
proc normalizeValue*(x, radius: float, energyRange: keV, backgroundTime: Hour): keV⁻¹•cm⁻²•s⁻¹ =
  let pixelSizeRatio = 65536 / (1.4 * 1.4).cm²
  let σ = Sigma
  ## This comes for integration with `sagemath` over the gaussian weighting. See the notes.
  let area = -2*π*(σ*σ * exp(-1/2 * radius*radius / (σ*σ)) - (σ*σ))
  let energyRange = energyRange * 2.0 # we look at (factor 2 for radius)
  let factor = area / pixelSizeRatio * # area in cm²
    energyRange *
    backgroundTime.to(Second)
  result = x / factor
#+end_src


**** Error propagation of background interpolation              :noexport:
:PROPERTIES:
:CUSTOM_ID: sec:background_interpolation_uncertainty
:END:

For obvious reasons the background interpolation suffers from
statistical uncertainties. Ideally, we compute the resulting error
from the statistical uncertainty for the points by propagating the
errors through the whole computation. That is from the nearest
neighbor lookup, through the sum of the weighted distance calculation
and then the normalization.

We'll use [[https://github.com/SciNim/Measuremancer]].

#+begin_src nim :tangle /tmp/background_interpolation_error_propagation.nim
import datamancer, measuremancer, unchained, seqmath
#+end_src
Start by importing some data taken from running the main
program. These are the distances at some energy at pixel (127, 127) to
the nearest neighbors.
#+begin_src nim :tangle /tmp/background_interpolation_error_propagation.nim
when isMainModule:
  const data = """
dists
32.14
31.89
29.41
29.12
27.86
21.38
16.16
16.03
"""
#+end_src
Parse and look at it:
#+begin_src nim :tangle /tmp/background_interpolation_error_propagation.nim
when isMainModule:
  var df = parseCsvString(data)
  echo df
#+end_src
#+RESULTS:
| Dataframe |  with | 1 | columns | and | 8 | rows: |
|       Idx | dists |   |         |     |   |       |
|    dtype: | float |   |         |     |   |       |
|         0 | 32.14 |   |         |     |   |       |
|         1 | 31.89 |   |         |     |   |       |
|         2 | 29.41 |   |         |     |   |       |
|         3 | 29.12 |   |         |     |   |       |
|         4 | 27.86 |   |         |     |   |       |
|         5 | 21.38 |   |         |     |   |       |
|         6 | 16.16 |   |         |     |   |       |
|         7 | 16.03 |   |         |     |   |       |
|           |       |   |         |     |   |       |

Now import the required transformations of the code, straight from the limit code
(we will remove all unnecessary bits). First get the radius and sigma
that we used here:
#+begin_src nim :tangle /tmp/background_interpolation_error_propagation.nim
when isMainModule:
  let Radius = 33.3
  let Sigma = Radius / 3.0
  let EnergyRange = 0.3.keV
#+end_src
and now the functions:
#+begin_src nim :tangle /tmp/background_interpolation_error_propagation.nim
template compValue(tup: untyped, byCount = false): untyped =
  if byCount:
    tup.size.float
  else:
    # weigh by distance using gaussian of radius being 3 sigma
    let dists = tup # `NOTE:` not a tuple here anymore
    var val = 0.0
    for d in items(dists):
      val += smath.gauss(d, mean = 0.0, sigma = Sigma)
    val

defUnit(cm²)    
proc normalizeValue*[T](x: T, radius, σ: float, energyRange: keV, byCount = false): auto =
  let pixelSizeRatio = 65536 / (1.4 * 1.4).cm²
  var area: float
  if byCount:
    # case for regular circle with weights 1
    area = π * radius * radius # area in pixel
  else:
    area = -2*Pi*(σ*σ * exp(-1/2 * radius*radius / (σ*σ)) - (σ*σ))
    
  let energyRange = energyRange * 2.0 #radius / 6.0 / 256.0 * 12.0 * 2.0 # fraction of full 12 keV range
                                                # we look at (factor 2 for radius)
  let backgroundTime = 3300.h.to(Second)
  let factor = area / pixelSizeRatio * # area in cm²
    energyRange *
    backgroundTime
  result = x / factor
#+end_src
=compValue= computes the weighted (or unweighted) distance measure and
=normalizeValue= computes the correct normalization based on the
radius. The associated area is obtained using the integration shown in
the previous section (using sagemath).

Let's check if we can run the computation and see what we get
#+begin_src nim :tangle /tmp/background_interpolation_error_propagation.nim
when isMainModule:
  let dists = df["dists", float]
  echo "Weighted value : ", compValue(dists)
  echo "Normalized value : ", compValue(dists).normalizeValue(Radius, Sigma, EnergyRange)
#+end_src
#+RESULTS:
:RESULTS:
| Weighted value   |                         0.9915005015651535 |
| Normalized value | 6.07539968974599e-06 CentiMeter⁻²•Second⁻¹ |
:END:

values that seem reasonable.

To compute the associated errors, we need to promote the functions we
use above to work with =Measurement[T]= objects. =normalizeValue= we
can just make generic (DONE). For =compValue= we still need a Gaussian
implementation (note: we don't have errors associated with $μ$ and $σ$
for now. We might add that.).

The logic for the error calculation / getting an uncertainty from the
set of clusters in the search radius is somewhat subtle.

Consider the unweighted case: If we have $N$ clusters, we associate an
uncertainty to these number of clusters to $ΔN = √N$. Why is that?
Because:
\[
N = Σ_i (1 ± 1) =: f
\]
leads to precisely that result using linear error propagation! Each
value has an uncertainty of $√1$. Computing the uncertainty of a
single value just yields $√((∂(N)/∂N)² ΔN²) = ΔN$. Doing the same of
the *sum* of elements, just means 
\[
ΔN =  √( Σ_i (∂f/∂N_i)²(ΔN_i)² ) = √( Σ_i 1² ) = √N
\]
precisely what we expect.

We can then just treat the gaussian in the same way, namely:
\[
f = Σ_i (1 ± 1) * \text{gauss}(\vec{x} - \vec{x_i}, μ = 0, σ)
\]
and transform it the same way. This has the effect that points that
are further away contribute less than those closer!

This is implemented here (thanks to =Measuremancer=, damn):
#+begin_src nim :tangle /tmp/background_interpolation_error_propagation.nim
proc gauss*[T](x: T, μ, σ: float): T =
  let
    arg = (x - μ) / σ
    res = exp(-0.5 * arg * arg)
  result = res

proc compMeasureValue*[T](tup: Tensor[T], σ: float, byCount: bool = false): auto =
  if byCount:
    let dists = tup # only a tuple in real interp code
    let num = tup.size.float
    var val = 0.0 ± 0.0
    for d in items(dists):
      val = val + (1.0 ± 1.0) * 1.0 # last * 1.0 represents the weight that is one !this holds!
    doAssert val == (num ± sqrt(num)) # sanity check that our math works out
    val
  else:
    # weigh by distance using gaussian of radius being 3 sigma
    let dists = tup # `NOTE:` not a tuple here anymore
    var val = 0.0 ± 0.0
    for d in items(dists):
      let gv = (1.0 ± 1.0) * gauss(d, μ = 0.0, σ = σ) # equivalent to unweighted but with gaussian weights
      val = val + gv
    val
#+end_src

Time to take our data and plug it into the two procedures:
#+begin_src nim :tangle /tmp/background_interpolation_error_propagation.nim
when isMainModule:
  let dists = df["dists", float]
  echo "Weighted values (byCount) : ", compMeasureValue(dists, σ = Sigma, byCount = true)
  echo "Normalized value (byCount) : ", compMeasureValue(dists, σ = Sigma, byCount = true)
    .normalizeValue(Radius, Sigma, EnergyRange, byCount = true)
  echo "Weighted values (gauss) : ", compMeasureValue(dists, σ = Sigma, byCount = false)
  echo "Normalized value (gauss) : ", compMeasureValue(dists, σ = Sigma, byCount = false)
    .normalizeValue(Radius, Sigma, EnergyRange)
#+end_src
#+RESULTS:
:RESULTS:
Weighted values (byCount) : 8.00 ± 2.83
Normalized value (byCount) : 1.08e-05 ± 3.81e-06 CentiMeter⁻²•Second⁻¹
Weighted values (gauss) : 0.992 ± 0.523
Normalized value (gauss) : 6.08e-06 ± 3.20e-06 CentiMeter⁻²•Second⁻¹
:END:

The result mostly makes sense: Namely, in the case of the gaussian, we
essentially have "less" statistics, because we weigh the events
further away less. The result is a larger error on the weighted case
with less statistics.

*Note:* In this particular case the computed background rate is
significantly lower (but almost within 1σ!) than in the non weighted
case. This is expected and also essentially proving the correctness of
the uncertainty. The distances of the points in the input data is
simply quite far away for all values.

***** Random sampling to simulate background uncertainty

We'll do a simple Monte Carlo experiment to assess the uncertainties
from a statistical point of view and compare with the results obtained
in the section above.

First do the sampling of backgrounds part:
#+begin_src nim :tangle /tmp/background_uncertainty_mc.nim
import std / [random, math, strformat, strutils]

const outDir = "/home/basti/org/Figs/statusAndProgress/background_interpolation/uncertainty"

import ./sampling_helpers
proc sampleBackgroundClusters(rng: var Rand, num: int,
                              sampleFn: (proc(x: float): float)
                             ): seq[tuple[x, y: int]] =
  ## Samples a number `num` of background clusters distributed over the whole chip.
  result = newSeq[tuple[x, y: int]](num)
  # sample in `y` from function
  let ySamples = sampleFrom(sampleFn, 0.0, 255.0, num)
  for i in 0 ..< num:
    result[i] = (x: rng.rand(255),
                 y: ySamples[i].round.int)

import ggplotnim, sequtils
proc plotClusters(s: seq[tuple[x, y: int]], suffix: string) =
  let df = toDf({"x" : s.mapIt(it.x), "y" : s.mapIt(it.y)})
  let outname = &"{outDir}/clusters{suffix}.pdf"
  ggplot(df, aes("x", "y")) + geom_point(size = some(1.0)) +
    ggtitle(&"Sampling bias: {suffix}. Num clusters: {s.len}") +
    ggsave(outname)

import unchained
defUnit(keV⁻¹•cm⁻²•s⁻¹)
proc computeNumClusters(backgroundRate: keV⁻¹•cm⁻²•s⁻¹, energyRange: keV): float =
  ## computes the number of clusters we need to simulate a certain background level
  let goldArea = 5.mm * 5.mm
  let area = 1.4.cm * 1.4.cm
  let time = 3300.h
  # let clusters = 10000 # about 10000 clusters in total chip background
  result = backgroundRate * area * time.to(Second) * energyRange

import arraymancer, measuremancer
import ./background_interpolation_error_propagation


import numericalnim
proc compClusters(fn: (proc(x: float): float), numClusters: int): float =
  proc hFn(x: float, ctx: NumContext[float, float]): float =
    (numClusters / (256.0 * fn(127.0))) * fn(x)
  result = simpson(hfn, 0.0, 256.0)
  doAssert almostEqual(hFn(127.0, newNumContext[float, float]()), numClusters / 256.0)

proc computeToy(rng: var Rand, numClusters: int, radius, σ: float, energyRange: keV,
                sampleFn: (proc(x: float): float),
                correctNumClusters = false,
                verbose = false, suffix = ""): tuple[m: Measurement[keV⁻¹•cm⁻²•s⁻¹],
                                                     num: int] =
  var numClusters = numClusters
  if correctNumClusters:
    numClusters = compClusters(sampleFn, numClusters).round.int
  let clusters = rng.sampleBackgroundClusters(numClusters.int, sampleFn)
  if verbose:
    plotClusters(clusters, suffix)

  # generate a kd tree based on the data

  let tTree = stack([clusters.mapIt(it.x.float).toTensor,
                     clusters.mapIt(it.y.float).toTensor], axis = 1)
  let kd = kdTree(tTree, leafSize = 16, balancedTree = true)
  let tup = kd.queryBallPoint([127.float, 127.float].toTensor, radius)

  let m = compMeasureValue(tup[0], σ = radius / 3.0, byCount = false)
      .normalizeValue(radius, σ, energyRange)
  let num = tup[0].len
  if verbose:
    echo "Normalized value (gauss) : ", m, " based on ", num, " clusters in radius"
  result = (m: m, num: num)

let radius = 33.3
let σ = radius / 3.0      
let energyRange = 0.3.keV
let num = computeNumClusters(5e-6.keV⁻¹•cm⁻²•s⁻¹, energyRange * 2.0).round.int

var rng = initRand(1337)
import sugar
# first look at / generate some clusters to see sampling works
discard rng.computeToy(num, radius, σ, energyRange, sampleFn = (x => 1.0), verbose = true, suffix = "_constant_gold_region_rate")
# should be the same number of clusters!
discard rng.computeToy(num, radius, σ, energyRange, sampleFn = (x => 1.0),
                       correctNumClusters = true,
                       verbose = true, suffix = "_constant_gold_region_rate_corrected")
# now again with more statistics
discard rng.computeToy(100 * num, radius, σ, energyRange, sampleFn = (x => 1.0), verbose = true, suffix = "_constant")
# should be the same number of clusters!
discard rng.computeToy(100 * num, radius, σ, energyRange, sampleFn = (x => 1.0),
                       correctNumClusters = true,
                       verbose = true, suffix = "_constant_corrected")
# linear
discard rng.computeToy(100 * num, radius, σ, energyRange, sampleFn = (x => x), verbose = true, suffix = "_linear")
# should be the same number of clusters!
discard rng.computeToy(100 * num, radius, σ, energyRange, sampleFn = (x => x),
                       correctNumClusters = true,
                       verbose = true, suffix = "_linear_corrected")
# square
discard rng.computeToy(100 * num, radius, σ, energyRange, sampleFn = (x => x*x), verbose = true, suffix = "_square")
# number of clusters should differ!
discard rng.computeToy(100 * num, radius, σ, energyRange, sampleFn = (x => x*x),
                       correctNumClusters = true,
                       verbose = true, suffix = "_square_corrected")
# exp
discard rng.computeToy(100 * num, radius, σ, energyRange, sampleFn = (x => exp(x/64.0)), verbose = true, suffix = "_exp64")
# number of clusters should differ!
discard rng.computeToy(100 * num, radius, σ, energyRange, sampleFn = (x => exp(x/64.0)),
                       correctNumClusters = true,
                       verbose = true, suffix = "_exp64_corrected") 

proc performToys(nmc: int,
                 numClusters: int,
                 sampleFn: (proc(x: float): float),
                 suffix: string,
                 correctNumClusters = true): DataFrame =
  var numClusters = numClusters
  if correctNumClusters:
    echo "Old number of clusters: ", numClusters
    numClusters = compClusters(sampleFn, numClusters).round.int
    echo "Corrected number of clusters: ", numClusters
  var data = newSeq[Measurement[keV⁻¹•cm⁻²•s⁻¹]](nmc)
  var clustersInRadius = newSeq[int](nmc)
  for i in 0 ..< nmc:
    if i mod 500 == 0:
      echo "Iteration: ", i
    let (m, numInRadius) = rng.computeToy(numClusters, radius, σ, energyRange, sampleFn = sampleFn)
    data[i] = m
    clustersInRadius[i] = numInRadius

  let df = toDf({ "values" : data.mapIt(it.value.float),
                      "errors" : data.mapIt(it.error.float),
                      "numInRadius" : clustersInRadius })
  ggplot(df, aes("values")) + geom_histogram(bins = 500) +
    ggsave(&"{outDir}/background_uncertainty_mc_{suffix}.pdf")
  ggplot(df, aes("errors")) + geom_histogram(bins = 500) +
    ggsave(&"{outDir}/background_uncertainty_mc_errors_{suffix}.pdf")
  if numClusters < 500:
    ggplot(df, aes("numInRadius")) + geom_bar() +
      ggsave(&"{outDir}/background_uncertainty_mc_numInRadius_{suffix}.pdf")
  else:
    ggplot(df, aes("numInRadius")) + geom_histogram(bins = clustersInRadius.max) +
      ggsave(&"{outDir}/background_uncertainty_mc_numInRadius_{suffix}.pdf")
    
  let dfG = df.gather(["values", "errors"], key = "Type", value = "Value")
  ggplot(dfG, aes("Value", fill = "Type")) +
    geom_histogram(bins = 500, position = "identity", hdKind = hdOutline, alpha = some(0.5)) +
    ggtitle("Sampling bias: {suffix}. NMC = {nmc}, numClusters = {int}") +
    ggsave(&"{outDir}/background_uncertainty_mc_combined_{suffix}.pdf")
    
  result = dfG
  result["sampling"] = suffix

proc performAllToys(nmc, numClusters: int, suffix = "", correctNumClusters = true) =
  var df = newDataFrame()
  df.add performToys(nmc, numClusters, (x => 1.0), "constant", correctNumClusters)
  df.add performToys(nmc, numClusters, (x => x), "linear", correctNumClusters)
  df.add performToys(nmc, numClusters, (x => x*x), "square", correctNumClusters)
  df.add performToys(nmc, numClusters, (x => exp(x/64.0)), "exp_x_div_64", correctNumClusters)
  #df = if numClusters < 100: df.filter(f{`Value` < 2e-5}) else: df
  let suffixClean = suffix.strip(chars = {'_'})
  let pltVals = ggplot(df, aes("Value", fill = "sampling")) +
    facet_wrap("Type") + 
    geom_histogram(bins = 500, position = "identity", hdKind = hdOutline, alpha = some(0.5)) +
    prefer_rows() +
    ggtitle(&"Comp diff. sampling biases, {suffixClean}. NMC = {nmc}, numClusters = {numClusters}")
    #ggsave(&"{outDir}/background_uncertainty_mc_all_samplers{suffix}.pdf", height = 600, width = 800)

  # stacked version of number in radius
  let width = if numClusters < 100: 800.0 else: 1000.0
  # stacked version
  ggplot(df.filter(f{`Type` == "values"}), aes("numInRadius", fill = "sampling")) +
    geom_bar(position = "stack") + 
    scale_x_discrete() +
    xlab("# cluster in radius") +
    ggtitle(&"# clusters in interp radius, {suffixClean}. NMC = {nmc}, numClusters = {numClusters}") +
    ggsave(&"{outDir}/background_uncertainty_mc_all_samplers_numInRadius_stacked{suffix}.pdf", height = 600, width = width)
  # ridgeline version
  ggplot(df.filter(f{`Type` == "values"}), aes("numInRadius", fill = "sampling")) +
    ggridges("sampling", overlap = 1.3) + 
    geom_bar(position = "identity") + 
    scale_x_discrete() +
    xlab("# cluster in radius") +
    ggtitle(&"# clusters in interp radius, {suffixClean}. NMC = {nmc}, numClusters = {numClusters}") +
    ggsave(&"{outDir}/background_uncertainty_mc_all_samplers_numInRadius_ridges{suffix}.pdf", height = 600, width = width)
    
  var pltNum: GgPlot
  # non stacked bar/histogram with alpha
  if numClusters < 100:
    pltNum = ggplot(df.filter(f{`Type` == "values"}), aes("numInRadius", fill = "sampling")) +
      geom_bar(position = "identity", alpha = some(0.5)) +
      scale_x_discrete() + 
      ggtitle(&"# clusters in interp radius, {suffixClean}. NMC = {nmc}, numClusters = {numClusters}")
  else:
    let binEdges = toSeq(0 .. df["numInRadius", int].max + 1).mapIt(it.float - 0.5)
    pltNum = ggplot(df.filter(f{`Type` == "values"}), aes("numInRadius", fill = "sampling")) +
      geom_histogram(breaks = binEdges, hdKind = hdOutline, position = "identity", alpha = some(0.5)) +
      ggtitle(&"# clusters in interp radius, {suffixClean}. NMC = {nmc}, numClusters = {numClusters}")# +
  ggmulti([pltVals, pltNum], fname = &"{outDir}/background_uncertainty_mc_all_samplers{suffix}.pdf",
          widths = @[800], heights = @[600, 300])

# first regular MC  
const nmc = 100_000
performAllToys(nmc, num, suffix = "_uncorrected", correctNumClusters = false)
# and now the artificial increased toy example
performAllToys(nmc div 10, 10 * num, "_uncorrected_artificial_statistics", correctNumClusters = false)
## and now with cluster correction
performAllToys(nmc, num, suffix = "_corrected", correctNumClusters = true)
# and now the artificial increased toy example
performAllToys(nmc div 10, 10 * num, "_corrected_artificial_statistics", correctNumClusters = true)
#+end_src

#+begin_src nim :tangle /tmp/sampling_helpers.nim
import random, seqmath, sequtils, algorithm

proc cdf[T](data: T): T =
  result = data.cumSum()
  result.applyIt(it / result[^1])

proc sampleFromCdf[T](data, cdf: seq[T]): T =
  # sample an index based on this CDF
  let idx = cdf.lowerBound(rand(1.0))
  result = data[idx]

proc sampleFrom*[T](data: seq[T],
                   start, stop: T,
                   numSamples: int): seq[T] =
  # get the normalized (to 1) CDF for this radius
  let points = linspace(start, stop, data.len)
  let cdfD = cdf(data)
  result = newSeq[T](numSamples)
  for i in 0 ..< numSamples:
    # sample an index based on this CDF
    let idx = cdfD.lowerBound(rand(1.0))
    result[i] = points[idx]

proc sampleFrom*[T](fn: (proc(x: T): T), start, stop: T,
                   numSamples: int,
                   numInterp = 10_000): seq[T] =
  # get the normalized (to 1) CDF for this radius
  let points = linspace(start, stop, numInterp)
  let data = points.mapIt(fn(it))
  let cdfD = cdf(data)
  result = newSeq[T](numSamples)
  for i in 0 ..< numSamples:
    # sample an index based on this CDF
    let idx = cdfD.lowerBound(rand(1.0))
    result[i] = points[idx]
#+end_src

So, from these Monte Carlo toy experiments, we can gleam quite some
insight.

We have implemented unbiased clusters as well as biased clusters.

First one example for the four different cluster samplers, with the
condition each time that the *number of total clusters* is the same as
in the constant background rate case:
#+begin_center
#+CAPTION: Example of an unbiased cluster sampling. Sampled 100 times (for better
#+CAPTION: visibility of the distribution) as many
#+CAPTION: clusters as predicted for our background data taking.
#+NAME: unbiased_sampled_background_clusters_uncorrected
[[~/org/Figs/statusAndProgress/background_interpolation/uncertainty/clusters_constant.pdf]]
#+end_center

#+begin_center
#+CAPTION: Example of a linearly biased cluster sampling. Sampled 100 times (for better
#+CAPTION: visibility of the distribution) as many
#+CAPTION: clusters as predicted for our background data taking.
#+NAME: linear_sampled_background_clusters_uncorrected
[[~/org/Figs/statusAndProgress/background_interpolation/uncertainty/clusters_linear.pdf]]
#+end_center

#+begin_center
#+CAPTION: Example of a squarely biased cluster sampling. Sampled 100 times (for better
#+CAPTION: visibility of the distribution) as many
#+CAPTION: clusters as predicted for our background data taking.
#+NAME: square_sampled_background_clusters_uncorrected
[[~/org/Figs/statusAndProgress/background_interpolation/uncertainty/clusters_square.pdf]]
#+end_center

#+begin_center
#+CAPTION: Example of a $\exp(x/64)$ biased cluster sampling. Sampled 100 times (for better
#+CAPTION: visibility of the distribution) as many
#+CAPTION: clusters as predicted for our background data taking.
#+NAME: exp64_sampled_background_clusters_uncorrected
[[~/org/Figs/statusAndProgress/background_interpolation/uncertainty/clusters_exp64.pdf]]
#+end_center

With these in place, we performed two sets of Monte Carlo experiments
to compute the value & uncertainty of the center point =(127, 127)=
using the gaussian weighted nearest neighbor interpolation from the
previous section.

This is done for all four different samplers and the obtained values
and their errors (propagated via =Measuremancer=) plotted as a
histogram

Once for the number of expected clusters (based on the gold region
background rate), fig. [[background_uncertainty_mc_all_samplers]] and once
for a lower statistics, but much 10 times higher number of clusters,
fig. [[background_uncertainty_mc_all_samplers_artificial_statistics]]

#+begin_center
#+CAPTION: Comparison of four different samplers (unbiased + 3 biased), showing the
#+CAPTION: result of \num{100000} MC toy experiments based on the expected number of
#+CAPTION: clusters if the same background rate of the gold region covered the whole chip.
#+CAPTION: Below a bar chart of the number of clusters found inside the radius.
#+CAPTION: The number of clusters corresponds to about =5e-6 keV⁻¹•cm⁻²•s⁻¹= over the
#+CAPTION: whole chip.
#+NAME: background_uncertainty_mc_all_samplers_uncorrected
[[~/org/Figs/statusAndProgress/background_interpolation/uncertainty/background_uncertainty_mc_all_samplers_uncorrected.pdf]]
#+end_center

#+begin_center
#+CAPTION: Comparison of four different samplers (unbiased + 3 biased), showing the
#+CAPTION: result of \num{10000} MC toy experiments based on the 10 times the expected number of
#+CAPTION: clusters if the same background rate of the gold region covered the whole chip.
#+CAPTION: Below a histogram of the number of clusters found inside the radius.
#+CAPTION: The number of clusters corresponds to about =5e-5 keV⁻¹•cm⁻²•s⁻¹= over the
#+CAPTION: whole chip.
#+NAME: background_uncertainty_mc_all_samplers_uncorrected_artificial_statistics
[[~/org/Figs/statusAndProgress/background_interpolation/uncertainty/background_uncertainty_mc_all_samplers_uncorrected_artificial_statistics.pdf]]
#+end_center

First of all there is some visible structure in the low statistics
figure (fig. [[background_uncertainty_mc_all_samplers_uncorrected]]). The
meaning of it, is not entirely clear to me. Initially, we thought it
might be an integer effect of 0, 1, 2, ... clusters within the radius
and the additional slope being from the distance these clusters are
away from the center. Further away, less weight, less background
rate. But looking at the number of clusters in the radius (lowest plot
in the figure), this explanation alone does not really seem to explain it.

For the high statistics case, we can see that the mean of the
distribution shifts lower and lower, the more extreme the bias is.
This is likely, because the bias causes a larger and larger number of
clusters to land near the top corner of the chip, meaning that there
are less and less clusters found within the point of interpolation.
Comparing the number of clusters in radius figure for this case shows
that indeed, the square and exponential bias case show a peak at lower
energies.

Therefore, I also computed a correction function to compute a biased
distribution that matches the background rate exactly at the center of
the chip, but therefore allows for a larger number of sampled clusters
in total.

We know that (projecting onto the y axis alone), there are:

\[
∫_0^{256} f(x) dx = N
\]

where $N$ is the total number of clusters we draw and $f(x)$ the
function we use to sample. For the constant case, this means that we
have a rate of $N / 256$ clusters per pixel along the y axis
(i.e. per row).

So in order to correct for this and compute the new required number of
clusters in total that gives us the same rate of $N / 256$ in the
center, we can:

\[
∫_0^{256} \frac{N}{256 · f(127)} f(x) dx = N'
\]

where the point $f(127)$ is simply the value of the "background rate"
the function we currently use produces as is in the center of the
chip.

Given our definition of the functions (essentially as primitive
~f(x)= x~, ~f(x) = x * x~, etc. we expect the linear function to match
the required background rate of the constant case exactly in the
middle, i.e. at 127. And this is indeed the case (as can be seen in
the new linear plot below, fig. [[linear_sampled_background_clusters_corrected]]).

This correction has been implemented. The equivalent figures to the
cluster distributions from further above are:

#+begin_center
#+CAPTION: Example of an unbiased cluster sampling with the applied correction. 
#+CAPTION: Sampled 100 times (for better visibility of the distribution) as many
#+CAPTION: clusters as predicted for our background data taking.
#+CAPTION: As expected the number of clusters is still the same number as above.
#+NAME: unbiased_sampled_background_clusters_corrected
[[~/org/Figs/statusAndProgress/background_interpolation/uncertainty/clusters_constant_corrected.pdf]]
#+end_center

#+begin_center
#+CAPTION: Example of a linearly biased cluster sampling with the applied correction.
#+CAPTION: Sampled 100 times (for better
#+CAPTION: visibility of the distribution) as many
#+CAPTION: clusters as predicted for our background data taking.
#+NAME: linear_sampled_background_clusters_corrected
[[~/org/Figs/statusAndProgress/background_interpolation/uncertainty/clusters_linear_corrected.pdf]]
#+end_center

#+begin_center
#+CAPTION: Example of a squarely biased cluster sampling with the applied correction. Sampled 100 times (for better
#+CAPTION: visibility of the distribution) as many
#+CAPTION: clusters as predicted for our background data taking.
#+CAPTION: The correction means that the total number of clusters is now almost 2500 more than
#+CAPTION: in the uncorrected case.
#+NAME: square_sampled_background_clusters_corrected
[[~/org/Figs/statusAndProgress/background_interpolation/uncertainty/clusters_square_corrected.pdf]]
#+end_center

#+begin_center
#+CAPTION: Example of a $\exp(x/64)$ biased cluster sampling with the applied correction. Sampled 100 times (for better
#+CAPTION: visibility of the distribution) as many
#+CAPTION: clusters as predicted for our background data taking.
#+CAPTION: The correction means that the total number of clusters is now almost double the
#+CAPTION: amount in the uncorrected case.
#+NAME: exp64_sampled_background_clusters_corrected
[[~/org/Figs/statusAndProgress/background_interpolation/uncertainty/clusters_exp64_corrected.pdf]]
#+end_center

The correction works nicely. It is visible that in the center the
density seems to be the same as in the constant case.

From here we can again look at the same plots as above, i.e. the
corrected monte carlo plots:


#+begin_center
#+CAPTION: Comparison of four different samplers (unbiased + 3 biased), showing the
#+CAPTION: result of \num{100000} MC toy experiments based on the expected number of
#+CAPTION: clusters such that the background is biased and produces the same background
#+CAPTION: rate as in the gold region in the constant case.
#+CAPTION: Below a bar chart of the number of clusters found inside the radius.
#+CAPTION: The number of clusters corresponds to about =5e-6 keV⁻¹•cm⁻²•s⁻¹= over the
#+CAPTION: whole chip.
#+NAME: background_uncertainty_mc_all_samplers_corrected
[[~/org/Figs/statusAndProgress/background_interpolation/uncertainty/background_uncertainty_mc_all_samplers_corrected.pdf]]
#+end_center

#+begin_center
#+CAPTION: Comparison of four different samplers (unbiased + 3 biased), showing the
#+CAPTION: result of \num{10000} MC toy experiments based on the 10 times the expected number of
#+CAPTION: clusters such that the background is biased and produces the same background
#+CAPTION: rate as in the gold region in the constant case.
#+CAPTION: Below a histogram of the number of clusters found inside the radius.
#+CAPTION: The number of clusters corresponds to about =5e-5 keV⁻¹•cm⁻²•s⁻¹= over the
#+CAPTION: whole chip.
#+NAME: background_uncertainty_mc_all_samplers_uncorrected_artificial_statistics
[[~/org/Figs/statusAndProgress/background_interpolation/uncertainty/background_uncertainty_mc_all_samplers_corrected_artificial_statistics.pdf]]
#+end_center

It can be nicely seen that the mean of the value is now again at the
same place for all samplers! This is reassuring, because it implies
that any systematic uncertainty due to such a bias in our real data is
*probably* negligible, as the effects will never be as strong as
simulated here.

Secondly, we can nicely see that the computed uncertainty for a single
element seems to follow nicely the actual width of the distribution.

In particular this is visible in the artificial high statistics case,
where the mean value of the error is comparable to the width of the
=value= histogram.


**** Sampling of background interpolation

Take interpolation. For sampling purposes we need to sample from
background according to rate. How?
Take a grid in x, y, E of N grid cells. Take background in those
volumes & normalize to # counts in tracking time. Then Poisson sample
from that as mean. Sample an (x, y, E) "position" in that cube.

10x10x20 ? cells used. Plot of the cells, normalized to counts.


*** Candidates [/]
:PROPERTIES:
:CUSTOM_ID: sec:limit:ingredients:candidates
:END:

Finally, the candidates are the X-ray like clusters remaining after
the background rejection algorithm has been applied for the data taken
during the solar tracking. For the computation of the expected limit
the set of candidates is drawn from the background rate distribution
via sampling from a Poisson distribution with the mean of the
background rate. As our background model is an interpolation instead
of a binned model with Poisson distributed bins, we create a grid of
$(x, y, E) = (10, 10, 20)$ cells from the interpolation, which we
scale such that they contain the expected number of candidates from
each cell after the solar tracking duration, $b_{ijk}$. Then we can
walk over the entire grid and sample from a Poisson distribution for
each grid cell with mean $λ_{ijk} = b_{ijk}$. For all sampled
candidates $κ_{ijk}$ in each grid cell, we then compute a random
position and energy from uniform distributions along each dimension.

A slice of the grid cells centered at $\SI{2.75}{keV}$ is shown in
fig. \ref{fig:limit:expected_counts}, with the color indicating how
many candidates are expected in each cell after the solar tracking duration.

A set of toy candidates generated in this manner is shown in
fig. \ref{fig:limit:toy_candidate_set}. Each point represents one toy
candidate at its cluster center position. The color scale represents
the energy of each cluster in \si{keV}.

\begin{figure}[htbp]
  \centering
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{~/org/Figs/statusAndProgress/limitSanityChecks/candidate_sampling_grid_index_5.pdf}
    \caption{Grid of expected counts}
    \label{fig:limit:expected_counts}
  \end{subfigure}%
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{~/org/Figs/statusAndProgress/limitSanityChecks/example_candidates_1.pdf}
    \caption{Toy candidate set}
    \label{fig:limit:toy_candidate_set}
  \end{subfigure}%
  \label{fig:limit:toy_candidates}
  \caption{\subref{fig:limit:expected_counts} Expected counts in $(10, 10, 20)$ cells, centered around
  $\SI{2.75}{keV}$ obtained from background interpolation and normalized back to counts in solar tracking within
  the volume of the grid cell. 
  \subref{fig:limit:toy_candidate_set} A set of toy candidates drawn from cells of expected counts using a Poisson distribution
  with mean based on each grid cell. Each point is the center of a cluster with the color scale showing the energy of that cluster.
  }
\end{figure}

- [ ] *MOVE THE SAMPLING PART* to the section about expected limits?

** Systematics
:PROPERTIES:
:CUSTOM_ID: sec:limit:systematics
:END:

As explained previously in sec. [[#sec:limit:method_systematics]] we
introduce 4 different nuisance parameters to handle systematics. These
are split by those impacting the signal, the background and each
position axis independently.

Tab. [[tab:limit:systematic_uncertainties]] shows the different systematic
uncertainties we consider, whether they affect signal, background or
the position, their value and finally potential biases due to some
imperfect knowledge. Note that the listed software efficiency is an
upper bound. The explicit value depends on the parameter setup for
which we compute a limit, as each setup with differing software
efficiency can have differing uncertainties. Further note that the
accuracy given is purely the result of our estimation on the signal or
background of the underlying systematic assuming some uncertainty. It
does not strictly speaking reflect our knowledge of it to that extent.

All individual systematic uncertainties are combined in the form of a
euclidean distance

\[
\bar{σ} = \sqrt{\sum_i σ_i²}
\]

for each type of systematic ($s$, $b$). The combined uncertainties
come out to

\begin{align*}
σ_s &\leq \SI{3.38}{\percent} \text{ (assuming } σ_{\text{software}} = \SI{2}{\%} \text{)} \\
σ_b &= \SI{0.28}{\percent} \\
σ_{xy} &= \SI{5}{\percent} \text{ (fixed, uncertainty numbers are bounds)}
\end{align*}

where again the final $σ_s$ depends on the specific setup and the
given value is for a case of $σ_{\text{software}} = \SI{2}{\%}$, which
is a bound larger than the observed uncertainties. The position
uncertainty is fixed by hand to $\SI{5}{\%}$ due to lack of knowledge
about parameters that could be used to calculate a specific value. The
numbers in the table represent bounds about the maximum deviation possible.

#+CAPTION: Overview of the different systematics that are considered as well as possible
#+CAPTION: biases due to our understanding.
#+NAME: tab:limit:systematic_uncertainties
#+ATTR_LATEX: :booktabs t
| Uncertainty                               | s or b?  | rel. σ [%] | bias?                           |
|-------------------------------------------+----------+------------+---------------------------------+
| Earth <-> Sun distance                    | s        |     0.7732 | none                            |
| Window thickness (± 10nm)                 | s        |     0.5807 | none                            |
| Solar models                              | s        |      $< 1$ | none                            |
| Magnet length (- 1cm)                     | s        |     0.2159 | likely 9.26m                    |
| Magnet bore diameter (± 0.5mm)            | s        |    2.32558 | measurements indicate 42.x - 43 |
| Window rotation (30° ± 0.5°)              | s        |    0.18521 | none                            |
| Nuisance parameter integration routine    |          |            |                                 |
| Software efficiency                       | s        |       $<2$ | none                            |
|-------------------------------------------+----------+------------+---------------------------------+
| Gas gain time binning                     | b        |    0.26918 | to 0                            |
| Reference dist interp (CDL morphing)      | b        |     0.0844 | none                            |
| Gas gain variation                        | ?        |            |                                 |
| Random coincidences in septem/line veto   |          |            |                                 |
| Background interpolation (params & shape) | b        |          ? | none                            |
| Energy calibration                        |          |            |                                 |
|-------------------------------------------+----------+------------+---------------------------------+
| Alignment (signal, related mounting)      | s (pos.) |     0.5 mm | none                            |
| Detector mounting precision (±0.25mm)     | s (pos.) |    0.25 mm |                                 |
| Gas gain vs charge calib fit              |          |          ? | none                            |

- [ ] Missing systematics:
  - Effective area uncertainty of telescope (Bias to positive!)
  - See ~statusAndProgress~ section about it!

# | note                                                         | reference                                          |
# |--------------------------------------------------------------+----------------------------------------------------|
# |                                                              | [[#sec:uncertain:distance_earth_sun]]                  |
# |                                                              | [[#sec:uncertain:window_thickness]]                    |
# | unclear from plot, need to look at code                      | [[~/org/Figs/lennert_seb_comparison_solar_models.png]] |
# |                                                              | [[#sec:magnet_length_bore_uncertainty]]                |
# |                                                              | [[#sec:magnet_length_bore_uncertainty]]                |
# | rotation seems to be same in both data takings               | [[#sec:window_rotation_uncertainty]]                   |
# | For performance reasons less precise integrations.           |                                                    |
# | Eff. ε_photo < 2, but ε_escape > 3% (less reliable). Choose!  | [[#sec:uncertain:software_efficiency]]                 |
# |--------------------------------------------------------------+----------------------------------------------------|
# | Computed background clusters for different gas gain binnings | [[#sec:uncertain:gas_gain_binning]]                    |
# |                                                              | [[#sec:uncertain:cdl_morphing]]                        |
# | Partially encoded / fixed w/ gas gain time binning.          |                                                    |
# |                                                              | [[#sec:uncertain:random_coincidences]]                 |
# | From error prop. But unclear interpretation. Statistical.    | [[#sec:uncertain:background_interpolation]]            |
# |                                                              | [[#sec:uncertain:energy_calibration]]                  |
# |--------------------------------------------------------------+----------------------------------------------------|
# | From X-ray finger & laser alignment                          | [[#sec:window_rotation_uncertainty]]                   |
# | M6 screws in 6.5mm holes. Results in misalignment, above.    |                                                    |

*** Derivation of the systematics                               :noexport:

Copy over from =[status]=. 

*** TODO Update numbers for systematics relating to software efficiency :noexport:

There was the bug mentioned in ~StatusAndProgress~ section
[[#sec:uncertain:software_efficiency]] about the code there using wrong
energies & DFs.
#+begin_src sh
./determineEffectiveEfficiency ~/CastData/data/CalibrationRuns2017_Reco.h5 ~/CastData/data/CalibrationRuns2018_Reco.h5 --real
#+end_src
See the latest update there too.


*** Computing the combined uncertainties                        :extended:
:PROPERTIES:
:CUSTOM_ID: sec:systematics:combined_uncertainties
:END:

#+begin_src nim
import math
let ss = [0.77315941, # based on real tracking dates # 3.3456, <- old number for Sun ⇔ Earth using min/max perihelion/aphelion
          0.5807,
          1.0,
          0.2159,
          2.32558,
          0.18521]
          #1.727] # software efficiency of LnL method. Included in `mcmc_limit` directly!
let bs = [0.26918,
          0.0844]
proc total(vals: openArray[float]): float =
  for x in vals:
    result += x * x
  result = sqrt(result)

echo "Combined uncertainty signal: ", total(ss) / 100.0
echo "Combined uncertainty background: ", total(bs) / 100.0

echo "Position: ", sqrt(pow((0.5 / 7.0), 2) + pow((0.25 / 7.0), 2))

#+end_src

#+RESULTS:
| Combined  | uncertainty         | signal:     |  0.02724743263827172 |
| Combined  | uncertainty         | background: | 0.002821014576353691 |
| Position: | 0.07985957062499248 |             |                      |

Compared to 4.582 % we're now down to 3.22%! (in each case including
already the software efficiency, which we don't actually include
anymore here, but in ~mcmc_limit~).
Without the software efficiency we're down to 2.7%!

**** Old results

These were the numbers that still used the Perihelion/Aphelion based
distances for the systematic of Sun ⇔ Earth distance.

| Combined  | uncertainty         | signal:     |  0.04582795952309026 |
| Combined  | uncertainty         | background: | 0.002821014576353691 |
| Position: | 0.07985957062499248 |             |                      |

*NOTE*: The value used here is not the one that was used in most mcmc
limit calculations. There we used:
#+begin_src nim
    σ_sig = 0.04692492913207222,
#+end_src
which comes out from assuming 2% uncertainty for the software
efficiency instead of the ~1.727~ that now show up in the code!


*** Signal [0/3]                                                :extended:
- [ ] *signal position* (i.e. the spot of the raytracing result)
  - to be implemented as a nuisance parameter (actually 2) in the
    limit calculation code. 
- [ ] pointing precision of the CAST magnet
  - check the reports of the CAST sun filming. That should give us a
    good number for the alignment accuracy
- [ ] detector and telescope alignment
  - detector alignment goes straight into the signal position one. The
    telescope alignment can be estimated maybe from the geometer
    measurements. In any case that will also directly impact the
    placement / shape of the axion image. So this should be
    redundant. Still need to check the geometer measurements to get a
    good idea here.
    - [X] compute center based on X-ray finger run
    - [X] find image of laser alignment with plastic target
    - [ ] find geometer measurements and see where they place us (good
      for relative from 2017/18 to end of 2018) 

*** Signal rate & efficiency [5/7]                              :extended:
- [ ] (solar model)
  - [X] look into the work by Lennert & Sebastian. What does their study
    of different solar models imply for different fluxes?
    - [ ] check absolute number for 
- [X] axion rate as a function of distance Earth ⇔ Sun (depends on time
  data was taken)
  - [X] simple: compute different rate based on perihelion &
    aphelion. Difference is measure for > 1σ uncertainty on flux
  - [ ] more complex: compute actual distance at roughly times when data
    taking took place. Compare those numbers with the AU distance used
    in the ray tracer & in axion flux (=expRate= in code).    
- [X] telescope and window efficiencies
  - [X] window: especially uncertainty of window thickness: Yevgen
    measured thickness of 3 samples using ellipsometry and got values
    O(350 nm)!
    Norcada themselves say 300 ± 10 nm
    - compute different absorptions for the 300 ± 10 nm case
      (integrated over some energy range) and for the extrema (Yevgen). That
      should give us a number in flux one might lose / gain. 
- [X] window rotation (position of the strongbacks), different for two run
  periods & somewhat uncertain
  - [X] measurement: look at occupancy of calibration runs. This *should*
    give us a well defined orientation for the strongback. From that
    we can adjust the raytracing. Ideally this does not count as a
    systematic as we can measure it (I think, but need to do!)
    - [X] need to look at X-ray finger runs reconstructed & check
      occupancy to compare with occupancies of the calibration data
    - [X] determine the actual loss based on the rotation uncertainty
      if plugged into raytracer & computed total signal?
- [X] magnet length, diameter and field strength (9 T?)
  - magnet length sometimes reported as 9.25 m, other times as 9.26
    - [X] compute conversion probability for 9.26 ± 0.01 m. Result affects
      signal. Get number.
  - diameter sometimes reported as 43 mm, sometimes 42.5 (iirc, look
    up again!), but numbers given by Theodoros from a measurement for
    CAPP indicated essentially 43 (with some measured uncertainty!)
    - [X] treated the same way as magnet length. Adjust area accordingly &
      get number for the possible range.
- [ ] Software signal efficiency due to linear logL interpolation, for
  classification signal / background
  - [ ] what we already did: took two bins surrounding a center bin and
    interpolated the middle one.
    -> what is difference between interpolated and real? This is a measure for
    its uncertainty.
- [X] detector mounting precision:
  - [X] 6 mounting holes, a M6. Hole size 6.5 mm. Thus, easily 0.25mm
    variation is possible (discussed with Tobi).
  - [X] plug can be moved about ±0.43mm away from the center. On
    septemboard variance of plugs is ±0.61mm.
    
**** Distance Earth ⇔ Sun
:PROPERTIES:
:CUSTOM_ID: sec:uncertain:distance_earth_sun
:END:


The distance between Earth and the Sun varies between:

Aphelion:        152100000 km
Perihelion:      147095000 km 
Semi-major axis: 149598023 km
#+REF: https://en.wikipedia.org/wiki/Earth

which first of all is a variation of a bit more than 3% or about ~1.5%
from one AU. The naive interpretation of the effect on the signal
variation would then be 1 / (1.015²) = ~0.971, a loss of about 3% for
the increase from the semi-major axis to the aphelion (or the inverse
for an increase to the aphelion).

In more explicit numbers:
#+begin_src nim
import math

proc flux(r: float): float =
  result = 1 / (r * r)

let f_au = flux(149598023)
let f_pe = flux(147095000)
let f_ap = flux(152100000)
echo "Flux at 1 AU: ", f_au
echo "Flux at Perihelion: ", f_pe
echo "Flux at Aphelion: ", f_ap

echo "Flux decrease from 1 AU to Perihelion: ", f_au / f_pe
echo "Flux increase from 1 AU to Aphelion: ", f_au / f_ap
echo "Mean of increase & decrease: ", (abs(1.0 - f_au / f_pe) + abs(1.0 - f_au / f_ap)) / 2.0
echo "Total flux difference: ", f_pe / f_ap
#+end_src

#+RESULTS:
| Flux  | at       | 1           |                   AU: | 4.468361401371663e-17 |                     |             |                    |
| Flux  | at       | Perihelion: | 4.621725831202688e-17 |                       |                     |             |                    |
| Flux  | at       | Aphelion:   | 4.322565390688589e-17 |                       |                     |             |                    |
| Flux  | decrease | from        |                     1 | AU                    | to                  | Perihelion: | 0.9668166318314223 |
| Flux  | increase | from        |                     1 | AU                    | to                  | Aphelion:   |  1.033729046875066 |
| Mean  | of       | increase    |                     & | decrease:             | 0.03345620752182193 |             |                    |
| Total | flux     | difference: |     1.069209002866338 |                       |                     |             |                    |
| Total | flux     | difference: |   0.06691241504364387 |                       |                     |             |                    |

***** *UPDATE*: <2023-07-01 Sat 15:50>

In section [[#sec:journal:01_07_23_sun_earth_dist]] of the ~journal.org~
we discuss the real distances during the CAST trackings. The numbers
we actually need to care about are the following:

#+begin_src
Mean distance during trackings = 0.9891144450781392
Variance of distance during trackings = 1.399449924353128e-05
Std of distance during trackings = 0.003740922245052853
#+end_src

referring to the CSV file:
[[~/org/resources/sun_earth_distance_cast_solar_trackings.csv]]

where the numbers are in units of 1 AU.

So the absolute numbers come out to:
#+begin_src nim
import unchained
const mean = 0.9891144450781392
echo "Actual distance = ", mean.AU.to(km)
#+end_src

#+RESULTS:
: Actual distance = 1.47969e+08 km

This means an improvement in flux, following the code snippet above:
#+begin_src nim
import math, unchained, measuremancer

proc flux[T](r: T): T =
  result = 1 / (r * r)

let mean = 0.9891144450781392.AU.to(km).float ± 0.003740922245052853.AU.to(km).float
echo "Flux increase from 1 AU to our actual mean: ", pretty(flux(mean) / flux(1.AU.to(km).float), precision = 8)
#+end_src

#+RESULTS:
: Flux increase from 1 AU to our actual mean: 1.0221318 ± 0.0077315941

Which comes out to be an equivalent of 0.773% for the signal
uncertainty now!

This is a really nice improvement from the 3.3% we had before! It
should bring the signal uncertainty from ~4.5% down to close to 3%
probably.

This number was reproduced using ~readOpacityFile~ as well by (see
~journal.org~ on <2023-07-03 Mon 14:09> for more details):
#+begin_src nim
import ggplotnim
let df1 = readCsv("~/org/resources/differential_flux_sun_earth_distance/solar_axion_flux_differential_g_ae_1e-13_g_ag_1e-12_g_aN_1e-15_1AU.csv")
  .filter(f{`type` == "Total flux"})
let df2 = readCsv("~/org/resources/differential_flux_sun_earth_distance/solar_axion_flux_differential_g_ae_1e-13_g_ag_1e-12_g_aN_1e-15_0.989AU.csv")
  .filter(f{`type` == "Total flux"})
let max1AU = df1["diffFlux", float].max
let max0989AU = df2["diffFlux", float].max

echo "Ratio of 1 AU to 0.989 AU = ", max0989AU / max1AU
#+end_src
#+RESULTS:
: Ratio of 1 AU to 0.989 AU = 1.022131825899129
Bang on!


**** Variation of window thickness
:PROPERTIES:
:CUSTOM_ID: sec:uncertain:window_thickness
:END:

The thickness of the SiN windows will vary somewhat. Norcada says they
are within 10nm of 300nm thickness. Measurements done by Yevgen rather
imply variations on the O(50 nm). Difficult to know which numbers to
trust. The thickness goes into the transmission according to
Beer-Lambert's law. Does this imply quadratically?

I'm a bit confused playing around with the Henke tool.

TODO: get a data file for 1 μm and for 2 μm and check what the
difference is.

#+begin_src nim
import ggplotnim
let df1 = readCsv("/home/basti/org/resources/si_nitride_1_micron_5_to_10_kev.txt", sep = ' ')
  .mutate(f{"TSq" ~ `Transmission` * `Transmission`})
let df2 = readCsv("/home/basti/org/resources/si_nitride_2_micron_5_to_10_kev.txt", sep = ' ')
let df = bind_rows(df1, df2, id = "id")
ggplot(df, aes("Energy[eV]", "Transmission", color = "id")) +
  geom_line() +
  geom_line(data = df1, aes = aes(y = "TSq"), color = "purple", lineType = ltDashed) + 
  ggsave("/tmp/transmissions.pdf")
# compute the ratio
let dfI = inner_join(df1.rename(f{"T1" <- "Transmission"}),
                     df2.rename(f{"T2" <- "Transmission"}), by = "Energy[eV]")
  .mutate(f{"Ratio" ~ `T1` / `T2`})
echo dfI
ggplot(dfI, aes("Energy[eV]", "Ratio")) + geom_line() + ggsave("/tmp/ratio_transmissions_1_to_2_micron.pdf")

#+end_src

#+RESULTS:
| Dataframe |   with | 7      |                     columns |    and |    101 |      rows: |       |
|       Idx |    TSq | id     | prevVals_GGPLOTNIM_INTERNAL |     T1 |     T2 | Energy[eV] | Ratio |
|    dtype: |  float | string |                       float |  float |  float |      float | float |
|         0 | 0.8987 | id     |                           0 |  0.948 | 0.8987 |       5000 | 1.055 |
|         1 | 0.9014 | id     |                           0 | 0.9494 | 0.9014 |       5050 | 1.053 |
|         2 |  0.904 | id     |                           0 | 0.9508 |  0.904 |       5100 | 1.052 |
|         3 | 0.9065 | id     |                           0 | 0.9521 | 0.9065 |       5150 |  1.05 |
|         4 | 0.9089 | id     |                           0 | 0.9534 | 0.9089 |       5200 | 1.049 |
|         5 | 0.9113 | id     |                           0 | 0.9546 | 0.9113 |       5250 | 1.048 |
|         6 | 0.9135 | id     |                           0 | 0.9558 | 0.9135 |       5300 | 1.046 |
|         7 | 0.9157 | id     |                           0 | 0.9569 | 0.9157 |       5350 | 1.045 |
|         8 | 0.9178 | id     |                           0 |  0.958 | 0.9179 |       5400 | 1.044 |
|         9 | 0.9199 | id     |                           0 | 0.9591 | 0.9199 |       5450 | 1.043 |
|        10 | 0.9219 | id     |                           0 | 0.9602 | 0.9219 |       5500 | 1.042 |
|        11 | 0.9238 | id     |                           0 | 0.9612 | 0.9238 |       5550 |  1.04 |
|        12 | 0.9257 | id     |                           0 | 0.9621 | 0.9257 |       5600 | 1.039 |
|        13 | 0.9275 | id     |                           0 | 0.9631 | 0.9275 |       5650 | 1.038 |
|        14 | 0.9292 | id     |                           0 |  0.964 | 0.9292 |       5700 | 1.037 |
|        15 | 0.9309 | id     |                           0 | 0.9649 | 0.9309 |       5750 | 1.036 |
|        16 | 0.9326 | id     |                           0 | 0.9657 | 0.9326 |       5800 | 1.036 |
|        17 | 0.9342 | id     |                           0 | 0.9665 | 0.9342 |       5850 | 1.035 |
|        18 | 0.9357 | id     |                           0 | 0.9673 | 0.9357 |       5900 | 1.034 |
|        19 | 0.9372 | id     |                           0 | 0.9681 | 0.9372 |       5950 | 1.033 |
|           |        |        |                             |        |        |            |       |

The resulting =Ratio= here kind of implies that we're missing
something....
Ah, no. The =Ratio= thing was a brain fart. Just squaring the 1μm
thing does indeed reproduce the 2μm case! All good here.

So how do we get the correct value then for e.g. 310nm when having
300nm?

If my intuition is correct (we'll check with a few other numbers in a
minute) then essentially the following holds:

\[
T_{xd} = (T_d)^x
\]

where =T_d= is the transmission of the material at thickness =d= and
we get the correct transmission for a different thickness that is a
multiple =x= of =d= by the given power-law relation.

Let's apply this to the files we have for the 300nm window and see
what we get if we also add 290 and 300 nm.

#+begin_src nim
import ggplotnim, strformat, math

proc readFile(fname: string): DataFrame =
  result = readCsv(fname, sep = ' ')
    .rename(f{"Energy / eV" <- "PhotonEnergy(eV)"})
    .mutate(f{"E / keV" ~ c"Energy / eV" / 1000.0})

let sinDf = readFile("../resources/Si3N4_density_3.44_thickness_0.3microns.txt")
  .mutate(f{float: "T310" ~ pow(`Transmission`, 310.0 / 300.0)})
  .mutate(f{float: "T290" ~ pow(`Transmission`, 290.0 / 300.0)})
var sin1Mu = readFile("../resources/Si3N4_density_3.44_thickness_1microns.txt")
.mutate(f{float: "Transmission" ~ pow(`Transmission`, 0.3 / 1.0)})
sin1Mu["Setup"] = "T300_from1μm"
var winDf = sinDf.gather(["Transmission", "T310", "T290"], key = "Setup", value = "Transmission")
ggplot(winDf, aes("E / keV", "Transmission", color = "Setup")) +
  geom_line() +
  geom_line(data = sin1Mu, lineType = ltDashed, color = "purple") +
  xlim(0.0, 3.0, outsideRange = "drop") +
  xMargin(0.02) + yMargin(0.02) +
  margin(top = 1.5) + 
  ggtitle("Impact of 10nm uncertainty on window thickness. Dashed line: 300nm transmission computed " &
    "from 1μm via power law T₃₀₀ = T₁₀₀₀^{0.3/1}") + 
  ggsave("/home/basti/org/Figs/statusAndProgress/window_uncertainty_transmission.pdf", width = 853, height = 480)
#+end_src

#+RESULTS:

Plot

[[~/org/Figs/statusAndProgress/window_uncertainty_transmission.pdf]]

shows us the impact on the transmission of the uncertainty on the
window thickness. In terms of such transmission the impact seems
almost negligible as long as it's small. However, to get an accurate
number, we should check the integrated effect on the axion flux after
conversion & going through the window. That then takes into account
the energy dependence and thus gives us a proper number of the impact
on the signal.

#+begin_src nim
import sequtils, math, unchained, datamancer
import numericalnim except linspace, cumSum
# import ./background_interpolation

defUnit(keV⁻¹•cm⁻²)

type
  Context = object
    integralBase: float
    efficiencySpl: InterpolatorType[float]
defUnit(keV⁻¹•cm⁻²•s⁻¹)
defUnit(keV⁻¹•m⁻²•yr⁻¹)
defUnit(cm⁻²)
defUnit(keV⁻¹•cm⁻²)
proc readAxModel(): DataFrame =
  let upperBin = 10.0
  proc convert(x: float): float =
    result = x.keV⁻¹•m⁻²•yr⁻¹.to(keV⁻¹•cm⁻²•s⁻¹).float
  result = readCsv("/home/basti/CastData/ExternCode/AxionElectronLimit/axion_diff_flux_gae_1e-13_gagamma_1e-12.csv")
    .mutate(f{"Energy / keV" ~ c"Energy / eV" / 1000.0},
            f{float: "Flux / keV⁻¹•cm⁻²•s⁻¹" ~ convert(idx("Flux / keV⁻¹ m⁻² yr⁻¹"))})
    .filter(f{float: c"Energy / keV" <= upperBin})

proc detectionEff(spl: InterpolatorType[float], energy: keV): UnitLess =
  # window + gas
  if energy < 0.001.keV or energy > 10.0.keV: return 0.0
  result = spl.eval(energy.float)    

proc initContext(thickness: NanoMeter): Context =

  let combEffDf = readCsv("/home/basti/org/resources/combined_detector_efficiencies.csv")
    .mutate(f{float: "Efficiency" ~ pow(idx("300nm SiN"), thickness / 300.nm)}) ## no-op if input is also 300nm
  let effSpl = newCubicSpline(combEffDf["Energy [keV]", float].toRawSeq,
                              combEffDf["Efficiency", float].toRawSeq) # effective area included in raytracer
  let axData = readAxModel()
  let axModel = axData
    .mutate(f{"Flux" ~ idx("Flux / keV⁻¹•cm⁻²•s⁻¹") * detectionEff(effSpl, idx("Energy / keV").keV) })
  let integralBase = simpson(axModel["Flux", float].toRawSeq,
                             axModel["Energy / keV", float].toRawSeq)
  result = Context(integralBase: integralBase,
                   efficiencySpl: effSpl)

defUnit(cm²)
defUnit(keV⁻¹)
func conversionProbability(): UnitLess =
  ## the conversion probability in the CAST magnet (depends on g_aγ)
  ## simplified vacuum conversion prob. for small masses
  let B = 9.0.T
  let L = 9.26.m
  let g_aγ = 1e-12.GeV⁻¹ # ``must`` be same as reference in Context
  result = pow( (g_aγ * B.toNaturalUnit * L.toNaturalUnit / 2.0), 2.0 )

defUnit(cm⁻²•s⁻¹)
defUnit(m⁻²•yr⁻¹)
proc expRate(integralBase: float): UnitLess =
  let trackingTime = 190.h
  let areaBore = π * (2.15 * 2.15).cm²
  result = integralBase.cm⁻²•s⁻¹ * areaBore * trackingTime.to(s) * conversionProbability()

let ctx300 = initContext(300.nm)
let rate300 = expRate(ctx300.integralBase)
let ctx310 = initContext(310.nm)
let rate310 = expRate(ctx310.integralBase)
let ctx290 = initContext(290.nm)
let rate290 = expRate(ctx290.integralBase)

echo "Decrease: 300 ↦ 310 nm: ", rate310 / rate300
echo "Increase: 300 ↦ 290 nm: ", rate290 / rate300
echo "Total change: ", rate290 / rate310
echo "Averaged difference: ", (abs(1.0 - rate310 / rate300) + abs(1.0 - rate290 / rate300)) / 2.0
#+end_src

#+RESULTS:
| Decrease: |         300 |                    ↦ |      310 | nm: | 0.994254 | UnitLess |
| Increase: |         300 |                    ↦ |      290 | nm: |  1.00587 | UnitLess |
| Total     |     change: |              1.01168 | UnitLess |     |          |          |
| Averaged  | difference: | 0.005806760566511471 |          |     |          |          |

**** Magnet length & bore diameter
:PROPERTIES:
:CUSTOM_ID: sec:magnet_length_bore_uncertainty
:END:

Length was reported to be 9.25m in the original CAST proposal,
compared to the since then reported 9.26m.

Conversion probability scales by length quadratically, so the change in
flux should thus also just be quadratic.

The bore diameter was also given as 42.5mm (iirc) initially, but later
as 43mm. The amount of flux scales by the area.

#+begin_src nim
import math
echo 9.25 / 9.26 # Order 0.1% 
echo pow(42.5 / 2.0, 2.0) / pow(43 / 2.0, 2.0) # Order 2.3% 
#+end_src

#+RESULTS:
| 0.9989200863930886 |
| 0.9768793942671714 |

With the conversion probability:

\[
P_{a↦γ, \text{vacuum}} = \left(\frac{g_{aγ} B L}{2} \right)^2 \left(\frac{\sin\left(\delta\right)}{\delta}\right)^2
\]

The change in conversion probability from a variation in magnet length
is thus (using the simplified form if δ is small:
#+begin_src nim
import unchained, math
func conversionProbability(L: Meter): UnitLess =
  ## the conversion probability in the CAST magnet (depends on g_aγ)
  ## simplified vacuum conversion prob. for small masses
  let B = 9.0.T
  let g_aγ = 1e-12.GeV⁻¹ # ``must`` be same as reference in Context
  result = pow( (g_aγ * B.toNaturalUnit * L.toNaturalUnit / 2.0), 2.0 )
let P26 = conversionProbability(9.26.m)
let P25 = conversionProbability(9.25.m)
let P27 = conversionProbability(9.25.m)

echo "Change from 9.26 ↦ 9.25 m = ", P26 / P25
echo "Change from 9.25 ↦ 9.27 m = ", P27 / P25
echo "Relative change = ", (abs(1.0 - P27 / P26) + abs(1.0 - P25 / P26)) / 2.0
#+end_src

#+RESULTS:
| Change   | from   | 9.26 | ↦                    | 9.25 | m | = | 1.00216 | UnitLess |
| Change   | from   | 9.25 | ↦                    | 9.27 | m | = |       1 | UnitLess |
| Relative | change |    = | 0.002158661000424611 |      |   |   |         |          |

And now for the area:

As it only goes into the expected rate by virtue of, well, being the
area we integrate over, we simply need to look at the change in area
from a change in bore radius. 
#+begin_src nim
proc expRate(integralBase: float): UnitLess =
  let trackingTime = 190.h
  let areaBore = π * (2.15 * 2.15).cm²
  result = integralBase.cm⁻²•s⁻¹ * areaBore * trackingTime.to(s) * conversionProbability()
#+end_src

#+begin_src nim
import unchained, math
defUnit(MilliMeter²)
proc boreArea(diameter: MilliMeter): MilliMeter² =
  result = π * (diameter / 2.0)^2
let areaD = boreArea(43.mm)
let areaS = boreArea(42.5.mm)
let areaL = boreArea(43.5.mm)

echo "Change from 43 ↦ 42.5 mm = ", areaS / areaD
echo "Change from 43 ↦ 43.5 mm = ", areaL / areaD
echo "Relative change = ", (abs(1.0 - areaL / areaD) + abs(1.0 - areaS / areaD)) / 2.0
#+end_src

#+RESULTS:
| Change   | from   | 43 | ↦                   | 42.5 | mm | = | 0.976879 | UnitLess |
| Change   | from   | 43 | ↦                   | 43.5 | mm | = |  1.02339 | UnitLess |
| Relative | change |  = | 0.02325581395348841 |      |    |   |          |          |


**** Window rotation & alignment precision [2/2]
:PROPERTIES:
:CUSTOM_ID: sec:window_rotation_uncertainty
:END:

Rotation of the window. Initially we assumed that the rotation was
different in the two different data taking periods.

We can check the rotation by looking at the occupancy runs taken in
the 2017 dataset and in the 2018 dataset.

The 2017 occupancy (filtered to only use events in eccentricity 1 -
1.4) is

[[~/org/Figs/statusAndProgress/systematics/occupancy_clusters_run83_187_chip3_ckQuantile_80.0_region_crAll_eccentricity_1.0_1.3_applyAll_true_.pdf]]

and for 2018:

[[~/org/Figs/statusAndProgress/systematics/occupancy_clusters_run239_304_chip3_ckQuantile_80.0_region_crAll_eccentricity_1.0_1.3_applyAll_true.pdf]]

They imply that the angle was indeed the same (compare with the sketch
of our windows in fig. [[300nm_sin_norcada_window_layout]]). However,
there seems to be a small shift in y between the two, which seems hard
to explain. Such a shift *only* makes sense (unless I'm missing
something!) if there is a shift between the *chip* and the *window*,
but not for any kind of installation shift or shift in the position of
the 55Fe source. I suppose a slight change in how the window is
mounted on the detector can already explain it? This is < 1mm after
all.

In terms of the rotation angle, we'll just read it of using Inkscape.

It comes out to pretty much _exactly_ 30°, see
fig. [[fig:window_rotation]]. I suppose this makes sense given the number
of screws (6?). Still, this implies that the window was mounted
perfectly aligned with some line relative to 2 screws. Not that it
matters.

#+CAPTION: Measurement of the rotation angle of the window in 2018 data taking (2017 is the same)
#+CAPTION: using Inkscape. Comes out to ~30° (with maybe 0.5° margin for error, aligned for
#+CAPTION: exactly 30° for the picture, but some variation around that all looks fine).
#+NAME: fig:window_rotation
[[~/org/Figs/statusAndProgress/systematics/window_rotation_2018.png]]

Need to check the number used in the raytracing code. There we have
(also see discussion with Johanna in Discord):
#+begin_src nim
  case wyKind
  of wy2017:
    result = degToRad(10.8)
  of wy2018:
    result = degToRad(71.5)
  of wyIAXO:
    result = degToRad(20.0) # who knows
#+end_src
so an angle of 71.5 (2018) and 10.8 (2017). Very different from the
number we get in Inkscape based on the calibration runs.

She used the following plot:

[[~/org/Talks/CCM_2018_Apr/figs/xray_finger_side.pdf]]

to extract the angles.

The impact of this on the signal only depends on where the strongbacks
are compared to the axion image.

Fig. [[fig:axion_image_71_5deg]] shows the axion image for the rotation of
71.5° (Johanna from X-ray finger) and fig. [[fig:axion_image_30deg]] shows the same for a rotation of
30° (our measurement). The 30° case matches nicely with the extraction
of fig. [[fig:window_rotation]].

#+CAPTION: Axion image for a window setup rotated to 71.5° (the number Johanna read off
#+CAPTION: from the X-ray finger run).
#+NAME: fig:axion_image_71_5deg
[[~/org/Figs/statusAndProgress/systematics/axion_image_2018_71_5deg.pdf]]

#+CAPTION: Axion image for a window setup rotated to 30° (the number we read off from
#+CAPTION: from the calibration runs).
#+NAME: fig:axion_image_30deg
[[~/org/Figs/statusAndProgress/systematics/axion_image_2018_30deg.pdf]]


From here there are 2 things to do:
- [X] reconstruct the X-ray finger runs & check the rotation of those
  again using the same occupancy plots as for the calibration runs. 
- [X] compute the integrated signal for the 71.5°, 30° and 30°±0.5°
  cases and see how the signal differs. The latter will be the number
  for the systematic we'll use. We do that by just summing the
  raytracing output.

To do the latter, we need to add an option to write the CSV files in
the raytracer first.

#+begin_src nim
import datamancer

proc print(fname: string): float =
  let hmap = readCsv(fname)
  result = hmap["photon flux", float].sum
let f71 = print("/home/basti/org/resources/axion_images_systematics/axion_image_2018_71_5deg.csv")
let f30 = print("/home/basti/org/resources/axion_images_systematics/axion_image_2018_30deg.csv")
let f29 = print("/home/basti/org/resources/axion_images_systematics/axion_image_2018_29_5deg.csv")
let f31 = print("/home/basti/org/resources/axion_images_systematics/axion_image_2018_30_5deg.csv")

echo f71
echo f30
echo "Ratio : ", f30 / f71
echo "Ratio f29 / f31 ", f29 / f31
echo "Difference ", (abs(1.0 - (f29/f30)) + abs(1.0 - (f31/f30))) / 2.0
#+end_src

#+RESULTS:
| 2.177037651356148e-05 |                      |                   |     |                   |
| 2.211544112107869e-05 |                      |                   |     |                   |
| Ratio                 | :                    | 1.015850190156439 |     |                   |
| Ratio                 | f29                  |                 / | f31 | 1.003711153363915 |
| Difference            | 0.001852083217886047 |                   |     |                   |

Now on to the reconstruction of the X-ray finger run.

I copied the X-ray finger runs from tpc19 over to
[[file:~/CastData/data/XrayFingerRuns/]]. The run of interest is mainly
the run 189, as it's the run done with the detector installed as in
2017/18 data taking.

#+begin_src sh :results none
cd /dev/shm # store here for fast access & temporary
cp ~/CastData/data/XrayFingerRuns/XrayFingerRun2018.tar.gz .
tar xzf XrayFingerRun2018.tar.gz
raw_data_manipulation -p Run_189_180420-09-53 --runType xray --out xray_raw_run189.h5
reconstruction -i xray_raw_run189.h5 --out xray_reco_run189.h5 # make sure `config.toml` for reconstruction uses `default` clustering!
reconstruction -i xray_reco_run189.h5 --only_charge
reconstruction -i xray_reco_run189.h5 --only_gas_gain
reconstruction -i xray_reco_run189.h5 --only_energy_from_e
plotData --h5file xray_reco_run189.h5 --runType=rtCalibration -b bGgPlot --ingrid --occupancy --config plotData.toml
#+end_src

which gives us the following plot:

#+CAPTION: Occupancies of cluster centers of the X-ray finger run (189) in 2018.
#+CAPTION: Shows the same rotation as the calibration runs here.
#+NAME: fig:occupancy_cluster_xray_finger_run_189
[[~/org/Figs/statusAndProgress/systematics/occupancy_clusters_run189_chip3_ckQuantile_95.0_region_crAll_eccentricity_1.0_1.3_applyAll_true.pdf]]

With many more plots here:
[[file:~/org/Figs/statusAndProgress/xrayFingerRun/run189/]]

Also see the relevant section in sec. [[#sec:cast:alignment]].


Using =TimepixAnalysis/Tools/printXyDataset= we can now compute the
center of the X-ray finger run.

#+begin_src sh :results drawer
cd ~/CastData/ExternCode/TimepixAnalysis/Tools/
./printXyDataset -f /dev/shm/xray_reco_run189.h5 -c 3 -r 189 \
                 --dset centerX --reco \
                 --cuts '("eccentricity", 0.9, 1.4)' \
                 --cuts '("centerX", 3.0, 11.0)' \
                 --cuts '("centerY", 3.0, 11.0)'

./printXyDataset -f /dev/shm/xray_reco_run189.h5 -c 3 -r 189 \
                 --dset centerY --reco \
                 --cuts '("eccentricity", 0.9, 1.4)' \
                 --cuts '("centerX", 3.0, 11.0)' \
                 --cuts '("centerY", 3.0, 11.0)'
#+end_src

#+RESULTS:
:results:
Parsing ("eccentricity", 0.9, 1.4)
vals: @["\"eccentricity\"", " 0.9", " 1.4"]
Parsed it to (dset: "eccentricity", lower: 0.9, upper: 1.4)
Parsing ("centerX", 3.0, 11.0)
vals: @["\"centerX\"", " 3.0", " 11.0"]
Parsed it to (dset: "centerX", lower: 3.0, upper: 11.0)
Parsing ("centerY", 3.0, 11.0)
vals: @["\"centerY\"", " 3.0", " 11.0"]
Parsed it to (dset: "centerY", lower: 3.0, upper: 11.0)
Dataset: centerX
Min: 3.000375816993464
Max: 10.9998178807947
Mean: 7.658099408977982
Sum: 144470.045350371
Parsing ("eccentricity", 0.9, 1.4)
vals: @["\"eccentricity\"", " 0.9", " 1.4"]
Parsed it to (dset: "eccentricity", lower: 0.9, upper: 1.4)
Parsing ("centerX", 3.0, 11.0)
vals: @["\"centerX\"", " 3.0", " 11.0"]
Parsed it to (dset: "centerX", lower: 3.0, upper: 11.0)
Parsing ("centerY", 3.0, 11.0)
vals: @["\"centerY\"", " 3.0", " 11.0"]
Parsed it to (dset: "centerY", lower: 3.0, upper: 11.0)
Dataset: centerY
Min: 3.002725
Max: 10.99956806282723
Mean: 6.448868421386536
Sum: 121657.902769457
:end:

So we get a mean of:
- centerX: 7.658
- centerY: 6.449 

meaning we are ~0.5 mm away from the center in either direction. Given
that there is distortion due to the magnet optic, uncertainty about
the location of X-ray finger & emission characteristic, using a
variation of 0.5mm seems reasonable.

This also matches more or less the laser alignment we did initially,
see fig. [[fig:cast_laser_alignment_target]].

#+CAPTION: Laser alignment using target on flange at CAST. Visible deviation is ~0.5mm
#+CAPTION: more or less.
#+NAME: fig:cast_laser_alignment_target
[[~/org/Figs/statusAndProgress/CAST_detector_laser_alignment_target.jpg]]

***** TODO Question about signal & window

One thing we currently do not take into account is that when varying
the signal position using the nuisance parameters, we move the window
strongback *with* the position...

In principle we're not allowed to do that. The strongbacks are part of
the detector & not the signal (but are currently convolved into the
image).

The strongback position depends on the detector mounting precision
only.

So if the main peak was exactly on the strongback, we'd barely see anything!

**** Integration routines for nuisance parameters

For performance reasons we cannot integrate out the nuisance
parameters using the most sophisticated algorithms. Maybe in the end
we could assign a systematic by computing a few "accurate"
integrations (e.g. integrating out $θ_x$ and $θ_y$) with adaptive
gauss and then with our chosen method and compare the result on the
limit? Could just be a "total" uncertainty on the limit w/o changing
any parameters. 

*** Detector behavior [0/1]                                     :extended:

- [ ] drift in # hits in ⁵⁵Fe. "Adaptive gas gain" tries to minimize this,
  maybe variation of mean energy over time after application a measure for uncertainty?
  -> should mainly have an effect on *software signal efficiency*.
  - goes into S of limit likelihood (ε), which is currently assumed a
    constant number

- [ ] veto random coincidences

**** Random coincidences
:PROPERTIES:
:CUSTOM_ID: sec:uncertain:random_coincidences
:END:

Come up with equation to compute rate of random coincidences.

Inputs:
- area of chip
- rate of cosmics
- shutter length
- physical time scale of background events

to compute rate of random coincidences.

Leads to effective reduction in live data taking time (increased dead
time). 


*** Background [0/2]                                            :extended:
- [ ] background interpolation
  - we already did: study of statistical uncertainty (both MC as well
    as via error propagation) 
    - [X] extract from error propagation code
      unclear what to do with these numbers!
- [ ] septem veto can suffer from uncertainties due to possible random
  coincidences of events on outer chip that veto a center event, which
  are not actually correlated.
  In our current application of it, this implies a) a lower background
  rate, but b) a lower software signal efficiency *as we might also
  remove real photons*. So its effect is on ε as ell.
  - [ ] think about random coincidences, derive some formula similar
    to lab course to compute chance

**** Background interpolation [0/1]
:PROPERTIES:
:CUSTOM_ID: sec:uncertain:background_interpolation
:END:

Ref: [[#sec:background_interpolation_uncertainty]]
and =ntangle= this file and run
=/tmp/background_interpolation_error_propagation.nim= for the
background interpolation with =Measuremancer= error propagation.

For an input of 8 clusters in a search radius around a point we get
numbers such as:
=Normalized value (gauss) : 6.08e-06 ± 3.20e-06 CentiMeter⁻²•Second⁻¹=
so an error that is almost 50% of the input.

However, keep in mind that this is for a small area around the
specific point. Just purely from Poisson statistics we expect an
uncertainty of 2.82 for 8 events
\[
ΔN = √8 = 2.82
\]

As such this makes sense (the number is larger due to the gaussian
nature of the distance calculation etc.) and just being a weighted sum
of =1 ± 1= terms error propagated.

If we compute the same for a larger number of points, the error should
go down, which can be seen comparing
fig. [[background_uncertainty_mc_all_samplers_corrected]] with
fig. [[background_uncertainty_mc_all_samplers_uncorrected_artificial_statistics]]
(where the latter has artificially increased statistics).

As this is purely a statistical effect, I'm not sure how to quantify
any kind of systematic errors.

The systematics come into play, due to the:
- choice of radius & sigma
- choice of gaussian weighting
- choice of "energy radius"

- [ ] look at background interpolation uncertainty section linked
  above. Modify to also include a section about a flat model that
  varies the different parameters going into the interpolation.
- [ ] use existing code to compute a systematic based on the kind of
  background model. Impact of background hypothesis? 

*** Energy calibration, likelihood method [0/1]                 :extended:
- [ ] the energy calibration as a whole has many uncertainties (due to
  detector variation, etc.)
  - gas gain time binning:
    - [ ] compute everything up to background rate for no time binning, 90
      min and maybe 1 or 2 other values. Influence on σ_b is the
      change in background that we see from this (will be a lot of
      work, but useful to make things more reproducible).
  - [ ] compute energy of 55Fe peaks after energy
    calibration. Variation gives indication for systematic influence.

**** Gas gain time binning
:PROPERTIES:
:CUSTOM_ID: sec:uncertain:gas_gain_binning
:END:

We need to investigate the impact of the gas gain binning on the
background rate. How do we achieve that?

Simplest approach:
1. Compute gas gain slices for different cases (no binning, 30 min
   binning, 90 min binning, 240 min binning ?)
2. calculate energy based on the used gas gain binning
3. compute the background rate for each case
4. compare amount of background after that.

Question:
Do we need to recompute the gas gain for the calibration data as well?
Yes, as the gas gain slices directly go into the 'gain fit' that needs
to be done in order to compute the energy for any cluster.

So, the whole process is only made complicated by the fact that we
need to change the =config.toml= file in between runs. In the future
this should be a CL argument. For the time being, we can use the same
approach as in
=/home/basti/CastData/ExternCode/TimepixAnalysis/Tools/backgroundRateDifferentEffs/backgroundRateDifferentEfficiencies.nim=
where we simply read the toml file, rewrite the single line and write
it back.

Let's write a script that does mainly steps 1 to 3 for us.
#+begin_src nim :tangle /tmp/compute_systematic_gas_gain_intervals.nim
import shell, strformat, strutils, sequtils, os

# an interval of 0 implies _no_ gas gain interval, i.e. full run
const intervals = [0, 30, 90, 240]
const Tmpl = "$#Runs$#_Reco.h5"
const Path = "/home/basti/CastData/data/systematics/"
const TomlFile = "/home/basti/CastData/ExternCode/TimepixAnalysis/Analysis/ingrid/config.toml"    

proc rewriteToml(path: string, interval: int) =
  ## rewrites the given TOML file in the `path` to use the `interval`
  ## instead of the existing value
  var data = readFile(path).splitLines
  for l in mitems(data):
    if interval == 0 and l.startsWith("fullRunGasGain"):
      l = "fullRunGasGain = true"
    elif interval != 0 and l.startsWith("fullRunGasGain"):
      l = "fullRunGasGain = false"
    elif interval != 0 and l.startsWith("gasGainInterval"):
      l = "gasGainInterval = " & $interval
  writeFile(path, data.join("\n"))

proc computeGasGainSlices(fname: string, interval: int) =
  let (res, err, code) = shellVerboseErr:
    one:
      cd ~/CastData/data/systematics
      reconstruction ($fname) "--only_gas_gain"
  if code != 0:
    raise newException(Exception, "Error calculating gas gain for interval " & $interval)

proc computeGasGainFit(fname: string, interval: int) =
  let (res, err, code) = shellVerboseErr:
    one:
      cd ~/CastData/data/systematics
      reconstruction ($fname) "--only_gain_fit"
  if code != 0:
    raise newException(Exception, "Error calculating gas gain fit for interval " & $interval)

proc computeEnergy(fname: string, interval: int) =
  let (res, err, code) = shellVerboseErr:
    one:
      cd ~/CastData/data/systematics
      reconstruction ($fname) "--only_energy_from_e"
  if code != 0:
    raise newException(Exception, "Error calculating energy for interval " & $interval)

proc computeLikelihood(f, outName: string, interval: int) =
  let args = { "--altCdlFile" : "~/CastData/data/CDL_2019/calibration-cdl-2018.h5",
               "--altRefFile" : "~/CastData/data/CDL_2019/XrayReferenceFile2018.h5",
               "--cdlYear" : "2018",
               "--region" : "crGold"}
  let argStr = args.mapIt(it[0] & " " & it[1]).join(" ")           
  let (res, err, code) = shellVerboseErr:
    one:
      cd ~/CastData/data/systematics
      likelihood ($f) "--h5out" ($outName) ($argStr)
  if code != 0:
    raise newException(Exception, "Error computing likelihood cuts for interval " & $interval)

#proc plotBackgroundRate(f1, f2: string, eff: float) =
#  let suffix = &"_eff_{eff}"
#  let (res, err, code) = shellVerboseErr:
#    one:
#      cd ~/CastData/ExternCode/TimepixAnalysis/Plotting/plotBackgroundRate
#      ./plotBackgroundRate ($f1) ($f2) "--suffix" ($suffix)
#      ./plotBackgroundRate ($f1) ($f2) "--separateFiles --suffix" ($suffix)
#  if code != 0:
#    raise newException(Exception, "Error plotting background rate for eff " & $eff)

let years = [2017, 2018]
let calibs = years.mapIt(Tmpl % ["Calibration", $it])
let backs = years.mapIt(Tmpl % ["Data", $it])
    
copyFile(TomlFile, "/tmp/toml_file.backup")
for interval in intervals:    
  ## rewrite toml file
  rewriteToml(TomlFile, interval)
  ## compute new gas gain for new interval for all files
  for f in concat(calibs, backs):
    computeGasGainSlices(f, interval)
  ## use gas gain slices to compute gas gain fit
  for f in calibs:
    computeGasGainFit(f, interval)
  ## compute energy based on new gain fit
  for f in concat(calibs, backs):
    computeEnergy(f, interval)
  ## compute likelihood based on new energies
  var logFs = newSeq[string]()
  for b in backs:
    let yr = if "2017" in b: "2017" else: "2018"
    let fname = &"out/lhood_{yr}_interval_{interval}.h5"
    logFs.add fname
    ## XXX: need to redo likelihood computation!!
    computeLikelihood(b, fname, interval)
## plot background rate for all combined? or just plot cluster centers? can all be done later...
#plotBackgroundRate(log, eff)
#+end_src

#+begin_src nim
import shell, strformat, strutils, sequtils, os

# an interval of 0 implies _no_ gas gain interval, i.e. full run
const intervals = [0, 30, 90, 240]
const Tmpl = "$#Runs$#_Reco.h5"
echo (Tmpl % ["Data", "2017"]).extractFilename
#+end_src

#+RESULTS:
: DataRuns2017_Reco.h5

The resulting files are found in
[[~/CastData/data/systematics/out/]]
or
[[~/CastData/data/systematics/]]
on my laptop.

Let's extract the number of clusters found on the center chip (gold
region) for each of the intervals:
#+begin_src sh :results drawer
cd ~/CastData/data/systematics
for i in 0 30 90 240 
  do echo Inteval: $i
     extractClusterInfo -f lhood_2017_interval_$i.h5 --short --region crGold
     extractClusterInfo -f lhood_2018_interval_$i.h5 --short --region crGold     
done
#+end_src

#+RESULTS:
:results:
Inteval: 0
Found 497 clusters in region: crGold
Found 244 clusters in region: crGold
Inteval: 30
Found 499 clusters in region: crGold
Found 244 clusters in region: crGold
Inteval: 90
Found 500 clusters in region: crGold
Found 243 clusters in region: crGold
Inteval: 240
Found 497 clusters in region: crGold
Found 244 clusters in region: crGold
:end:

The numbers pretty much speak for themselves.

#+begin_src nim
let nums = { 0 : 497 + 244,
             30 : 499 + 244,
             90 : 500 + 243,
             240 : 497 + 244 }
# reference is 90
let num90 = nums[2][1]
var minVal = Inf
var maxVal = 0.0
for num in nums:
  let rat = num[1] / num90
  echo "Ratio of ", num, " = ", rat
  minVal = min(minVal, rat)
  maxVal = max(maxVal, rat)
echo "Deviation: ", maxVal - minVal
#+end_src

#+RESULTS:
| Ratio      | of                   | (0,   | 741) | = | 0.9973082099596231 |
| Ratio      | of                   | (30,  | 743) | = |                1.0 |
| Ratio      | of                   | (90,  | 743) | = |                1.0 |
| Ratio      | of                   | (240, | 741) | = | 0.9973082099596231 |
| Deviation: | 0.002691790040376896 |       |      |   |                    |

*NOTE*: The one 'drawback' of this approach taken here is the following:
the CDL data was _not_ reconstructed using the changed gas gain
data. *However* that is much less important, as we assume constant
gain over the CDL runs anyway more or less / want to pick the most
precise description of our data!

**** Interpolation of reference distributions (CDL morphing) [/]
:PROPERTIES:
:CUSTOM_ID: sec:uncertain:cdl_morphing
:END:

We already did the study of the variation in the interpolation for the
reference distributions. To estimate the systematic uncertainty
related to that, we should simply look at the computation of the
"intermediate" distributions again and compare the real numbers to the
interpolated ones. The deviation can be done per bin. The average &
some quantiles should be a good number to refer to as a systematic.

The =cdlMorphing= tool
[[file:~/CastData/ExternCode/TimepixAnalysis/Tools/cdlMorphing/cdlMorphing.nim]]
is well suited to this. We will compute the difference between the
morphed and real data for each bin & sum the squares for each
target/filter (those that are morphed, so not the outer two of
course).

Running the tool now yields the following output:
#+begin_src
Target/Filter: Cu-EPIC-0.9kV = 0.0006215219861090395
Target/Filter: Cu-EPIC-2kV = 0.0007052150065674744
Target/Filter: Al-Al-4kV = 0.001483398679126846
Target/Filter: Ag-Ag-6kV = 0.001126063558474516
Target/Filter: Ti-Ti-9kV = 0.0006524420692883554
Target/Filter: Mn-Cr-12kV = 0.0004757207676502019
Mean difference 0.0008440603445360723
#+end_src

So we really have a miniscule difference there.

- [ ] also compute the background rate achieved using no CDL morphing
  vs using it. 

**** Energy calibration [/]
:PROPERTIES:
:CUSTOM_ID: sec:uncertain:energy_calibration
:END:

- [ ] compute peaks of 55Fe energy. What is variation?

**** Software efficiency systematic [/]
:PROPERTIES:
:CUSTOM_ID: sec:uncertain:software_efficiency
:END:

In order to guess at the systematic uncertainty of the software
efficiency, we can push all calibration data through the likelihood
cuts and evaluate the real efficiency that way.

This means the following:
- compute likelihood values for all calibration runs
- for each run, remove extreme outliers using rough RMS transverse &
  eccentricity cuts
- filter to 2 energies (essentially a secondary cut), the photopeak
  and escape peak
- for each peak, push through likelihood cut. # after / # before is
  software efficiency at that energy
The variation we'll see over all runs tells us something about the
systematic uncertainty & potential bias.

*UPDATE*: The results presented below the code were computed with the
code snippet here *as is* (and multiple arguments of course, check
=zsh_history= at home for details). A modified version now also lives
at [[file:~/CastData/ExternCode/TimepixAnalysis/Tools/determineEffectiveEfficiency.nim]]

*UPDATE2* :<2022-08-24 Wed 19:15> While working on the below code for
the script mentioned in the first update, I noticed a bug in the
=filterEvents= function:
#+begin_src nim
      of "Escapepeak":
        let dset = 5.9.toRefDset()
        let xrayCuts = xrayCutsTab[dset]
        result.add applyFilters(df)
      of "Photopeak":
        let dset = 2.9.toRefDset()
        let xrayCuts = xrayCutsTab[dset]
        result.add applyFilters(df)
#+end_src
the energies are exchanged and =applyFilters= is applied to =df= and
not =subDf= as it should here!
- [ ] Investigate the effect for the systematics of CAST!
  -> <2023-03-20 Mon 21:53>: I just had a short look at this. It seems
  like this is the correct output:
  :RESULTS:
DataFrame with 3 columns and 67 rows:
     Idx    Escapepeak     Photopeak     RunNumber
  dtype:         float         float           int
       0        0.6579        0.7542            83
       1        0.6452         0.787            88
       2        0.6771        0.7667            93
       3        0.7975        0.7599            96
       4         0.799        0.7605           102
       5        0.8155        0.7679           108
       6        0.7512        0.7588           110
       7        0.8253        0.7769           116
       8        0.7766        0.7642           118
       9        0.7752        0.7765           120
      10        0.7556        0.7678           122
      11        0.7788        0.7711           126
      12        0.7749        0.7649           128
      13        0.8162        0.7807           145
      14        0.8393        0.7804           147
      15        0.7778          0.78           149
      16        0.8153         0.778           151
      17        0.7591        0.7873           153
      18        0.8229        0.7819           155
      19        0.8341        0.7661           157
      20        0.7788        0.7666           159
      21        0.7912        0.7639           161
      22        0.8041        0.7675           163
      23        0.7884         0.777           165
      24        0.8213        0.7791           167
      25        0.7994        0.7833           169
      26        0.8319        0.7891           171
      27        0.8483        0.7729           173
      28        0.7973        0.7733           175
      29         0.834        0.7771           177
      30         0.802         0.773           179
      31        0.7763        0.7687           181
      32        0.8061         0.766           183
      33        0.7916        0.7799           185
      34        0.8131        0.7745           187
      35        0.8366        0.8256           239
      36        0.8282        0.8035           241
      37        0.8072        0.8045           243
      38         0.851        0.8155           245
      39        0.7637        0.8086           247
      40        0.8439        0.8135           249
      41        0.8571        0.8022           251
      42        0.7854        0.7851           253
      43        0.8159        0.7843           255
      44         0.815        0.7827           257
      45        0.8783        0.8123           259
      46        0.8354        0.8094           260
      47           0.8         0.789           262
      48        0.8038        0.8097           264
      49        0.7926        0.7937           266
      50        0.8275        0.7961           269
      51        0.8514        0.8039           271
      52        0.8089        0.7835           273
      53        0.8134        0.7789           275
      54        0.8168        0.7873           277
      55        0.8198        0.7886           280
      56        0.8447        0.7833           282
      57        0.7876        0.7916           284
      58        0.8093        0.8032           286
      59        0.7945        0.8059           288
      60        0.8407        0.7981           290
      61        0.7824          0.78           292
      62        0.7885        0.7869           294
      63        0.7933        0.7823           296
      64         0.837        0.7834           300
      65        0.7594        0.7826           302
      66        0.8333        0.7949           304

Std Escape = 0.04106537728575545
Std Photo = 0.01581231947284212
Mean Escape = 0.8015071105396809
Mean Photo = 0.7837728948033928
  :END:
  So a bit worse than initially thought...
  
#+begin_src nim :tangle /tmp/calibration_software_eff_syst.nim
import std / [os, strutils, random, sequtils, stats, strformat]
import nimhdf5, cligen
import numericalnim except linspace


import ingrid / private / [likelihood_utils, hdf5_utils, ggplot_utils, geometry, cdl_cuts]
import ingrid / calibration
import ingrid / calibration / [fit_functions]
import ingrid / ingrid_types
import ingridDatabase / [databaseRead, databaseDefinitions, databaseUtils]

# cut performed regardless of logL value on the data, since transverse
# rms > 1.5 cannot be a physical photon, due to diffusion in 3cm drift
# distance
const RmsCleaningCut = 1.5

let CdlFile = "/home/basti/CastData/data/CDL_2019/calibration-cdl-2018.h5"
let RefFile = "/home/basti/CastData/data/CDL_2019/XrayReferenceFile2018.h5"

proc drawNewEvent(rms, energy: seq[float]): int =
  let num = rms.len - 1
  var idx = rand(num)
  while rms[idx] >= RmsCleaningCut or
        (energy[idx] <= 4.5 or energy[idx] >= 7.5):
    idx = rand(num)
  result = idx

proc computeEnergy(h5f: H5File, pix: seq[Pix], group: string, a, b, c, t, bL, mL: float): float =
  let totalCharge = pix.mapIt(calibrateCharge(it.ch.float, a, b, c, t)).sum
  # compute mean of all gas gain slices in this run (most sensible)
  let gain = h5f[group / "chip_3/gasGainSlices", GasGainIntervalResult].mapIt(it.G).mean
  let calibFactor = linearFunc(@[bL, mL], gain) * 1e-6
  # now calculate energy for all hits
  result = totalCharge * calibFactor
  
proc generateFakeData(h5f: H5File, nFake: int, energy = 3.0): DataFrame =
  ## For each run generate `nFake` fake events
  let refSetTuple = readRefDsets(RefFile, yr2018)
  result = newDataFrame()
  for (num, group) in runs(h5f):
    # first read all x / y / tot data
    echo "Run number: ", num
    let xs = h5f[group / "chip_3/x", special_type(uint8), uint8]
    let ys = h5f[group / "chip_3/y", special_type(uint8), uint8]
    let ts = h5f[group / "chip_3/ToT", special_type(uint16), uint16]
    let rms = h5f[group / "chip_3/rmsTransverse", float]
    let cX = h5f[group / "chip_3/centerX", float]
    let cY = h5f[group / "chip_3/centerY", float]    
    let energyInput = h5f[group / "chip_3/energyFromCharge", float]
    let chipGrp = h5f[(group / "chip_3").grp_str]
    let chipName = chipGrp.attrs["chipName", string]
    # get factors for charge calibration
    let (a, b, c, t) = getTotCalibParameters(chipName, num)    
    # get factors for charge / gas gain fit
    let (bL, mL) = getCalibVsGasGainFactors(chipName, num, suffix = $gcIndividualFits)
    var count = 0
    var evIdx = 0

    when false:
      for i in 0 ..< xs.len:
        if xs[i].len < 150 and energyInput[i] > 5.5:
          # recompute from data
          let pp = toSeq(0 ..< xs[i].len).mapIt((x: xs[i][it], y: ys[i][it], ch: ts[i][it]))
          let newEnergy = h5f.computeEnergy(pp, group, a, b, c, t, bL, mL)
          echo "Length ", xs[i].len , " w/ energy ", energyInput[i], " recomp ", newEnergy
          let df = toDf({"x" : pp.mapIt(it.x.int), "y" : pp.mapIt(it.y.int), "ch" : pp.mapIt(it.ch.int)})
          ggplot(df, aes("x", "y", color = "ch")) +
            geom_point() +
            ggtitle("funny its real") + 
            ggsave("/tmp/fake_event_" & $i & ".pdf")
          sleep(200)
      if true: quit()
    
    # to store fake data
    var energies = newSeqOfCap[float](nFake)
    var logLs = newSeqOfCap[float](nFake)
    var rmss = newSeqOfCap[float](nFake)
    var eccs = newSeqOfCap[float](nFake)
    var ldivs = newSeqOfCap[float](nFake)
    var frins = newSeqOfCap[float](nFake)
    var cxxs = newSeqOfCap[float](nFake)
    var cyys = newSeqOfCap[float](nFake)
    var lengths = newSeqOfCap[float](nFake)
    while count < nFake:
      # draw index from to generate a fake event
      evIdx = drawNewEvent(rms, energyInput)
      # draw number of fake pixels
      # compute ref # pixels for this event taking into account possible double counting etc.
      let basePixels = (energy / energyInput[evIdx] * xs[evIdx].len.float)
      let nPix = round(basePixels + gauss(sigma = 10.0)).int  # ~115 pix as reference in 3 keV (26 eV), draw normal w/10 around
      if nPix < 4:
        echo "Less than 4 pixels: ", nPix, " skipping"
        continue
      var pix = newSeq[Pix](nPix)
      var seenPix: set[uint16] = {}
      let evNumPix = xs[evIdx].len

      if nPix >= evNumPix:
        echo "More pixels to draw than available! ", nPix, " vs ", evNumPix, ", skipping!"
        continue
      if not inRegion(cX[evIdx], cY[evIdx], crSilver):
        echo "Not in silver region. Not a good basis"
        continue
      
      var pIdx = rand(evNumPix - 1)
      for j in 0 ..< nPix:
        # draw pix index
        while pIdx.uint16 in seenPix:
          pIdx = rand(evNumPix - 1)
        seenPix.incl pIdx.uint16
        pix[j] = (x: xs[evIdx][pIdx], y: ys[evIdx][pIdx], ch: ts[evIdx][pIdx])
      # now draw
      when false:
        let df = toDf({"x" : pix.mapIt(it.x.int), "y" : pix.mapIt(it.y.int), "ch" : pix.mapIt(it.ch.int)})
        ggplot(df, aes("x", "y", color = "ch")) +
          geom_point() +
          ggsave("/tmp/fake_event.pdf")
        sleep(200)

      # reconstruct event
      let inp = (pixels: pix, eventNumber: 0, toa: newSeq[uint16](), toaCombined: newSeq[uint64]())
      let recoEv = recoEvent(inp, -1,
                             num, searchRadius = 50,
                             dbscanEpsilon = 65,
                             clusterAlgo = caDefault)
      if recoEv.cluster.len > 1 or recoEv.cluster.len == 0:
        echo "Found more than 1 or 0 cluster! Skipping"
        continue
      # compute charge
      let energy = h5f.computeEnergy(pix, group, a, b, c, t, bL, mL)

      # puhhh, now the likelihood...
      let ecc = recoEv.cluster[0].geometry.eccentricity
      let ldiv = recoEv.cluster[0].geometry.lengthDivRmsTrans
      let frin = recoEv.cluster[0].geometry.fractionInTransverseRms
      let logL = calcLikelihoodForEvent(energy, 
                                        ecc,
                                        ldiv,
                                        frin,
                                        refSetTuple)
      # finally done
      energies.add energy
      logLs.add logL
      rmss.add recoEv.cluster[0].geometry.rmsTransverse
      eccs.add ecc
      ldivs.add ldiv
      frins.add frin
      cxxs.add recoEv.cluster[0].centerX
      cyys.add recoEv.cluster[0].centerY
      lengths.add recoEv.cluster[0].geometry.length
      inc count
    let df = toDf({ "energyFromCharge" : energies,
                    "likelihood" : logLs,
                    "runNumber" : num,
                    "rmsTransverse" : rmss,
                    "eccentricity" : eccs,
                    "lengthDivRmsTrans" : ldivs,
                    "centerX" : cxxs,
                    "centerY" : cyys,
                    "length" : lengths,
                    "fractionInTransverseRms" : frins })
    result.add df

proc applyLogLCut(df: DataFrame, cutTab: CutValueInterpolator): DataFrame =
  result = df.mutate(f{float: "passLogL?" ~ (block:
                                               #echo "Cut value: ", cutTab[idx(igEnergyFromCharge.toDset())], " at dset ", toRefDset(idx(igEnergyFromCharge.toDset())), " at energy ", idx(igEnergyFromCharge.toDset()) 
                                               idx(igLikelihood.toDset()) < cutTab[idx(igEnergyFromCharge.toDset())])})

proc readRunData(h5f: H5File): DataFrame =
  result = h5f.readDsets(chipDsets =
    some((chip: 3,
          dsets: @[igEnergyFromCharge.toDset(),
                   igRmsTransverse.toDset(),
                   igLengthDivRmsTrans.toDset(),
                   igFractionInTransverseRms.toDset(),
                   igEccentricity.toDset(),
                   igCenterX.toDset(),
                   igCenterY.toDset(),
                   igLength.toDset(),
                   igLikelihood.toDset()])))

proc filterEvents(df: DataFrame, energy: float = Inf): DataFrame =
  let xrayCutsTab {.global.} = getXrayCleaningCuts()  
  template applyFilters(dfI: untyped): untyped {.dirty.} =
    let minRms = xrayCuts.minRms
    let maxRms = xrayCuts.maxRms
    let maxLen = xrayCuts.maxLength
    let maxEcc = xrayCuts.maxEccentricity
    dfI.filter(f{float -> bool: idx(igRmsTransverse.toDset()) < RmsCleaningCut and
      inRegion(idx("centerX"), idx("centerY"), crSilver) and
      idx("rmsTransverse") >= minRms and
      idx("rmsTransverse") <= maxRms and
      idx("length") <= maxLen and
      idx("eccentricity") <= maxEcc
    })
  if "Peak" in df:
    doAssert classify(energy) == fcInf
    result = newDataFrame()
    for (tup, subDf) in groups(df.group_by("Peak")):
      case tup[0][1].toStr
      of "Escapepeak":
        let dset = 5.9.toRefDset()
        let xrayCuts = xrayCutsTab[dset]
        result.add applyFilters(df)
      of "Photopeak":
        let dset = 2.9.toRefDset()
        let xrayCuts = xrayCutsTab[dset]
        result.add applyFilters(df)
      else: doAssert false, "Invalid name"
  else:
    doAssert classify(energy) != fcInf
    let dset = energy.toRefDset()
    let xrayCuts = xrayCutsTab[dset]
    result = applyFilters(df)

proc splitPeaks(df: DataFrame): DataFrame =
  let eD = igEnergyFromCharge.toDset()
  result = df.mutate(f{float -> string: "Peak" ~ (
    if idx(eD) < 3.5 and idx(eD) > 2.5:
      "Escapepeak"
    elif idx(eD) > 4.5 and idx(eD) < 7.5:
      "Photopeak"
    else:
      "None")})
    .filter(f{`Peak` != "None"})
  
proc handleFile(fname: string, cutTab: CutValueInterpolator): DataFrame =
  ## Given a single input file, performs application of the likelihood cut for all
  ## runs in it, split by photo & escape peak. Returns a DF with column indicating
  ## the peak, energy of each event & a column whether it passed the likelihood cut.
  ## Only events that are pass the input cuts are stored.
  let h5f = H5open(fname, "r")
  randomize(423)
  result = newDataFrame()
  let data = h5f.readRunData()
    .splitPeaks()
    .filterEvents()
    .applyLogLCut(cutTab)
  result.add data
  when false:
    ggplot(result, aes("energyFromCharge")) +
      geom_histogram(bins = 200) +
      ggsave("/tmp/ugl.pdf")
  discard h5f.close()

proc handleFakeData(fname: string, energy: float, cutTab: CutValueInterpolator): DataFrame =
  let h5f = H5open(fname, "r")
  var data = generateFakeData(h5f, 5000, energy = energy)
    .filterEvents(energy)
    .applyLogLCut(cutTab)
  result = data
  discard h5f.close()  

proc getIndices(dset: string): seq[int] =
  result = newSeq[int]()
  applyLogLFilterCuts(CdlFile, RefFile, dset, yr2018, igEnergyFromCharge):
    result.add i

proc plotRefHistos(df: DataFrame, energy: float, cutTab: CutValueInterpolator,
                   dfAdditions: seq[tuple[name: string, df: DataFrame]] = @[]) =
  # map input fake energy to reference dataset
  let grp = energy.toRefDset()

  let passedInds = getIndices(grp)
  let h5f = H5open(RefFile, "r")
  let h5fC = H5open(CdlFile, "r")
  const xray_ref = getXrayRefTable()
  #for (i, grp) in pairs(xray_ref):

  var dfR = newDataFrame()
  for dset in IngridDsetKind:
    try:
      let d = dset.toDset()
      if d notin df: continue # skip things not in input
      ## first read data from CDL file (exists for sure)
      ## extract all CDL data that passes the cuts used to generate the logL histograms
      var cdlFiltered = newSeq[float](passedInds.len)
      let cdlRaw = h5fC[cdlGroupName(grp, "2019", d), float]
      for i, idx in passedInds:
        cdlFiltered[i] = cdlRaw[idx]
      echo "Total number of elements ", cdlRaw.len, " filtered to ", passedInds.len
      dfR[d] = cdlFiltered
      ## now read histograms from RefFile, if they exist (not all datasets do)
      if grp / d in h5f:
        let dsetH5 = h5f[(grp / d).dset_str]
        let (bins, data) = dsetH5[float].reshape2D(dsetH5.shape).split(Seq2Col)
        let fname = &"/tmp/{grp}_{d}_energy_{energy:.1f}.pdf"
        echo "Storing histogram in : ", fname
        # now add fake data
        let dataSum = simpson(data, bins)
        let refDf = toDf({"bins" : bins, "data" : data})
          .mutate(f{"data" ~ `data` / dataSum})
        let df = df.filter(f{float: idx(d) <= bins[^1]})
        ggplot(refDf, aes("bins", "data")) +
          geom_histogram(stat = "identity", hdKind = hdOutline, alpha = 0.5) +
          geom_histogram(data = df, aes = aes(d), bins = 200, alpha = 0.5,
                         fillColor = "orange", density = true, hdKind = hdOutline) +
          ggtitle(&"{d}. Orange: fake data from 'reducing' 5.9 keV data @ {energy:.1f}. Black: CDL ref {grp}") +
          ggsave(fname, width = 1000, height = 600)
    except AssertionError:
      continue

  # get effect of logL cut on CDL data
  dfR = dfR.applyLogLCut(cutTab)
  var dfs = @[("Fake", df), ("Real", dfR)]
  if dfAdditions.len > 0:
    dfs = concat(dfs, dfAdditions)
  var dfPlot = bind_rows(dfs, "Type")
  echo "Rough filter removes: ", dfPlot.len
  dfPlot = dfPlot.filter(f{`lengthDivRmsTrans` <= 50.0 and `eccentricity` <= 5.0})
  echo "To ", dfPlot.len, " elements"
  ggplot(dfPlot, aes("lengthDivRmsTrans", "fractionInTransverseRms", color = "eccentricity")) +
    facet_wrap("Type") + 
    geom_point(size = 1.0, alpha = 0.5) +
    ggtitle(&"Fake energy: {energy:.2f}, CDL dataset: {grp}") +
    ggsave(&"/tmp/scatter_colored_fake_energy_{energy:.2f}.png", width = 1200, height = 800)

  # plot likelihood histos
  ggplot(dfPlot, aes("likelihood", fill = "Type")) +
    geom_histogram(bins = 200, alpha = 0.5, hdKind = hdOutline) +
    ggtitle(&"Fake energy: {energy:.2f}, CDL dataset: {grp}") +
    ggsave(&"/tmp/histogram_fake_energy_{energy:.2f}.pdf", width = 800, height = 600)
    
  discard h5f.close()    
  discard h5fC.close()


  echo "DATASET : ", grp, "--------------------------------------------------------------------------------"
  echo "Efficiency of logL cut on filtered CDL data (should be 80%!) = ", dfR.filter(f{idx("passLogL?") == true}).len.float / dfR.len.float
  echo "Elements passing using `passLogL?` ", dfR.filter(f{idx("passLogL?") == true}).len, " vs total ", dfR.len
  let (hist, bins) = histogram(dfR["likelihood", float].toRawSeq, 200, (0.0, 30.0))
  ggplot(toDf({"Bins" : bins, "Hist" : hist}), aes("Bins", "Hist")) +
    geom_histogram(stat = "identity") +
    ggsave("/tmp/usage_histo_" & $grp & ".pdf")  
  let cutval = determineCutValue(hist, eff = 0.8)
  echo "Effficiency from `determineCutValue? ", bins[cutVal]

proc main(files: seq[string], fake = false, real = false, refPlots = false,
          energies: seq[float] = @[]) =
  ## given the input files of calibration runs, walks all files to determine the
  ## 'real' software efficiency for them & generates a plot
  let cutTab = calcCutValueTab(CdlFile, RefFile, yr2018, igEnergyFromCharge)
  var df = newDataFrame()
  if real and not fake:
    for f in files:
      df.add handleFile(f, cutTab)
    var effEsc = newSeq[float]()
    var effPho = newSeq[float]()
    var nums = newSeq[int]()
    for (tup, subDf) in groups(df.group_by(@["runNumber", "Peak"])):
      echo "------------------"
      echo tup
      #echo subDf
      let eff = subDf.filter(f{idx("passLogL?") == true}).len.float / subDf.len.float
      echo "Software efficiency: ", eff
      if tup[1][1].toStr == "Escapepeak":
        effEsc.add eff
      elif tup[1][1].toStr == "Photopeak":
        effPho.add eff
        # only add in one branch
        nums.add tup[0][1].toInt
      echo "------------------"
    let dfEff = toDf({"Escapepeak" : effEsc, "Photopeak" : effPho, "RunNumber" : nums})
    echo dfEff.pretty(-1)
    let stdEsc = effEsc.standardDeviationS
    let stdPho = effPho.standardDeviationS
    let meanEsc = effEsc.mean
    let meanPho = effPho.mean
    echo "Std Escape = ", stdEsc
    echo "Std Photo = ", stdPho
    echo "Mean Escape = ", meanEsc
    echo "Mean Photo = ", meanPho    
    ggplot(dfEff.gather(["Escapepeak", "Photopeak"], "Type", "Value"), aes("Value", fill = "Type")) +
      geom_histogram(bins = 20, hdKind = hdOutline, alpha = 0.5) +
      ggtitle(&"σ_escape = {stdEsc:.4f}, μ_escape = {meanEsc:.4f}, σ_photo = {stdPho:.4f}, μ_photo = {meanPho:.4f}") + 
      ggsave("/tmp/software_efficiencies_cast_escape_photo.pdf", width = 800, height = 600)
    
    for (tup, subDf) in groups(df.group_by("Peak")):
      case tup[0][1].toStr
      of "Escapepeak": plotRefHistos(df, 2.9, cutTab)
      of "Photopeak": plotRefHistos(df, 5.9, cutTab)
      else: doAssert false, "Invalid data: " & $tup[0][1].toStr
  if fake and not real:
    var effs = newSeq[float]()
    for e in energies:
      if e > 5.9:
        echo "Warning: energy above 5.9 keV not allowed!"
        return
      df = newDataFrame()
      for f in files:
        df.add handleFakeData(f, e, cutTab)
      plotRefHistos(df, e, cutTab)
        
      echo "Done generating for energy ", e
      effs.add(df.filter(f{idx("passLogL?") == true}).len.float / df.len.float)
    let dfL = toDf({"Energy" : energies, "Efficiency" : effs})
    echo dfL
    ggplot(dfL, aes("Energy", "Efficiency")) +
      geom_point() +
      ggtitle("Software efficiency from 'fake' events") + 
      ggsave("/tmp/fake_software_effs.pdf")
  if fake and real:
    doAssert files.len == 1, "Not more than 1 file supported!"
    let f = files[0]
    let dfCast = handleFile(f, cutTab)

    for (tup, subDf) in groups(dfCast.group_by("Peak")):
      case tup[0][1].toStr
      of "Escapepeak":
        plotRefHistos(handleFakeData(f, 2.9, cutTab), 2.9, cutTab, @[("CAST", subDf)])
      of "Photopeak":
        plotRefHistos(handleFakeData(f, 5.9, cutTab), 5.9, cutTab, @[("CAST", subDf)])        
      else: doAssert false, "Invalid data: " & $tup[0][1].toStr
    
  #if refPlots:
  #  plotRefHistos()    
  
when isMainModule:
  dispatch main
#+end_src


*UPDATE* <2022-05-06 Fri 12:32>: The discussion about the results of
the above code here is limited to the results relevant for the
systematic of the software efficiency. For the debugging of the
unexpected software efficiencies computed for the calibration photo &
escape peaks, see section
[[#sec:debug_software_efficiency_cdl_mapping_bug]].

After the debugging session trying to figure out why the hell the
software efficiency is so different, here are finally the results of
this study.

The software efficiencies for the escape & photo peak energies from
the calibration data at CAST are determined as follows:
- filter to events with =rmsTransverse= <= 1.5
- filter to events within the silver region
- filter to events passing the 'X-ray cuts'
- for escape & photo peak each filter to energies of 1 & 1.5 keV
  around the peak
The remaining events are then used as the "basis" for the
evaluation. From here the likelihood cut method is applied to all
clusters.
In the final step the ratio of clusters passing the logL cut over all
clusters is computed, which gives the effective software efficiency
for the data.

For all 2017 and 2018 runs this gives:
#+begin_src sh
Dataframe with 3 columns and 67 rows:
           Idx    Escapepeak     Photopeak     RunNumber
        dtype:         float         float           int
             0        0.6886         0.756            83
             1        0.6845         0.794            88
             2        0.6789        0.7722            93
             3        0.7748        0.7585            96
             4        0.8111         0.769           102
             5        0.7979         0.765           108
             6        0.7346        0.7736           110
             7        0.7682        0.7736           116
             8        0.7593         0.775           118
             9        0.7717        0.7754           120
            10        0.7628        0.7714           122
            11        0.7616        0.7675           126
            12        0.7757        0.7659           128
            13        0.8274        0.7889           145
            14        0.7974        0.7908           147
            15        0.7969        0.7846           149
            16        0.7919        0.7853           151
            17        0.7574        0.7913           153
            18         0.835        0.7887           155
            19        0.8119        0.7755           157
            20        0.7738        0.7763           159
            21        0.7937        0.7736           161
            22        0.7801         0.769           163
            23           0.8        0.7801           165
            24        0.8014         0.785           167
            25        0.7922         0.787           169
            26        0.8237        0.7945           171
            27        0.8392         0.781           173
            28        0.8092        0.7756           175
            29        0.8124        0.7864           177
            30         0.803        0.7818           179
            31        0.7727        0.7742           181
            32        0.7758        0.7676           183
            33        0.7993        0.7817           185
            34        0.8201        0.7757           187
            35         0.824        0.8269           239
            36        0.8369        0.8186           241
            37        0.7953        0.8097           243
            38        0.8205        0.8145           245
            39         0.775        0.8117           247
            40        0.8368        0.8264           249
            41        0.8405        0.8105           251
            42        0.7804         0.803           253
            43        0.8177        0.7907           255
            44         0.801        0.7868           257
            45         0.832        0.8168           259
            46        0.8182        0.8074           260
            47        0.7928        0.7995           262
            48        0.7906        0.8185           264
            49        0.7933        0.8039           266
            50        0.8026         0.811           269
            51        0.8328        0.8086           271
            52        0.8024        0.7989           273
            53        0.8065        0.7911           275
            54         0.807        0.8006           277
            55        0.7895        0.7963           280
            56        0.8133        0.7918           282
            57        0.7939        0.8037           284
            58        0.7963        0.8066           286
            59        0.8104        0.8181           288
            60        0.8056         0.809           290
            61         0.762        0.7999           292
            62        0.7659        0.8021           294
            63        0.7648          0.79           296
            64        0.7868        0.7952           300
            65        0.7815        0.8036           302
            66        0.8276        0.8078           304
#+end_src

with the following statistical summaries:
#+begin_src sh
Std Escape = 0.03320160467567293
Std Photo = 0.01727763707839311
Mean Escape = 0.7923601424260915
Mean Photo = 0.7909126317171645
#+end_src
(where =Std= really is the standard deviation. For the escape data
this is skewed due to the first 3 runs as visible in the DF output
above).



The data as a histogram:

#+CAPTION: Histogram of the effective software efficiencies for escape and photopeak
#+CAPTION: data at CAST for all 2017/18 calibration runs. The low efficiency outliers
#+CAPTION: are the first 3 calibration runs in 2017.
[[~/org/Figs/statusAndProgress/systematics/software_efficiencies_cast_escape_photo.pdf]]

Further, we can also ask for the behavior of fake data now. Let's
generate a set and look at the effective efficiency of fake data.

#+CAPTION: Fake effective software efficiencies at different energies. Clusters are generated
#+CAPTION: from valid 5.9 keV Photopeak clusters (that pass the required cuts) by randomly removing
#+CAPTION: a certain number of pixels until the desired energy is reached. Given the
#+CAPTION: approach, the achieved efficiencies seem fine.
[[~/org/Figs/statusAndProgress/systematics/fake_effective_software_efficiencies.pdf]]

#+CAPTION: Histograms showing the different distributions of the properties for the generated fake
#+CAPTION: data compared to the real reference data from the CDL. At the lowest energies the
#+CAPTION: properties start to diverge quite a bit, likely explaining the lower efficiency there.
[[~/org/Figs/statusAndProgress/systematics/histograms_properties_fake_vs_referenc_data.pdf]]

#+CAPTION: Scatter plots of the different parameters going into the logL cut method comparing
#+CAPTION: the CDL reference data & the fake generated data. The cuts (X-ray for fake &
#+CAPTION: X-ray + reference for CDL) are applied.
[[~/org/Figs/statusAndProgress/systematics/scatter_plots_logL_parameters_fake_vs_ref_data.pdf]]


*NOTE*: One big TODO is the following:
- [ ] Currently the cut values for the LogL are computed using a
  histogram of 200 bins, resulting in significant variance already in
  the CDL data of around 1%. By increasing the number of bins this
  variance goes to 0 (eventually it depends on the number of data
  points). In theory I don't see why we can't compute the cut value
  purely based on the unbinned data. Investigate / do this!
- [ ] Choose the final uncertainty for this variable that we want to use.

  
***** (While generating fake data) Events with large energy, but few pixels
:PROPERTIES:
:CUSTOM_ID: sec:large_events_few_pixels_tot
:END:

While developing some fake data using existing events in the photo
peak & filtering out pixels to end up at ~3 keV, I noticed the
prevalence of events with <150 pixels & ~6 keV energy.

Code produced by splicing in the following code into the body of =generateFakeData=.
#+begin_src nim
    for i in 0 ..< xs.len:
      if xs[i].len < 150 and energyInput[i] > 5.5:
        # recompute from data
        let pp = toSeq(0 ..< xs[i].len).mapIt((x: xs[i][it], y: ys[i][it], ch: ts[i][it]))
        let newEnergy = h5f.computeEnergy(pp, group, a, b, c, t, bL, mL)
        echo "Length ", xs[i].len , " w/ energy ", energyInput[i], " recomp ", newEnergy
        let df = toDf({"x" : pp.mapIt(it.x.int), "y" : pp.mapIt(it.y.int), "ch" : pp.mapIt(it.ch.int)})
        ggplot(df, aes("x", "y", color = "ch")) +
          geom_point() +
          ggtitle("funny its real") + 
          ggsave("/tmp/fake_event.pdf")
        sleep(200)
    if true: quit()
#+end_src

This gives about 100 events that fit the criteria out of a total of
O(20000). A ratio of 1/200 seems probably reasonable for absorption of
X-rays at 5.9 keV.

While plotting them I noticed that they all share that they are
incredibly dense, like:
[[file:~/org/Figs/statusAndProgress/exampleEvents/event_few_pixels_large_energy.pdf]]

These events must be events where the X-ray to photoelectron
conversion happens very close to the grid!
This is one argument "in favor" of using ~ToT~ instead of ToA on the
Timepix1 and more importantly a good reason to keep using the ~ToT~
values instead of pure pixel counting for at least some events!

- [ ] We should look at number of pixels vs. energy as a scatter plot to see
what this gives us.



** Putting it all together :noexport:

- [ ] I don't think this is necessary now?

First: show the basic algorithm in pseudo code to compute a likelihood
value for a set of parameters ($g_{ae}²$, and the $θ$ values).

- expected rate,
- nuisance parameter penalty terms
- loop over candidates, for each candidate
  - compute signal
  - compute background
  -> combine

From here either integrate out $θ$ manually, or sample via MCMC. Goes
straight to next section.  

** MCMC to sample the distribution and compute a limit [0/0]
:PROPERTIES:
:CUSTOM_ID: sec:limit:mcmc_calc_limit
:END:

The Metropolis-Hastings algorithm - as mentioned in
sec. [[#sec:limit:method_mcmc]] - is used to evaluate the integral over
the nuisance parameters to get the posterior likelihood.

Instead of building a very long MCMC, we opt to construct 3 Markov
chains with \num{150000} links to avoid a bias introduced by the
starting parameters. Ideally, one would construct even more chains,
but a certain number of steps from the starting parameter are usually
needed to get into the parameter space of large contributions to the
integral (unless the starting parameters are chosen in a very confined
region, which itself is problematic in terms of bias).

The MCMC is built based on 5 dimensional vectors $\vec{x}$,

\[
\vec{x} = \mtrix{g_{ae}² & θ_s & θ_b & θ_x & θ_y}^T
\]

containing the coupling constant of interest squared as the first
entry and the four nuisance parameters after. Here we mention the
axion-electron coupling constant $g_{ae}$ squared, but generally it
can also be for example $g_{aγ}²$ or $g_{ae}²·g_{aγ}²$, depending on
the search to be conducted. 

Our initial starting vector $\vec{x_i}$  is randomly sampled by

\[
\vec{x} = \vektor{
  \text{rand}([0, 5]) · g_{\text{ref}} \\
  \text{rand}([-0.4, 0.4]) \\
  \text{rand}([-0.4, 0.4]) \\
  \text{rand}([-0.5, 0.5]) \\
  \text{rand}([-0.5, 0.5]) \\
  }
\]

where $\text{rand}$ refers to a uniform random sampler in the given
interval and $g_{\text{ref}}$ is a reference coupling parameter of
choice, which also depends on the specific search.

Our default reference coupling constant for $g_{ae}²$ is $g_{ae}² =
\num{1e-21}$, allowing for a range of parameters in the expected
parameter space. The nuisance parameters are allowed to vary in a
large region, given the standard deviations of $σ < 0.05$ for all four
nuisance parameters. In the updating stage to propose a new vector, we
use the following:

\[
\vec{x_{i+1}} = \vec{x_i} +
  \vektor{
    \text{rand}( -0.5 · 3 g_{\text{ref}}, 0.5 · 3 g_{\text{ref}} ) \\
    \text{rand}( -0.5 · 0.025, 0.5 · 0.025 ) \\
    \text{rand}( -0.5 · 0.025, 0.5 · 0.025 ) \\
    \text{rand}( -0.5 · 0.05,  0.5 · 0.05 ) \\
    \text{rand}( -0.5 · 0.05,  0.5 · 0.05 ) \\
  }
\]

This combination leads to an acceptance rate of the new proposal
typically between $\SIrange{20}{25}{\%}$. After all three chains are
built, the first \num{50000} links each are thrown out as burn-in to
make sure we only include meaningful parameter space.

The parameter space for each of the 5 elements is restricted based on
the following

\[
\vektor{
  g = [0, ∞] \\
  θ_s = [-1, 1] \\
  θ_b = [-0.8, 1] \\
  θ_x = [-1, 1] \\
  θ_y = [-1, 1]
},
\]

meaning we restrict ourselves to physical coupling constants and give
loose bounds on the nuisance parameters. In particular for the $θ_b$
parameter the restriction to values larger than $θ_b > -0.8$ is due to
the singularity in $\mathcal{L}_M$ at $θ_b = -1$. For all realistic
values for the systematic uncertainty $σ_b$ the region of $θ_b \ll 1$
has no physical meaning anyway. But for unit tests and sanity checks
of the implementation, larger uncertainties are tested for, which
cause computational issues if this restriction was not in place.

Example Markov chains can be seen in fig. \ref{fig:limit:mcmc_example}
where we see the different nuisance parameters of the chain and how
they are distributed. As expected for our comparatively small values
of $σ$, the chain is centered around 0 for each nuisance
parameter. And the coupling constant in
fig. \ref{fig:limit:mcmc_theta_x_y} also shows a clear increase
towards low values. 

\begin{figure}[htbp]
  \centering
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{~/org/Figs/statusAndProgress/limitCalculation/MCMC/mcmc_lines_long_0_likelihood_chain_burnin_20000_thetas_sb.png}
    \caption{\includegraphics[width=0.03\textwidth]{/home/basti/org/Figs/panda-face_emoji_bubu.png} $θ_s$ vs. $θ_b$}
    \label{fig:limit:mcmc_theta_s_b}
  \end{subfigure}%
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{~/org/Figs/statusAndProgress/limitCalculation/MCMC/mcmc_lines_long_0_likelihood_chain_burnin_20000.png}
    \caption{$g²$ vs. $θ_y$}
    \label{fig:limit:mcmc_theta_x_y}
  \end{subfigure}%
  \label{fig:limit:mcmc_example}
  \caption{\subref{fig:limit:mcmc_theta_s_b} MCMC of the $θ_s$ nuisance parameter against $θ_b$ with the
  coupling constant as the color scale. \subref{fig:limit:mcmc_theta_x_y} MCMC of the coupling constant
  against $θ_x$ and $θ_y$ in color. Both show a clear centering to values around 0, with the coupling
  constant a decrease in population towards larger couplings.
  }
\end{figure}

The resulting three Markov chains are finally used to compute the
marginal posterior likelihood function by computing the histogram of
all sampled $g$ values. The distribution of the sampled $g$ values is
that of the marginal posterior likelihood. This then allows to compute
a limit by computing the empirical distribution function of the
sampled $g$ values and extracting the value corresponding to the
$95^{\text{th}}$ percentile. An example for this is shown in
fig. [[fig:limit:mcmc_calc_limit:limit]].

#+CAPTION: Example likelihood as a function of $g_{ae}²$ for a set of toy candidates with
#+CAPTION: the limit indicated at the intersection of the blue and red colored areas.
#+CAPTION: Blue is the lower 95-th percentile of the integral over the likelihood
#+CAPTION: function and red the upper 5-th.
#+NAME: fig:limit:mcmc_calc_limit:limit
[[~/org/Figs/statusAndProgress/limitCalculation/mcmc_histo_example_limit_determination.pdf]]


*** TODO TODOs for this section [6/13] :noexport:

- [ ] *NOTE*: 5e-21 is 7e-11 for g_ae. That's lower than the old limit
  if g_aγ = 1e-12. Should we increase this to be sure we do not bias ourselves?

- [ ] *POSSIBLY* move part about calculation of likelihood and limit
  to a separate section after?  


- [X] *MENTION* the cutoff we use at background values -0.8 for
  example to avoid the singularity! Generally mention it in the
  discussion of the 4-fold posterior integral.

- [ ] *REFERENCE PAPER* about multiple chains for bias reduction!
  -> Did not find it quickly. Will have to search.
- [X] Explanation of the MCMC method *OR* reference to previous
  explanation
- [X] Mention restrictions of MCMC parameter space
- [X] Explicit number of chains, burn in and # of samples
- [X] ACCEPTANCE RATE
- [X] STEP SIZE  

  
- [ ] Example of MCMC plots
- [ ] Example of likelihood space by histogram of sampled g_ae²
- [ ] Mention some of the other performance optimizations we make?
  - [X] Caching of some values? -> Mentioned in background
    interpolation section.
  - [ ] What else do we do? No other caches. Things like custom code
    paths for distances are irrelevant.

- [ ] *WHERE do we mention numerical integration slowness*?
- [ ] WHERE do we mention sanity checks?

*** Relevant code for init & number of chains of MCMC :noexport:

#+begin_src nim
  const g_ae²Ref = 1e-21 * 1e-12^2 ## This is the reference we want to keep constant!
  # ...
      const nChains = 3
      ## Burn in of 50,000 was deemed fine even for extreme walks in L = 0 space
      const BurnIn = 50_000
      var totalChain = newSeq[seq[float]]()
      for i in 0 ..< nChains:
        let start = @[rnd.rand(0.0 .. 5.0) * couplingRef, #1e-21, # g_ae²
                      rnd.rand(-0.4 .. 0.4), rnd.rand(-0.4 .. 0.4), # θs, θb
                      rnd.rand(-0.5 .. 0.5), rnd.rand(-0.5 .. 0.5)] # θx, θy
        echo "\t\tInitial chain state: ", start
        let (chain, acceptanceRate) = rnd.build_MH_chain(start, @[3.0 * couplingRef, 0.025, 0.025, 0.05, 0.05], 150_000, fn)

#+end_src

** Expected limits of different setups
:PROPERTIES:
:CUSTOM_ID: sec:limit:expected_limits
:END:

One interesting approach to compute the expected limit usually
employed in binned likelihood approaches is the so called 'Asimov
dataset'. The idea is to compute the limit based on the dataset, which
matches exactly the expectation value for each Poisson in each
bin. This has the tremendous computational advantage of providing an
expected limit by only computing the limit for a *single* toy
candidate set. Unfortunately, for an unbinned approach this is less
straightforward, because there is no explicit mean of expectation
values anymore. On the other hand, the Asimov dataset calculation does
not provide information about the possible spread of all limits due to
the statistical variation possible in toy candidate sets. 
- [ ] *ASIMOV DATASET* and how we can / cannot use it
  -> Check note somewhere else here about using c_i = rational number
  for expected counts based on ~expCount~. Could that work? I think
  so.

In our case then, we fall back to computing an expected limit based on
toy candidate sets that we draw from a discretized, grid version of the background
interpolation, as explained in sec. [[#sec:limit:ingredients:candidates]].

We compute expected limits for different parameters, due to the signal
efficiency penalties that these imply. In a first rough 'scan' of
expected limits we compute an expected limit based on \num{1000} toy
limits for all setups we consider. Then we narrow it down and compute
\num{15000} toy limits for the best few setups. Finally, we compute
\num{30000} toys for the best setup we find. The resulting setup is
the one for which we unblind the solar tracking data. 


Tab. [[tab:limit:expected_limits]] shows the different setups with their
respective expected limits. Also shown is the limit achieved in case
no candidate is observed as a theoretical lower bound on the
limit. This 'no candidate' limit scales down with the total
efficiency, as one would expect. All limits are given as limits on
$g_{ae}·g_{aγ}$ based on a fixed $g_{aγ} = \SI{1e-12}{GeV⁻¹}$. The
table does not show all setups that were initially considered. Further
considerations of other possible parameters were excluded in
preliminary studies on their effect on the expected limit.

In particular:
- the scintillator veto is always used. It does not come with an
  efficiency penalty and therefore there is no reason not to activate
  it.
- different FADC veto efficiencies as well as disabling it completely
  were considered. The current $ε_{\text{FADC}} = 0.98$ efficiency was
  deemed optimal. Harder cuts do not yield significant improvements.
- the potential eccentricity cutoff for the line veto, as discussed in
  sec. [[#sec:background:line_veto:eccentricity_cutoff]] is fully
  disabled, as the efficiency gains do not outweigh the positive
  effect on the expected limit in practice.

Further, the table shows a few cases multiple times, with more or less
statistics (~nmc~ column). The reason is simply that it highlights
that \num{1000} toy sets is not enough to make a rigorous estimate of
the expected limit. Compare row 3 with row 5 (MLP at $\SI{84.74}{\%}$
efficiency with \num{1000} and \num{15000} toys) for an example of a
better expected limit at low statistics. In the best setup the
difference was even bigger (yielding an expected limit around
$\SI{7.46e-23}{GeV⁻¹}$), but it is left out as to not appear as the
'best case'.

Based on this study, the MLP produces the best expected limit,
specifically at a software efficiency of $\SI{91.07}{\%}$ using the
scintillator veto, FADC veto (at $ε_{\text{FADC}} = \SI{98}{\%}$) and
the line veto, but disregarding the septem veto. The software
efficiency corresponds to a target software efficiency based on the
simulated X-ray data of $\SI{95}{\%}$. This is the setup we will
mainly consider for the data unblinding.

\footnotesize
#+CAPTION: The FADC veto is always in use at an efficiency of $ε_{\text{FADC}} = 0.98$ and so is the
#+CAPTION: scintillator veto. These settings were defined in preliminary studies of the expected limits.
#+CAPTION: Note the efficiencies associated with the septem veto $ε_{\text{septem}} = 0.7841$ and the line veto
#+CAPTION: $ε_{\text{line}} = 0.8602$ and combined $ε_{\text{septem+line}} = 0.7325$, which are implicitly
#+CAPTION: included based on the 'Septem' and 'Line' column values into the total efficiency.
#+NAME: tab:limit:expected_limits
#+ATTR_LATEX: :booktabs t :environment longtable :align lrllllll
|       ε | ~nmc~ | Type | Septem | Line  | Total eff. | Limit no signal [GeV⁻¹] | Expected limit [GeV⁻¹] |
|--------+-------+------+--------+-------+------------+------------------------+-----------------------|
| 0.9107 | 30000 | MLP  | false  | true  |     0.7677 |             5.9559e-23 |            7.5824e-23 |
| 0.9718 | 15000 | MLP  | false  | true  |     0.8192 |             5.8374e-23 |            7.6252e-23 |
| 0.8474 |  1000 | MLP  | false  | true  |     0.7143 |             6.1381e-23 |             7.643e-23 |
| 0.9718 |  1000 | MLP  | false  | true  |     0.8192 |             5.8374e-23 |            7.6619e-23 |
| 0.8474 | 15000 | MLP  | false  | true  |     0.7143 |             6.1381e-23 |            7.6698e-23 |
|    0.9 |  1000 | LnL  | false  | true  |     0.7587 |             6.0434e-23 |            7.7375e-23 |
| 0.7926 | 15000 | MLP  | false  | true  |     0.6681 |             6.2843e-23 |            7.8222e-23 |
| 0.7926 |  1000 | MLP  | false  | true  |     0.6681 |             6.2843e-23 |            7.8575e-23 |
| 0.7398 |  1000 | MLP  | false  | true  |     0.6237 |             6.5704e-23 |             7.941e-23 |
| 0.7398 | 15000 | MLP  | false  | true  |     0.6237 |             6.5704e-23 |            7.9913e-23 |
|    0.8 |  1000 | LnL  | false  | true  |     0.6744 |             6.3147e-23 |            8.0226e-23 |
| 0.9718 |  1000 | MLP  | true   | true  |     0.6976 |             6.2431e-23 |            8.0646e-23 |
| 0.9107 |  1000 | MLP  | true   | true  |     0.6538 |              6.432e-23 |            8.0878e-23 |
| 0.9718 |  1000 | MLP  | true   | false |     0.7468 |             5.9835e-23 |            8.1654e-23 |
| 0.9107 |  1000 | MLP  | true   | false |     0.6998 |             6.2605e-23 |            8.2216e-23 |
| 0.8474 |  1000 | MLP  | true   | true  |     0.6083 |             6.6739e-23 |            8.2488e-23 |
|    0.9 |  1000 | LnL  | true   | true  |     0.6461 |             6.4725e-23 |            8.3284e-23 |
| 0.8474 |  1000 | MLP  | true   | false |     0.6511 |             6.4585e-23 |             8.338e-23 |
| 0.7926 |  1000 | MLP  | true   | true  |      0.569 |             6.8883e-23 |            8.3784e-23 |
| 0.7926 |  1000 | MLP  | true   | false |      0.609 |             6.6309e-23 |            8.4116e-23 |
|    0.8 |  1000 | LnL  | true   | true  |     0.5743 |             6.8431e-23 |            8.5315e-23 |
|    0.8 |  1000 | LnL  | true   | true  |     0.5743 |              6.875e-23 |            8.5437e-23 |
| 0.7398 |  1000 | MLP  | true   | true  |     0.5311 |             7.1279e-23 |            8.5511e-23 |
| 0.7398 |  1000 | MLP  | true   | false |     0.5685 |             6.9024e-23 |            8.6142e-23 |
|    0.7 |  1000 | LnL  | true   | true  |     0.5025 |             7.2853e-23 |            8.9271e-23 |
\normalsize

The distribution of all toy limits for this best setup can be seen in
fig. [[fig:limit:expected_limits:toy_limit_histogram]]. It shows both the
limit for the case without any candidates (red line) as well as the
expected limit (blue line). Depending on the number of candidates that
are inside the signal sensitive region (in regions of the solar axion
image with significant flux expectation) based on $\ln(1 + s_i/b_i) >
0.5$, the limits are split into histograms of different colors. Based
on the location of these histograms and the expected limit, the most
likely case for the real candidates seems to be 1 or 2 candidates in
that region. Note that there are some toy limits that are below the
red line for the case without candidates. This is expected, because
the calculation of each limit is based on the MCMC evaluation of the
likelihood. As such it is a statistical random process and the red
line itself is a single sample. Further, the purple histogram for "0"
candidates is *not* equivalent to the red line, because the definition
of the number of signal sensitive candidates is an arbitrary
cutoff. For the red line literally _no candidates_ at all are
considered and the limit is based purely on the
$\exp(-s_{\text{tot}})$ term of the likelihood.

#+CAPTION: Distribution of toy limits resulting in the best expected limit based on $\num{30000}$
#+CAPTION: toy limits. The expected limit -- the median -- is shown as a blue line. The red line
#+CAPTION: shows the limit for the case without any candidates. The different colored histograms correspond
#+CAPTION: to toy sets with a different number of toy candidates in the signal sensitive region, defined
#+CAPTION: by $\ln(1 + s_i/b_i) > 0.5$. The most likely number of candidates in the sensitive region
#+CAPTION: seems to be 1 or 2.
#+NAME: fig:limit:expected_limits:toy_limit_histogram
#+ATTR_LATEX: :width 0.85\textwidth
[[~/org/Figs/statusAndProgress/mc_limit_lkMCMC_skInterpBackground_nmc_30000_uncertainty_ukUncertain_σs_0.0328_σb_0.0028_posUncertain_puUncertain_σp_0.0500nmc_30k_pretty.pdf]]

*** Verification

Because of the significant complexity of the limit calculation, a
large number of sanity checks was written. They are used to verify all
internal results are consistent with expectation. They include things
like verifying the background interpolation reproduces a compatible
background rate or the individual $s_i$ terms of the likelihood
reproduce the total $s_{\text{tot}}$ term, while producing sensible numbers.

The details of this verification are left out of the main thesis, but
can be found in the extended version after this section.

- [ ] *INSERT REFERENCE TO THE SANITY CHECKS*

*** Limit sanity checks                                          :optional:

- [ ] *INSERT DISCUSSION* Of the sanity checks here!


*** TODOs for this section [/] :noexport:
- [ ] *SHOW DIFFERENT EXPECTED LIMITS*
  We want to have different expected limits given certain assumptions,
  i.e. for the case of only using the lnL cut, using lnL +
  scintillator (well, maybe that's too fine of a difference?), lnL +
  septem, lnL + septem + line, lnL + line etc.
  This way we get a better overview of which setup is actually the
  best to compute a limit.
  Note that it's important especially because the vetoes have an
  impact on tracking & background time and therefore change the
  expected signal!

- [ ] *EXPLAIN* how the statistical uncertainty is baked into the
  calculation of the expected limit due to our calculation of
  candidates that are sampled from the background model! That is after
  all, our only source of statistical uncertainties (in terms of the
  candidates that we actually observe!)

*** Example of the candidates in sensitive region                :noexport:

- [ ] Show a plot with a few more words about the candidates in
  sensitive region from a plot.

*** Expected limits with ~nmc = 1000~  :noexport:

This is our baseline analysis. It is already fixed to using the FADC
veto always and disabling the eccentricity cutoff of the line veto.

This is copy pasted directly from ~statusAndProgress~ section:

[[sec:limit:expected_limits_different_setups_test]]

See ~journal.org~ for more details around the calculation around this time!
#+begin_src sh
./generateExpectedLimitsTable \
    --path ~/org/resources/lhood_lnL_04_07_23/limits/ \
    --prefix mc_limit_lkMCMC_skInterpBackground_nmc_1000 \
    --path ~/org/resources/lhood_MLP_06_07_23/limits/ \
    --prefix mc_limit_lkMCMC_skInterpBackground_nmc_1000_uncertainty
#+end_src
#+RESULTS:

*** Expected limits with more statistics :noexport:

Again, straight from the ~statusAndProgress~ section about it.


For the best case:
#+begin_src sh
./generateExpectedLimitsTable --path ~/org/resources/lhood_MLP_06_07_23/limits/ --prefix mc_limit_lkMCMC_skInterpBackground_nmc_30000_
#+end_src
|      ε | Type | Scinti | FADC | ε_FADC | Septem | Line | eccLineCut | ε_Septem | ε_Line | ε_SeptemLine | Total eff. | Limit no signal [GeV⁻¹] | Expected limit [GeV⁻¹] | Exp. limit variance [GeV⁻¹] | Exp. limit σ [GeV⁻¹] |
|--------+------+--------+------+--------+--------+------+------------+----------+--------+--------------+------------+-------------------------+------------------------+-----------------------------+----------------------|
| 0.9107 | MLP  | true   | true |   0.98 | false  | true |          1 |   0.7841 | 0.8602 |       0.7325 |     0.7677 |              5.9559e-23 |             7.5824e-23 |                  6.0632e-51 |           7.7866e-26 |


For the next worse cases:
#+begin_src sh
./generateExpectedLimitsTable \
    --path ~/org/resources/lhood_MLP_06_07_23/limits/ \
    --prefix mc_limit_lkMCMC_skInterpBackground_nmc_15000_
#+end_src
|      ε | Type | Scinti | FADC | ε_FADC | Septem | Line | eccLineCut | ε_Septem | ε_Line | ε_SeptemLine | Total eff. | Limit no signal [GeV⁻¹] | Expected limit [GeV⁻¹] | Exp. limit variance [GeV⁻¹] | Exp. limit σ [GeV⁻¹] |
|--------+------+--------+------+--------+--------+------+------------+----------+--------+--------------+------------+-------------------------+------------------------+-----------------------------+----------------------|
| 0.9718 | MLP  | true   | true |   0.98 | false  | true |          1 |   0.7841 | 0.8602 |       0.7325 |     0.8192 |              5.8374e-23 |             7.6252e-23 |                  1.6405e-50 |           1.2808e-25 |
| 0.8474 | MLP  | true   | true |   0.98 | false  | true |          1 |   0.7841 | 0.8602 |       0.7325 |     0.7143 |              6.1381e-23 |             7.6698e-23 |                  1.4081e-50 |           1.1866e-25 |
| 0.7926 | MLP  | true   | true |   0.98 | false  | true |          1 |   0.7841 | 0.8602 |       0.7325 |     0.6681 |              6.2843e-23 |             7.8222e-23 |                  1.3589e-50 |           1.1657e-25 |
| 0.7398 | MLP  | true   | true |   0.98 | false  | true |          1 |   0.7841 | 0.8602 |       0.7325 |     0.6237 |              6.5704e-23 |             7.9913e-23 |                  1.6073e-50 |           1.2678e-25 |



*** Older expected limit table with different FADC percentiles and eccentricity line cutoffs :noexport:

This is copy pasted directly from ~statusAndProgress~ section:

[[sec:limit:expected_limits_different_setups_test]]

Ideally we want to rerun this! Different ε line cutoff vetoes and FADC vetoes!



<2023-03-20 Mon 12:44>
Expected limits from
[[file:~/org/resources/lhood_limits_automation_with_nn_support/]]
which should be more or less correct now (but they lack the
eccentricity line veto cut value, so it's 0 in all columns!

#+begin_src sh
cd $TPA/Tools/generateExpectedLimitsTable
./generateExpectedLimitsTable --path ~/org/resources/lhood_limits_automation_with_nn_support/limits
#+end_src

*NOTE*: These have different rows for different ε line veto cutoffs,
but the table does not highlight that fact! 0.8602 corresponds to ε =
1.0, i.e. disable the cutoff.


| ε_lnL | Scinti | FADC  | ε_FADC | Septem | Line  | eccLineCut | ε_Septem | ε_Line | ε_SeptemLine | Total eff. | Limit no signal | Expected Limit |
|-------+--------+-------+--------+--------+-------+------------+----------+--------+--------------+------------+-----------------+----------------|
|   0.9 | true   | true  |   0.98 | false  | true  |          0 |   0.7841 | 0.8602 |       0.7325 |     0.7587 |      3.7853e-21 |     7.9443e-23 |
|   0.9 | true   | false |   0.98 | false  | true  |          0 |   0.7841 | 0.8602 |       0.7325 |     0.7742 |      3.6886e-21 |     8.0335e-23 |
|   0.9 | true   | true  |   0.98 | false  | true  |          0 |   0.7841 | 0.8794 |       0.7415 |     0.7757 |      3.6079e-21 |     8.1694e-23 |
|   0.8 | true   | true  |   0.98 | false  | true  |          0 |   0.7841 | 0.8602 |       0.7325 |     0.6744 |      4.0556e-21 |     8.1916e-23 |
|   0.8 | true   | true  |   0.98 | false  | true  |          0 |   0.7841 | 0.8602 |       0.7325 |     0.6744 |      4.0556e-21 |     8.1916e-23 |
|   0.9 | true   | true  |   0.98 | false  | true  |          0 |   0.7841 | 0.8946 |       0.7482 |     0.7891 |      3.5829e-21 |     8.3198e-23 |
|   0.8 | true   | true  |   0.98 | false  | true  |          0 |   0.7841 | 0.8794 |       0.7415 |     0.6895 |      3.9764e-21 |     8.3545e-23 |
|   0.8 | true   | true  |    0.9 | false  | true  |          0 |   0.7841 | 0.8602 |       0.7325 |     0.6193 |      4.4551e-21 |     8.4936e-23 |
|   0.9 | true   | true  |   0.98 | false  | true  |          0 |   0.7841 | 0.9076 |        0.754 |     0.8005 |      3.6208e-21 |     8.5169e-23 |
|   0.8 | true   | true  |   0.98 | false  | true  |          0 |   0.7841 | 0.8946 |       0.7482 |     0.7014 |      3.9491e-21 |     8.6022e-23 |
|   0.8 | true   | true  |   0.98 | false  | true  |          0 |   0.7841 | 0.9076 |        0.754 |     0.7115 |      3.9686e-21 |     8.6462e-23 |
|   0.9 | true   | false |   0.98 | true   | true  |          0 |   0.7841 | 0.8602 |       0.7325 |     0.6593 |      4.2012e-21 |     8.6684e-23 |
|   0.7 | true   | true  |   0.98 | false  | true  |          0 |   0.7841 | 0.8602 |       0.7325 |     0.5901 |      4.7365e-21 |       8.67e-23 |
|   0.9 | true   | true  |   0.98 | true   | true  |          0 |   0.7841 | 0.8602 |       0.7325 |     0.6461 |      4.3995e-21 |     8.6766e-23 |
|   0.7 | true   | false |   0.98 | false  | true  |          0 |   0.7841 | 0.8602 |       0.7325 |     0.6021 |      4.7491e-21 |     8.7482e-23 |
|   0.8 | true   | true  |   0.98 | true   | true  |          0 |   0.7841 | 0.8602 |       0.7325 |     0.5743 |      4.9249e-21 |     8.7699e-23 |
|   0.8 | true   | true  |   0.98 | false  | false |          0 |   0.7841 | 0.8602 |       0.7325 |      0.784 |      3.6101e-21 |     8.8059e-23 |
|   0.8 | true   | true  |    0.8 | false  | true  |          0 |   0.7841 | 0.8602 |       0.7325 |     0.5505 |      5.1433e-21 |      8.855e-23 |
|   0.7 | true   | true  |   0.98 | false  | true  |          0 |   0.7841 | 0.8794 |       0.7415 |     0.6033 |      4.4939e-21 |     8.8649e-23 |
|   0.8 | true   | true  |   0.98 | true   | false |          0 |   0.7841 | 0.8602 |       0.7325 |     0.6147 |      4.5808e-21 |     8.8894e-23 |
|   0.9 | true   | false |   0.98 | true   | false |          0 |   0.7841 | 0.8602 |       0.7325 |     0.7057 |      3.9383e-21 |     8.9504e-23 |
|   0.7 | true   | true  |   0.98 | false  | true  |          0 |   0.7841 | 0.8946 |       0.7482 |     0.6137 |      4.5694e-21 |     8.9715e-23 |
|   0.8 | true   | true  |    0.9 | true   | true  |          0 |   0.7841 | 0.8602 |       0.7325 |     0.5274 |      5.3406e-21 |     8.9906e-23 |
|   0.9 | true   | true  |    0.9 | true   | true  |          0 |   0.7841 | 0.8602 |       0.7325 |     0.5933 |       4.854e-21 |          9e-23 |
|   0.8 | false  | false |   0.98 | false  | false |          0 |   0.7841 | 0.8602 |       0.7325 |        0.8 |      3.5128e-21 |     9.0456e-23 |
|   0.8 | true   | false |   0.98 | false  | false |          0 |   0.7841 | 0.8602 |       0.7325 |        0.8 |      3.5573e-21 |     9.0594e-23 |
|   0.7 | true   | true  |   0.98 | false  | true  |          0 |   0.7841 | 0.9076 |        0.754 |     0.6226 |      4.5968e-21 |     9.0843e-23 |
|   0.7 | true   | true  |   0.98 | true   | true  |          0 |   0.7841 | 0.8602 |       0.7325 |     0.5025 |       5.627e-21 |     9.1029e-23 |
|   0.8 | true   | true  |    0.9 | false  | false |          0 |   0.7841 | 0.8602 |       0.7325 |       0.72 |      3.8694e-21 |     9.1117e-23 |
|   0.8 | true   | true  |    0.9 | true   | false |          0 |   0.7841 | 0.8602 |       0.7325 |     0.5646 |       4.909e-21 |     9.2119e-23 |
|   0.7 | true   | false |   0.98 | true   | true  |          0 |   0.7841 | 0.8602 |       0.7325 |     0.5128 |      5.5669e-21 |     9.3016e-23 |
|   0.7 | true   | false |   0.98 | true   | false |          0 |   0.7841 | 0.8602 |       0.7325 |     0.5489 |      5.3018e-21 |     9.3255e-23 |
|   0.7 | true   | true  |    0.9 | true   | true  |          0 |   0.7841 | 0.8602 |       0.7325 |     0.4615 |      6.1471e-21 |     9.4509e-23 |
|   0.8 | true   | true  |    0.8 | false  | false |          0 |   0.7841 | 0.8602 |       0.7325 |       0.64 |      4.5472e-21 |     9.5113e-23 |
|   0.8 | true   | true  |    0.8 | true   | true  |          0 |   0.7841 | 0.8602 |       0.7325 |     0.4688 |      5.8579e-21 |     9.5468e-23 |
|   0.8 | true   | true  |    0.8 | true   | false |          0 |   0.7841 | 0.8602 |       0.7325 |     0.5018 |      5.6441e-21 |     9.5653e-23 |





** Solar tracking candidates
:PROPERTIES:
:CUSTOM_ID: sec:limit:candidates
:END:

Based on the best performing setup we can now look at the solar
tracking candidates. [fn:data_unblinding] In this setup, based on the
background model a total of \num{1449} candidates are expected over
the entire chip. [fn:expected_clusters] Computing the real candidates
yields a total of \num{1610} clusters over the chip, a somewhat
meaningful excess. However, this excess is mostly from very low energy
contributions below \SI{1}{keV} and potentially a sign of additional
activity from noise related phenomena due to the tracking motor
activity (which caused the significant FADC noise
activity). Restricting the clusters to $\SIrange{1}{12}{keV}$ yields
\num{219} cluster candidates where \num{210} would be expected
(\num{4145} clusters in background). A figure comparing the rate
observed for the candidates compared to the background is shown in
fig. \ref{fig:limit:rate_candidates_background}.

Fig. [[fig:limit:candidates]] shows all solar tracking candidates that are
observed with the method yielding the best expected limit. Their
energy is color coded and written above each cluster within the center
$\SI{85}{pixel}$ radius of the chip center. The axion image is
underlaid to provide a visual reference of the importance of each
cluster. Very few candidates of relevant energies are seen within the
region of interest. Based on the previously mentioned $\ln(1 +
s_i/b_i) > 0.5$ condition it is in fact 0 candidates in the sensitive
region. See fig. \ref{fig:limit:candidates_s_b} for an overview of the
weighting of each candidate in this way.

The candidates that are obtained for other setups of those shown in
tab. [[tab:limit:expected_limits]], see appendix
[[sec:appendix:solar_candidates]] and the full version of the thesis for
even more detail.

#+CAPTION: Overview of all solar tracking candidates given the MLP@\SI{91}{\%} setup
#+CAPTION: including all vetoes except the septem veto. \num{1610} clusters are observed,
#+CAPTION: with the majority low energy clusters in the chip corners. The axion image
#+CAPTION: is underlaid the area and the energy of each cluster is color coded. For all
#+CAPTION: candidates within a radius of \SI{85}{pixel} of the center the energy is also
#+CAPTION: written above. We can see by eye that very few candidates of relevant energies
#+CAPTION: are present in the region of expected signal.
#+NAME: fig:limit:candidates
[[~/org/Figs/statusAndProgress/trackingCandidates/background_cluster_centersmlp_0.95_scinti_fadc_line_tracking_candidates_axion_image_with_energy_radius_85.pdf]]


\begin{figure}[htbp]
  \centering
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{~/org/Figs/statusAndProgress/trackingCandidates/real_candidates_signal_over_background.pdf}
    \caption{$\ln(1 + s_i/b_i)$}
    \label{fig:limit:candidates_s_b}
  \end{subfigure}%
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{~/org/Figs/statusAndProgress/trackingCandidates/rate_real_candidates_vs_background_rate_crAll_mlp_0.95_scinti_fadc_line.pdf}
    \caption{Rate candidates vs. background}
    \label{fig:limit:rate_candidates_background}
  \end{subfigure}%
  \label{fig:limit:candidates_s_b_rates}
  \caption{\subref{fig:limit:candidates_s_b} Weighting of each candidate based on $\ln(1 + s_i/b_i)$.
  The largest weight given is for the \SI{5.4}{keV} cluster seen in fig. \ref{fig:limit:candidates}
  near x pixel 100 and tops out at slighlty above $\num{0.2}$. 
  \subref{fig:limit:rate_candidates_background} Rate of background and candidate clusters over the entire
  chip as a log plot. Candidate excess mainly below \SI{1}{keV}.
  }
\end{figure}


[fn:expected_clusters] The background model contains \num{28627}
clusters in this case. \SI{3156.8}{h} of background data and
\SI{159.8}{h} of tracking data yields a
$\num{28627}·\frac{3156.8}{159.8} = 1449.12$ clusters.

[fn:data_unblinding] The actual data unblinding of the candidates
presented in this section was only done after the analysis of the
previous sections was fully complete. A presentation with discussion
took place first inside our own group and later with the relevant
members of the CAST collaboration to ascertain that our analysis
appears sound.

*** TODO for this section [/]                                    :noexport:

- [X] Create a time series plot of all tracking candidates to verify
  that indeed their timestamp indicates that they were part of
  trackings. Or better yet export the timestamps as text strings to a
  CSV file?
  We should verify these things are correct, in particular given thqe
  mismatch of expected number of candidates and real ones.
  Or is it due to energy filtering?
  -> See: [[file:~/org/resources/candidate_cluster_dates.csv]]

- [ ] *CREATE APPENDIX FOR OTHER CANDIDATE PLTOS!!!*  

*** Rate plot comparing background to candidates                 :noexport:

Combined:
#+begin_src sh
plotBackgroundRate \
    /home/basti/org/resources/lhood_limits_10_05_23_mlp_sEff_0.99/lhood_c18_R2_crAll_sEff_0.95_scinti_fadc_line_mlp_mlp_tanh300_msecheckpoint_epoch_485000_loss_0.0055_acc_0.9933_vQ_0.99.h5 \
    /home/basti/org/resources/lhood_limits_10_05_23_mlp_sEff_0.99/lhood_c18_R3_crAll_sEff_0.95_scinti_fadc_line_mlp_mlp_tanh300_msecheckpoint_epoch_485000_loss_0.0055_acc_0.9933_vQ_0.99.h5 \
    /home/basti/Sync/lhood_tracking_scinti_line_mlp_0.95_2017.h5 \
    /home/basti/Sync/lhood_tracking_scinti_line_mlp_0.95_2018.h5 \
    --centerChip 3 \
    --names "Background" --names "Background" --names "Candidates" --names "Candidates" \
    --title "Rate over whole chip, MLP@91 % + line veto," \
    --showNumClusters \
    --region crAll \
    --showTotalTime \
    --topMargin 1.5 \
    --energyDset energyFromCharge \
    --energyMin 0.2 \
    --outfile rate_real_candidates_vs_background_rate_crAll_mlp_0.95_scinti_fadc_line.pdf \
    --outpath ~/org/Figs/statusAndProgress/trackingCandidates/ \
    --logPlot \
    --hideErrors \
    --quiet
#+end_src

[[~/org/Figs/statusAndProgress/trackingCandidates/rate_real_candidates_vs_background_rate_crAll_mlp_0.95_scinti_fadc_line.pdf]]

Below 1 keV the candidates are _always_ in excess. At higher energies
maybe there is still a slight excess, yes, but it *seems* (may not be
though) to be more in line with expectation. 


*** Perform the data unblinding and produce plots                :noexport:

1. run ~likelihood~ with ~--tracking~
2. compute ~mcmc_limit_calculation~ based on the files
  

*** Generate plots for real candidates                           :noexport:

Using the files created in the previous section, let's create some plots.

(from ~journal.org~)

#+begin_src sh
plotBackgroundClusters \
    /home/basti/Sync/lhood_tracking_scinti_line_mlp_0.95_2017.h5 \
    /home/basti/Sync/lhood_tracking_scinti_line_mlp_0.95_2018.h5 \
    --title "MLP@95+FADC+Scinti+Line tracking clusters" \
    --outpath ~/org/Figs/statusAndProgress/trackingCandidates/ \
    --suffix "mlp_0.95_scinti_fadc_line_tracking_candidates_axion_image_with_energy_radius_85" \
    --energyMin 0.2 --energyMax 12.0 \
    --filterNoisyPixels \
    --axionImage /home/basti/org/resources/axion_images/axion_image_2018_1487_93_0.989AU.csv \
    --energyText \
    --colorBy energy \
    --energyTextRadius 85.0
#+end_src

[[~/org/Figs/statusAndProgress/trackingCandidates/background_cluster_centersmlp_0.95_scinti_fadc_line_tracking_candidates_axion_image_with_energy_radius_85.pdf]]


*** Number of background clusters expected                       :noexport:

This is part of the ~sanityCheckBackgroundSampling~ procedure in the
limit code.

Running it on the MLP@95 + line veto files yields:
#+begin_src sh
mcmc_limit_calculation sanity --limitKind lkMCMC
#+end_src

- [ ] *FURTHER INVESTIGATE* the missing time!

** Observed limits

With the data fully unblinded and the solar tracking candidates known,
we can now compute the observed limits.

We will start with the main coupling constant of interest in the
context of this thesis, the axion-electron coupling $g_{ae}$ in
sec. [[#sec:limit:observed_axion_electron]].

Afterwards, we will also shortly present the limits achievable by this
detector, dataset and methodology on the axion-photon coupling
$g_{aγ}$ as well as the Chameleon coupling constant $β$.

*** Axion-electron coupling
:PROPERTIES:
:CUSTOM_ID: sec:limit:observed_axion_electron
:END:

Based on the candidates presented in sec. [[#sec:limit:candidates]] we
compute an observed limit of

\[
g_{ae} · g_{aγ} = \SI{6.56e-23}{GeV⁻¹},
\]

which is approaching the best possible limit achievable given this
setup, due to the distribution of candidates and absence of any highly
significant candidates. The expected limit for this case was
$\left(g_{ae} · g_{aγ}\right)_{\text{exp}} = \SI{7.56e-23}{GeV⁻¹}$ and the limit
without any candidates at all $\left(g_{ae} · g_{aγ}\right)_{\text{no
candidates}} = \SI{5.96e-23}{GeV⁻¹}$.

This is in contrast to the current observed best limit by CAST in 2013
cite:Barth_2013, which achieved

\[
\left(g_{ae} · g_{aγ}\right)_{\text{CAST2013}} = \SI{8.1e-23}{GeV⁻¹}.
\]

Fig. [[fig:limit:observed_axion_electron]] shows the marginal posterior
likelihood function for the observed solar tracking candidates. The
limit is at the $95^{\text{th}}$ percentile of the histogram, shown by
the intersection of the blue and red filling. In addition the yellow
line shows values based on a numerical integration using Romberg's
method *CITE ME* at 10 different coupling constants. This is a cross
validation of the MCMC result. [fn:romberg_integration_performance] 

- [ ] *REPLACE THIS FIG BY VERSION WITH g_ae·g_aγ!!!*
- [ ] *ALSO PRODUCE A χ² PLOT OF THE SPACE!*  
#+CAPTION: The marginal posterior likelihood function for the solar candidates
#+CAPTION: with the observed limit at the intersection between the blue and
#+CAPTION: red filling. Also shown is a line based on a numerical integration of
#+CAPTION: the 4-fold integral at 10 steps as a cross check of the MCMC.
#+NAME: fig:limit:observed_axion_electron
[[~/org/Figs/statusAndProgress/trackingCandidates/mcmc_real_limit_likelihood_g_ae2_with_numerical_int.pdf]] 

[fn:romberg_integration_performance] Note though, while the
calculation of the observed limit via the MCMC takes about
$\SI{10}{s}$, the numerical integration using Romberg's method took
$\sim\SI{1.5}{h}$ for only 10 points. And that is only using a
integration level of 5 (one often uses 8 for Romberg for
accuracy). This highlights the need for Monte Carlo methods,
_especially_ for expected limits.

*** TODOs for this section [/] 
For limit:  
- [ ] *likelihood space* of the real candidates as plot
- [ ] try to fit an exponential to the likelihood and provide
  that. Should give a better way to combine it maybe?
- [ ] ?  

- [ ] *CITE ROMBERG*  

- [ ] *PROVIDE AT LEAST A TABLE* of the limits we get for a few other setups!

*** Axion-photon coupling

- [ ] Show differential solar flux
- [ ] Show axion image for Primakoff origin

- [ ] How is the axion emission modeled for the axion-photon flux?
  -> Figure out, just need that!
  
*** Chameleon coupling

- [ ] For chameleon mainly just need to look at how Christoph sampled
  from the solar radius to compute the axion image in his case. We can
  then plug that into the raytracer.

** Comparison to 2013 limit (using their method)

** Calculate expected limits                                      :noexport:

- [ ] *MOVE THIS* to appropriate section above.

Running the ~runLimits~ code with ~nmc = 100~ took

#+begin_src sh
Computing all limits took 2297.087612867355 s
#+end_src

so a bit more than half an hour. This implies in a single day we can
get about 4800 MC samples essentially.




* Outlook                                                             :Part5:
:PROPERTIES:
:CUSTOM_ID: sec:outlook
:END:

- [ ] *OUTLOOK* in terms of *DETECTOR*
- [ ] *OUTLOOK* in terms of *ANALYSIS*
  -> Combine this thesis data with other searches!  

Timepix3 based detector will be big improvement, as long readout times
without time information are probably the biggest issue (and partially
biggest mistake) in the data taking campaign. Further, not requiring
any analog readout parts like an FADC is a big help as handling EMI
properly is complex.

- [ ] *REWRITE ME*
  -> then refer to Tobi thesis 
In a test background run using a GridPix3, which was pointed towards
the zenith, 50 clusters remained after the likelihood cut. Using the
duration of the clusters, another 15 could be removed, showing the
usefulness of the approach. For more information on this see the upcoming
cite:schiffer_phd. 

*TODO: get correct numbers!*

Radiopure materials will also help to further reduce the background
that is still present in the detector of this thesis.

For a BabyIAXO detector the 7 GridPix layout is a must due to the
power of the septem-veto. 

Finally, a new setup should make sure to at the very least have
multiple temperature sensors, but more importantly likely use some
kind of temperature control to avoid strong gas gain variations.

One positive aspect of a new Timepix3 based detector is that the full
analysis procedure explained in the context of this thesis is already
written with Timepix3 based detectors in mind. While some additions
will likely be desired, the basis is already there and given a dataset
for a Timepix3 based detector a limit calculation could be done in a
matter of days.

This is also one additional reason why I felt it important to provide
a thesis that is fully reproducible in order to make it easier for
other people to actually use it as such.

And hey, maybe soon we can compute a limit on the Axio-Chameleon cite:brax2023axiochameleons?


* Summary & conclusion                                                :Part5:
:PROPERTIES:
:CUSTOM_ID: sec:summary
:END:

#+BIBLIOGRAPHY: references

# Use biblatex for the bibliography
# Add bibliography to Table of Contents
# Comment out this command if your references are printed for each chapter.


#+LATEX: \printbibliography[heading=bibintoc]
#+LATEX: \appendix
# * Appendix                                                         :Appendix:
# #+LATEX: \minitoc

# TODO: need backend specific sections here I think

* Bibliography                                                         :html:

<<bibliographystyle link>>
bibliographystyle:unsrtnat

<<bibliography link>>
bibliography:references.bib



* Configurations                                                   :Appendix:
:PROPERTIES:
:CUSTOM_ID: sec:appendix:configuration
:END:

- [ ] This can be seen from a slightly wider lens. I.e. not only
  strictly configuration files, but also things like the used TOF
  versions?

** TOS configuration file [/]
:PROPERTIES:
:CUSTOM_ID: sec:appendix:configuration:tos_config
:END:

This is the configuration file as it was used at CAST during the data
taking periods.

- [ ] *ALSO LINK AN ONLINE VERSION?*

#+begin_src toml
[General]
sAddress_fadc = 1
baseAddress_hv = 0x4000

[HvModule]
setKillEnable                   = true
# Voltage and Current RampSped currently set to arbitrary value
# in percent / second
moduleVoltageRampSpeed          = 0.1
moduleCurrentRampSpeed          = 50
# checkModuleTimeInterval       = 60, checks the status of the
# module every 60 seconds during a Run, between two events
checkModuleTimeInterval         = 60

# if this flag is set to true, anode and grid
# will be coupled to one group
[HvGroups]
anodeGridGroupFlag              = true
# grid is master channel of set on group
anodeGridGroupMasterChannel     = 5
anodeGridGroupNumber            = 0
monitorTripGroupFlag            = true
monitorTripGroupNumber          = 1
rampingGroupFlag                = true
rampingGroupNumber              = 2                          
gridChannelNumber               = 5
anodeChannelNumber              = 6
cathodeChannelNumber            = 9

[HvChannels]
# grid, anode and cathode settings
# all currents given in A (vmecontrol shows mA)
0_Name                          = grid
0_Number                        = 5
0_VoltageSet                    = 300
0_VoltageNominal                = 500
0_VoltageBound                  = 10
0_CurrentSet                    = 0.000050
0_CurrentNominal                = 0.000500 
0_CurrentBound                  = 0
                                
1_Name                          = anode
1_Number                        = 6
1_VoltageSet                    = 375
1_VoltageNominal                = 500
1_VoltageBound                  = 10
1_CurrentSet                    = 0.000050
1_CurrentNominal                = 0.000500
1_CurrentBound                  = 0
                                
2_Name                          = cathode
2_Number                        = 9
2_VoltageSet                    = 1875
2_VoltageNominal                = 2500
2_VoltageBound                  = 15
2_CurrentSet                    = 0.000050
2_CurrentNominal                = 0.000500
2_CurrentBound                  = 0
                                
3_Name                          = Ring1
3_Number                        = 7
3_VoltageSet                    = 415
3_VoltageNominal                = 500
3_VoltageBound                  = 15
3_CurrentSet                    = 0.000100
3_CurrentNominal                = 0.000500
3_CurrentBound                  = 0
                                
4_Name                          = Ring29
4_Number                        = 8
4_VoltageSet                    = 1830
4_VoltageNominal                = 2500
4_VoltageBound                  = 15
4_CurrentSet                    = 0.000100
4_CurrentNominal                = 0.000500
4_CurrentBound                  = 0
                                
6_Name                          = sipm
6_Number                        = 4
6_VoltageSet                    = 65.6
6_VoltageNominal                = 100
6_VoltageBound                  = 5
6_CurrentSet                    = 0.0005
6_CurrentNominal                = 0.0005
6_CurrentBound                  = 0
                                
# The veto paddle scintillator is commented out, as it was supplied
# with HV by an external CAEN HV power supply.
# 5_Name                        = szintillator
# 5_Number                      = 11
# #5_VoltageSet                 = 1300
# 5_VoltageSet                  = 0
# 5_VoltageNominal              = 2500
# 5_VoltageBound                = 5
# 5_CurrentSet                  = 0.002
# 5_CurrentNominal              = 0.002
# 5_CurrentBound                = 0

[Fadc] # FADC Settings
fadcTriggerType                 = 3 
fadcFrequency                   = 2
fadcPosttrig                    = 80
fadcPretrig                     = 15000
# was 2033 before, 1966 corresponds to -40 mV
fadcTriggerThresholdRegisterAll = 1966 
# run time of a single pedestal run for the FADC in ms
fadcPedestalRunTime             = 100
# number of acquisition runs done for each pedestal calibration
fadcPedestalNumRuns             = 10
# using channel 0 on FADC as trigger source, thus bit 0  1!
fadcChannelSource               = 1
# set FADC mode register (mainly to enable 14-bit readout)
fadcModeRegister                = 0b000

[Temperature] # temperature related parameters
safeUpperTempIMB                = 61
safeUpperTempSeptem             = 61
safeLowerTempIMB                = 0
safeLowerTempSeptem             = 0
#+end_src


** TOF versions used at CAST [/]
:PROPERTIES:
:CUSTOM_ID: sec:appendix:configuration:tof_versions
:END:

- [ ] *WHICH VERSIONS WERE USED*
- [ ] *WHERE ARE THE FILES*. Office computer?

* Calibrations                                                     :Appendix:
:PROPERTIES:
:CUSTOM_ID: sec:appendix:calibration
:END:

** Septemboard calibration [/]
:PROPERTIES:
:CUSTOM_ID: sec:appendix:septemboard_calibrations
:END:

- [ ] Show all calibrations for each of the runs.

That means, for all chips:
- [ ] THS optimization & equalization bits
- [ ] equalized matrix of all pixels  
- [ ] ~ToT~ calibration
- [ ] S-curves
- [ ] fit to 50% point of S-curves to get #electrons / THL DAC value    

** Calibration measurements of the veto scintillator paddle
:PROPERTIES:
:CUSTOM_ID: sec:appendix:scintillator_calibration_notes
:END:

The following is a set of notes taken when calibrating the
scintillator paddle in the laboratory of the RD51 group at CERN. It is
reproduced here for transparency and completeness.

*** Scintillator paddle calibrations [0/2]

This document contains the data for the calibration of the MM veto
scintillator. It is a 'report' created while data taking and thus may
contain conflicting information. Not to be understood as a simple
reference protocol.

The scintillator has a Canberra 2007 base, which accepts positive
HV. The PMT is a Bicron Corp. 31.49x15.74M2BC408/2-X, where the first
two numbers are the scintillators dimensions in inch.

=For calibration we're using $\SI{1400}{\volt}$, while Juanan
mentioned in his mail to use $\SI{1200}{\volt}$ during data taking.=

For calibration we're using an Ortec 9302 amplifier after the PMT with
a gain of 20. This is fed into an LRS 621CL discriminator. The PMT and
base are used at a HV of $+\SI{1200}{\volt}$.

Scintillator is of size $\SI{42}{\cm}$ times $\SI{82}{\cm}$. Which is
an area of
#+BEGIN_SRC nim
let x = 0.42
let y = 0.82
echo x * y 
#+END_SRC

#+RESULTS:
: 0.3444

- [ ] *TODO: CROSS CHECK THESE NUMBERS HERE*

At a cosmic muon rate of $\sim\SI{100}{\hertz \per \meter \squared
\steradian}$, the expected signal rate of munons is thus $\sim
\SI{33}{\hertz}$.

#+BEGIN_SRC nim
let area = 0.34
let total_muons = 60000.0
echo area * total_muons
#+END_SRC

#+RESULTS:
: 20400.0

- [ ] *UPDATE*: Muon rate about $\SI{1}{cm^{-2}.min^{-1}} \approx \SI{166.67}{m^{-2}.s^{-1}}$

**** Calibration
:PROPERTIES:
:ORDERED:  t
:END:

Threshold values are scaled by a factor of 10. Coincidence using
Theodoros 2 scintillator paddles in RD51 lab. 
- upper scinti: $\SI{-2070}{\volt}$
- lower scinti: $\SI{-2050}{\volt}$

Measurement time for each value: $\SI{10}{\minute}$

Note: The reason the coincidences are much lower than the single
scintillator counts is of course due to the much smaller coincidence
area of the small scintillators used for the measurement.

| Threshold / mV | Counts Szinti | Counts Coincidence |
|----------------+---------------+--------------------|
|         -301.9 |         24062 |                760 |
|           -399 |         13332 |                496 |
|           -498 |          6584 |                300 |
|           -603 |          3363 |                167 |
|           -699 |          1900 |                104 |
|           -802 |          1087 |                 83 |
|           -901 |           651 |                 54 |
|          -1005 |           523 |                 50 |
|          -1104 |           361 |                 32 |
|          -1203 |           231 |                 32 |
|          -1305 |           189 |                 38 |
|          -1400 |           151 |                 23 |
|          -1502 |            96 |                 14 |
|          -1602 |            78 |                 15 |
|          -1703 |            72 |                 10 |
|          -1802 |            58 |                 11 |
|                |               |                    |

Second set of measurements around interesting point of
$\SI{1000}{\milli\volt}$

| Threshold / mV | Counts Szinti | Counts Coincidence |
|----------------+---------------+--------------------|
|          -1200 |           259 |                 35 |
|          -1100 |           350 |                 34 |
|          -1000 |           456 |                 48 |
|           -900 |           774 |                 42 |

A third measurement using an amplifier after the PMT, since the output
signal of the PMT is so small (see mail of JuanAn). Now the HV was
lowered to $\SI{1200}{\volt}$ again, since it is not necessary.

#+NAME: tab-test
| Threshold / mV | Counts Szinti | Counts Coincidence |
|----------------+---------------+--------------------|
|           -598 |         31221 |                634 |
|           -700 |         30132 |                674 |
|           -804 |         28893 |                635 |
|           -903 |         28076 |                644 |
|          -1005 |         27012 |                684 |
|          -1103 |         25259 |                566 |
|          -1200 |         22483 |                495 |
|          -1303 |         19314 |                437 |
|          -1403 |         16392 |                356 |
|          -1505 |         13677 |                312 |
|          -1600 |         11866 |                267 |
|          -1701 |         10008 |                243 |
|                |               |                    |

|           -900 |         28263 |                892 |
|          -1000 |         26789 |                991 |


#+begin_src nim :var tbl=tab-test 
import ggplotnim, sequtils
proc parse(s: openArray[string]): seq[float] = s.filterIt(it.len > 0).mapIt(it.parseFloat)
let df = toDf({ "Thr" : tbl["Threshold / mV"].parse,
                "Szinti" : tbl["Counts Szinti"].parse,
                "Coinc" : tbl["Counts Coincidence"].parse })
ggplot(df, aes("Thr", "Szinti")) +
  geom_point() + geom_line() +
  ggsave("/t/test.pdf")
#+end_src

Export this table using org-table-export to
[[file:data/veto_szinti_counts.txt]]. Then remove unnecessary last line
and add # to beginning of first line.

- [ ] *REWRITE TO USE AN INLINE GGPLOTNIM PLOTTING AND SHOW DATA*

Then use [[file:PyS_mm_veto_szinti_calib.py]] to plot the data.

A threshold of $\SI{-110}{\milli\volt}$ was selected, after analysis.


* CAST data taking run list                                        :Appendix:
:PROPERTIES:
:CUSTOM_ID: sec:appendix:cast_run_list
:END:


\scriptsize
#+CAPTION: List of all runs recorded with the Septemboard detector during Run-2 and Run-3 at CAST.
#+CAPTION: The run type is listed as ~b~: background with possible tracking and ~c~: calibration with
#+CAPTION: the \cefe source.
#+NAME: tab:appendix:cast_run_list
#+ATTR_LATEX: :environment longtable :width \textwidth :spread
| Run # | Type | Start                  | End                    | Length        | # trackings | # frames | # FADC |
|-------+------+------------------------+------------------------+---------------+-------------+----------+--------|
| Run-2 |      |                        |                        |               |             |          |        |
|-------+------+------------------------+------------------------+---------------+-------------+----------+--------|
|    76 | b    | <2017-10-30 Mon 18:39> | <2017-11-02 Thu 5:24>  | 2 days 10:44  |           1 |    88249 |  19856 |
|    77 | b    | <2017-11-02 Thu 5:24>  | <2017-11-03 Fri 5:28>  | 1 days 00:03  |           1 |    36074 |   8016 |
|    78 | b    | <2017-11-03 Fri 5:28>  | <2017-11-03 Fri 20:45> | 0 days 15:17  |           1 |    23506 |   5988 |
|    79 | b    | <2017-11-03 Fri 20:46> | <2017-11-05 Sun 0:09>  | 1 days 03:22  |           1 |    40634 |   8102 |
|    80 | b    | <2017-11-05 Sun 0:09>  | <2017-11-05 Sun 23:50> | 0 days 23:40  |           1 |    35147 |   6880 |
|    81 | b    | <2017-11-05 Sun 23:54> | <2017-11-07 Tue 0:00>  | 1 days 00:06  |           1 |    35856 |   7283 |
|    82 | b    | <2017-11-07 Tue 0:01>  | <2017-11-08 Wed 15:58> | 1 days 15:56  |           2 |    59502 |  12272 |
|    83 | c    | <2017-11-08 Wed 16:27> | <2017-11-08 Wed 17:27> | 0 days 00:59  |           0 |     4915 |   4897 |
|    84 | b    | <2017-11-08 Wed 17:49> | <2017-11-09 Thu 19:01> | 1 days 01:11  |           1 |    37391 |   7551 |
|    85 | b    | <2017-11-09 Thu 19:01> | <2017-11-09 Thu 21:46> | 0 days 02:45  |           0 |     4104 |    899 |
|    86 | b    | <2017-11-09 Thu 21:47> | <2017-11-11 Sat 2:17>  | 1 days 04:29  |           1 |    42396 |   9656 |
|    87 | b    | <2017-11-11 Sat 2:17>  | <2017-11-12 Sun 14:29> | 1 days 12:11  |           2 |    54786 |  15123 |
|    88 | c    | <2017-11-12 Sun 14:30> | <2017-11-12 Sun 15:30> | 0 days 00:59  |           0 |     4943 |   4934 |
|    89 | b    | <2017-11-12 Sun 15:30> | <2017-11-13 Mon 18:27> | 1 days 02:57  |           1 |    25209 |   6210 |
|    90 | b    | <2017-11-13 Mon 19:14> | <2017-11-14 Tue 20:24> | 1 days 01:09  |           1 |    37497 |   8122 |
|    91 | b    | <2017-11-14 Tue 20:24> | <2017-11-15 Wed 21:44> | 1 days 01:20  |           1 |    37732 |   8108 |
|    92 | b    | <2017-11-15 Wed 21:45> | <2017-11-17 Fri 19:18> | 1 days 21:32  |           1 |    67946 |  14730 |
|    93 | c    | <2017-11-17 Fri 19:18> | <2017-11-17 Fri 20:18> | 0 days 01:00  |           0 |     4977 |   4968 |
|    94 | b    | <2017-11-17 Fri 20:48> | <2017-11-19 Sun 2:34>  | 1 days 05:46  |           1 |    44344 |   9422 |
|    95 | b    | <2017-11-19 Sun 2:35>  | <2017-11-23 Thu 10:41> | 4 days 08:06  |           1 |   154959 |  33112 |
|    96 | c    | <2017-11-23 Thu 10:42> | <2017-11-23 Thu 17:43> | 0 days 07:01  |           0 |    34586 |  34496 |
|    97 | b    | <2017-11-23 Thu 17:43> | <2017-11-26 Sun 1:41>  | 2 days 07:57  |           1 |    83404 |  18277 |
|    98 | b    | <2017-11-26 Sun 1:42>  | <2017-11-26 Sun 21:18> | 0 days 19:36  |           1 |    29202 |   6285 |
|    99 | b    | <2017-11-26 Sun 21:18> | <2017-11-28 Tue 6:46>  | 1 days 09:27  |           1 |    49921 |  10895 |
|   100 | b    | <2017-11-28 Tue 6:46>  | <2017-11-29 Wed 6:40>  | 0 days 23:53  |           1 |    35658 |   7841 |
|   101 | b    | <2017-11-29 Wed 6:40>  | <2017-11-29 Wed 20:18> | 0 days 13:37  |           1 |    20326 |   4203 |
|   102 | c    | <2017-11-29 Wed 20:19> | <2017-11-29 Wed 22:19> | 0 days 02:00  |           0 |     9919 |   9898 |
|   103 | b    | <2017-11-29 Wed 22:26> | <2017-12-01 Fri 6:46>  | 1 days 08:19  |           1 |    47381 |   7867 |
|   104 | b    | <2017-12-01 Fri 6:47>  | <2017-12-02 Sat 6:48>  | 1 days 00:00  |           1 |    35220 |   5866 |
|   105 | b    | <2017-12-02 Sat 6:48>  | <2017-12-03 Sun 6:39>  | 0 days 23:51  |           1 |    34918 |   5794 |
|   106 | b    | <2017-12-03 Sun 6:40>  | <2017-12-04 Mon 6:54>  | 1 days 00:14  |           1 |    35576 |   6018 |
|   107 | b    | <2017-12-04 Mon 6:54>  | <2017-12-04 Mon 13:38> | 0 days 06:44  |           1 |     9883 |   1641 |
|   108 | c    | <2017-12-04 Mon 13:39> | <2017-12-04 Mon 17:39> | 0 days 04:00  |           0 |    19503 |  19448 |
|   109 | b    | <2017-12-04 Mon 17:47> | <2017-12-05 Tue 11:20> | 0 days 17:32  |           1 |    28402 |   8217 |
|   110 | c    | <2017-12-05 Tue 11:20> | <2017-12-05 Tue 13:20> | 0 days 01:59  |           0 |     9804 |   9786 |
|   111 | b    | <2017-12-05 Tue 13:23> | <2017-12-05 Tue 16:17> | 0 days 02:53  |           0 |     4244 |    644 |
|   112 | b    | <2017-12-06 Wed 14:50> | <2017-12-10 Sun 6:46>  | 3 days 15:55  |           2 |   128931 |  19607 |
|   113 | b    | <2017-12-10 Sun 6:46>  | <2017-12-11 Mon 6:49>  | 1 days 00:03  |           1 |    35100 |   5174 |
|   114 | b    | <2017-12-11 Mon 6:50>  | <2017-12-11 Mon 18:33> | 0 days 11:43  |           1 |    17111 |   2542 |
|   115 | b    | <2017-12-11 Mon 18:36> | <2017-12-12 Tue 20:58> | 1 days 02:21  |           1 |    40574 |   9409 |
|   116 | c    | <2017-12-12 Tue 20:59> | <2017-12-12 Tue 22:59> | 0 days 02:00  |           0 |     9741 |   9724 |
|   117 | b    | <2017-12-12 Tue 23:56> | <2017-12-13 Wed 21:29> | 0 days 21:33  |           1 |    31885 |   5599 |
|   118 | c    | <2017-12-13 Wed 21:30> | <2017-12-13 Wed 23:30> | 0 days 02:00  |           0 |     9771 |   9748 |
|   119 | b    | <2017-12-14 Thu 0:07>  | <2017-12-14 Thu 17:04> | 0 days 16:57  |           1 |    25434 |   4903 |
|   120 | c    | <2017-12-14 Thu 17:04> | <2017-12-14 Thu 21:04> | 0 days 04:00  |           0 |    19308 |  19261 |
|   121 | b    | <2017-12-14 Thu 21:07> | <2017-12-15 Fri 19:22> | 0 days 22:14  |           1 |    33901 |   6947 |
|   122 | c    | <2017-12-15 Fri 19:22> | <2017-12-16 Sat 1:20>  | 0 days 05:57  |           0 |    29279 |  29208 |
|   123 | b    | <2017-12-16 Sat 1:21>  | <2017-12-17 Sun 1:06>  | 0 days 23:45  |           1 |    34107 |   3380 |
|   124 | b    | <2017-12-17 Sun 1:06>  | <2017-12-19 Tue 2:57>  | 2 days 01:50  |           2 |    71703 |   7504 |
|   125 | b    | <2017-12-19 Tue 2:57>  | <2017-12-19 Tue 16:20> | 0 days 13:22  |           1 |    19262 |   1991 |
|   126 | c    | <2017-12-19 Tue 16:21> | <2017-12-19 Tue 19:21> | 0 days 02:59  |           0 |    14729 |  14689 |
|   127 | b    | <2017-12-19 Tue 19:27> | <2017-12-22 Fri 0:17>  | 2 days 04:50  |           1 |    75907 |   7663 |
|   128 | c    | <2017-12-22 Fri 0:18>  | <2017-12-22 Fri 9:23>  | 0 days 09:05  |           0 |    44806 |  44709 |
|   145 | c    | <2018-02-17 Sat 17:18> | <2018-02-17 Sat 20:40> | 0 days 03:22  |           0 |    16797 |  16796 |
|   146 | b    | <2018-02-17 Sat 20:41> | <2018-02-18 Sun 18:12> | 0 days 21:30  |           1 |    32705 |   3054 |
|   147 | c    | <2018-02-18 Sun 18:12> | <2018-02-18 Sun 20:12> | 0 days 01:59  |           0 |    10102 |  10102 |
|   148 | b    | <2018-02-18 Sun 20:46> | <2018-02-19 Mon 17:24> | 0 days 20:37  |           1 |    31433 |   3120 |
|   149 | c    | <2018-02-19 Mon 17:25> | <2018-02-19 Mon 19:25> | 0 days 02:00  |           0 |     9975 |   9975 |
|   150 | b    | <2018-02-19 Mon 19:53> | <2018-02-20 Tue 17:36> | 0 days 21:42  |           1 |    33192 |   3546 |
|   151 | c    | <2018-02-20 Tue 17:36> | <2018-02-20 Tue 19:36> | 0 days 01:59  |           0 |     9907 |   9907 |
|   152 | b    | <2018-02-20 Tue 21:54> | <2018-02-21 Wed 18:05> | 0 days 20:10  |           1 |    30809 |   3319 |
|   153 | c    | <2018-02-21 Wed 18:05> | <2018-02-21 Wed 20:05> | 0 days 01:59  |           0 |    10103 |  10102 |
|   154 | b    | <2018-02-21 Wed 21:10> | <2018-02-22 Thu 17:23> | 0 days 20:12  |           1 |    30891 |   3426 |
|   155 | c    | <2018-02-22 Thu 17:23> | <2018-02-22 Thu 19:23> | 0 days 02:00  |           0 |     9861 |   9861 |
|   156 | b    | <2018-02-23 Fri 6:06>  | <2018-02-23 Fri 17:41> | 0 days 11:35  |           1 |    17686 |   1866 |
|   157 | c    | <2018-02-23 Fri 17:41> | <2018-02-23 Fri 19:41> | 0 days 01:59  |           0 |     9962 |   9962 |
|   158 | b    | <2018-02-23 Fri 19:42> | <2018-02-26 Mon 8:46>  | 2 days 13:03  |           1 |    93205 |   9893 |
|   159 | c    | <2018-02-26 Mon 8:46>  | <2018-02-26 Mon 12:46> | 0 days 04:00  |           0 |    19879 |  19878 |
|   160 | b    | <2018-02-26 Mon 14:56> | <2018-03-01 Thu 10:24> | 2 days 19:28  |           1 |   103145 |  11415 |
|   161 | c    | <2018-03-01 Thu 10:26> | <2018-03-01 Thu 14:26> | 0 days 04:00  |           0 |    19944 |  19943 |
|   162 | b    | <2018-03-01 Thu 17:07> | <2018-03-04 Sun 20:16> | 3 days 03:08  |           3 |   114590 |  11897 |
|   163 | c    | <2018-03-04 Sun 20:17> | <2018-03-04 Sun 22:17> | 0 days 02:00  |           0 |    10093 |  10093 |
|   164 | b    | <2018-03-04 Sun 22:57> | <2018-03-06 Tue 19:15> | 1 days 20:18  |           2 |    67456 |   6488 |
|   165 | c    | <2018-03-06 Tue 19:15> | <2018-03-06 Tue 23:15> | 0 days 04:00  |           0 |    19882 |  19879 |
|   166 | b    | <2018-03-07 Wed 0:50>  | <2018-03-07 Wed 18:28> | 0 days 17:38  |           1 |    26859 |   2565 |
|   167 | c    | <2018-03-07 Wed 18:29> | <2018-03-07 Wed 20:29> | 0 days 02:00  |           0 |     9938 |   9938 |
|   168 | b    | <2018-03-07 Wed 20:37> | <2018-03-13 Tue 16:54> | 5 days 20:16  |           0 |   213545 |  20669 |
|   169 | c    | <2018-03-13 Tue 16:55> | <2018-03-13 Tue 22:55> | 0 days 06:00  |           0 |    29874 |  29874 |
|   170 | b    | <2018-03-13 Tue 23:19> | <2018-03-14 Wed 21:01> | 0 days 21:42  |           1 |    33098 |   3269 |
|   171 | c    | <2018-03-14 Wed 21:01> | <2018-03-14 Wed 23:01> | 0 days 02:00  |           0 |     9999 |   9999 |
|   172 | b    | <2018-03-14 Wed 23:06> | <2018-03-15 Thu 17:57> | 0 days 18:50  |           1 |    28649 |   2773 |
|   173 | c    | <2018-03-15 Thu 17:59> | <2018-03-15 Thu 19:59> | 0 days 01:59  |           0 |     9898 |   9897 |
|   174 | b    | <2018-03-15 Thu 20:39> | <2018-03-16 Fri 16:27> | 0 days 19:48  |           1 |    30163 |   2961 |
|   175 | c    | <2018-03-16 Fri 16:28> | <2018-03-16 Fri 18:28> | 0 days 01:59  |           0 |    10075 |  10075 |
|   176 | b    | <2018-03-16 Fri 18:35> | <2018-03-17 Sat 20:55> | 1 days 02:19  |           1 |    40084 |   3815 |
|   177 | c    | <2018-03-17 Sat 20:55> | <2018-03-17 Sat 22:55> | 0 days 01:59  |           0 |     9967 |   9966 |
|   178 | b    | <2018-03-17 Sat 23:31> | <2018-03-22 Thu 17:40> | 4 days 18:09  |           5 |   174074 |  17949 |
|   179 | c    | <2018-03-22 Thu 17:41> | <2018-03-22 Thu 19:41> | 0 days 01:59  |           0 |     9887 |   9887 |
|   180 | b    | <2018-03-22 Thu 20:47> | <2018-03-24 Sat 18:10> | 1 days 21:22  |           1 |    69224 |   7423 |
|   181 | c    | <2018-03-24 Sat 18:10> | <2018-03-24 Sat 22:10> | 0 days 04:00  |           0 |    20037 |  20036 |
|   182 | b    | <2018-03-24 Sat 23:32> | <2018-03-26 Mon 19:46> | 1 days 19:14  |           2 |    65888 |   6694 |
|   183 | c    | <2018-03-26 Mon 19:47> | <2018-03-26 Mon 23:47> | 0 days 03:59  |           0 |    20026 |  20026 |
|   184 | b    | <2018-03-27 Tue 0:32>  | <2018-03-30 Fri 14:18> | 3 days 13:45  |           0 |   130576 |  12883 |
|   185 | c    | <2018-03-30 Fri 14:18> | <2018-03-30 Fri 18:18> | 0 days 03:59  |           0 |    19901 |  19901 |
|   186 | b    | <2018-03-30 Fri 19:03> | <2018-04-11 Wed 16:03> | 11 days 21:00 |           0 |   434087 |  42830 |
|   187 | c    | <2018-04-11 Wed 16:04> | <2018-04-11 Wed 20:04> | 0 days 04:00  |           0 |    19667 |  19665 |
|   188 | b    | <2018-04-11 Wed 20:53> | <2018-04-17 Tue 10:53> | 5 days 14:00  |           0 |   204281 |  20781 |
|-------+------+------------------------+------------------------+---------------+-------------+----------+--------|
| Run-3 |      |                        |                        |               |             |          |        |
|-------+------+------------------------+------------------------+---------------+-------------+----------+--------|
|   239 | c    | <2018-10-20 Sat 18:31> | <2018-10-20 Sat 20:31> | 0 days 02:00  |           0 |     9565 |   9518 |
|   240 | b    | <2018-10-21 Sun 14:54> | <2018-10-22 Mon 16:15> | 1 days 01:21  |           1 |    38753 |   4203 |
|   241 | c    | <2018-10-22 Mon 16:16> | <2018-10-22 Mon 18:16> | 0 days 02:00  |           0 |     9480 |   9426 |
|   242 | b    | <2018-10-22 Mon 18:44> | <2018-10-23 Tue 22:08> | 1 days 03:24  |           1 |    41933 |   4843 |
|   243 | c    | <2018-10-23 Tue 22:09> | <2018-10-24 Wed 0:09>  | 0 days 01:59  |           0 |     9488 |   9429 |
|   244 | b    | <2018-10-24 Wed 0:32>  | <2018-10-24 Wed 19:24> | 0 days 18:52  |           1 |    28870 |   3317 |
|   245 | c    | <2018-10-24 Wed 19:25> | <2018-10-24 Wed 21:25> | 0 days 01:59  |           0 |     9573 |   9530 |
|   246 | b    | <2018-10-24 Wed 21:59> | <2018-10-25 Thu 16:18> | 0 days 18:18  |           1 |    27970 |   2987 |
|   247 | c    | <2018-10-25 Thu 16:19> | <2018-10-25 Thu 18:19> | 0 days 01:59  |           0 |     9389 |   9334 |
|   248 | b    | <2018-10-25 Thu 18:25> | <2018-10-26 Fri 22:29> | 1 days 04:04  |           1 |    42871 |   4544 |
|   249 | c    | <2018-10-26 Fri 22:30> | <2018-10-27 Sat 0:30>  | 0 days 02:00  |           0 |     9473 |   9431 |
|   250 | b    | <2018-10-27 Sat 1:31>  | <2018-10-27 Sat 22:26> | 0 days 20:54  |           1 |    31961 |   3552 |
|   251 | c    | <2018-10-27 Sat 22:26> | <2018-10-28 Sun 0:26>  | 0 days 01:59  |           0 |     9551 |   9503 |
|   253 | c    | <2018-10-28 Sun 19:18> | <2018-10-28 Sun 21:39> | 0 days 02:20  |           0 |    11095 |  11028 |
|   254 | b    | <2018-10-28 Sun 21:40> | <2018-10-29 Mon 23:03> | 1 days 01:23  |           1 |    38991 |   4990 |
|   255 | c    | <2018-10-29 Mon 23:03> | <2018-10-30 Tue 1:03>  | 0 days 02:00  |           0 |     9378 |   9330 |
|   256 | b    | <2018-10-30 Tue 1:49>  | <2018-10-31 Wed 22:18> | 1 days 20:29  |           1 |    68315 |   8769 |
|   257 | c    | <2018-10-31 Wed 22:19> | <2018-11-01 Thu 0:19>  | 0 days 01:59  |           0 |     9648 |   9592 |
|   258 | b    | <2018-11-01 Thu 0:20>  | <2018-11-01 Thu 16:15> | 0 days 15:55  |           1 |    24454 |   3103 |
|   259 | c    | <2018-11-01 Thu 16:16> | <2018-11-01 Thu 17:31> | 0 days 01:14  |           0 |     5900 |   5864 |
|   260 | c    | <2018-11-01 Thu 17:39> | <2018-11-01 Thu 19:09> | 0 days 01:30  |           0 |     7281 |   7251 |
|   261 | b    | <2018-11-01 Thu 19:39> | <2018-11-04 Sun 15:23> | 2 days 19:43  |           3 |   103658 |  12126 |
|   262 | c    | <2018-11-04 Sun 15:24> | <2018-11-04 Sun 21:24> | 0 days 05:59  |           0 |    28810 |  28681 |
|   263 | b    | <2018-11-05 Mon 0:35>  | <2018-11-05 Mon 20:28> | 0 days 19:52  |           1 |    30428 |   3610 |
|   264 | c    | <2018-11-05 Mon 20:28> | <2018-11-05 Mon 22:28> | 0 days 01:59  |           0 |     9595 |   9544 |
|   265 | b    | <2018-11-05 Mon 22:52> | <2018-11-07 Wed 22:14> | 1 days 23:21  |           1 |    72514 |   8429 |
|   266 | c    | <2018-11-07 Wed 22:14> | <2018-11-08 Thu 0:14>  | 0 days 01:59  |           0 |     9555 |   9506 |
|   267 | b    | <2018-11-08 Thu 2:05>  | <2018-11-08 Thu 6:54>  | 0 days 04:48  |           0 |     7393 |    929 |
|   268 | b    | <2018-11-09 Fri 6:15>  | <2018-11-09 Fri 17:20> | 0 days 11:04  |           1 |    16947 |   1974 |
|   269 | c    | <2018-11-09 Fri 17:20> | <2018-11-09 Fri 21:20> | 0 days 04:00  |           0 |    19382 |  19302 |
|   270 | b    | <2018-11-09 Fri 21:27> | <2018-11-11 Sun 21:02> | 1 days 23:34  |           2 |    72756 |   8078 |
|   271 | c    | <2018-11-11 Sun 21:03> | <2018-11-11 Sun 23:46> | 0 days 02:43  |           0 |    13015 |  12944 |
|   272 | b    | <2018-11-12 Mon 0:09>  | <2018-11-14 Wed 19:07> | 2 days 18:58  |           3 |   102360 |  11336 |
|   273 | c    | <2018-11-14 Wed 19:08> | <2018-11-14 Wed 21:08> | 0 days 01:59  |           0 |     9535 |   9471 |
|   274 | b    | <2018-11-14 Wed 21:28> | <2018-11-17 Sat 18:14> | 2 days 20:45  |           3 |   105187 |  12101 |
|   275 | c    | <2018-11-17 Sat 18:14> | <2018-11-17 Sat 20:57> | 0 days 02:43  |           0 |    13179 |  13116 |
|   276 | b    | <2018-11-17 Sat 22:08> | <2018-11-22 Thu 2:26>  | 4 days 04:17  |           2 |   153954 |  19640 |
|   277 | c    | <2018-11-22 Thu 2:26>  | <2018-11-22 Thu 16:14> | 0 days 13:48  |           0 |    66052 |  65749 |
|   278 | b    | <2018-11-22 Thu 16:14> | <2018-11-23 Fri 10:51> | 0 days 18:36  |           0 |    28164 |   3535 |
|   279 | b    | <2018-11-24 Sat 10:51> | <2018-11-26 Mon 14:58> | 2 days 04:07  |           2 |    79848 |   9677 |
|   280 | c    | <2018-11-26 Mon 14:59> | <2018-11-26 Mon 18:59> | 0 days 04:00  |           0 |    19189 |  19112 |
|   281 | b    | <2018-11-26 Mon 19:02> | <2018-11-28 Wed 18:07> | 1 days 23:04  |           1 |    72230 |   8860 |
|   282 | c    | <2018-11-28 Wed 18:07> | <2018-11-28 Wed 20:51> | 0 days 02:43  |           0 |    12924 |  12860 |
|   283 | b    | <2018-11-28 Wed 22:31> | <2018-12-01 Sat 14:38> | 2 days 16:07  |           3 |    98246 |  11965 |
|   284 | c    | <2018-12-01 Sat 14:39> | <2018-12-01 Sat 18:39> | 0 days 03:59  |           0 |    19017 |  18904 |
|   285 | b    | <2018-12-01 Sat 19:06> | <2018-12-03 Mon 19:39> | 2 days 00:33  |           2 |    74405 |   8887 |
|   286 | c    | <2018-12-04 Tue 15:57> | <2018-12-04 Tue 17:57> | 0 days 02:00  |           0 |     9766 |   9715 |
|   287 | b    | <2018-12-04 Tue 19:07> | <2018-12-05 Wed 15:08> | 0 days 20:01  |           1 |    30598 |   3393 |
|   288 | c    | <2018-12-05 Wed 17:28> | <2018-12-05 Wed 19:28> | 0 days 02:00  |           0 |     9495 |   9443 |
|   289 | b    | <2018-12-05 Wed 23:07> | <2018-12-06 Thu 19:11> | 0 days 20:03  |           1 |    30629 |   3269 |
|   290 | c    | <2018-12-06 Thu 19:11> | <2018-12-06 Thu 21:11> | 0 days 02:00  |           0 |     9457 |   9394 |
|   291 | b    | <2018-12-06 Thu 23:14> | <2018-12-08 Sat 13:39> | 1 days 14:24  |           2 |    58602 |   6133 |
|   292 | c    | <2018-12-08 Sat 13:39> | <2018-12-08 Sat 15:39> | 0 days 02:00  |           0 |     9475 |   9426 |
|   293 | b    | <2018-12-08 Sat 17:42> | <2018-12-10 Mon 21:50> | 2 days 04:07  |           1 |    79677 |   8850 |
|   294 | c    | <2018-12-10 Mon 21:50> | <2018-12-10 Mon 23:50> | 0 days 02:00  |           0 |     9514 |   9467 |
|   295 | b    | <2018-12-11 Tue 0:54>  | <2018-12-11 Tue 20:31> | 0 days 19:37  |           1 |    29981 |   3271 |
|   296 | c    | <2018-12-11 Tue 20:31> | <2018-12-11 Tue 22:31> | 0 days 02:00  |           0 |     9565 |   9517 |
|   297 | b    | <2018-12-12 Wed 0:14>  | <2018-12-13 Thu 18:30> | 1 days 18:15  |           2 |    68124 |  12530 |
|   298 | b    | <2018-12-13 Thu 18:39> | <2018-12-15 Sat 6:41>  | 1 days 12:01  |           1 |    53497 |      0 |
|   299 | b    | <2018-12-15 Sat 6:43>  | <2018-12-15 Sat 18:13> | 0 days 11:29  |           1 |    17061 |      0 |
|   300 | c    | <2018-12-15 Sat 18:38> | <2018-12-15 Sat 20:38> | 0 days 02:00  |           0 |     9466 |   9415 |
|   301 | b    | <2018-12-15 Sat 21:34> | <2018-12-17 Mon 14:17> | 1 days 16:43  |           2 |    62454 |   7751 |
|   302 | c    | <2018-12-17 Mon 14:18> | <2018-12-17 Mon 16:18> | 0 days 01:59  |           0 |     9616 |   9577 |
|   303 | b    | <2018-12-17 Mon 16:52> | <2018-12-18 Tue 16:41> | 0 days 23:48  |           1 |    36583 |   4571 |
|   304 | c    | <2018-12-19 Wed 9:33>  | <2018-12-19 Wed 11:33> | 0 days 01:59  |           0 |     9531 |   9465 |
|   306 | b    | <2018-12-20 Thu 6:55>  | <2018-12-20 Thu 11:53> | 0 days 04:58  |           1 |     7546 |    495 |

\normalsize

* CAST data taking notes [0/1]                            :Appendix:noexport:
:PROPERTIES:
:CUSTOM_ID: sec:appendix:cast_data_taking_notes
:END:

*NOTE*: This can't be properly exported to the full thesis in a
PDF. The layout is all sorts of broken.

This file contains a run list of all runs taken during the data taking
period starting in October 2017.

It lists each run, separated as data or calibration run and includes
notes about chip settings etc. (in case there were changes).

- [ ] *HAVE A NOEXPORT SECTION EACH ABOUT:*
  - CAST detector documentation
  - Shifter documentation
  - ..?


** Run table

This section contains a table of the different runs, which identifies
what type each run is, when it started and ended.

The type column describes the run as either
- 'd' == data runs
- 'c' == calibration run
- 'x' == experimental, related to development, problems etc

*** Run in 2017

| Run # | Type {d, c} | Start                  | End                    | Length       | Backup? | Notes                                      |
|-------+-------------+------------------------+------------------------+--------------+---------+--------------------------------------------|
|    76 | d           | <2017-10-30 Mon 18:39> | <2017-11-02 Thu 5:24>  | 2 days 10:44 | y       |                                            |
|    77 | d           | <2017-11-02 Thu 05:24> | <2017-11-03 Fri 5:28>  | 1 days 00:03 | y       |                                            |
|    78 | d           | <2017-11-03 Fri 05:28> | <2017-11-03 Fri 20:45> | 0 days 15:17 | y       |                                            |
|    79 | d           | <2017-11-03 Fri 20:46> | <2017-11-05 Sun 0:09>  | 1 days 03:22 | y       |                                            |
|    80 | d           | <2017-11-05 Sun 00:09> | <2017-11-05 Sun 23:50> | 0 days 23:40 | y       |                                            |
|    81 | d           | <2017-11-05 Sun 23:54> | <2017-11-07 Tue 0:00>  | 1 days 00:06 | y       |                                            |
|    82 | d           | <2017-11-07 Thu 00:01> | <2017-11-08 Wed 15:58> | 1 days 15:56 | y       |                                            |
|    83 | c           | <2017-11-08 Wed 16:27> | <2017-11-08 Wed 17:27> | 0 days 00:59 | y       |                                            |
|    84 | d           | <2017-11-08 Wed 17:49> | <2017-11-09 Thu 19:01> | 1 days 01:11 | y       |                                            |
|    85 | d           | <2017-11-09 Thu 19:01> | <2017-11-09 Thu 21:46> | 0 days 02:45 | y       |                                            |
|    86 | d           | <2017-11-09 Thu 21:47> | <2017-11-11 Sat 2:17>  | 1 days 04:29 | y       |                                            |
|    87 | d           | <2017-11-11 Sat 2:17>  | <2017-11-12 Sun 14:29> | 1 days 12:11 | y       |                                            |
|    88 | c           | <2017-11-12 Sun 14:30> | <2017-11-12 Sun 15:30> | 0 days 0:59  | y       |                                            |
|    89 | d           | <2017-11-12 Sun 15:30> | <2017-11-13 Mon 18:27> | 1 days 2:57  | y       | See note below about length                |
|    90 | d           | <2017-11-13 Mon 19:14> | <2017-11-14 Tue 20:24> | 1 days 1:09  | y       |                                            |
|    91 | d           | <2017-11-14 Tue 20:24> | <2017-11-15 Wed 21:44> | 1 days 1:20  | y       |                                            |
|    92 | d           | <2017-11-15 Wed 21:45> | <2017-11-17 Fri 19:18> | 1 days 21:32 | y       | No Run on <2017-11-16 Th>                  |
|    93 | c           | <2017-11-17 Fri 19:18> | <2017-11-17 Fri 20:18> | 0 days 1:00  | y       |                                            |
|    94 | d           | <2017-11-17 Fri 20:48> | <2017-11-19 Sun 2:34>  | 1 days 5:46  | y       |                                            |
|    95 | d           | <2017-11-19 Sun 2:35>  | <2017-11-23 Thu 10:41> | 4 days 8:06  | y       | Beginning of GRID                          |
|    96 | c           | <2017-11-23 Thu 10:42> | <2017-11-23 Thu 17:43> | 0 days 7:01  | y       | Long calibration for statistics            |
|    97 | d           | <2017-11-23 Thu 17:43> | <2017-11-26 Sun 1:41>  | 2 days 7:57  | y       | ~4 min of tracking lost on 25/11, see note |
|    98 | d           | <2017-11-26 Sun 1:42>  | <2017-11-26 Sun 21:18> | 0 days 19:36 | y       |                                            |
|    99 | d           | <2017-11-26 Sun 21:18> | <2017-11-28 Tue 6:46>  | 1 days 9:27  | y       |                                            |
|   100 | d           | <2017-11-28 Tue 6:46>  | <2017-11-29 Wed 6:40>  | 0 days 23:53 | y       |                                            |
|   101 | d           | <2017-11-29 Wed 6:40>  | <2017-11-29 Wed 20:18> | 0 days 13:37 | y       | FADC amp settings changed, see below       |
|   102 | c           | <2017-11-29 Wed 20:19> | <2017-11-29 Wed 22:19> | 0 days 2:00  | y       |                                            |
|   103 | d           | <2017-11-29 Wed 22:26> | <2017-12-01 Fri 6:46>  | 1 days 8:19  | y       |                                            |
|   104 | d           | <2017-12-01 Fri 6:47>  | <2017-12-02 Sat 6:48>  | 1 days 0:00  | y       |                                            |
|   105 | d           | <2017-12-02 Sat 6:48>  | <2017-12-03 Sun 6:39>  | 0 days 23:51 | y       |                                            |
|   106 | d           | <2017-12-03 Sun 6:40>  | <2017-12-04 Mon 6:54>  | 1 days 0:14  | y       |                                            |
|   107 | d           | <2017-12-04 Mon 6:54>  | <2017-12-04 Mon 13:38> | 0 days 6:44  | y       |                                            |
|   108 | c           | <2017-12-04 Mon 13:39> | <2017-12-04 Mon 17:39> | 0 days 4:00  | y       |                                            |
|   109 | d           | <2017-12-04 Mon 17:47> | <2017-12-05 Tue 11:20> | 0 days 17:32 | y       | A lot of noise during this shift           |
|   110 | c           | <2017-12-05 Tue 11:20> | <2017-12-05 Tue 13:20> | 0 days 1:59  | y       |                                            |
|   111 | d           | <2017-12-05 Tue 13:23> | <2017-12-05 Tue 16:17> | 0 days 2:53  | y       | gas interlock box fuse burned, early stop  |
|   112 | d           | <2017-12-06 Wed 14:50> | <2017-12-10 Sun 6:46>  | 3 days 15:55 | y       | FADC: int. time: 50->100->50ns + quench    |
|   113 | d           | <2017-12-10 Sun 6:46>  | <2017-12-11 Mon 6:49>  | 1 days 0:03  | y       |                                            |
|   114 | d           | <2017-12-11 Mon 6:50>  | <2017-12-11 Mon 18:33> | 0 days 11:43 | y       |                                            |
|   115 | d           | <2017-12-11 Mon 18:36> | <2017-12-12 Tue 20:58> | 1 days 2:21  | y       |                                            |
|   116 | c           | <2017-12-12 Tue 20:59> | <2017-12-12 Tue 22:59> | 0 days 2:00  | y       |                                            |
|   117 | d           | <2017-12-12 Tue 23:56> | <2017-12-13 Wed 21:29> | 0 days 21:33 | y       |                                            |
|   118 | c           | <2017-12-13 Wed 21:30> | <2017-12-13 Wed 23:30> | 0 days 2:00  | y       |                                            |
|   119 | d           | <2017-12-14 Thu 0:07>  | <2017-12-14 Thu 17:04> | 0 days 16:57 | y       |                                            |
|   120 | c           | <2017-12-14 Thu 17:04> | <2017-12-14 Thu 21:04> | 0 days 4:00  | y       |                                            |
|   121 | d           | <2017-12-14 Thu 21:07> | <2017-12-15 Fri 19:22> | 0 days 22:14 | y       | Jochen: FADC int. time: 50->100ns, c note  |
|   122 | c           | <2017-12-15 Fri 19:22> | <2017-12-16 Sat 1:20>  | 0 days 5:57  | y       |                                            |
|   123 | d           | <2017-12-16 Sat 1:21>  | <2017-12-17 Sun 1:06>  | 0 days 23:45 | y       |                                            |
|   124 | d           | <2017-12-17 Sun 1:06>  | <2017-12-19 Tue 2:57>  | 2 days 1:50  | y       |                                            |
|   125 | d           | <2017-12-19 Tue 2:57>  | <2017-12-19 Tue 16:20> | 0 days 13:22 | y       |                                            |
|   126 | c           | <2017-12-19 Tue 16:21> | <2017-12-19 Tue 19:21> | 0 days 2:59  | y       |                                            |
|   127 | d           | <2017-12-19 Tue 19:27> | <2017-12-22 Fri 0:17>  | 2 days 4:50  | y       |                                            |
|   128 | c           | <2017-12-22 Fri 0:18>  | <2017-12-22 Fri 9:23>  | 0 days 9:05  | y       | Final run of 2017                          |

*** Run 1 in 2018

| Run # | Type {d, c} | Start                  | End                    | Length        | # trackings | Backup? | Notes                                                             |
|-------+-------------+------------------------+------------------------+---------------+-------------+---------+-------------------------------------------------------------------|
|   137 | d           | <2018-02-15 Thu 5:34>  | <2018-02-15 Thu 17:08> | 0 days 11:34  |             | y*      | WARNING: do not use, THL problems, see note!                      |
|   138 | c           | <2018-02-15 Thu 17:09> | <2018-02-15 Thu 19:34> | 0 days 2:24   |             | y*      | Seems like gas amplification down by factor 2!                    |
|   139 | c           | <2018-02-15 Thu 20:31> | <2018-02-15 Thu 21:53> | 0 days 1:22   |             | y*      | Central THL 450 -> 400 from here on! (if result good)             |
|   140 | d           | <2018-02-15 Thu 21:53> | <2018-02-16 Fri 17:59> | 0 days 20:05  |             | y*      | Running THL from Run 139.                                         |
|   141 | x (c)       | <2018-02-16 Fri 18:00> | <2018-02-17 Sat 13:28> | 0 days 19:28  |             | y*      | Calibration run over night, showcasing increasing [Power Problem] |
|   142 | x (d)       | <2018-02-17 Sat 14:04> | <2018-02-17 Sat 16:17> | 0 days 2:12   |             | y*      | Background data run w/ THL 400 and problems, see [Power Problem]  |
|   143 | x (c)       | <2018-02-17 Sat 16:18> | <2018-02-17 Sat 16:26> | 0 days 0:07   |             | y*      | More calibration for testing                                      |
|   144 | x (c)       | <2018-02-17 Sat 16:32> | <2018-02-17 Sat 17:18> | 0 days 0:45   |             | y*      | Calibration run in which [Power Problem] was fixed                |
|   145 | c           | <2018-02-17 Sat 17:18> | <2018-02-17 Sat 20:40> | 0 days 3:22   |             | y       | Proper calibration run w/ THL = 450 and fixed [Power Problem]     |
|   146 | d           | <2018-02-17 Sat 20:41> | <2018-02-18 Sun 18:12> | 0 days 21:30  |             | y       | First good shift after power supply problem                       |
|   147 | c           | <2018-02-18 Sun 18:12> | <2018-02-18 Sun 20:12> | 0 days 1:59   |             | y       | Calibration run                                                   |
|   148 | d           | <2018-02-18 Sun 20:46> | <2018-02-19 Mon 17:24> | 0 days 20:37  |             | y       |                                                                   |
|   149 | c           | <2018-02-19 Mon 17:25> | <2018-02-19 Mon 19:25> | 0 days 2:00   |             | y       |                                                                   |
|   150 | d           | <2018-02-19 Mon 19:53> | <2018-02-20 Tue 17:36> | 0 days 21:42  |             | y       |                                                                   |
|   151 | c           | <2018-02-20 Tue 17:36> | <2018-02-20 Tue 19:36> | 0 days 1:59   |             | y       |                                                                   |
|   152 | d           | <2018-02-20 Tue 21:54> | <2018-02-21 Wed 18:05> | 0 days 20:10  |             | y       |                                                                   |
|   153 | c           | <2018-02-21 Wed 18:05> | <2018-02-21 Wed 20:05> | 0 days 1:59   |             | y       |                                                                   |
|   154 | d           | <2018-02-21 Wed 21:10> | <2018-02-22 Thu 17:23> | 0 days 20:12  |             | y       |                                                                   |
|   155 | c           | <2018-02-22 Thu 17:23> | <2018-02-22 Thu 19:23> | 0 days 1:59   |             | y       |                                                                   |
|   156 | d           | <2018-02-23 Fri 6:06>  | <2018-02-23 Fri 17:41> | 0 days 11:35  |             | y       |                                                                   |
|   157 | c           | <2018-02-23 Fri 17:41> | <2018-02-23 Fri 19:41> | 0 days 1:59   |             | y       |                                                                   |
|   158 | d           | <2018-02-23 Fri 19:42> | <2018-02-26 Mon 8:46>  | 2 days 13:03  |             | y       |                                                                   |
|   159 | c           | <2018-02-26 Mon 8:46>  | <2018-02-26 Mon 12:46> | 0 days 4:00   |             | y       |                                                                   |
|   160 | d           | <2018-02-26 Mon 14:56> | <2018-03-01 Thu 10:24> | 2 days 19:28  |             | y       |                                                                   |
|   161 | c           | <2018-03-01 Thu 10:26> | <2018-03-01 Thu 14:26> | 0 days 4:00   |             | y       |                                                                   |
|   162 | d           | <2018-03-01 Thu 17:07> | <2018-03-04 Sun 20:16> | 3 days 3:08   |             | y       |                                                                   |
|   163 | c           | <2018-03-04 Sun 20:17> | <2018-03-04 Sun 22:17> | 0 days 2:00   |             | y       |                                                                   |
|   164 | d           | <2018-03-04 Sun 22:57> | <2018-03-06 Tue 19:15> | 1 days 20:18  |             | y       |                                                                   |
|   165 | c           | <2018-03-06 Tue 19:15> | <2018-03-06 Tue 23:15> | 0 days 4:00   |             | y       |                                                                   |
|   166 | d           | <2018-03-07 Wed 0:50>  | <2018-03-07 Wed 18:28> | 0 days 17:38  |             | y       |                                                                   |
|   167 | c           | <2018-03-07 Wed 18:29> | <2018-03-07 Wed 20:29> | 0 days 2:00   |             | y       |                                                                   |
|   168 | d           | <2018-03-07 Wed 20:37> | <2018-03-13 Tue 16:54> | 5 days 20:16  |             | y       |                                                                   |
|   169 | c           | <2018-03-13 Tue 16:55> | <2018-03-13 Tue 22:55> | 0 days 6:00   |             | y       |                                                                   |
|   170 | d           | <2018-03-13 Tue 23:19> | <2018-03-14 Wed 21:01> | 0 days 21:42  |           1 | y       | First shift including sun tracking                                |
|   171 | c           | <2018-03-14 Wed 21:01> | <2018-03-14 Wed 23:01> | 0 days 2:00   |             | y       |                                                                   |
|   172 | d           | <2018-03-14 Wed 23:06> | <2018-03-15 Thu 17:57> | 0 days 18:50  |           1 | y       |                                                                   |
|   173 | c           | <2018-03-15 Thu 17:59> | <2018-03-15 Thu 19:59> | 0 days 1:59   |             | y       |                                                                   |
|   174 | d           | <2018-03-15 Thu 20:39> | <2018-03-16 Fri 16:27> | 0 days 19:48  |           1 | y       |                                                                   |
|   175 | c           | <2018-03-16 Fri 16:28> | <2018-03-16 Fri 18:28> | 0 days 1:59   |             | y       |                                                                   |
|   176 | d           | <2018-03-16 Fri 18:35> | <2018-03-17 Sat 20:55> | 1 days 2:19   |           1 | y       |                                                                   |
|   177 | c           | <2018-03-17 Sat 20:55> | <2018-03-17 Sat 22:55> | 0 days 1:59   |             | y       |                                                                   |
|   178 | d           | <2018-03-17 Sat 23:31> | <2018-03-22 Thu 17:40> | 4 days 18:09  |           5 | y       |                                                                   |
|   179 | c           | <2018-03-22 Thu 17:41> | <2018-03-22 Thu 19:41> | 0 days 1:59   |             | y       |                                                                   |
|   180 | d           | <2018-03-22 Thu 20:47> | <2018-03-24 Sat 18:10> | 1 days 21:22  |           2 | y       |                                                                   |
|   181 | c           | <2018-03-24 Sat 18:10> | <2018-03-24 Sat 22:10> | 0 days 4:00   |             | y       |                                                                   |
|   182 | d           | <2018-03-24 Sat 23:32> | <2018-03-26 Mon 19:46> | 1 days 19:14  |           2 | y       | Last run including tracking                                       |
|   183 | c           | <2018-03-26 Mon 19:47> | <2018-03-26 Mon 23:47> | 0 days 3:59   |             | y       |                                                                   |
|   184 | d           | <2018-03-27 Tue 0:32>  | <2018-03-30 Fri 14:18> | 3 days 13:45  |             | y       |                                                                   |
|   185 | c           | <2018-03-30 Fri 14:18> | <2018-03-30 Fri 18:18> | 0 days 3:59   |             | y       |                                                                   |
|   186 | d           | <2018-03-30 Fri 19:03> | <2018-04-11 Wed 16:03> | 11 days 21:00 |             | y       |                                                                   |
|   187 | c           | <2018-04-11 Wed 16:04> | <2018-04-11 Wed 20:04> | 0 days 4:00   |             | y       |                                                                   |
|   188 | d           | <2018-04-11 Wed 20:53> | <2018-04-17 Tue 10:53> | 5 days 14:00  |             | y       | Last background data run of 2017/18                               |
|   189 | X*          | <2018-04-20 Fri 9:53>  | <2018-04-21 Sat 18:39> | 1 days 08:45  |             | y       | X-ray finger run <2018-04-20 Fri>                                 |

y* == located in 2018/BadRuns folder to not mix up with 'good' data
X* == X-ray finger run

*** Run 2 in 2018

Data taking for the second data taking period starts on
<2018-10-20 Sat 18:33> with a 2h calibration run after the detector
was finally fixed on <2018-10-19 Fri>. The issue was a bad soldering
joint on the Phoenix connector on the intermediate board.

| Run # | Type {d, c} | Start | End | Length       | # trackings | Backup? | Notes                  |
|-------+-------------+-------+-----+--------------+-------------+---------+------------------------|
|   239 | c           |       |     | 0 days 02:00 |             |         |                        |
|   240 | d           |       |     |              |           1 |         | no B field!            |
|   297 | d           |       |     |              |             |         | crazy noise at the end |
|   298 | d           |       |     |              |             |         | run without FADC       |


| Run # | Type          | DataType | Start                  | End                    | Length       | # trackings | # frames | # FADC Backup? | Backup? | Notes |
|-------+---------------+----------+------------------------+------------------------+--------------+-------------+----------+----------------+---------+-------|
|   239 | rtCalibration | rfNewTos | <1970-01-01 Thu 1:00>  | <1970-01-01 Thu 1:00>  | 0 days 02:00 |             |        0 |              0 | y       |       |
|   240 | rtBackground  | rfNewTos | <2018-10-21 Sun 14:54> | <2018-10-22 Mon 16:15> | 1 days 01:21 |             |    38753 |           4203 | y       |       |
|   239 | rtCalibration | rfNewTos | <2018-10-20 Sat 18:31> | <2018-10-20 Sat 20:31> | 0 days 02:00 |             |     9565 |           9518 | y       |       |
|   240 | rtBackground  | rfNewTos | <2018-10-21 Sun 14:54> | <2018-10-22 Mon 16:15> | 1 days 01:21 |             |    38753 |           4203 | y       |       |
|   239 | rtCalibration | rfNewTos | <2018-10-20 Sat 18:31> | <2018-10-20 Sat 20:31> | 0 days 02:00 |             |     9565 |           9518 | y       |       |
|   240 | rtBackground  | rfNewTos | <2018-10-21 Sun 14:54> | <2018-10-22 Mon 16:15> | 1 days 01:21 |             |    38753 |           4203 | y       |       |
|   239 | rtCalibration | rfNewTos | <2018-10-20 Sat 18:31> | <2018-10-20 Sat 20:31> | 0 days 02:00 |             |     9565 |           9518 | y       |       |
|   240 | rtBackground  | rfNewTos | <2018-10-21 Sun 14:54> | <2018-10-22 Mon 16:15> | 1 days 01:21 |             |    38753 |           4203 | y       |       |
|   241 | rtCalibration | rfNewTos | <2018-10-22 Mon 16:16> | <2018-10-22 Mon 18:16> | 0 days 02:00 |             |     9480 |           9426 | y       |       |
|   242 | rtBackground  | rfNewTos | <2018-10-22 Mon 18:44> | <2018-10-23 Tue 22:08> | 1 days 03:24 |             |    41933 |           4843 | y       |       |
|   243 | rtCalibration | rfNewTos | <2018-10-23 Tue 22:09> | <2018-10-24 Wed 0:09>  | 0 days 01:59 |             |     9488 |           9429 | y       |       |
|   244 | rtBackground  | rfNewTos | <2018-10-24 Wed 0:32>  | <2018-10-24 Wed 19:24> | 0 days 18:52 |             |    28870 |           3317 | y       |       |
|   245 | rtCalibration | rfNewTos | <2018-10-24 Wed 19:25> | <2018-10-24 Wed 21:25> | 0 days 01:59 |             |     9573 |           9530 | y       |       |
|   246 | rtBackground  | rfNewTos | <2018-10-24 Wed 21:59> | <2018-10-25 Thu 16:18> | 0 days 18:18 |             |    27970 |           2987 | y       |       |
|   247 | rtCalibration | rfNewTos | <2018-10-25 Thu 16:19> | <2018-10-25 Thu 18:19> | 0 days 01:59 |             |     9389 |           9334 | y       |       |
|   248 | rtBackground  | rfNewTos | <2018-10-25 Thu 18:25> | <2018-10-26 Fri 22:29> | 1 days 04:04 |             |    42871 |           4544 | y       |       |
|   249 | rtCalibration | rfNewTos | <2018-10-26 Fri 22:30> | <2018-10-27 Sat 0:30>  | 0 days 02:00 |             |     9473 |           9431 | y       |       |
|   250 | rtBackground  | rfNewTos | <2018-10-27 Sat 1:31>  | <2018-10-27 Sat 22:26> | 0 days 20:54 |             |    31961 |           3552 | y       |       |
|   251 | rtCalibration | rfNewTos | <2018-10-27 Sat 22:26> | <2018-10-28 Sun 0:26>  | 0 days 01:59 |             |     9551 |           9503 | y       |       |
|   252 | rtNone        | rfNewTos | <2018-10-28 Sun 0:59>  | <2018-10-28 Sun 2:20>  | 0 days 01:20 |             |     2060 |            214 | y       |       |
|   253 | rtCalibration | rfNewTos | <2018-10-28 Sun 19:18> | <2018-10-28 Sun 21:39> | 0 days 02:20 |             |    11095 |          11028 | y       |       |
|   254 | rtBackground  | rfNewTos | <2018-10-28 Sun 21:40> | <2018-10-29 Mon 23:03> | 1 days 01:23 |             |    38991 |           4990 | y       |       |
|   255 | rtCalibration | rfNewTos | <2018-10-29 Mon 23:03> | <2018-10-30 Tue 1:03>  | 0 days 02:00 |             |     9378 |           9330 | y       |       |
|   256 | rtBackground  | rfNewTos | <2018-10-30 Tue 1:49>  | <2018-10-31 Wed 22:18> | 1 days 20:29 |             |    68315 |           8769 | y       |       |
|   257 | rtCalibration | rfNewTos | <2018-10-31 Wed 22:19> | <2018-11-01 Thu 0:19>  | 0 days 01:59 |             |     9648 |           9592 | y       |       |
|   258 | rtBackground  | rfNewTos | <2018-11-01 Thu 0:20>  | <2018-11-01 Thu 16:15> | 0 days 15:55 |             |    24454 |           3103 | y       |       |
|   259 | rtCalibration | rfNewTos | <2018-11-01 Thu 16:16> | <2018-11-01 Thu 17:31> | 0 days 01:14 |             |     5900 |           5864 | y       |       |
|   260 | rtCalibration | rfNewTos | <2018-11-01 Thu 17:39> | <2018-11-01 Thu 19:09> | 0 days 01:30 |             |     7281 |           7251 | y       |       |
|   261 | rtBackground  | rfNewTos | <2018-11-01 Thu 19:39> | <2018-11-04 Sun 15:23> | 2 days 19:43 |             |   103658 |          12126 | y       |       |
|   262 | rtCalibration | rfNewTos | <2018-11-04 Sun 15:24> | <2018-11-04 Sun 21:24> | 0 days 05:59 |             |    28810 |          28681 | y       |       |
|   263 | rtBackground  | rfNewTos | <2018-11-05 Mon 0:35>  | <2018-11-05 Mon 20:28> | 0 days 19:52 |             |    30428 |           3610 | y       |       |
|   264 | rtCalibration | rfNewTos | <2018-11-05 Mon 20:28> | <2018-11-05 Mon 22:28> | 0 days 01:59 |             |     9595 |           9544 | y       |       |
|   265 | rtBackground  | rfNewTos | <2018-11-05 Mon 22:52> | <2018-11-07 Wed 22:14> | 1 days 23:21 |             |    72514 |           8429 | y       |       |
|   266 | rtCalibration | rfNewTos | <2018-11-07 Wed 22:14> | <2018-11-08 Thu 0:14>  | 0 days 01:59 |             |     9555 |           9506 | y       |       |
|   267 | rtBackground  | rfNewTos | <2018-11-08 Thu 2:05>  | <2018-11-08 Thu 6:54>  | 0 days 04:48 |             |     7405 |            930 | y       |       |
|   268 | rtBackground  | rfNewTos | <2018-11-09 Fri 6:15>  | <2018-11-09 Fri 17:20> | 0 days 11:04 |             |    16947 |           1974 | y       |       |
|   269 | rtCalibration | rfNewTos | <2018-11-09 Fri 17:20> | <2018-11-09 Fri 21:20> | 0 days 04:00 |             |    19382 |          19302 | y       |       |
|   270 | rtBackground  | rfNewTos | <2018-11-09 Fri 21:27> | <2018-11-11 Sun 21:02> | 1 days 23:34 |             |    72756 |           8078 | y       |       |
|   271 | rtCalibration | rfNewTos | <2018-11-11 Sun 21:03> | <2018-11-11 Sun 23:46> | 0 days 02:43 |             |    13015 |          12944 | y       |       |
|   272 | rtBackground  | rfNewTos | <2018-11-12 Mon 0:09>  | <2018-11-14 Wed 19:07> | 2 days 18:58 |             |   102360 |          11336 | y       |       |
|   273 | rtCalibration | rfNewTos | <2018-11-14 Wed 19:08> | <2018-11-14 Wed 21:08> | 0 days 01:59 |             |     9535 |           9471 | y       |       |
|   274 | rtBackground  | rfNewTos | <2018-11-14 Wed 21:28> | <2018-11-17 Sat 18:14> | 2 days 20:45 |             |   105187 |          12101 | y       |       |
|   275 | rtCalibration | rfNewTos | <2018-11-17 Sat 18:14> | <2018-11-17 Sat 20:57> | 0 days 02:43 |             |    13179 |          13116 | y       |       |
|   276 | rtBackground  | rfNewTos | <2018-11-17 Sat 22:08> | <2018-11-22 Thu 2:26>  | 4 days 04:17 |             |   153954 |          19640 | y       |       |
|   277 | rtCalibration | rfNewTos | <2018-11-22 Thu 2:26>  | <2018-11-22 Thu 16:14> | 0 days 13:48 |             |    66052 |          65749 | y       |       |
|   278 | rtBackground  | rfNewTos | <2018-11-22 Thu 16:14> | <2018-11-23 Fri 10:51> | 0 days 18:36 |             |    28899 |           3581 | y       |       |
|   279 | rtBackground  | rfNewTos | <2018-11-24 Sat 10:51> | <2018-11-26 Mon 14:58> | 2 days 04:07 |             |    79848 |           9677 | y       |       |
|   280 | rtCalibration | rfNewTos | <2018-11-26 Mon 14:59> | <2018-11-26 Mon 18:59> | 0 days 04:00 |             |    19189 |          19112 | y       |       |
|   281 | rtBackground  | rfNewTos | <2018-11-26 Mon 19:02> | <2018-11-28 Wed 18:07> | 1 days 23:04 |             |    72230 |           8860 | y       |       |
|   282 | rtCalibration | rfNewTos | <2018-11-28 Wed 18:07> | <2018-11-28 Wed 20:51> | 0 days 02:43 |             |    12924 |          12860 | y       |       |
|   283 | rtBackground  | rfNewTos | <2018-11-28 Wed 22:31> | <2018-12-01 Sat 14:38> | 2 days 16:07 |             |    98246 |          11965 | y       |       |
|   284 | rtCalibration | rfNewTos | <2018-12-01 Sat 14:39> | <2018-12-01 Sat 18:39> | 0 days 03:59 |             |    19017 |          18904 | y       |       |
|   285 | rtBackground  | rfNewTos | <2018-12-01 Sat 19:06> | <2018-12-03 Mon 19:39> | 2 days 00:33 |             |    74431 |           8888 | y       |       |
|   286 | rtCalibration | rfNewTos | <2018-12-04 Tue 15:57> | <2018-12-04 Tue 17:57> | 0 days 02:00 |             |     9766 |           9715 | y       |       |
|   287 | rtBackground  | rfNewTos | <2018-12-04 Tue 19:07> | <2018-12-05 Wed 15:08> | 0 days 20:01 |             |    30622 |           3395 | y       |       |
|   288 | rtCalibration | rfNewTos | <2018-12-05 Wed 17:28> | <2018-12-05 Wed 19:28> | 0 days 02:00 |             |     9495 |           9443 | y       |       |
|   289 | rtBackground  | rfNewTos | <2018-12-05 Wed 23:07> | <2018-12-06 Thu 19:11> | 0 days 20:03 |             |    30629 |           3269 | y       |       |
|   290 | rtCalibration | rfNewTos | <2018-12-06 Thu 19:11> | <2018-12-06 Thu 21:11> | 0 days 02:00 |             |     9457 |           9394 | y       |       |
|   291 | rtBackground  | rfNewTos | <2018-12-06 Thu 23:14> | <2018-12-08 Sat 13:39> | 1 days 14:24 |             |    58602 |           6133 | y       |       |
|   292 | rtCalibration | rfNewTos | <2018-12-08 Sat 13:39> | <2018-12-08 Sat 15:39> | 0 days 02:00 |             |     9475 |           9426 | y       |       |
|   293 | rtBackground  | rfNewTos | <2018-12-08 Sat 17:42> | <2018-12-10 Mon 21:50> | 2 days 04:07 |             |    79677 |           8850 | y       |       |
|   294 | rtCalibration | rfNewTos | <2018-12-10 Mon 21:50> | <2018-12-10 Mon 23:50> | 0 days 02:00 |             |     9514 |           9467 | y       |       |
|   295 | rtBackground  | rfNewTos | <2018-12-11 Tue 0:54>  | <2018-12-11 Tue 20:31> | 0 days 19:37 |             |    29981 |           3271 | y       |       |
|   296 | rtCalibration | rfNewTos | <2018-12-11 Tue 20:31> | <2018-12-11 Tue 22:31> | 0 days 02:00 |             |     9565 |           9517 | y       |       |
|   297 | rtBackground  | rfNewTos | <2018-12-12 Wed 0:14>  | <2018-12-13 Thu 18:30> | 1 days 18:15 |             |    68124 |          12530 | y       |       |
|   298 | rtBackground  | rfNewTos | <2018-12-13 Thu 18:39> | <2018-12-15 Sat 6:41>  | 1 days 12:01 |             |    53497 |              0 | y       |       |
|   299 | rtBackground  | rfNewTos | <2018-12-15 Sat 6:43>  | <2018-12-15 Sat 18:13> | 0 days 11:29 |             |    17061 |              0 | y       |       |
|   300 | rtCalibration | rfNewTos | <2018-12-15 Sat 18:38> | <2018-12-15 Sat 20:38> | 0 days 02:00 |             |     9466 |           9415 | y       |       |
|   301 | rtBackground  | rfNewTos | <2018-12-15 Sat 21:34> | <2018-12-17 Mon 14:17> | 1 days 16:43 |             |    62454 |           7751 | y       |       |
|   302 | rtCalibration | rfNewTos | <2018-12-17 Mon 14:18> | <2018-12-17 Mon 16:18> | 0 days 01:59 |             |     9616 |           9577 | y       |       |
|   303 | rtBackground  | rfNewTos | <2018-12-17 Mon 16:52> | <2018-12-18 Tue 16:41> | 0 days 23:48 |             |    36583 |           4571 | y       |       |
|   304 | rtCalibration | rfNewTos | <2018-12-19 Wed 9:33>  | <2018-12-19 Wed 11:33> | 0 days 01:59 |             |     9531 |           9465 | y       |       |
|   305 | rtCalibration | rfNewTos | <2018-12-19 Wed 13:24> | <2018-12-20 Thu 3:23>  | 0 days 13:58 |             |    32655 |          25702 | y*      |       |
|   306 | rtBackground  | rfNewTos | <2018-12-20 Thu 6:55>  | <2018-12-20 Thu 11:53> | 0 days 04:58 |             |     7574 |            496 | y       |       |


NOTE: Run 305 is considered a *bad* run now! 
See [[docs/statusAndProgress.org]] for more information. It is *not* a
calibration run. Probably it's just a background run, but the
confusion makes me not want to trust it!

y* == located in 2018/BadRuns folder to not mix up with 'good' data
X* == X-ray finger run


X-ray finger runs are apparently number 21 (old before data taking??)
and then 189 at the end of the 2017/18 data taking in Apr 2018. See
[[file:analysis.org]] for reference to run 21.


*** Total run time

After Run 96:
- Currently 23.85625 days (552.5 hours) of data taking (raw, beginning to end)
  - of that were 10 hours calibration
  - 17 shifts a 90 minutes => 25.5 hours
  => - 517 hours background
     - 10 hours calibration
     - 25.5 hours tracking

After Run 116 (since excl. 96):
- another 449.95 hours of data taking
  - of that were 16 shifts => ~24 hours tracking
  - 10 hours of calibration
=> - 417.95 hours of background
   - 24 hours of tracking
   - 10 hours of calibration

Combined so far:
- background: 934.95 h
- tracking: 49.5 h
- calibration: 20 h

Up to incl. 128 (since excl. 116):
- 4 days, 100 hours, 271 minutes background
- 200.5 hours of background + tracking
- 8 shifts since run 117 incl = 12 hours
- calibration: 22 hours
=> - 188 h background
   - 12 h tracking
   - 22 h calibration

Combined Runs 2017:
- background: 1123 h
- tracking: 61.5 h
- calibration: 42 h


Run time 2018 from run 145 to run 167:
- total time: 8 days + 211 hours + 578 minutes = 17 days +
  4 hours + 38 minutes
- background + tracking: 15 days + 5 hours + 3.5 minutes ~ 365 h
- background (90 minutes tracking): 342.5 h
- tracking: 22.5 h
- calibration: 31 hours + 17 minutes ~ 31.5 h

Run time 2018 from run 168 to Run 187:
- background + tracking: 26 days + 172 hours + 265 minutes = 33 days + 8 hours +
  25 minutes ~ 800.5 h
- # trackings: 13
- background: 781 h
- tracking: 19.5 h
- calibration: 32 hours


Total time of Run 2017 / 2018:
background: 2288 h
tracking: 103.5 h
calibration: 105.5 h



** Data runs

This section covers the data runs, which took place.

The runs, unless otherwise stated, are using the FSR files as in the
Septem H calibration Git repo in the folder fsr_in_use. These are the
THL values, which were obtained during calibration, but have THL+50
for chips 1, 2, 5 and 6.
The HV values are the ones documented in the detector documentation
[[file:Doc/Detector/CastDetectorDocumentation.org]]
(as of <2017-11-09 Do 19:05>).

[[file:/ssh:tpc@tpc00:/volume1/cast/data/2017_CAST-Run/DataRuns/Run_76_171030-18-39.tar.gz]]

[[file:/ssh:tpc@tpc00:/volume1/cast/data/2017_CAST-Run/DataRuns/Run_77_171102-05-24.tar.gz]]

[[file:/ssh:tpc@tpc00:/volume1/cast/data/2017_CAST-Run/DataRuns/Run_78_171103-05-28.tar.gz]]

[[file:/ssh:tpc@tpc00:/volume1/cast/data/2017_CAST-Run/DataRuns/Run_79_171103-20-46.tar.gz]]

[[file:/ssh:tpc@tpc00:/volume1/cast/data/2017_CAST-Run/DataRuns/Run_80_171105-00-09.tar.gz]]

[[file:/ssh:tpc@tpc00:/volume1/cast/data/2017_CAST-Run/DataRuns/Run_81_171105-23-54.tar.gz]]

[[file:/ssh:tpc@tpc00:/volume1/cast/data/2017_CAST-Run/DataRuns/Run_82_171107-00-01.tar.gz]]

*NOTE:* this was the first run with the correct SiPM HV setting of $\SI{65.6}{\volt}$
[[file:/ssh:tpc@tpc00:/volume1/cast/data/2017_CAST-Run/DataRuns/Run_84_171108-17-49.tar.gz]]

*NOTE*: this run was stopped early to fix the src/waitconditions.cpp
bug, mentioned in the note below.
[[file:/ssh:tpc@tpc00:/volume1/cast/data/2017_CAST-Run/DataRuns/Run_85_171109-19-01.tar.gz]]

*NOTE: VERY IMPORTANT*
In all runs above, there was a bug in src/waitconditions.cpp, which
cause the FADC and scintillator values to be written the all
subsequent files, from an event in which the FADC triggered until the
next (in case of non-subsequent FADC events).
Therefore: for analysis, we need to take into account
- fadcReadout == 1.
Otherwise we read random scintillator trigger values as well as FADC
trigger clock cycles.

All runs below are without the aforementioned error.

[[file:/ssh:tpc@tpc00:/volume1/cast/data/2017_CAST-Run/DataRuns/Run_86_171109-21-47.tar.gz]]

[[file:/ssh:tpc@tpc00:/volume1/cast/data/2017_CAST-Run/DataRuns/Run_87_171111-02-17.tar.gz]]

*NOTE:* During Run 89 the byobu buffer containing TOS got stuck on
<2017-11-12 So 19:35> due to <F7> being pressed (which eventually
pauses the thread). Was called by Cristian at <2017-11-13 Mo 5:55>
roughly. I fixed the issue. Therefore the length given in the table is
misleading, as it does not show the actual time of data taking of that
run.

[[file:/ssh:tpc@tpc00:/volume1/cast/data/2017_CAST-Run/DataRuns/Run_89_171112-15-30.tar.gz]]

[[file:/ssh:tpc@tpc00:/volume1/cast/data/2017_CAST-Run/DataRuns/Run_90_171113-19-14.tar.gz]]

[[file:/ssh:tpc@tpc00:/volume1/cast/data/2017_CAST-Run/DataRuns/Run_91_171114-20-24.tar.gz]]

[[file:/ssh:tpc@tpc00:/volume1/cast/data/2017_CAST-Run/DataRuns/Run_92_171115-21-45.tar.gz]]

[[file:/ssh:tpc@tpc00:/volume1/cast/data/2017_CAST-Run/DataRuns/Run_94_171117-20-48.tar.gz]]

*NOTE:* The following run contains the beginning (and most of it) of
the GRID measurement. It only contains a single tracking, that's why
the run is so long.

[[file:/ssh:tpc@tpc00:/volume1/cast/data/2017_CAST-Run/DataRuns/Run_95_171119-02-35.tar.gz]]

*NOTE:* Regarding Run 97, morning shift on 25/11/17, from the elog:
#+BEGIN_SRC
Remarks: At approx. 7:02 there was an error message in the Slow
Control program indicating that some file could not be saved because
there was not enough memory. After about a minute, the Slow Control PC
restarted by itself. At about the same time, an error message appeared
in the Tracking program, probably related to the fact that it could
not communicate with the Slow Control program in order to read
proximity sensor data. After the restart of the Slow control PC and
Slow control program, things went back to normal. Because of all that,
we lost first 3-4 minutes of tracking
#+END_SRC

*NOTE: Still need to add links to previous runs (97, 98, 99)*

*NOTE:* Regarding Run 101: Because of pretty bad noise during the run,
I decided to play around with the main amplifier of the analogue
signal. I changed the settings as followed:
- Diff: 50 ns -> 20 ns (one to left)
- Coarse gain: 6x -> 10x (one to right)
this got rid of (almost ?) all of the noise. However, obviously this
changes the FADC data completely. The shapes are slightly altered (a
little steeper)

*NOTE:* Regarding Run 109: CRAZY amounts of noise during that
run. Interestingly, the only difference between the previous runs from
what I can tell (although I wasn't present in the shift), was that the
main light in the LHCb part of the hall was turned on. Maybe this
causes the electricity circuit to be working under high load,
producing a lot more noise? Will keep an eye on this.

*NOTE:* Regarding Run 111: Was stopped early, because I tried to debug
the noise and in doing so burned a fuse in the gas interlock box by
connecting the NIM crate to a wrong power cable.
-> no shift on 6/12/17, background data being taken again since <2017-12-06 Wed 14:55>.

*NOTE:* Regarding Run 112: Another change of FADC settings due to crazy amounts of
noise. Changed integration time from:
- 50ns -> 100ns
gets rid of all noise for now. However, shapes are much smoother than
before, might make differentiation much harder later.
This was done at around <2017-12-07 Thu 8:00>
*Also:* The power cable from the main amplifier to the pre amplifier
was not properly inserted. Did that before changing the settings,
seeemed to help, but noise eventually returned.

*NOTE:* Regarding Run 112 and previous note: Turned down integration
time from 100ns to 50ns again at around <2017-12-08 Fri 17:50>.

*NOTE:* Regarding Run 112: Run is so long, because after problems with
fuse, had quench on <2017-12-07 Thu 18:31:53>.

*NOTE:* Regarding Run 121: Jochen set the FADC main amplifier
integration time from 50 to 100 ns again. Happened at around
<2017-12-15 Fri 10:20>. Maybe 5 min later.

*** Notes 2018

After Run 137 (first run of 2018) I did a calibration run only to
notice that we didn't recover 220 electrons anymore, but
rather 110. THLscan revealed that indeed the THL values of the central
chips (and potentially all others) changed.

Previously we used:
Chip #: 3
THL: 450

Now a value of:
THL: 400

produces no noise, if run without source on 2.4s frames, and recovers
all electrons again, it seems.

- Calibration Run 138 uses THL 450
- Calibration Run 139 uses THL 400

Change is reflected in CAST calibration Git repository, in fsr_as_used
folder!

**** Power Problem

Problem due to power supply, causing what looked like changes to the
thresholds of all chips.

See the following mail for a short explanation:
[[file:Mails/cast_power_supply_problem_thlshift/power_supply_problem.org]]

**** Run 297 and 298
There was some pretty crazy noise in run 297 (see Sergios video on
WhatsApp), so I disabled the FADC on <2018-12-13 Thu 18:40> for Run
298, since I'm heading down to CERN on <2018-12-14 Fri>.


** Calibration runs

This scetion covers the calibration runs, which took place.

[[file:/ssh:tpc@tpc00:/volume1/cast/data/2017_CAST-Run/CalibrationRuns/Run_83_171108-16-27.tar.gz]]

[[file:/ssh:tpc@tpc00:/volume1/cast/data/2017_CAST-Run/CalibrationRuns/Run_88_171112-14-30.tar.gz]]

[[file:/ssh:tpc@tpc00:/volume1/cast/data/2017_CAST-Run/CalibrationRuns/Run_93_171117-19-18.tar.gz]]

[[file:/ssh:tpc@tpc00:/volume1/cast/data/2017_CAST-Run/CalibrationRuns/Run_96_171123-10-42.tar.gz]]



** Automatically generated run list

The following run list is created by the =writeRunList= tool:
 [[file:~/CastData/ExternCode/TimepixAnalysis/Tools/writeRunList/writeRunList.nim]]
based on the tracking logs.

 | Run # | Type          | DataType | Start                  | End                    | Length        | # trackings | # frames | # FADC | Backup? | Notes |
 |-------+---------------+----------+------------------------+------------------------+---------------+-------------+----------+--------+---------+-------|
 |    76 | rtBackground  | rfNewTos | <2017-10-30 Mon 18:39> | <2017-11-02 Thu 5:24>  | 2 days 10:44  |           1 |    88249 |  19856 | y       |       |
 |    77 | rtBackground  | rfNewTos | <2017-11-02 Thu 5:24>  | <2017-11-03 Fri 5:28>  | 1 days 00:03  |           1 |    36074 |   8016 | y       |       |
 |    78 | rtBackground  | rfNewTos | <2017-11-03 Fri 5:28>  | <2017-11-03 Fri 20:45> | 0 days 15:17  |           1 |    23506 |   5988 | y       |       |
 |    79 | rtBackground  | rfNewTos | <2017-11-03 Fri 20:46> | <2017-11-05 Sun 0:09>  | 1 days 03:22  |           1 |    40634 |   8102 | y       |       |
 |    80 | rtBackground  | rfNewTos | <2017-11-05 Sun 0:09>  | <2017-11-05 Sun 23:50> | 0 days 23:40  |           1 |    35147 |   6880 | y       |       |
 |    81 | rtBackground  | rfNewTos | <2017-11-05 Sun 23:54> | <2017-11-07 Tue 0:00>  | 1 days 00:06  |           1 |    35856 |   7283 | y       |       |
 |    82 | rtBackground  | rfNewTos | <2017-11-07 Tue 0:01>  | <2017-11-08 Wed 15:58> | 1 days 15:56  |           2 |    59502 |  12272 | y       |       |
 |    83 | rtCalibration | rfNewTos | <2017-11-08 Wed 16:27> | <2017-11-08 Wed 17:27> | 0 days 00:59  |           0 |     4915 |   4897 | y       |       |
 |    84 | rtBackground  | rfNewTos | <2017-11-08 Wed 17:49> | <2017-11-09 Thu 19:01> | 1 days 01:11  |           1 |    37391 |   7551 | y       |       |
 |    85 | rtBackground  | rfNewTos | <2017-11-09 Thu 19:01> | <2017-11-09 Thu 21:46> | 0 days 02:45  |           0 |     4104 |    899 | y       |       |
 |    86 | rtBackground  | rfNewTos | <2017-11-09 Thu 21:47> | <2017-11-11 Sat 2:17>  | 1 days 04:29  |           1 |    42396 |   9656 | y       |       |
 |    87 | rtBackground  | rfNewTos | <2017-11-11 Sat 2:17>  | <2017-11-12 Sun 14:29> | 1 days 12:11  |           2 |    54786 |  15123 | y       |       |
 |    88 | rtCalibration | rfNewTos | <2017-11-12 Sun 14:30> | <2017-11-12 Sun 15:30> | 0 days 00:59  |           0 |     4943 |   4934 | y       |       |
 |    89 | rtBackground  | rfNewTos | <2017-11-12 Sun 15:30> | <2017-11-13 Mon 18:27> | 1 days 02:57  |           1 |    25209 |   6210 | y       |       |
 |    90 | rtBackground  | rfNewTos | <2017-11-13 Mon 19:14> | <2017-11-14 Tue 20:24> | 1 days 01:09  |           1 |    37497 |   8122 | y       |       |
 |    91 | rtBackground  | rfNewTos | <2017-11-14 Tue 20:24> | <2017-11-15 Wed 21:44> | 1 days 01:20  |           1 |    37732 |   8108 | y       |       |
 |    92 | rtBackground  | rfNewTos | <2017-11-15 Wed 21:45> | <2017-11-17 Fri 19:18> | 1 days 21:32  |           1 |    67946 |  14730 | y       |       |
 |    93 | rtCalibration | rfNewTos | <2017-11-17 Fri 19:18> | <2017-11-17 Fri 20:18> | 0 days 01:00  |           0 |     4977 |   4968 | y       |       |
 |    94 | rtBackground  | rfNewTos | <2017-11-17 Fri 20:48> | <2017-11-19 Sun 2:34>  | 1 days 05:46  |           1 |    44344 |   9422 | y       |       |
 |    95 | rtBackground  | rfNewTos | <2017-11-19 Sun 2:35>  | <2017-11-23 Thu 10:41> | 4 days 08:06  |           1 |   154959 |  33112 | y       |       |
 |    96 | rtCalibration | rfNewTos | <2017-11-23 Thu 10:42> | <2017-11-23 Thu 17:43> | 0 days 07:01  |           0 |    34586 |  34496 | y       |       |
 |    97 | rtBackground  | rfNewTos | <2017-11-23 Thu 17:43> | <2017-11-26 Sun 1:41>  | 2 days 07:57  |           1 |    83404 |  18277 | y       |       |
 |    98 | rtBackground  | rfNewTos | <2017-11-26 Sun 1:42>  | <2017-11-26 Sun 21:18> | 0 days 19:36  |           1 |    29202 |   6285 | y       |       |
 |    99 | rtBackground  | rfNewTos | <2017-11-26 Sun 21:18> | <2017-11-28 Tue 6:46>  | 1 days 09:27  |           1 |    49921 |  10895 | y       |       |
 |   100 | rtBackground  | rfNewTos | <2017-11-28 Tue 6:46>  | <2017-11-29 Wed 6:40>  | 0 days 23:53  |           1 |    35658 |   7841 | y       |       |
 |   101 | rtBackground  | rfNewTos | <2017-11-29 Wed 6:40>  | <2017-11-29 Wed 20:18> | 0 days 13:37  |           1 |    20326 |   4203 | y       |       |
 |   102 | rtCalibration | rfNewTos | <2017-11-29 Wed 20:19> | <2017-11-29 Wed 22:19> | 0 days 02:00  |           0 |     9919 |   9898 | y       |       |
 |   103 | rtBackground  | rfNewTos | <2017-11-29 Wed 22:26> | <2017-12-01 Fri 6:46>  | 1 days 08:19  |           1 |    47381 |   7867 | y       |       |
 |   104 | rtBackground  | rfNewTos | <2017-12-01 Fri 6:47>  | <2017-12-02 Sat 6:48>  | 1 days 00:00  |           1 |    35220 |   5866 | y       |       |
 |   105 | rtBackground  | rfNewTos | <2017-12-02 Sat 6:48>  | <2017-12-03 Sun 6:39>  | 0 days 23:51  |           1 |    34918 |   5794 | y       |       |
 |   106 | rtBackground  | rfNewTos | <2017-12-03 Sun 6:40>  | <2017-12-04 Mon 6:54>  | 1 days 00:14  |           1 |    35576 |   6018 | y       |       |
 |   107 | rtBackground  | rfNewTos | <2017-12-04 Mon 6:54>  | <2017-12-04 Mon 13:38> | 0 days 06:44  |           1 |     9883 |   1641 | y       |       |
 |   108 | rtCalibration | rfNewTos | <2017-12-04 Mon 13:39> | <2017-12-04 Mon 17:39> | 0 days 04:00  |           0 |    19503 |  19448 | y       |       |
 |   109 | rtBackground  | rfNewTos | <2017-12-04 Mon 17:47> | <2017-12-05 Tue 11:20> | 0 days 17:32  |           1 |    28402 |   8217 | y       |       |
 |   110 | rtCalibration | rfNewTos | <2017-12-05 Tue 11:20> | <2017-12-05 Tue 13:20> | 0 days 01:59  |           0 |     9804 |   9786 | y       |       |
 |   111 | rtBackground  | rfNewTos | <2017-12-05 Tue 13:23> | <2017-12-05 Tue 16:17> | 0 days 02:53  |           0 |     4244 |    644 | y       |       |
 |   112 | rtBackground  | rfNewTos | <2017-12-06 Wed 14:50> | <2017-12-10 Sun 6:46>  | 3 days 15:55  |           2 |   128931 |  19607 | y       |       |
 |   113 | rtBackground  | rfNewTos | <2017-12-10 Sun 6:46>  | <2017-12-11 Mon 6:49>  | 1 days 00:03  |           1 |    35100 |   5174 | y       |       |
 |   114 | rtBackground  | rfNewTos | <2017-12-11 Mon 6:50>  | <2017-12-11 Mon 18:33> | 0 days 11:43  |           1 |    17111 |   2542 | y       |       |
 |   115 | rtBackground  | rfNewTos | <2017-12-11 Mon 18:36> | <2017-12-12 Tue 20:58> | 1 days 02:21  |           1 |    40574 |   9409 | y       |       |
 |   116 | rtCalibration | rfNewTos | <2017-12-12 Tue 20:59> | <2017-12-12 Tue 22:59> | 0 days 02:00  |           0 |     9741 |   9724 | y       |       |
 |   117 | rtBackground  | rfNewTos | <2017-12-12 Tue 23:56> | <2017-12-13 Wed 21:29> | 0 days 21:33  |           1 |    31885 |   5599 | y       |       |
 |   118 | rtCalibration | rfNewTos | <2017-12-13 Wed 21:30> | <2017-12-13 Wed 23:30> | 0 days 02:00  |           0 |     9771 |   9748 | y       |       |
 |   119 | rtBackground  | rfNewTos | <2017-12-14 Thu 0:07>  | <2017-12-14 Thu 17:04> | 0 days 16:57  |           1 |    25434 |   4903 | y       |       |
 |   120 | rtCalibration | rfNewTos | <2017-12-14 Thu 17:04> | <2017-12-14 Thu 21:04> | 0 days 04:00  |           0 |    19308 |  19261 | y       |       |
 |   121 | rtBackground  | rfNewTos | <2017-12-14 Thu 21:07> | <2017-12-15 Fri 19:22> | 0 days 22:14  |           1 |    33901 |   6947 | y       |       |
 |   122 | rtCalibration | rfNewTos | <2017-12-15 Fri 19:22> | <2017-12-16 Sat 1:20>  | 0 days 05:57  |           0 |    29279 |  29208 | y       |       |
 |   123 | rtBackground  | rfNewTos | <2017-12-16 Sat 1:21>  | <2017-12-17 Sun 1:06>  | 0 days 23:45  |           1 |    34107 |   3380 | y       |       |
 |   124 | rtBackground  | rfNewTos | <2017-12-17 Sun 1:06>  | <2017-12-19 Tue 2:57>  | 2 days 01:50  |           2 |    71703 |   7504 | y       |       |
 |   125 | rtBackground  | rfNewTos | <2017-12-19 Tue 2:57>  | <2017-12-19 Tue 16:20> | 0 days 13:22  |           1 |    19262 |   1991 | y       |       |
 |   126 | rtCalibration | rfNewTos | <2017-12-19 Tue 16:21> | <2017-12-19 Tue 19:21> | 0 days 02:59  |           0 |    14729 |  14689 | y       |       |
 |   127 | rtBackground  | rfNewTos | <2017-12-19 Tue 19:27> | <2017-12-22 Fri 0:17>  | 2 days 04:50  |           1 |    75907 |   7663 | y       |       |
 |   128 | rtCalibration | rfNewTos | <2017-12-22 Fri 0:18>  | <2017-12-22 Fri 9:23>  | 0 days 09:05  |           0 |    44806 |  44709 | y       |       |
 |   145 | rtCalibration | rfNewTos | <2018-02-17 Sat 17:18> | <2018-02-17 Sat 20:40> | 0 days 03:22  |           0 |    16797 |  16796 | y       |       |
 |   146 | rtBackground  | rfNewTos | <2018-02-17 Sat 20:41> | <2018-02-18 Sun 18:12> | 0 days 21:30  |           1 |    32705 |   3054 | y       |       |
 |   147 | rtCalibration | rfNewTos | <2018-02-18 Sun 18:12> | <2018-02-18 Sun 20:12> | 0 days 01:59  |           0 |    10102 |  10102 | y       |       |
 |   148 | rtBackground  | rfNewTos | <2018-02-18 Sun 20:46> | <2018-02-19 Mon 17:24> | 0 days 20:37  |           1 |    31433 |   3120 | y       |       |
 |   149 | rtCalibration | rfNewTos | <2018-02-19 Mon 17:25> | <2018-02-19 Mon 19:25> | 0 days 02:00  |           0 |     9975 |   9975 | y       |       |
 |   150 | rtBackground  | rfNewTos | <2018-02-19 Mon 19:53> | <2018-02-20 Tue 17:36> | 0 days 21:42  |           1 |    33192 |   3546 | y       |       |
 |   151 | rtCalibration | rfNewTos | <2018-02-20 Tue 17:36> | <2018-02-20 Tue 19:36> | 0 days 01:59  |           0 |     9907 |   9907 | y       |       |
 |   152 | rtBackground  | rfNewTos | <2018-02-20 Tue 21:54> | <2018-02-21 Wed 18:05> | 0 days 20:10  |           1 |    30809 |   3319 | y       |       |
 |   153 | rtCalibration | rfNewTos | <2018-02-21 Wed 18:05> | <2018-02-21 Wed 20:05> | 0 days 01:59  |           0 |    10103 |  10102 | y       |       |
 |   154 | rtBackground  | rfNewTos | <2018-02-21 Wed 21:10> | <2018-02-22 Thu 17:23> | 0 days 20:12  |           1 |    30891 |   3426 | y       |       |
 |   155 | rtCalibration | rfNewTos | <2018-02-22 Thu 17:23> | <2018-02-22 Thu 19:23> | 0 days 02:00  |           0 |     9861 |   9861 | y       |       |
 |   156 | rtBackground  | rfNewTos | <2018-02-23 Fri 6:06>  | <2018-02-23 Fri 17:41> | 0 days 11:35  |           1 |    17686 |   1866 | y       |       |
 |   157 | rtCalibration | rfNewTos | <2018-02-23 Fri 17:41> | <2018-02-23 Fri 19:41> | 0 days 01:59  |           0 |     9962 |   9962 | y       |       |
 |   158 | rtBackground  | rfNewTos | <2018-02-23 Fri 19:42> | <2018-02-26 Mon 8:46>  | 2 days 13:03  |           1 |    93205 |   9893 | y       |       |
 |   159 | rtCalibration | rfNewTos | <2018-02-26 Mon 8:46>  | <2018-02-26 Mon 12:46> | 0 days 04:00  |           0 |    19879 |  19878 | y       |       |
 |   160 | rtBackground  | rfNewTos | <2018-02-26 Mon 14:56> | <2018-03-01 Thu 10:24> | 2 days 19:28  |           1 |   103145 |  11415 | y       |       |
 |   161 | rtCalibration | rfNewTos | <2018-03-01 Thu 10:26> | <2018-03-01 Thu 14:26> | 0 days 04:00  |           0 |    19944 |  19943 | y       |       |
 |   162 | rtBackground  | rfNewTos | <2018-03-01 Thu 17:07> | <2018-03-04 Sun 20:16> | 3 days 03:08  |           3 |   114590 |  11897 | y       |       |
 |   163 | rtCalibration | rfNewTos | <2018-03-04 Sun 20:17> | <2018-03-04 Sun 22:17> | 0 days 02:00  |           0 |    10093 |  10093 | y       |       |
 |   164 | rtBackground  | rfNewTos | <2018-03-04 Sun 22:57> | <2018-03-06 Tue 19:15> | 1 days 20:18  |           2 |    67456 |   6488 | y       |       |
 |   165 | rtCalibration | rfNewTos | <2018-03-06 Tue 19:15> | <2018-03-06 Tue 23:15> | 0 days 04:00  |           0 |    19882 |  19879 | y       |       |
 |   166 | rtBackground  | rfNewTos | <2018-03-07 Wed 0:50>  | <2018-03-07 Wed 18:28> | 0 days 17:38  |           1 |    26859 |   2565 | y       |       |
 |   167 | rtCalibration | rfNewTos | <2018-03-07 Wed 18:29> | <2018-03-07 Wed 20:29> | 0 days 02:00  |           0 |     9938 |   9938 | y       |       |
 |   168 | rtBackground  | rfNewTos | <2018-03-07 Wed 20:37> | <2018-03-13 Tue 16:54> | 5 days 20:16  |           0 |   213545 |  20669 | y       |       |
 |   169 | rtCalibration | rfNewTos | <2018-03-13 Tue 16:55> | <2018-03-13 Tue 22:55> | 0 days 06:00  |           0 |    29874 |  29874 | y       |       |
 |   170 | rtBackground  | rfNewTos | <2018-03-13 Tue 23:19> | <2018-03-14 Wed 21:01> | 0 days 21:42  |           1 |    33098 |   3269 | y       |       |
 |   171 | rtCalibration | rfNewTos | <2018-03-14 Wed 21:01> | <2018-03-14 Wed 23:01> | 0 days 02:00  |           0 |     9999 |   9999 | y       |       |
 |   172 | rtBackground  | rfNewTos | <2018-03-14 Wed 23:06> | <2018-03-15 Thu 17:57> | 0 days 18:50  |           1 |    28649 |   2773 | y       |       |
 |   173 | rtCalibration | rfNewTos | <2018-03-15 Thu 17:59> | <2018-03-15 Thu 19:59> | 0 days 01:59  |           0 |     9898 |   9897 | y       |       |
 |   174 | rtBackground  | rfNewTos | <2018-03-15 Thu 20:39> | <2018-03-16 Fri 16:27> | 0 days 19:48  |           1 |    30163 |   2961 | y       |       |
 |   175 | rtCalibration | rfNewTos | <2018-03-16 Fri 16:28> | <2018-03-16 Fri 18:28> | 0 days 01:59  |           0 |    10075 |  10075 | y       |       |
 |   176 | rtBackground  | rfNewTos | <2018-03-16 Fri 18:35> | <2018-03-17 Sat 20:55> | 1 days 02:19  |           1 |    40084 |   3815 | y       |       |
 |   177 | rtCalibration | rfNewTos | <2018-03-17 Sat 20:55> | <2018-03-17 Sat 22:55> | 0 days 01:59  |           0 |     9967 |   9966 | y       |       |
 |   178 | rtBackground  | rfNewTos | <2018-03-17 Sat 23:31> | <2018-03-22 Thu 17:40> | 4 days 18:09  |           5 |   174074 |  17949 | y       |       |
 |   179 | rtCalibration | rfNewTos | <2018-03-22 Thu 17:41> | <2018-03-22 Thu 19:41> | 0 days 01:59  |           0 |     9887 |   9887 | y       |       |
 |   180 | rtBackground  | rfNewTos | <2018-03-22 Thu 20:47> | <2018-03-24 Sat 18:10> | 1 days 21:22  |           1 |    69224 |   7423 | y       |       |
 |   181 | rtCalibration | rfNewTos | <2018-03-24 Sat 18:10> | <2018-03-24 Sat 22:10> | 0 days 04:00  |           0 |    20037 |  20036 | y       |       |
 |   182 | rtBackground  | rfNewTos | <2018-03-24 Sat 23:32> | <2018-03-26 Mon 19:46> | 1 days 19:14  |           2 |    65888 |   6694 | y       |       |
 |   183 | rtCalibration | rfNewTos | <2018-03-26 Mon 19:47> | <2018-03-26 Mon 23:47> | 0 days 03:59  |           0 |    20026 |  20026 | y       |       |
 |   184 | rtBackground  | rfNewTos | <2018-03-27 Tue 0:32>  | <2018-03-30 Fri 14:18> | 3 days 13:45  |           0 |   130576 |  12883 | y       |       |
 |   185 | rtCalibration | rfNewTos | <2018-03-30 Fri 14:18> | <2018-03-30 Fri 18:18> | 0 days 03:59  |           0 |    19901 |  19901 | y       |       |
 |   186 | rtBackground  | rfNewTos | <2018-03-30 Fri 19:03> | <2018-04-11 Wed 16:03> | 11 days 21:00 |           0 |   434087 |  42830 | y       |       |
 |   187 | rtCalibration | rfNewTos | <2018-04-11 Wed 16:04> | <2018-04-11 Wed 20:04> | 0 days 04:00  |           0 |    19667 |  19665 | y       |       |
 |   188 | rtBackground  | rfNewTos | <2018-04-11 Wed 20:53> | <2018-04-17 Tue 10:53> | 5 days 14:00  |           0 |   204281 |  20781 | y       |       |
 |   239 | rtCalibration | rfNewTos | <2018-10-20 Sat 18:31> | <2018-10-20 Sat 20:31> | 0 days 02:00  |           0 |     9565 |   9518 | y       |       |
 |   240 | rtBackground  | rfNewTos | <2018-10-21 Sun 14:54> | <2018-10-22 Mon 16:15> | 1 days 01:21  |           1 |    38753 |   4203 | y       |       |
 |   241 | rtCalibration | rfNewTos | <2018-10-22 Mon 16:16> | <2018-10-22 Mon 18:16> | 0 days 02:00  |           0 |     9480 |   9426 | y       |       |
 |   242 | rtBackground  | rfNewTos | <2018-10-22 Mon 18:44> | <2018-10-23 Tue 22:08> | 1 days 03:24  |           1 |    41933 |   4843 | y       |       |
 |   243 | rtCalibration | rfNewTos | <2018-10-23 Tue 22:09> | <2018-10-24 Wed 0:09>  | 0 days 01:59  |           0 |     9488 |   9429 | y       |       |
 |   244 | rtBackground  | rfNewTos | <2018-10-24 Wed 0:32>  | <2018-10-24 Wed 19:24> | 0 days 18:52  |           1 |    28870 |   3317 | y       |       |
 |   245 | rtCalibration | rfNewTos | <2018-10-24 Wed 19:25> | <2018-10-24 Wed 21:25> | 0 days 01:59  |           0 |     9573 |   9530 | y       |       |
 |   246 | rtBackground  | rfNewTos | <2018-10-24 Wed 21:59> | <2018-10-25 Thu 16:18> | 0 days 18:18  |           1 |    27970 |   2987 | y       |       |
 |   247 | rtCalibration | rfNewTos | <2018-10-25 Thu 16:19> | <2018-10-25 Thu 18:19> | 0 days 01:59  |           0 |     9389 |   9334 | y       |       |
 |   248 | rtBackground  | rfNewTos | <2018-10-25 Thu 18:25> | <2018-10-26 Fri 22:29> | 1 days 04:04  |           1 |    42871 |   4544 | y       |       |
 |   249 | rtCalibration | rfNewTos | <2018-10-26 Fri 22:30> | <2018-10-27 Sat 0:30>  | 0 days 02:00  |           0 |     9473 |   9431 | y       |       |
 |   250 | rtBackground  | rfNewTos | <2018-10-27 Sat 1:31>  | <2018-10-27 Sat 22:26> | 0 days 20:54  |           1 |    31961 |   3552 | y       |       |
 |   251 | rtCalibration | rfNewTos | <2018-10-27 Sat 22:26> | <2018-10-28 Sun 0:26>  | 0 days 01:59  |           0 |     9551 |   9503 | y       |       |
 |   253 | rtCalibration | rfNewTos | <2018-10-28 Sun 19:18> | <2018-10-28 Sun 21:39> | 0 days 02:20  |           0 |    11095 |  11028 | y       |       |
 |   254 | rtBackground  | rfNewTos | <2018-10-28 Sun 21:40> | <2018-10-29 Mon 23:03> | 1 days 01:23  |           1 |    38991 |   4990 | y       |       |
 |   255 | rtCalibration | rfNewTos | <2018-10-29 Mon 23:03> | <2018-10-30 Tue 1:03>  | 0 days 02:00  |           0 |     9378 |   9330 | y       |       |
 |   256 | rtBackground  | rfNewTos | <2018-10-30 Tue 1:49>  | <2018-10-31 Wed 22:18> | 1 days 20:29  |           1 |    68315 |   8769 | y       |       |
 |   257 | rtCalibration | rfNewTos | <2018-10-31 Wed 22:19> | <2018-11-01 Thu 0:19>  | 0 days 01:59  |           0 |     9648 |   9592 | y       |       |
 |   258 | rtBackground  | rfNewTos | <2018-11-01 Thu 0:20>  | <2018-11-01 Thu 16:15> | 0 days 15:55  |           1 |    24454 |   3103 | y       |       |
 |   259 | rtCalibration | rfNewTos | <2018-11-01 Thu 16:16> | <2018-11-01 Thu 17:31> | 0 days 01:14  |           0 |     5900 |   5864 | y       |       |
 |   260 | rtCalibration | rfNewTos | <2018-11-01 Thu 17:39> | <2018-11-01 Thu 19:09> | 0 days 01:30  |           0 |     7281 |   7251 | y       |       |
 |   261 | rtBackground  | rfNewTos | <2018-11-01 Thu 19:39> | <2018-11-04 Sun 15:23> | 2 days 19:43  |           3 |   103658 |  12126 | y       |       |
 |   262 | rtCalibration | rfNewTos | <2018-11-04 Sun 15:24> | <2018-11-04 Sun 21:24> | 0 days 05:59  |           0 |    28810 |  28681 | y       |       |
 |   263 | rtBackground  | rfNewTos | <2018-11-05 Mon 0:35>  | <2018-11-05 Mon 20:28> | 0 days 19:52  |           1 |    30428 |   3610 | y       |       |
 |   264 | rtCalibration | rfNewTos | <2018-11-05 Mon 20:28> | <2018-11-05 Mon 22:28> | 0 days 01:59  |           0 |     9595 |   9544 | y       |       |
 |   265 | rtBackground  | rfNewTos | <2018-11-05 Mon 22:52> | <2018-11-07 Wed 22:14> | 1 days 23:21  |           1 |    72514 |   8429 | y       |       |
 |   266 | rtCalibration | rfNewTos | <2018-11-07 Wed 22:14> | <2018-11-08 Thu 0:14>  | 0 days 01:59  |           0 |     9555 |   9506 | y       |       |
 |   267 | rtBackground  | rfNewTos | <2018-11-08 Thu 2:05>  | <2018-11-08 Thu 6:54>  | 0 days 04:48  |           0 |     7393 |    929 | y       |       |
 |   268 | rtBackground  | rfNewTos | <2018-11-09 Fri 6:15>  | <2018-11-09 Fri 17:20> | 0 days 11:04  |           1 |    16947 |   1974 | y       |       |
 |   269 | rtCalibration | rfNewTos | <2018-11-09 Fri 17:20> | <2018-11-09 Fri 21:20> | 0 days 04:00  |           0 |    19382 |  19302 | y       |       |
 |   270 | rtBackground  | rfNewTos | <2018-11-09 Fri 21:27> | <2018-11-11 Sun 21:02> | 1 days 23:34  |           2 |    72756 |   8078 | y       |       |
 |   271 | rtCalibration | rfNewTos | <2018-11-11 Sun 21:03> | <2018-11-11 Sun 23:46> | 0 days 02:43  |           0 |    13015 |  12944 | y       |       |
 |   272 | rtBackground  | rfNewTos | <2018-11-12 Mon 0:09>  | <2018-11-14 Wed 19:07> | 2 days 18:58  |           3 |   102360 |  11336 | y       |       |
 |   273 | rtCalibration | rfNewTos | <2018-11-14 Wed 19:08> | <2018-11-14 Wed 21:08> | 0 days 01:59  |           0 |     9535 |   9471 | y       |       |
 |   274 | rtBackground  | rfNewTos | <2018-11-14 Wed 21:28> | <2018-11-17 Sat 18:14> | 2 days 20:45  |           3 |   105187 |  12101 | y       |       |
 |   275 | rtCalibration | rfNewTos | <2018-11-17 Sat 18:14> | <2018-11-17 Sat 20:57> | 0 days 02:43  |           0 |    13179 |  13116 | y       |       |
 |   276 | rtBackground  | rfNewTos | <2018-11-17 Sat 22:08> | <2018-11-22 Thu 2:26>  | 4 days 04:17  |           2 |   153954 |  19640 | y       |       |
 |   277 | rtCalibration | rfNewTos | <2018-11-22 Thu 2:26>  | <2018-11-22 Thu 16:14> | 0 days 13:48  |           0 |    66052 |  65749 | y       |       |
 |   278 | rtBackground  | rfNewTos | <2018-11-22 Thu 16:14> | <2018-11-23 Fri 10:51> | 0 days 18:36  |           0 |    28164 |   3535 | y       |       |
 |   279 | rtBackground  | rfNewTos | <2018-11-24 Sat 10:51> | <2018-11-26 Mon 14:58> | 2 days 04:07  |           2 |    79848 |   9677 | y       |       |
 |   280 | rtCalibration | rfNewTos | <2018-11-26 Mon 14:59> | <2018-11-26 Mon 18:59> | 0 days 04:00  |           0 |    19189 |  19112 | y       |       |
 |   281 | rtBackground  | rfNewTos | <2018-11-26 Mon 19:02> | <2018-11-28 Wed 18:07> | 1 days 23:04  |           1 |    72230 |   8860 | y       |       |
 |   282 | rtCalibration | rfNewTos | <2018-11-28 Wed 18:07> | <2018-11-28 Wed 20:51> | 0 days 02:43  |           0 |    12924 |  12860 | y       |       |
 |   283 | rtBackground  | rfNewTos | <2018-11-28 Wed 22:31> | <2018-12-01 Sat 14:38> | 2 days 16:07  |           3 |    98246 |  11965 | y       |       |
 |   284 | rtCalibration | rfNewTos | <2018-12-01 Sat 14:39> | <2018-12-01 Sat 18:39> | 0 days 03:59  |           0 |    19017 |  18904 | y       |       |
 |   285 | rtBackground  | rfNewTos | <2018-12-01 Sat 19:06> | <2018-12-03 Mon 19:39> | 2 days 00:33  |           2 |    74405 |   8887 | y       |       |
 |   286 | rtCalibration | rfNewTos | <2018-12-04 Tue 15:57> | <2018-12-04 Tue 17:57> | 0 days 02:00  |           0 |     9766 |   9715 | y       |       |
 |   287 | rtBackground  | rfNewTos | <2018-12-04 Tue 19:07> | <2018-12-05 Wed 15:08> | 0 days 20:01  |           1 |    30598 |   3393 | y       |       |
 |   288 | rtCalibration | rfNewTos | <2018-12-05 Wed 17:28> | <2018-12-05 Wed 19:28> | 0 days 02:00  |           0 |     9495 |   9443 | y       |       |
 |   289 | rtBackground  | rfNewTos | <2018-12-05 Wed 23:07> | <2018-12-06 Thu 19:11> | 0 days 20:03  |           1 |    30629 |   3269 | y       |       |
 |   290 | rtCalibration | rfNewTos | <2018-12-06 Thu 19:11> | <2018-12-06 Thu 21:11> | 0 days 02:00  |           0 |     9457 |   9394 | y       |       |
 |   291 | rtBackground  | rfNewTos | <2018-12-06 Thu 23:14> | <2018-12-08 Sat 13:39> | 1 days 14:24  |           2 |    58602 |   6133 | y       |       |
 |   292 | rtCalibration | rfNewTos | <2018-12-08 Sat 13:39> | <2018-12-08 Sat 15:39> | 0 days 02:00  |           0 |     9475 |   9426 | y       |       |
 |   293 | rtBackground  | rfNewTos | <2018-12-08 Sat 17:42> | <2018-12-10 Mon 21:50> | 2 days 04:07  |           1 |    79677 |   8850 | y       |       |
 |   294 | rtCalibration | rfNewTos | <2018-12-10 Mon 21:50> | <2018-12-10 Mon 23:50> | 0 days 02:00  |           0 |     9514 |   9467 | y       |       |
 |   295 | rtBackground  | rfNewTos | <2018-12-11 Tue 0:54>  | <2018-12-11 Tue 20:31> | 0 days 19:37  |           1 |    29981 |   3271 | y       |       |
 |   296 | rtCalibration | rfNewTos | <2018-12-11 Tue 20:31> | <2018-12-11 Tue 22:31> | 0 days 02:00  |           0 |     9565 |   9517 | y       |       |
 |   297 | rtBackground  | rfNewTos | <2018-12-12 Wed 0:14>  | <2018-12-13 Thu 18:30> | 1 days 18:15  |           2 |    68124 |  12530 | y       |       |
 |   298 | rtBackground  | rfNewTos | <2018-12-13 Thu 18:39> | <2018-12-15 Sat 6:41>  | 1 days 12:01  |           1 |    53497 |      0 | y       |       |
 |   299 | rtBackground  | rfNewTos | <2018-12-15 Sat 6:43>  | <2018-12-15 Sat 18:13> | 0 days 11:29  |           1 |    17061 |      0 | y       |       |
 |   300 | rtCalibration | rfNewTos | <2018-12-15 Sat 18:38> | <2018-12-15 Sat 20:38> | 0 days 02:00  |           0 |     9466 |   9415 | y       |       |
 |   301 | rtBackground  | rfNewTos | <2018-12-15 Sat 21:34> | <2018-12-17 Mon 14:17> | 1 days 16:43  |           2 |    62454 |   7751 | y       |       |
 |   302 | rtCalibration | rfNewTos | <2018-12-17 Mon 14:18> | <2018-12-17 Mon 16:18> | 0 days 01:59  |           0 |     9616 |   9577 | y       |       |
 |   303 | rtBackground  | rfNewTos | <2018-12-17 Mon 16:52> | <2018-12-18 Tue 16:41> | 0 days 23:48  |           1 |    36583 |   4571 | y       |       |
 |   304 | rtCalibration | rfNewTos | <2018-12-19 Wed 9:33>  | <2018-12-19 Wed 11:33> | 0 days 01:59  |           0 |     9531 |   9465 | y       |       |
 |   306 | rtBackground  | rfNewTos | <2018-12-20 Thu 6:55>  | <2018-12-20 Thu 11:53> | 0 days 04:58  |           1 |     7546 |    495 | y       |       |

** Automatically calculated total run times

Run period 2 (= 2017/18): 
#+BEGIN_SRC 
Type: rtBackground
         trackingDuration: 4 days, 10 hours, and 20 seconds
         nonTrackingDuration: 14 weeks, 2 days, 1 hour, 23 minutes, and 18 seconds
Type: rtCalibration
         trackingDuration: 0 nanoseconds
         nonTrackingDuration: 4 days, 11 hours, 25 minutes, and 20 seconds
#+END_SRC

Which amounts to:
Calibration data: 107.42 h
Background data: 2401.40 h
Tracking data: 106 h

Run period 3 (= Oct-Dec 2018):
#+BEGIN_SRC
Type: rtBackground
         trackingDuration: 3 days, 2 hours, 17 minutes, and 53 seconds
         nonTrackingDuration: 6 weeks, 4 days, 20 hours, 54 minutes, and 29 seconds
Type: rtCalibration
         trackingDuration: 0 nanoseconds
         nonTrackingDuration: 3 days, 15 hours, 3 minutes, and 45 seconds
#+END_SRC

Which amounts to:
Calibration data: 87.05 h
Background data: 1124.9 h
Tracking data: 74.29 h

So in total:
Calibration data: 194.47 h
Background data: 3526.3 h
Tracking data: 180.29 h

** InGrid temperature from shift forms

Due to the bug in TOS that caused the temperature log files to be
placed in =./TOS/log/= instead of the respective run folders, the
files were overwritten periodically it seems.

The only temperature information we still have from it thus is the
shift forms.

There are also pictures of every shift form in my googlo photos.

| Run number | Date                   | Temp / ° | Notes                                          |
|------------+------------------------+----------+------------------------------------------------|
|         76 | <2017-10-17 Tue 05:00> |        - | Not written down yet                           |
|         77 | <2017-11-02 Thu 05:25> |    40.50 |                                                |
|         78 | <2017-11-03 Fri 05:25> |    40.30 |                                                |
|         79 | <2017-11-04 Sat 05:30> |    40.63 |                                                |
|         80 | <2017-11-05 Sun 05:30> |    40.80 |                                                |
|         81 | <2017-11-06 Mon 06:07> |    40.27 |                                                |
|         82 | <2017-11-07 Tue 06:00> |    40.10 |                                                |
|         82 | <2017-11-08 Wed 05:32> |        - | Wrote 2nd val: 22.73                           |
|         84 | <2017-11-09 Thu 05:33> |    40.22 |                                                |
|         86 | <2017-11-10 Fri 05:32> |    40.13 |                                                |
|         87 | <2017-11-11 Sat 05:36> |    40.00 |                                                |
|         87 | <2017-11-12 Sun 06:07> |    40.31 |                                                |
|         89 | <2017-11-13 Mon 05:40> |    39.87 |                                                |
|         90 | <2017-11-14 Tue 05:40> |    39.74 |                                                |
|         91 | <2017-11-15 Wed 05:35> |    39.68 |                                                |
|         92 | <2017-11-17 Fri 05:36> |    39.72 |                                                |
|         94 | <2017-11-18 Sat 05:38> |    39.72 |                                                |
|         95 | <2017-11-19 Sun 05:44> |    39.70 |                                                |
|         97 | <2017-11-25 Sat 05:52> |    40.35 |                                                |
|         98 | <2017-11-26 Sun 06:28> |    39.70 |                                                |
|         99 | <2017-11-27 Mon 06:25> |    39.61 |                                                |
|        100 | <2017-11-28 Tue 06:35> |    38.93 |                                                |
|        101 | <2017-11-29 Wed 06:35> |    38.72 |                                                |
|        103 | <2017-11-30 Thu 06:32> |    39.70 |                                                |
|        104 | <2017-12-01 Fri 06:42> |    38.68 |                                                |
|        105 | <2017-12-02 Sat 06:40> |    39.46 |                                                |
|        106 | <2017-12-03 Sun 06:36> |    39.46 |                                                |
|        107 | <2017-12-04 Mon 06:50> |    38.67 |                                                |
|        109 | <2017-12-05 Tue 06:35> |    38.31 |                                                |
|        112 | <2017-12-07 Thu 06:45> |    38.46 |                                                |
|        112 | <2017-12-09 Sat 06:38> |    39.79 |                                                |
|        113 | <2017-12-10 Sun 06:37> |    38.56 |                                                |
|        114 | <2017-12-11 Mon 06:45> |    38.86 |                                                |
|        115 | <2017-12-12 Tue 06:45> |    39.86 |                                                |
|        117 | <2017-12-13 Wed 06:41> |    39.71 |                                                |
|        119 | <2017-12-14 Thu 06:42> |    40.32 |                                                |
|        121 | <2017-12-15 Fri 06:45> |    39.95 |                                                |
|        123 | <2017-12-16 Sat 06:42> |    40.03 |                                                |
|        124 | <2017-12-17 Sun 06:47> |    39.96 |                                                |
|        124 | <2017-12-18 Mon 06:45> |    40.07 |                                                |
|        125 | <2017-12-19 Tue 06:44> |    40.61 |                                                |
|        127 | <2017-12-20 Wed 06:46> |    40.75 |                                                |
|        137 | <2018-02-15 Thu 05:48> |        - | Start of 2018 data taking, temp readout broken |
|        140 | <2018-02-16 Fri 05:44> |        - |                                                |
|        146 | <2018-02-18 Sun 06:15> |        - |                                                |
|        148 | <2018-02-19 Sun 06:12> |        - |                                                |
|        150 | <2018-02-20 Tue 06:00> |        - |                                                |
|        152 | <2018-02-21 Wed 06:12> |        - |                                                |
|        154 | <2018-02-22 Thu 06:14> |        - |                                                |
|        156 | <2018-02-23 Fri 05:58> |        - |                                                |
|        158 | <2018-02-24 Sat 06:15> |        - |                                                |
|        160 | <2018-02-28 Wed 06:01> |        - |                                                |
|        162 | <2018-03-02 Fri 05:58> |        - |                                                |
|        162 | <2018-03-03 Tue 06:03> |        - |                                                |
|        162 | <2018-03-04 Sun 05:57> |        - |                                                |
|        164 | <2018-03-05 Mon 06:00> |        - |                                                |
|        164 | <2018-03-06 Tue 06:00> |        - |                                                |
|        166 | <2018-03-07 Wed 05:44> |        - |                                                |
|        170 | <2018-03-14 Wed 05:35> |        - |                                                |
|        172 | <2018-03-15 Thu 05:29> |        - |                                                |
|        174 | <2018-03-16 Fri 05:30> |        - |                                                |
|        176 | <2018-03-17 Sat 05:25> |        - |                                                |
|        178 | <2018-03-18 Sun 05:24> |        - |                                                |
|        178 | <2018-03-19 Mon 05:22> |        - |                                                |
|        178 | <2018-03-20 Tue 05:16> |        - |                                                |
|        178 | <2018-03-21 Wed 05:15> |        - |                                                |
|        178 | <2018-03-22 Thu 05:12> |        - |                                                |
|        180 | <2018-03-23 Fri 05:12> |        - |                                                |
|        180 | <2018-03-24 Sat 05:17> |        - |                                                |
|        182 | <2018-03-25 Sun 06:07> |        - |                                                |
|        182 | <2018-03-26 Mon 06:07> |        - |                                                |
|        240 | <2018-10-22 Mon 06:38> |    48.27 | Begin of Run 3 period                          |
|        242 | <2018-10-23 Tue 06:38> |    48.61 |                                                |
|        244 | <2018-10-24 Wed 06:45> |    48.70 |                                                |
|        246 | <2018-10-25 Thu 06:47> |    49.83 |                                                |
|        248 | <2018-10-26 Fri 06:47> |    49.37 |                                                |
|        250 | <2018-10-27 Sat 06:46> |    47.55 |                                                |
|        252 | <2018-10-28 Sun 05:40> |    47.16 |                                                |
|        254 | <2018-10-29 Mon 05:55> |    45.77 |                                                |
|        256 | <2018-10-30 Tue 05:33> |    45.56 |                                                |
|        258 | <2018-11-01 Thu 05:50> |    46.15 |                                                |
|        261 | <2018-11-02 Fri 06:10> |    46.65 |                                                |
|        261 | <2018-11-03 Sat 05:53> |    47.02 |                                                |
|        261 | <2018-11-04 Sun 06:14> |    47.97 |                                                |
|        263 | <2018-11-05 Mon 06:14> |    47.94 |                                                |
|        265 | <2018-11-06 Tue 05:53> |    47.27 |                                                |
|        268 | <2018-11-09 Fri 06:00> |    47.66 |                                                |
|        270 | <2018-11-10 Sat 06:04> |    48.39 |                                                |
|        270 | <2018-11-11 Sun 06:11> |    48.43 |                                                |
|        272 | <2018-11-12 Mon 06:10> |    48.69 |                                                |
|        272 | <2018-11-13 Tue 06:04> |    48.59 |                                                |
|        272 | <2018-11-14 Wed 06:10> |    48.96 |                                                |
|        274 | <2018-11-15 Thu 06:10> |    48.57 |                                                |
|        274 | <2018-11-16 Fri 06:07> |    48.60 |                                                |
|        274 | <2018-11-17 Sat 06:03> |    47.98 |                                                |
|        276 | <2018-11-18 Sun 06:05> |    47.70 |                                                |
|        276 | <2018-11-19 Mon 06:10> |    47.36 |                                                |
|        278 | <2018-11-24 Sat 06:22> |    46.60 |                                                |
|        279 | <2018-11-25 Sun 06:29> |    46.74 |                                                |
|        279 | <2018-11-26 Mon 06:26> |    46.67 |                                                |
|        281 | <2018-11-27 Tue 06:20> |    46.57 |                                                |
|        283 | <2018-11-29 Thu 06:22> |    46.31 |                                                |
|        283 | <2018-11-30 Fri 06:26> |    46.76 |                                                |
|        283 | <2018-12-01 Sat 06:29> |    46.83 |                                                |
|        285 | <2018-12-02 Sun 06:26> |    46.75 |                                                |
|        285 | <2018-12-03 Mon 06:28> |    46.62 |                                                |
|        287 | <2018-12-05 Wed 06:28> |    47.10 |                                                |
|        289 | <2018-12-06 Thu 06:29> |    47.44 |                                                |
|        291 | <2018-12-07 Fri 06:31> |    46.23 |                                                |
|        291 | <2018-12-08 Sat 06:35> |    46.13 |                                                |
|        293 | <2018-12-09 Sun 06:32> |    46.02 |                                                |
|        293 | <2018-12-10 Mon 06:32> |    45.78 |                                                |
|        295 | <2018-12-11 Tue 06:33> |    45.41 |                                                |
|        297 | <2018-12-12 Wed 06:37> |    44.38 |                                                |
|        297 | <2018-12-13 Thu 06:35> |    44.50 |                                                |
|        298 | <2018-12-14 Fri 06:40> |    44.74 |                                                |
|        299 | <2018-12-15 Sat 06:43> |    44.50 |                                                |
|        301 | <2018-12-16 Sun 06:41> |    44.44 |                                                |
|        301 | <2018-12-17 Mon 06:26> |    45.03 |                                                |
|        303 | <2018-12-18 Tue 06:52> |    44.69 |                                                |
|        306 | <2018-12-20 Thu 06:30> |    40.04 |                                                |


* CAST operation procedures                                        :Appendix:
:PROPERTIES:
:CUSTOM_ID: sec:appendix:cast_operations
:END:

This chapter provides some guidance about typical operations necessary
during maintenance or installation of the detector / vacuum system.

** CAST terminology

- [ ] *HAVE A SCHEMATIC OF SORTS THAT SHOWS WHAT AIRPORT, JURA,
  SUNRISE, SUNSET MEANS*
  -> We don't have the time, but it would be fun to experiment with
  gaussian splatting based on all the CAST pictures we have to see if
  we could get an interactive 3D view of the CAST hall, haha.
  -> Realistically: do a simple schematic showing CAST magnet, tracks,
  telescope side etc and annotate.

** Ramping the HV

The high voltage supply can be controlled in two different
ways. Besides differing in usability terms (one is manual, the other
automatic), the main difference between the two is the HV interlock,
which is only partially usable in case of the manual HV control.
1. in the manual way using the Linux software supplied by iseg. On the
   InGrid-DAQ computer it is located in
   [[file:/home/ingrid/src/isegControl/isegControl][~/src/isegControl/isegControl]]. Depending on the setup of the
   machine, the software may need superuser rights to access the USB
   connection. With the software the given channel as shown in
   tab. [[tab:hv]] can be set up and the HV can be ramped up. Note: one
   needs to activate SetKillEnable such that the HV is shut down in
   case of a current trip (exceeding of specified current). One should
   then set 'groups' of different channels, so that grid, anode and
   ring 1 are shut down at the same time, in case of a current trip,
   as well as ring 29 and the cathode! In addition the trip current
   needs to be manually set about a factor 5 higher during ramping,
   because of capacitors, which need to be charged first. Otherwise
   the channels trip immediately. In this case the HV interlock is
   restricted to basic current restrictions. Anything detector related
   is not included!
2. in the automatic way via the TOS. The TOS takes care of everything
   mentioned above. To use the TOS for the HV control (and thus also
   use the complete HV interlock, as it exists at the moment), perform
   the following steps:
   1. check [[file:~/TOS/config/HFM_settings.ini][~/TOS/config/HFM_settings.ini]] and compare with tab. [[tab:hv]]
      whether settings seem reasonable
   2. after starting TOS and setting up the chips, call
      #+BEGIN_SRC sh
      > ActivateHFM
      #+END_SRC
      which will set up TOS to use the combined HV and FADC (due to
      both inside the same VME crate, they are intertwined). This
      configures the FADC and reads the desired HV settings, but does
      not set the HV settings on the module yet.
   3. to write the HV to the HV module, call
      #+BEGIN_SRC sh
      > InitHV
      #+END_SRC
      which will write the HV settings from HFM_settings.ini to the HV
      module. At the end it will ask the user, whether the HV should
      be ramped up:
      #+BEGIN_SRC sh
      Do you wish to ramp up the channels now? (Y / n)
      #+END_SRC
      If yes, the ramping progress will be shown via calls to the
      CheckModuleIsRamping() function (which can also be called
      manually in TOS). 
   This should properly ramp up all channels. It is possible that TOS
   fails to connect to the VME crate and hence is not able to ramp up
   the channels. The most likely reason for this is that the
   isegControl software is still open, since only one application can
   access a single USB interface at the same time.

** Vacuum

The vacuum system as described in sec. [[#sec:cast:vacuum_system]] usually does not
require manual intervention during normal operation.

For maintenance the following two sections describe how to pump the
system safely as well as how to flush it with nitrogen. Both processes
are rather delicate due to the sensitive $\ce{Si_x N_y}$
window. Pumping needs to be done slowly, $O(\SI{1}{\milli\bar \per
\second})$. To be able to do this, the needle valve
$V_{\text{Needle}}$ (cf. fig. [[fig:cast:vacuum-schematic]]) is installed. One
may separate the vacuum volume into two separate vacua. A bad vacuum
before the primary pump and after the turbo pumps T1 and T2 and a good
vacuum before the turbo pump T2. There are three connections from the
good vacuum to the bad one. 
1. through T2, closable via $V_{\text{T2}}$, $\SI{40}{\mm}$ tubing
2. through the needle valve $V_{\text{needle}}$, $\SI{16}{\mm}$ tubing
3. through T1 via the manipulator interstage, normally closed (see the
  note below), $\SI{25}{\mm}$ tubing
3 is mainly irrelevant for pumping purposes, since there is no valve
to open or close; it is always closed by a 2-O-ring seal to the good
vacuum. While 1 is the main path for pumping during operation, it is 2
which is used during a pumping down or flushing of the system, since
it can be controlled very granularly. 

For both explanations below, it is very important to always think
about each step (are the correct valves open / closed? etc.). A small
mistake can lead to severe damage of the hardware (turbo pumps can
break, the window can rupture).

_Note_: There is a third very small vacuum volume before T1, which is
the volume up to the manipulator interstage. This volume is separate
from the main good vacuum chamber, due to a 2-O-ring seal on both ends
of the manipulator. Compare with fig. [[fig:cast:vacuum-schematic]] at the
location of the two clamped flanges 'above' the manipulator. One
2-O-ring seal is at the upper flange and one at the lower. This is
because the manipulator part furthest from the beampipe is under
air. In order to seal the air and the vacuum especially during
movement of the source, these seals are in place. However, while the
2-O-ring seals provide decent sealing, it is not perfect. This is why
the small turbo pump T1 is in place at all, to reduce the amount of
air, which might enter the system during source manipulation. Another
aspect to keep in mind is potential air, which can get trapped in
between the two O-rings. This air will be released during movement of
the seals. Especially after the system was open to air, it is expected
that a small pressure increase on $P_{\text{MM}}$ can be seen during
operation, despite T1 being in place. After several movement cycles,
$O(10)$, these peaks should be negligible.

*** Pumping the vacuum

Before pumping it is a good idea to connect two linear gauges to the
two $P_{\text{Linear}}$ pressure sensors.
To pump the system safely, perform the following steps:
1. Make sure every pump is turned off.
2. Make sure every valve in the system is closed:
   1. $V_{\text{Primary}}$
   2. $V_{\text{Leak}}$
   3. $V_{\text{T2}}$
   4. $V_{\text{Needle}}$
3. Connect a linear gauge to $P_{\text{P, Linear}}$ on the primary pump line.
4. Start the primary pump. Tubing up to $V_{\text{Primary}}$ will be
   pumped, visible on linear gauge connected to
   $P_{\text{P, Linear}}$. Check that the second linear gauge remains
   unchanged, if not $V_{\text{Primary}}$ and $V_{\text{Needle}}$ is
   open!
5. Once $P_{\text{P, Linear}}$ shows $\leq \SI{10}{\milli bar}$,
   slowly open $V_{\text{Primary}}$, again checking $P_{\text{N,
   Linear}}$ remains unchanged. This will increase the pressure on
   $P_{\text{P, Linear}}$ again until the volume is pumped.
6. This step is the most crucial. With $V_{\text{T2}}$ still closed,
   very carefully open $V_{\text{Needle}}$, while keeping an eye on
   $P_{\text{N, Linear}}$. Note that $V_{\text{Needle}}$ has two
   locking mechanisms. The knob at the end with the analog indicator
   and a general lock in front of that. While the analog indicator
   shows =000=, open the general lock. Then slowly start turning the
   knob. At around =300= the pressure on $P_{\text{N, Linear}}$ should
   slowly start to decrease. Keep turning the knob until you reach a
   pump rate of $O(\SI{1}{\milli \bar \per \second})$. You will have
   to keep opening the needle valve further, the lower the pressure is
   to keep the pump rate constant.
7. Once both linear gauges have equalized (up to different offsets),
   close the needle valve again.
8. Open $V_{\text{T2}}$.
9. Start T2 by turning on the power and pressing the right most
   button. Use the arrow buttons to select the 'actual RPM' setting to
   see that the turbo is spinning up. Final speed should be set to
   $\SI{1500}{\Hz}$.
10. While T2 is spinning up, start T1 by turning on the power at the
    back. There is no additional button to be pressed.

The system should now be in the following state:
- $V_{\text{Leak}}$ closed
- $V_{\text{Needle}}$ closed
- $V_{\text{T2}}$ open
- $V_{\text{Primary}}$ open
- T2 & T1 running
- Primary pump running
If so, the system is now pumping. Note that it may take several days
to reach a vacuum good enough to satisfy the interlock.

*** Flushing the system

Flushing the system is somewhat of a reverse of pumping the
system. Follow these steps to safely flush the system with
nitrogen. See section [[Nitrogen supply]] for an explanation of which
valves need to be operated to open the nitrogen line.
Before flushing the system connect two linear gauges to both
$P_{\text{Linear}}$ sensors.
1. Make sure the turbo pumps are turned off, if not yet turn both off
   and wait for them to have come to a halt.
2. Turn off the primary pump.
3. Close $V_{\text{T2}}$. $V_{\text{Leak}}$ and $V_{\text{Needle}}$
   should already be closed, while $V_{\text{T2}}$ and
   $V_{\text{Primary}}$ should still be open.
4. Connect the nitrogen line to the blind flange before
   $V_{\text{Leak}}$. 
5. Slowly open $V_{\text{Leak}}$, while checking both linear
   gauges. Make sure only the pressure on $P_{\text{P, Linear}}$
   increases, while $P_{\text{N, Linear}}$ remains under vacuum. If
   not, another valve is still open. Close $V_{\text{Leak}}$
   immediately again!
6. Keep flushing nitrogen, until $P_{\text{P, Linear}}$ gets close to
   $\SI{1000}{\milli\bar}$ (the sensors will never actually reach
   that value).
7. Close $V_{\text{Leak}}$ again to make sure you do not put the
   system over one atmosphere of pressure.
8. This step is the most crucial. With $V_{\text{T2}}$ still closed,
   very carefully open $V_{\text{Needle}}$, while keeping an eye on
   $P_{\text{N, Linear}}$. Note that $V_{\text{Needle}}$ has two
   locking mechanisms. The knob at the end with the analog indicator
   and a general lock in front of that. While the analog indicator
   shows =000=, open the general lock. Then slowly start turning the
   knob. At around =300= the pressure on $P_{\text{N, Linear}}$ should
   slowly start to increase. Keep turning the knob until you reach a
   pump rate of $O(\SI{1}{\milli \bar \per \second})$.
9. You will notice that the pressure on $P_{\text{P, Linear}}$ will
   start to decrease, since the air will distribute in a larger
   volume. Open $V_{\text{Leak}}$ again slightly to keep $P_{\text{P,
   Linear}}$ roughly constant.
10. Keep flushing with $\SI{1}{\milli\bar\per\second}$ until both
    sensors read $O(\SI{1000}{\milli\bar})$.
11. Close all valves in the system again.

This way the system is safely flushed with nitrogen. This helps to
pump faster after a short maintenance, because less humidity can enter
the system.

** COMMENT Gas supply

If the requirements of the gas supply interlock are satisfied
(cf. sec. [[Gas supply interlock]]), it is possible to flush the detector
with gas. For that, follow these steps:
1. Make sure the pressure controller is connected and running. Check
   InGrid-PLC computer in control room and see if pressure control
   software is running. If gas supply is currently closed, reported
   pressure inside the detector is usually reported to
   $\SIrange{960}{980}{\milli\bar}$.
2. Outside the building, open the main valve of the currently active
   gas bottle (check the arrow on the bottle selector mechanism). See
   fig. [[fig:gas-bottle-outside]].
3. Open the second valve near the bottle.
4. Pressure values should be:
   - gas bottle: $\sim\SIrange{30}{100}{\bar}$
   - pre-line pressure: $\sim\SI{7}{\bar}$
   - line pressure: $\sim\SI{0.45}{\bar}$
5. Activate the gas supply at the interlock box by turning the key to
   =Security on= and pressing the large button. See
   fig. [[fig:interlock-box]].
6. Go to the airport side of the magnet. Open the valve on the InGrid
   gas panel below the telescope platform. See
   fig. [[fig:ingrid-gas-panel]].
7. Slowly open the needle valve on the flow meter on the previous
   panel. Increase gas flow up to $\sim\SI{2}{\liter\per\hour}$.
8. Open the needle valve on the gas supply line on the side of the
   platform (see fig. [[fig:gas-needle-valve]]).
9. After $\SIrange{5}{10}{\minute}$ the pressure controller on the
   InGrid-PLC computer should report $\SI{1050}{\milli\bar}$.
# 7. Make sure the electrovalves on the second gas panel (see
#    fig. NOT HERE. NO PANEL) are open. THESE VALVES CANNOT BE CHECKED
#    EXPLICITLY. IF INTERLOCK BOX IS ACTIVE, WILL BE OPEN?! MAYBE CHECK
#    BOX POWER SUPPLY? NO LED I THINK.


Now the detector should be flushed with $\ce{Ar} /
\ce{iC_4H_{10}}$. Before turning on the HV, make sure to flush for at
least $\SI{12}{\hour}$ to be on the safe side.

#+CAPTION: Location of the Argon-Isobutane bottle and the main valves outside the building.
#+NAME: fig:gas-bottle-outside
[[file:~/org/Doc/Detector/figs/gas_bottles_outside.jpg]]


#+CAPTION: Location of the gas interlock box
#+NAME: fig:interlock-box
[[file:~/org/Doc/Detector/figs/gas_interlock_box.jpg]]

#+CAPTION: Location of the InGrid gas panel below the telescope platform on the airport side.
#+NAME: fig:ingrid-gas-panel
[[file:~/org/Doc/Detector/figs/gas_panel.jpg]]

#+CAPTION: Location of the needle valve on the gas supply line
#+NAME: fig:gas-needle-valve
<file to be inserted>

** Nitrogen supply

Nitrogen is supplied by a nitrogen bottle outside the building. To
open the nitrogen line, 5 valves need to be opened. The line ends in a
copper pipe on the airport side, which is usually there rolled up (the
copper is somewhat flexible).
1. Open the lever on the nitrogen bottle outside the building (see
   fig. [[fig:nitrogen-bottle-outside]]).
2. Open the valve next to the bottle.
3. Go to the gas lines next to the control room. The right most line
   (see fig. [[fig:valves-control-room]] and
   fig. [[fig:nitrogen-valve-control-room]]) is the nitrogen line. Open
   the valve.
4. Open the needle valve on the flow meter to the right of the
   previous valve.
5. Open the needle valve on the airport side of the magnet (see
   fig. [[fig:nitrogen-airport-side]]).
This should be all to open the nitrogen line. The flow through the
pipe is not too large, but it should be large enough to feel it on the
back of the hand.

#+CAPTION: Location of the Nitrogen bottle and the main valves outside the building.
#+NAME: fig:nitrogen-bottle-outside
[[file:~/org/Doc/Detector/figs/nitrogen_bottle.jpg]]

#+CAPTION: Location of the valves next to the control room
#+NAME: fig:valves-control-room
[[file:~/org/Doc/Detector/figs/nitrogen_valve_location.jpg]]

#+CAPTION: The actual nitrogen valve on the set of valves near the control room.
#+NAME: fig:nitrogen-valve-control-room
[[file:~/org/Doc/Detector/figs/nitrogen_valve.jpg]]


#+CAPTION: Location of the flow meter near the valve next to the control room
#+NAME: fig:nitrogen-flow-meter-control-room
[[file:~/org/Doc/Detector/figs/nitrogen_flow_meter.jpg]]

#+CAPTION: Location of the needle valve on the airport side
#+NAME: fig:nitrogen-airport-side
<file to be inserted>


#+BEGIN_EXPORT latex
\newpage
#+END_EXPORT

* Cabling & software setup [/]                                     :Appendix:
:PROPERTIES:
:CUSTOM_ID: sec:appendix:cabling_and_softwar_setup
:END:

- [ ] *CLEAN THIS UP TO MAKE IT STAND ON ITS OWN OR MERGE INTO ABOVE
  CAST OPERATION?*

The requires:
- Virtex:
  - power
  - 2 HDMI to intermediate board
  - mini USB into JTAG port on backside
  - RJ45 from ethernet port into 2nd ethernet card on DAQ PC

The RJ45 connection is only required to flash the firmware onto the
Virtex.

For the flashing of the firmware, take into account the USB driver
setup described in =void_settings.org=.

** Detector cabling [/]

- [ ] *CLEAN THIS UP*

The following is the cabling for the FADC and the scintillators to the
intermediate board. It's a useful reference when connecting the
cabling!

FADC Trig out:
- trig out -> level adapter (NIM module) into NIM IN, set to +NORM+ /
  *COMPL* (*this is active*)
  TTL out -> TTL signal clipper (TTL 5V -> 2.4V) into port marked =I=
  TTL signal clipper -> adapter board into *left LEMO at back* (viewed
  from behind the crate; cable still at CAST)

FPGA to FADC (shutter signal)
- Adapter board *right LEMO at back* (viewed from behind crate) ->
  level adapter (NIM module), set to *NORM* / +COMPL+ into TTL IN
  NIM out -> FADC EXT EN TRIG

Veto scintillator (*NOTE* this may be *WRONG*; input to adapter board
that is); signal order is reversed!
- Adapter board *left LEMO on top* (viewed from behind the crate,
  input number *1* on adapter board) -> discriminator OUT (top discriminator in NIM module)
  discriminator IN -> back of Amplifier Discriminator NIM module
  amplifier input -> veto scintillator signal
  The signal is needs to be *2.4 V TTL signal!*


** Ethernet connection with Virtex

The ethernet connection with the Virtex needs to be set up
manually.

On the one hand it is required to use a static IP address for the
secondary ethernet device and in addition we need to set an ARP entry
(note: the =arp= program is part of the =net-tools= package, name same
in ubuntu & void linux).

Under Ubuntu said setup can be done using the network manager. In Void
we need to use =ip= from the terminal.

The settings are as follows:
- IP address: 10.1.2.3
- Subnet: 24

Setup of the device can be done according to the example here:
https://docs.voidlinux.org/config/network/index.html
namely (with superuser rights):
#+begin_src sh
ip addr show
# check the name of the correct, secondary device
ip link set dev enp4s0 up # name on tpc19
ip addr add 10.1.2.3/24 brd + dev enp4s0
#+end_src
afterwards we can set the ARP entries (ref: https://confluence.team.uni-bonn.de/display/PHYGASDET/How+to+automatically+set+ARP+entries):
#+begin_src sh
arp -i enp4s0 -s 10.1.2.2 AA:BA:DD:EC:AD:E2
#+end_src

*** Make these two steps automatic under void

In principle it should be enough to set the above steps into =/etc/rc.local=.

** Setting up the chips in TOS

This section is specific to the Septemboard used at CAST.

Following the steps described in the shifter documentation
[[file:~/org/Doc/ShiftDocumentation/shifter_documentation.org]]
#+begin_src sh
#+BEGIN_SRC python
7 # number of chips
4 # preload
SetChipIDOffset
190
lf
# 7 times enter to load default paths
uma
1 # Matrix settings
0
1
1
0
LoadThreshold # load threshold equalisation files
4 # write matrix
3 # read out
3
ActivateHFM
SetFadcSettings
Run
1 # run time via # frames
0
0
0
2 # shutter range select
30 # shutter time select
0 # zero suppression
1 # FADC usage
0 # accept FADC settings
#+end_src

The above would launch a full background run.



* Window rupture and vacuum contamination                 :Appendix:noexport:
:PROPERTIES:
:CUSTOM_ID: sec:appendix:vacuum_contamination
:END:

*NOTE*: This section is a direct copy of my notes about the calculation of the
possible contamination of the LLNL telescope.



This file is a simple readme about the calculation of the potential
vacuum contamination of the system. The rough ideas are stated here.

** Calculation of vacuum volume
In order to calculate the total amount of gas, which entered the
vacuum volume, we first need to calculate the total volume of the
vacuum system. 

The calculation of the volume will be done in the Nim calculation
[[file:~/CAST/VacuumContamination/vacuum_contamination.nim][vacuum_contamination.nim]].

*** Tubing
The following lists the different pieces of vacuum piping, tubes etc.:
**** Static tubing:
This table lists the static tubes used in the vacuum system:

Table A:
| Diameter / mm | Length / cm | Index | Notes                                  |
|---------------+-------------+-------+----------------------------------------|
|            63 |          10 |     1 | telescope to gate valve                |
|            63 |          51 |     2 | telescope to detector, stainless steel |
|            63 |        21.5 |     3 | between st. steel tube & copper tube   |
|            25 |        33.7 |     4 | copper tube                            |
|            63 |          20 |     5 | bellow in front of telescope           |
|             ? |          50 |     6 | telescope                              |
|            40 |        15.5 |     7 | piece needle valve connects to         |
|            16 |          13 |     8 | Pirani <-> Mini turbo                  |
|            40 |          10 |    10 | 90 deg next to cross for needle valve  |


**** Flexible tubing:

Table B:
| Diameter / mm | Length / cm | Index | Notes                                         |
|---------------+-------------+-------+-----------------------------------------------|
|            16 |          25 |     1 | connection from needle valve to primary 1 / 4 |
|            16 |          25 |     2 | 2 / 4                                         |
|            16 |          25 |     3 | 3 / 4                                         |
|            16 |          25 |     4 | 4 / 4                                         |
|            16 |          40 |     5 | to needle valve                               |
|            25 |          90 |     6 | Pirani <-> Mini turbo                         |
|            25 |          80 |     7 | Mini turbo <-> T piece towards primary        |
|            40 |          50 |     8 | before turbo pump                             |
|            16 |         150 |     9 | connection to primary                         |
|            40 |          80 |    10 | needle valve to turbo, before index C3        |
|            40 |          80 |    11 | to turbo pump, after index C3                 |


**** T-pieces:

Table C:
| Diameter / mm | Length / cm | Index | Notes                                        |
|---------------+-------------+-------+----------------------------------------------|
|            40 | 18 x 21     |     1 | orthogonal to 3A                             |
|            16 | 7 x 4.5     |     2 | in between mini turbo, needle valve, primary |
|            40 | 10          |     3 | T-piece connection of P-MM                   |


**** Crosses:

Table D:
| Diameter / mm | Length / cm | Index | Notes                           |
|---------------+-------------+-------+---------------------------------|
|            16 | 10 x 10     |     1 | before primary                  |
|            40 | 14 x 14     |     2 | before turbo, behind gate valve |
|            40 | 14 x 14     |     3 | same as above                   |
|            40 | 14 x 14     |     4 | cross at needle valve           |




*** Implementation of tubing and calculation of volume
Given the tubing data as above, the nim module defines a TubesMap
datastructure, which is simply an object, with 4 different fields. One
for each type of vacuum tubing:
- static
- flexible
- T-pieces
- crosses
where for each a sequence of tuples is created given (diameter / mm,
length / cm).
This is defined in [[file:~/CAST/VacuumContamination/tubing.nim][tubing.nim]], which also offers a function to get
such a TubesMap for the data.

This tubing object is taken in the main() and handed to the
calcTotalVacuumVolume(), which uses the helper volume functions to
calculate the total vacuum volume.

In a functional style using map, we iterate over each of the fields ot
TubesMap one after another. For each item (a tuple of floats), an
anonymous function is used to calculate the volume of that specific
part. 

Finally, each volume element, which is added to the new sequence, is
summed after map is completed to give the total volume.

This amounts to 
#+BEGIN_LaTeX
$V_{\text{vacuum}} \approx 10.88 \, \mathrm{l}$.
#+END_LaTeX

** Calculation of potential influx of gas

Once we have the total volume of the vacuum system, we still need to
potential amount of gas, which entered the system. 

This can be separated into two parts. 
1. An static initial state given by the detector volume under the pressure at
   which the window burst:
#+BEGIN_LaTeX
$n_{\text{inital}} = \frac{p_{\text{burst}} V_{\text{det}}}{R T_{\text{amb}}}$.
#+END_LaTeX
2. And afterwards a dynamical flow, given by the compressed air tube,
   inserting gas until it was shut off, after
   $\SIrange{2}{5}{\second}$. For this one needs to consider the
   following. The compressed air tries to supply 6 bar. Assuming the
   last gauge sees $\SI{6}{\bar}$ the whole time. From there
   $\SI{2}{\meter}$ of tubing with about $\SI{3}{\milli \meter}$ inner
   diameter, results in a pressure of
   #+BEGIN_LaTeX
   $p_{\text{exit}} = p_i - z L \frac{\partial V}{\partial t}$
   #+END_LaTeX
   (or something like this?). $p_{\text{exit}}$ is the pressure at the
   end of the tube, i.e. inside the detector. $p_i$ the initial
   $\SI{6}{\bar}$, while $z$ is the specific impedance (per length) of
   the tube for the compressed air. The partial derivative should
   describe the flow of the gas. Analogous to currents, an impedance
   should drop the pressure inside the tube depending on the length
   $L$ due to the flow of gas inside it. Problematic to estimate
   impedance of the tubes. Look into Demtröder etc.
   
Given this, one can calculate the flow into the detector for the
time gas was still flowing. 
#+BEGIN_LaTeX
$n_{\text{total}} = n_{\text{initial}} + \frac{p_{\text{exit}}}{R T_{\text{amb}}}\frac{\mathrm{d}V}{\mathrm{d}t} t$
#+END_LaTeX
Or something similar...   

Alternative way to estimate total gas is to consider increase of
pressure inside of the $\sim \SI{11}{\liter}$ of vacuum volume while
the turbopumps and primary were still running. However, this is
probably less accurate, because this should be highly non-linear,
since the turbos shut off immediately (ramping down slowly,
i.e. pumping less and less). Primary kept pumping for about
$\SI{2}{\minute}$. 

Note: Upper parts remain for now, change approach of 2. point above.
We calculate the flow rate of the compressed air inside the tube using
the Poiseuille equation
#+BEGIN_LaTeX
$Q = \frac{\pi D^4 \Delta P}{128 \mu \Delta x}$,
#+END_LaTeX
where $Q$ is the flow rate in $\si{\liter\per\second}$, $D$ the
diameter of the tube, $\Delta P$ the pressure difference between both
ends of the tube, $\mu$ the dyanmic viscosity of air and $\Delta x$
the length of the tube. Regarding the dynamic viscosity, we use 
https://www.lmnoeng.com/Flow/GasViscosity.php
to calculate the viscosity of the compressed air. As a good
approximation, the dynamic viscosity is unchanged under pressure
changes
(https://www.quora.com/What-is-the-effect-of-pressure-on-viscosity-of-gases-and-liquids),
which means we can use the above calculator for air at
$\SI{20}{\celsius}$ to get a value of 
#+BEGIN_LaTeX
$\mu = \SI{1.8369247E-5}{\pascal \second}$.
#+END_LaTeX
In principle we need to check, whether the tube still contains laminar
flow, which we can do following:
https://engineering.stackexchange.com/questions/8004/how-to-calculate-flow-rate-of-water-through-a-pipe.
This calculation results in a value of
#+BEGIN_LaTeX
$Q_{\text{air, laminar}} = \SI{3.246}{\liter \per \second}$, 
#+END_LaTeX
which should be a good upper bound, since the equation is only valid
for laminar, incompressible fluids with a small pressure
gradient. Especially the last is definitely not valid, while the first
two are at least questionable.

Quick calculation of the Reynold's factor (to determine laminar or
turbulent flow), shows that (using velocity of flow $v$):
#+BEGIN_LaTeX
\[
v = \frac{Q}{A}
\]
\[
\mathrm{Re} = \frac{\rho d v}{\mu}
\]
#+END_LaTeX

#+BEGIN_SRC nim :exports both
import math
let v = 3.246e-3 / (PI * pow(1.5e-3, 2))
echo v
let Re = 1.225 * 1.5e-3 * v / (1.8369e-5)
echo Re
#+END_SRC

#+RESULTS:
| 459.2150624678154 |
| 45936.50592218471 | 
which shows that this calculation is wrong on several levels. The
speed of the flow is much too high I would assume. Although one thing
is to be noted: given a flow of compressed air into a vacuum, one
might expect a speed similar to the speed of sound of the inlet
pressure?!

At the same time, if one were to trust this, it suggests the flow to
be in the turbulent range (cp. laminar is $Re < \num{2300}$).

Maximal bound given by Bernouilli's principle
#+BEGIN_SRC nim :exports both
import math
let v = sqrt(2 * 6e5 / 1.2)
let Q = PI * pow(1.5e-3, 2) * v
echo v
echo Q * 1000
#+END_SRC

#+RESULTS:
|            1000.0 |
| 7.068583470577035 |
meaning a speed of $\SI{1000}{\meter \per \second}$ and a maximal flow
of $\SI{7.07}{\liter\per\second}$. 

On the other hand for a more practical value (ignoring more complex
calculations including turbulent, incompressible gases), see the
following plot from
http://www.engineeringtoolbox.com/air-flow-compressed-air-pipe-line-d_1280.html:

#+CAPTION: Compressed air capacities for different inner sizes. 1/8" roughly 3mm inner tube
#+NAME: fig::comp-air-capacity
[[file:~/org/Figs/compressed-air-pipeline-capacity-liter.png]]

shows a capacity of the compressed air line of
$\sim\SI{2}{\liter\per\second}$.

Thus, we can safely assume the calculated
$\SI{3.25}{\liter\per\second}$ to be a worst case scenario.

Hence, the total amount of air introduced into the system is:
#+BEGIN_LaTeX
$n_{\text{total}} = n_{\text{initial}} + \frac{Q_{\text{comp. air}} \cdot \SI{5}{\second} \cdot p_{\text{atm}}}{R T_{\text{amb}}}$,
where 
$Q_{\text{comp. air}} = \SI{3.246}{\liter\per\second}$
#+END_LaTeX
Ends up to be:
#+BEGIN_LaTeX
$n_{\text{total}} = n_{\text{initial}} + n_{\text{flow}} = \SI{0.0069}{\mol} + \SI{0.666}{\mol} = \SI{0.673}{\mol}$
#+END_LaTeX
Given in volume at normal conditions, this results in a gas volume of
#+BEGIN_LaTeX
$V_{\text{gas}} = \SI{16.4}{\liter}$
#+END_LaTeX

** Consider pumping of pumps

Since the pumps were still running during this period, they would have
extracted most of the gas immediately again. See last point in
previous section.

** Calculation of possible contamination

Finally, given the total vacuum volume and the amount of gas, which
entered the system, we can estimate the potential contamination. 

With the vacuum volume and the gas flowing into the system, the
maximum possible contamination can be estimated. The upper limit is of
course all contaminations in the gas forming a monolayer in the whole
vacuum system. Assuming a certain ppm contamination in the gas, the
maximum contamination is simply
#+BEGIN_LaTeX
$d_{\text{cont}} = \frac{n_{\text{total}} R T_{\text{amb}} \cdot q_{\text{cont}}}{A_{\text{vacuum}}}$
#+END_LaTeX
where $q_{\text{cont}}$ is the ppm contaminiation in the total gas
volume $nRT$, which enterd, while $A_{\text{vacuum}}$ is the total
surface area of all vacuum tubing. 

#+BEGIN_COMMENT 
However, first of all calculate the total amount of oil, which entered
the system assuming a (very high) 1 ppm, based on the calculated total
moles entering the vacuum of $n_{\text{total}} =
\SI{0.673}{\mol}$. This means a total of $\SI{6.73e-7}{\mol}$ of oil
entered the vacuum. 
#+END_COMMENT

However, first we estimate the amount of oil, which entered the system
from a typical oil contamination in compressed air. The ISO standard
ISO 8573-1:2010 defines different classes for compressed air in
different applications. Classes regarding oil contamination range from
0 to 4, with class 4 being the worst. Class 4 calls for
$\text{ppmv}_{\text{oil}} \leq \SI{5}{\milli
\gram\per\meter\cubed.}$. Thus, even if CERN's compressed air is a lot
worse than this, $\text{ppmv}_{\text{oil}} \approx
\SI{10}{\milli\gram\per\meter\cubed.}$ should be sufficient as a
baseline.

This means the entered air will contain about
#+BEGIN_SRC nim :exports both
import math
let air_vol = 16.4
let ppmv = 10e-3
echo ppmv * air_vol
#+END_SRC

#+RESULTS:
: 0.164

which is $\SI{0.164}{\mg}$ of oil. 

The telescope surface is not known exactly. No time to find out until
this needs to be done. Can be checked again later with Jaime / find
slides, paper about LLNL telescope, to get better numbers. Assuming 10
quarter shells of a radius of $\SI{5}{\centi\meter}$ (some larger,
some smaller radius), the telescope has an area of:
#+BEGIN_SRC nim :exports both nim
import math
let A = 10.0 * 0.5 * PI * 0.05 * 0.5
echo A
#+END_SRC

#+RESULTS:
: 0.3926990816987241
, i.e. an area of $A_{\text{telescope}} =
\SI{0.393}{\meter\squared}$. If all of the oil was placed on the
telescope, this would result in
#+BEGIN_SRC nim :exports both nim
import math
let A = 0.393
let oil_mg = 0.164
let ratio = oil_mg / A * 1e-4
echo ratio
#+END_SRC

#+RESULTS:
: 4.173027989821883e-05
A contamination of $d_{\text{max, cont}} =
\SI{41.73}{\nano\gram\per\cm\squared}$ is an upper bound on oil on the
telescope. Realistically, the telescope only has $< \frac{1}{10}$ of
the total vacuum surface area, while probably $> \SI{90}{\percent}$ of
the oil will have left the system via the pumps, pushing the
contamination as low as $d_{\text{cont}} <
\SI{0.4173}{\nano\gram\per\cm\squared}$. 

This may still sounds like quite a bit, but the assumptions made here
are all extremely conservative:
- 5 seconds with an open valve. More likely it was about
  \SI{3}{\second} => factor of $3/5$.
- flow of compressed air of $\SI{3.25}{\liter\per\second}$, more
  likely about $\SI{2}{\liter\per\second}$ => factor of another
  $2/3.25$.
- area of telescope vs total area of gas volume (hard to calculate due
  to flexible tubing) probably quite a bit less than 1 / 10 ?
- $\SI{10}{\percent}$ of oils sticking to surface probably also
  extremely unlikely. Maybe 2 orders of magnitude less?

#+BEGIN_SRC nim :exports both
let factors = (3.0 / 5.0) * (2.0 / 3.25) * 0.5 * 1e-2
echo factors
echo factors * 0.4173
#+END_SRC

#+RESULTS:
|  0.001846153846153846 |
| 0.0007704000000000001 | 
Meaning values as low as $d_{\text{cont}} <
\SI{0.7704}{\pico \gram\per\cm\squared}$ may even be more
reasonable. It is probably safe to say that this level is easily
reached if the telescope sits open during installation etc.



# A contamination of $d_{\text{max, cont}} =
# \SI{1.715e-10}{\mol\per\cm\squared}$. Realistically, the telescope only
# has $< \frac{1}{10}$ of the total vacuum surface area, while probably
# $> \SI{90}{\percent}$ of the oil will have left the system via the
# pumps, pushing the contamination as low as $d_{\text{cont}} <
# \SI{1.74e-12}{\mol\per\cm\squared}$.  Still a lot of particles, but
# probably much less than contaminating the telescope due to exposure to
# air.

# #+BEGIN_SRC nim :exports both nim
# let saw = 500.0
# echo 1.74e-12 * saw
# #+END_SRC

# #+RESULTS:
# : 8.7e-10

# This is about $\SI{8.7e-10}{\milli \gram \per \cm\squared}$ of oil. This
# still sounds like quite a bit, but the assumptions made here are all
# extremely conservative:
# - 5 seconds with an open valve. More likely it was about
#   \SI{3}{\second} => factor of $3/5$.
# - flow of compressed air of $\SI{3.25}{\liter\per\second}$, more
#   likely about $\SI{2}{\liter\per\second}$ => factor of another
#   $2/3.25$.
# - area of telescope vs total area of gas volume (hard to calculate due
#   to flexible tubing) probably quite a bit less than 1 / 10.
# - $\SI{10}{\percent}$ of oils sticking to surface probably also
#   extremely unlikely. Maybe 2 orders of magnitude less.

# #+BEGIN_SRC nim
# let factors = (3.0 / 5.0) * (2.0 / 3.25) * 0.5 * 1e-2
# echo factors
# echo factors * 2.6
# #+END_SRC

# #+RESULTS:
# | 0.001846153846153846 |
# |               0.0048 |
# => Would result in about $\SI{5}{\micro \gram \per \cm \squared}$.




# #+BEGIN_SRC nim :exports both
# import math
# let oil_g = 16.4e-3
# let g_per_mol = 500.0
# echo oil_g / g_per_mol

# echo 6.73e-7 * 500.0

# echo oil_g / 0.8

# #+END_SRC

# #+RESULTS:
# |  3.28e-05 |
# | 0.0003365 |
# |    0.0205 |


* Detector behavior over time                                      :Appendix:
:PROPERTIES:
:CUSTOM_ID: sec:appendix:detector_time_behavior
:END:

In section [[#sec:calib:detector_behavior_over_time]] we covered the
median charge and energies of clusters in the background and
calibration data. Here in the appendix

- [ ] *PROBABLY SCRAP THIS IF WE PUT BOTH DATA INTO ONE PLOT*

** Choice of gas gain binning time interval
:PROPERTIES:
:CUSTOM_ID: sec:appendix:choice_gas_gain_binning  
:END:

The following figures show the behavior of the different time
intervals for the choice of 'ideal' gas gain time binning for all run
periods (not in the sense of Run-2 and Run-3, but those split by
significant off time). In addition
fig. [[fig:appendix:calib:gof_tests_different_binnings]] shows the results
of applying a range of goodness of fit tests to the cluster data.

Note that the repository of this thesis contains even more figures
related to this in the ~Figs/behavior_over_time~ directory.

#+CAPTION: Kernel density estimation of the median energies split by the somewhat
#+CAPTION: distinct run periods and the time intervals used. A KDE instead of a histogram
#+CAPTION: is used as the binning has too large of an impact for the dataset.
#+NAME: fig:appendix:calib:median_energy_kde_intervals
[[~/phd/Figs/behavior_over_time/medianEnergy_kde_intervals.pdf]]

#+CAPTION: Comparison of the different time intervals in each run period using a set
#+CAPTION: of different goodness of fit tests. The $\SI{45}{min}$ interval seems optimal
#+CAPTION: in the 30/10/2017 period, but worse in others. The $\SI{90}{min}$ interval is
#+CAPTION: average in most cases.
#+NAME: fig:appendix:calib:gof_tests_different_binnings
[[~/phd/Figs/behavior_over_time/gofs_for_different_binnings.pdf]]

#+CAPTION: Equivalent plot to fig. [[fig:calib:median_energy_ridgeline_30_10_2017]] for data from
#+CAPTION: Feb 2018 to Apr 2018.
#+CAPTION: Ridgeline plot of a kernel density estimation (bandwidth based on Silverman's rule of thumb)
#+CAPTION: of the median cluster energies split by the used time intervals. The overlap of the individual ridges is for
#+CAPTION: easier visual comparison and a KDE was selected over a histogram due to strong
#+CAPTION: binning dependence of the resulting histograms. 
#+NAME: fig:calib:median_energy_ridgeline_Feb2018
[[~/phd/Figs/behavior_over_time/medianEnergy_kde_ridges_17_02_2018.pdf]]

#+CAPTION: Equivalent plot to fig. [[fig:calib:median_energy_ridgeline_30_10_2017]] for data from
#+CAPTION: Oct 2018 to Dec 2018.
#+CAPTION: Ridgeline plot of a kernel density estimation (bandwidth based on Silverman's rule of thumb)
#+CAPTION: of the median cluster energies split by the used time intervals. The overlap of the individual ridges is for
#+CAPTION: easier visual comparison and a KDE was selected over a histogram due to strong
#+CAPTION: binning dependence of the resulting histograms.
#+NAME: fig:calib:median_energy_ridgeline_Oct2018
[[~/phd/Figs/behavior_over_time/medianEnergy_kde_ridges_21_10_2018.pdf]]


** Correlation of gas gain and ambient CAST temperature

Let's look at the rest of the data not shown in
sec. [[#sec:calib:causes_variability]]. First in
fig. [[fig:appendix:correlation_ambient_temperature_gasgain_and_spectra_run2_2017]]
we see the same plot as
fig. [[fig:calib:correlation_ambient_temperature_gasgain_and_spectra]],
but only for Run-2 data from 2017. The anti-correlation is not quite
visible here, instead in parts it seems like the expected correlation
of temperature and gas gain is
visible. Fig. [[fig:appendix:correlation_ambient_temperature_gasgain_and_spectra_run2_2018]]
is the data for Feb 2018 to Apr 2018. Here there seems to be some of
the anti correlation, but less than in the Run-3 data presented in the
main of the discussion. Finally,
fig. [[fig:appendix:gas_gain_vs_ambient_temp_center]] shows the gas gain
of the center chip plotted directly against the ambient temperature at
CAST as a scatter plot. Here the anti correlation becomes very visible
as a global trend.


#+CAPTION: Normalized data for Run-2 (only 2017) of the temperature sensors from the CAST slow control log
#+CAPTION: files compared to the behavior of the mean peak position in the \cefe pixel spectra
#+CAPTION: (black points), the recovered temperature values recorded during each solar tracking
#+CAPTION: (blue points) and the gas gain values computed based on \SI{90}{min} of data for each
#+CAPTION: chip (smaller points using Viridis color scale). The shift log temperatures nicely
#+CAPTION: follow the trend of the general temperatures. In this period no real anti-correlation is
#+CAPTION: visible. Instead in parts it looks like the expected proportionality between temperature
#+CAPTION: and gas gain appears.
#+NAME: fig:appendix:correlation_ambient_temperature_gasgain_and_spectra_run2_2017
#+ATTR_LATEX: :float sideways
[[~/phd/Figs/behavior_over_time/correlation_fePixel_all_chips_gasgain_period_2017-10-30.pdf]]

#+CAPTION: Normalized data for Run-2 (only Feb. to Apr. of 2018) of the temperature sensors from the CAST slow control log
#+CAPTION: files compared to the behavior of the mean peak position in the \cefe pixel spectra
#+CAPTION: (black points), the recovered temperature values recorded during each solar tracking
#+CAPTION: (blue points) and the gas gain values computed based on \SI{90}{min} of data for each
#+CAPTION: chip (smaller points using Viridis color scale). The shift log temperatures nicely
#+CAPTION: follow the trend of the general temperatures. Here the anti correlation seems to
#+CAPTION: be visible in some parts, but also less extreme than in the end of 2018 Run-3 data,
#+CAPTION: presented in the main section.
#+NAME: fig:appendix:correlation_ambient_temperature_gasgain_and_spectra_run2_2018
#+ATTR_LATEX: :float sideways
[[~/phd/Figs/behavior_over_time/correlation_fePixel_all_chips_gasgain_period_2018-02-15.pdf]]

#+CAPTION: Gas gains of the center chip (by $\SI{90}{min}$ time slices) against the ambient
#+CAPTION: temperature at CAST. As a general trend the anti correlation is very visible. Also
#+CAPTION: visible though is that for the 2017 Run-2 data that effect does not really appear.
#+NAME: fig:appendix:gas_gain_vs_ambient_temp_center
[[~/phd/Figs/behavior_over_time/gain_vs_temp_center_chip.pdf]]



* CAST Detector Lab data                                           :Appendix:
:PROPERTIES:
:CUSTOM_ID: sec:appendix:cast_detector_lab
:END:

** All spectra split by run
:PROPERTIES:
:CUSTOM_ID: sec:appendix:cdl_spectra_by_run
:END:

\begin{figure}[htbp]
  \centering
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{/home/basti/phd/Figs/CDL/C-EPIC-0.6kV-2019_by_run.pdf}
    \caption{Pixel spectrum}
    \label{fig:appendix:cdl_pixel_cEpic0_6_by_run}
  \end{subfigure}%
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{/home/basti/phd/Figs/CDL/C-EPIC-0.6kVCharge-2019_by_run.pdf}
    \caption{Charge spectrum}
    \label{fig:appendix:cdl_charge_cEpic0_6_by_run}
  \end{subfigure}%
  \label{fig:appendix:cdl_cEpic0_6_by_run}
  \caption{\subref{fig:appendix:cdl_pixel_cEpic0_6_by_run} Pixel spectrum and \subref{fig:appendix:cdl_charge_cEpic0_6_by_run}
  charge spectrums of the raw data split by the data taking run.}
\end{figure}

\begin{figure}[htbp]
  \centering
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{/home/basti/phd/Figs/CDL/Cu-EPIC-0.9kV-2019_by_run.pdf}
    \caption{Pixel spectrum}
    \label{fig:appendix:cdl_pixel_cuEpic0_9_by_run}
  \end{subfigure}%
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{/home/basti/phd/Figs/CDL/Cu-EPIC-0.9kVCharge-2019_by_run.pdf}
    \caption{Charge spectrum}
    \label{fig:appendix:cdl_charge_cuEpic0_9_by_run}
  \end{subfigure}%
  \label{fig:appendix:cdl_cuEpic0_9_by_run}
  \caption{\subref{fig:appendix:cdl_pixel_cuEpic0_9_by_run} Pixel spectrum and \subref{fig:appendix:cdl_charge_cuEpic0_9_by_run}
  charge spectrums of the raw data split by the data taking run.}
\end{figure}

\begin{figure}[htbp]
  \centering
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{/home/basti/phd/Figs/CDL/Cu-EPIC-2kV-2019_by_run.pdf}
    \caption{Pixel spectrum}
    \label{fig:appendix:cdl_pixel_cuEpic2_by_run}
  \end{subfigure}%
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{/home/basti/phd/Figs/CDL/Cu-EPIC-2kVCharge-2019_by_run.pdf}
    \caption{Charge spectrum}
    \label{fig:appendix:cdl_charge_cuEpic2_by_run}
  \end{subfigure}%
  \label{fig:appendix:cdl_cuEpic2_by_run}
  \caption{\subref{fig:appendix:cdl_pixel_cuEpic2_by_run} Pixel spectrum and \subref{fig:appendix:cdl_charge_cuEpic2_by_run}
  charge spectrums of the raw data split by the data taking run.}
\end{figure}

\begin{figure}[htbp]
  \centering
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{/home/basti/phd/Figs/CDL/Al-Al-4kV-2019_by_run.pdf}
    \caption{Pixel spectrum}
    \label{fig:appendix:cdl_pixel_alAl4kV_by_run}
  \end{subfigure}%
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{/home/basti/phd/Figs/CDL/Al-Al-4kVCharge-2019_by_run.pdf}
    \caption{Charge spectrum}
    \label{fig:appendix:cdl_charge_alAl4kV_by_run}
  \end{subfigure}%
  \label{fig:appendix:cdl_charge_alAl4kV_by_run}
  \caption{\subref{fig:appendix:cdl_pixel_alAl4kV_by_run} Pixel spectrum and \subref{fig:appendix:cdl_charge_alAl4kV_by_run}
  charge spectrums of the raw data split by the data taking run.}
\end{figure}

\begin{figure}[htbp]
  \centering
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{/home/basti/phd/Figs/CDL/Ag-Ag-6kV-2019_by_run.pdf}
    \caption{Pixel spectrum}
    \label{fig:appendix:cdl_pixel_agAg6kV_by_run}
  \end{subfigure}%
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{/home/basti/phd/Figs/CDL/Ag-Ag-6kVCharge-2019_by_run.pdf}
    \caption{Charge spectrum}
    \label{fig:appendix:cdl_charge_agAg6kV_by_run}
  \end{subfigure}%
  \label{fig:appendix:cdl_charge_agAg6kV_by_run}
  \caption{\subref{fig:appendix:cdl_pixel_agAg6kV_by_run} Pixel spectrum and \subref{fig:appendix:cdl_charge_agAg6kV_by_run}
  charge spectrums of the raw data split by the data taking run.}
\end{figure}

\begin{figure}[htbp]
  \centering
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{/home/basti/phd/Figs/CDL/Ti-Ti-9kV-2019_by_run.pdf}
    \caption{Pixel spectrum}
    \label{fig:appendix:cdl_pixel_tiTi9kV_by_run}
  \end{subfigure}%
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{/home/basti/phd/Figs/CDL/Ti-Ti-9kVCharge-2019_by_run.pdf}
    \caption{Charge spectrum}
    \label{fig:appendix:cdl_charge_tiTi9kV_by_run}
  \end{subfigure}%
  \label{fig:appendix:cdl_charge_tiTi9kV_by_run}
  \caption{\subref{fig:appendix:cdl_pixel_tiTi9kV_by_run} Pixel spectrum and \subref{fig:appendix:cdl_charge_tiTi9kV_by_run}
  charge spectrums of the raw data split by the data taking run.}
\end{figure}


\begin{figure}[htbp]
  \centering
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{/home/basti/phd/Figs/CDL/Mn-Cr-12kV-2019_by_run.pdf}
    \caption{Pixel spectrum}
    \label{fig:appendix:cdl_pixel_mnCr12kV_by_run}
  \end{subfigure}%
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{/home/basti/phd/Figs/CDL/Mn-Cr-12kVCharge-2019_by_run.pdf}
    \caption{Charge spectrum}
    \label{fig:appendix:cdl_charge_mnCr12kV_by_run}
  \end{subfigure}%
  \label{fig:appendix:cdl_charge_mnCr12kV_by_run}
  \caption{\subref{fig:appendix:cdl_pixel_mnCr12kV_by_run} Pixel spectrum and \subref{fig:appendix:cdl_charge_mnCr12kV_by_run}
  charge spectrums of the raw data split by the data taking run.}
\end{figure}

\begin{figure}[htbp]
  \centering
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{/home/basti/phd/Figs/CDL/Cu-Ni-15kV-2019_by_run.pdf}
    \caption{Pixel spectrum}
    \label{fig:appendix:cdl_pixel_cuNi15kV_by_run}
  \end{subfigure}%
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{/home/basti/phd/Figs/CDL/Cu-Ni-15kVCharge-2019_by_run.pdf}
    \caption{Charge spectrum}
    \label{fig:appendix:cdl_charge_cuNi15kV_by_run}
  \end{subfigure}%
  \label{fig:appendix:cdl_charge_cuNi15kV_by_run}
  \caption{\subref{fig:appendix:cdl_pixel_cuNi15kV_by_run} Pixel spectrum and \subref{fig:appendix:cdl_charge_cuNi15kV_by_run}
  charge spectrums of the raw data split by the data taking run.}
\end{figure}

\clearpage

** All CDL spectra with line fits
:PROPERTIES:
:CUSTOM_ID: sec:appendix:cdl:all_spectra_fits
:END:

- [ ] *ADD TABLE OF THE FITS PERFORMED HERE?*
- [ ] *NEED TO REPLACE BELOW BY THE PLOTS FOR THE INDIVIDUAL RUNS*
  -> This is a good idea then to remove the pixel spectra that are
  still present here and replace them by side-by-side versions of the
  different runs.


  

Any missing parameter indicates it was fixed. For lines with an explicit 'fixed', all
parameters were fixed.

\begin{figure}[htbp]
  \centering
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{/home/basti/phd/Figs/CDL/C-EPIC-0.6kV-2019.pdf}
    \caption{Pixel spectrum}
    \label{fig:appendix:cdl_pixel_cEpic0_6}
  \end{subfigure}%
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{/home/basti/phd/Figs/CDL/C-EPIC-0.6kVCharge-2019.pdf}
    \caption{Charge spectrum}
    \label{fig:appendix:cdl_charge_cEpic0_6}
  \end{subfigure}%
  \label{fig:appendix:cdl_cEpic0_6}
  \caption{\subref{fig:appendix:cdl_pixel_cEpic0_6} Pixel spectrum including fit parameters and raw and cut data
  of the C-EPIC (target-filter) dataset at $\SI{0.6}{kV}$ voltage. \subref{fig:appendix:cdl_charge_cEpic0_6} equivalent
  for the charge spectrum.}
\end{figure}

\begin{figure}[htbp]
  \centering
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{/home/basti/phd/Figs/CDL/Cu-EPIC-0.9kV-2019.pdf}
    \caption{Pixel spectrum}
    \label{fig:appendix:cdl_pixel_cuEpic0_9}
  \end{subfigure}%
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{/home/basti/phd/Figs/CDL/Cu-EPIC-0.9kVCharge-2019.pdf}
    \caption{Charge spectrum}
    \label{fig:appendix:cdl_charge_cuEpic0_9}
  \end{subfigure}%
  \label{fig:appendix:cdl_cuEpic0_9}
  \caption{\subref{fig:appendix:cdl_pixel_cuEpic0_9} Pixel spectrum including fit parameters and raw and cut data
  of the Cu-EPIC (target-filter) dataset at $\SI{0.9}{kV}$ voltage. \subref{fig:appendix:cdl_charge_cuEpic0_9} equivalent
  for the charge spectrum.}
\end{figure}

\begin{figure}[htbp]
  \centering
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{/home/basti/phd/Figs/CDL/Cu-EPIC-2kV-2019.pdf}
    \caption{Pixel spectrum}
    \label{fig:appendix:cdl_pixel_cuEpic2}
  \end{subfigure}%
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{/home/basti/phd/Figs/CDL/Cu-EPIC-2kVCharge-2019.pdf}
    \caption{Charge spectrum}
    \label{fig:appendix:cdl_charge_cuEpic2}
  \end{subfigure}%
  \label{fig:appendix:cdl_cuEpic2}
  \caption{\subref{fig:appendix:cdl_pixel_cuEpic2} Pixel spectrum including fit parameters and raw and cut data
  of the Cu-EPIC (target-filter) dataset at $\SI{2.0}{kV}$ voltage. \subref{fig:appendix:cdl_charge_cuEpic2} equivalent
  for the charge spectrum.}
\end{figure}

\begin{figure}[htbp]
  \centering
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{/home/basti/phd/Figs/CDL/Al-Al-4kV-2019.pdf}
    \caption{Pixel spectrum}
    \label{fig:appendix:cdl_pixel_alAl4kV}
  \end{subfigure}%
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{/home/basti/phd/Figs/CDL/Al-Al-4kVCharge-2019.pdf}
    \caption{Charge spectrum}
    \label{fig:appendix:cdl_charge_alAl4kV}
  \end{subfigure}%
  \label{fig:appendix:cdl_charge_alAl4kV}
  \caption{\subref{fig:appendix:cdl_pixel_alAl4kV} Pixel spectrum including fit parameters and raw and cut data
  of the Al-Al (target-filter) dataset at $\SI{4}{kV}$ voltage. \subref{fig:appendix:cdl_charge_alAl4kV} equivalent
  for the charge spectrum.}
\end{figure}

\begin{figure}[htbp]
  \centering
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{/home/basti/phd/Figs/CDL/Ag-Ag-6kV-2019.pdf}
    \caption{Pixel spectrum}
    \label{fig:appendix:cdl_pixel_agAg6kV}
  \end{subfigure}%
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{/home/basti/phd/Figs/CDL/Ag-Ag-6kVCharge-2019.pdf}
    \caption{Charge spectrum}
    \label{fig:appendix:cdl_charge_agAg6kV}
  \end{subfigure}%
  \label{fig:appendix:cdl_charge_agAg6kV}
  \caption{\subref{fig:appendix:cdl_pixel_agAg6kV} Pixel spectrum including fit parameters and raw and cut data
  of the Ag-Ag (target-filter) dataset at $\SI{6}{kV}$ voltage. \subref{fig:appendix:cdl_charge_agAg6kV} equivalent
  for the charge spectrum.}
\end{figure}

\begin{figure}[htbp]
  \centering
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{/home/basti/phd/Figs/CDL/Ti-Ti-9kV-2019.pdf}
    \caption{Pixel spectrum}
    \label{fig:appendix:cdl_pixel_tiTi9kV}
  \end{subfigure}%
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{/home/basti/phd/Figs/CDL/Ti-Ti-9kVCharge-2019.pdf}
    \caption{Charge spectrum}
    \label{fig:appendix:cdl_charge_tiTi9kV}
  \end{subfigure}%
  \label{fig:appendix:cdl_charge_tiTi9kV}
  \caption{\subref{fig:appendix:cdl_pixel_tiTi9kV} Pixel spectrum including fit parameters and raw and cut data
  of the Ti-Ti (target-filter) dataset at $\SI{9}{kV}$ voltage. \subref{fig:appendix:cdl_charge_tiTi9kV} equivalent
  for the charge spectrum.}
\end{figure}


\begin{figure}[htbp]
  \centering
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{/home/basti/phd/Figs/CDL/Mn-Cr-12kV-2019.pdf}
    \caption{Pixel spectrum}
    \label{fig:appendix:cdl_pixel_mnCr12kV}
  \end{subfigure}%
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{/home/basti/phd/Figs/CDL/Mn-Cr-12kVCharge-2019.pdf}
    \caption{Charge spectrum}
    \label{fig:appendix:cdl_charge_mnCr12kV}
  \end{subfigure}%
  \label{fig:appendix:cdl_charge_mnCr12kV}
  \caption{\subref{fig:appendix:cdl_pixel_mnCr12kV} Pixel spectrum including fit parameters and raw and cut data
  of the Mn-Cr (target-filter) dataset at $\SI{12}{kV}$ voltage. \subref{fig:appendix:cdl_charge_mnCr12kV} equivalent
  for the charge spectrum.}
\end{figure}

\begin{figure}[htbp]
  \centering
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{/home/basti/phd/Figs/CDL/Cu-Ni-15kV-2019.pdf}
    \caption{Pixel spectrum}
    \label{fig:appendix:cdl_pixel_cuNi15kV}
  \end{subfigure}%
  \begin{subfigure}{0.5\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{/home/basti/phd/Figs/CDL/Cu-Ni-15kVCharge-2019.pdf}
    \caption{Charge spectrum}
    \label{fig:appendix:cdl_charge_cuNi15kV}
  \end{subfigure}%
  \label{fig:appendix:cdl_charge_cuNi15kV}c
  \caption{\subref{fig:appendix:cdl_pixel_cuNi15kV} Pixel spectrum including fit parameters and raw and cut data
  of the Cu-Ni (target-filter) dataset at $\SI{12}{kV}$ voltage. \subref{fig:appendix:cdl_charge_cuNi15kV} equivalent
  for the charge spectrum.}
\end{figure}


* CAST Detector Lab variations and fitting by run                  :Appendix:
:PROPERTIES:
:CUSTOM_ID: sec:appendix:fit_by_run_justification
:END:

The main difference between how the CAST detector lab data is treated
in cite:krieger2018search and in this thesis is that here we treat
each data taking run of the CDL data as one unit instead of all data
for one target/filter combination. This is mainly due to the strong
detector variation in the gas gain during the CDL data taking. As such
it is not possible to fit the spectra when combining all data from all
runs for a single target/filter combination. In the extreme cases
($\ce{Cu}-\ce{Ni}$ for example) this leads to visibly two main
peaks in the charge spectrum. As such each run is fit separately. The
one question about this approach is whether the cluster properties are
stable under gas gain
changes. Sec. [[#sec:appendix:fit_by_run:gas_gain_var_cluster_prop]] shows
that this is indeed the case. 

- [ ] *THINK ABOUT WHERE TO PUT TEMPERATURE INFORMATION FROM CDL
  DATA!*
  -> the temperature plot split by run is relatively
  interesting. Should appear at least in extended version.

** Influence of gas gain variations on cluster properties
:PROPERTIES:
:CUSTOM_ID: sec:appendix:fit_by_run:gas_gain_var_cluster_prop
:END:

The following figures are ridgeline plots of all relevant cluster
properties as introduced in sec. [[#sec:reco:cluster_geometry]]. For each
plot and each property all CDL runs are shown as kernel density
estimations. Outside the number of hits and total charge in a cluster
(which are expected to vary with gas gain of course) the properties
remain stable even in the cases that vary strongly.

- [ ] *ADD CAPTIONS AND LABELS TO THESE FIGURES!*
[[~/phd/Figs/CDL/C-EPIC-0.6kV_ridgeline_kde_by_run.pdf]]
[[~/phd/Figs/CDL/Cu-EPIC-0.9kV_ridgeline_kde_by_run.pdf]]
[[~/phd/Figs/CDL/Cu-EPIC-2kV_ridgeline_kde_by_run.pdf]]
[[~/phd/Figs/CDL/Al-Al-4kV_ridgeline_kde_by_run.pdf]]
[[~/phd/Figs/CDL/Ag-Ag-6kV_ridgeline_kde_by_run.pdf]]
[[~/phd/Figs/CDL/Ti-Ti-9kV_ridgeline_kde_by_run.pdf]]
[[~/phd/Figs/CDL/Mn-Cr-12kV_ridgeline_kde_by_run.pdf]]
[[~/phd/Figs/CDL/Cu-Ni-15kV_ridgeline_kde_by_run.pdf]]

** Data overview with pixel spectra [/]                           :noexport:

- [ ] *SHOULD THIS BE NOEXPORT OR NOT?*

#+CAPTION: Equivalent table to tab. [[tab:cdl:run_overview_tab]], but showing the fit results of the pixel
#+CAPTION: spectra. 
#+NAME: tab:cdl:run_overview_tab_pixels
|-----+-------+--------+--------+---------+-------------------------------+--------------+------------------------+-----------------------+-----------------------|
| Run | FADC? | Target | Filter | HV [kV] | Line                          | Energy [keV] | μ                      | σ                     | μ/σ                   |
|-----+-------+--------+--------+---------+-------------------------------+--------------+------------------------+-----------------------+-----------------------|
| 319 | y     | Cu     | Ni     |      15 | $\ce{Cu}$ $\text{K}_{\alpha}$ |         8.04 | $\num{3.2114(58)e+02}$ | $\num{1.826(57)e+01}$ | $\num{5.68(18)e-02}$  |
| 320 | n     | Cu     | Ni     |      15 |                               |              | $\num{3.1127(52)e+02}$ | $\num{2.280(48)e+01}$ | $\num{7.32(15)e-02}$  |
| 345 | y     | Cu     | Ni     |      15 |                               |              | $\num{2.6735(37)e+02}$ | $\num{2.007(34)e+01}$ | $\num{7.51(13)e-02}$  |
|-----+-------+--------+--------+---------+-------------------------------+--------------+------------------------+-----------------------+-----------------------|
| 315 | y     | Mn     | Cr     |      12 | $\ce{Mn}$ $\text{K}_{\alpha}$ |         5.89 | $\num{2.1680(98)e+02}$ | $\num{2.573(79)e+01}$ | $\num{1.187(37)e-01}$ |
| 323 | n     | Mn     | Cr     |      12 |                               |              | $\num{2.2649(29)e+02}$ | $\num{1.824(23)e+01}$ | $\num{8.05(10)e-02}$  |
| 347 | y     | Mn     | Cr     |      12 |                               |              | $\num{2.0058(31)e+02}$ | $\num{1.440(26)e+01}$ | $\num{7.18(13)e-02}$  |
|-----+-------+--------+--------+---------+-------------------------------+--------------+------------------------+-----------------------+-----------------------|
| 325 | y     | Ti     | Ti     |       9 | $\ce{Ti}$ $\text{K}_{\alpha}$ |         4.51 | $\num{1.810(12)e+02}$  | $\num{1.341(70)e+01}$ | $\num{7.41(39)e-02}$  |
| 326 | n     | Ti     | Ti     |       9 |                               |              | $\num{1.7558(61)e+02}$ | $\num{1.350(35)e+01}$ | $\num{7.69(20)e-02}$  |
| 349 | y     | Ti     | Ti     |       9 |                               |              | $\num{1.6036(90)e+02}$ | $\num{1.224(49)e+01}$ | $\num{7.63(31)e-02}$  |
|-----+-------+--------+--------+---------+-------------------------------+--------------+------------------------+-----------------------+-----------------------|
| 328 | y     | Ag     | Ag     |       6 | $\ce{Ag}$ $\text{L}_{\alpha}$ |         2.98 | $\num{1.1761(29)e+02}$ | $\num{1.091(25)e+01}$ | $\num{9.27(21)e-02}$  |
| 329 | n     | Ag     | Ag     |       6 |                               |              | $\num{1.1625(16)e+02}$ | $\num{1.190(13)e+01}$ | $\num{1.024(11)e-01}$ |
| 351 | y     | Ag     | Ag     |       6 |                               |              | $\num{1.0675(21)e+02}$ | $\num{1.139(17)e+01}$ | $\num{1.067(16)e-01}$ |
|-----+-------+--------+--------+---------+-------------------------------+--------------+------------------------+-----------------------+-----------------------|
| 332 | y     | Al     | Al     |       4 | $\ce{Al}$ $\text{K}_{\alpha}$ |         1.49 | $\num{5.769(15)e+01}$  | $\num{6.12(11)e+00}$  | $\num{1.061(20)e-01}$ |
| 333 | n     | Al     | Al     |       4 |                               |              | $\num{5.674(12)e+01}$  | $\num{7.18(10)e+00}$  | $\num{1.265(18)e-01}$ |
|-----+-------+--------+--------+---------+-------------------------------+--------------+------------------------+-----------------------+-----------------------|
| 335 | y     | Cu     | EPIC   |       2 | $\ce{Cu}$ $\text{L}_{\alpha}$ |        0.930 | $\num{3.542(35)e+01}$  | $\num{6.37(56)e+00}$  | $\num{1.80(16)e-01}$  |
| 336 | n     | Cu     | EPIC   |       2 |                               |              | $\num{3.309(38)e+01}$  | $\num{8.62(47)e+00}$  | $\num{2.60(15)e-01}$  |
| 337 | n     | Cu     | EPIC   |       2 |                               |              | $\num{3.392(56)e+01}$  | $\num{9.77(32)e+00}$  | $\num{2.88(11)e-01}$  |
|-----+-------+--------+--------+---------+-------------------------------+--------------+------------------------+-----------------------+-----------------------|
| 339 | y     | Cu     | EPIC   |     0.9 | $\ce{O }$ $\text{K}_{\alpha}$ |        0.525 | $\num{2.522(35)e+01}$  | $\num{6.32(58)e+00}$  | $\num{2.51(23)e-01}$  |
| 340 | n     | Cu     | EPIC   |     0.9 |                               |              | $\num{2.121(10)e+01}$  | $\num{5.49(16)e+00}$  | $\num{2.590(76)e-01}$ |
|-----+-------+--------+--------+---------+-------------------------------+--------------+------------------------+-----------------------+-----------------------|
| 342 | y     | C      | EPIC   |     0.6 | $\ce{C }$ $\text{K}_{\alpha}$ |        0.277 | $\num{1.907(12)e+01}$  | $\num{4.446(97)e+00}$ | $\num{2.331(53)e-01}$ |
| 343 | n     | C      | EPIC   |     0.6 |                               |              | $\num{1.7930(66)e+01}$ | $\num{5.243(51)e+00}$ | $\num{2.924(30)e-01}$ |
|-----+-------+--------+--------+---------+-------------------------------+--------------+------------------------+-----------------------+-----------------------|

* Morphing of CDL reference spectra [/]                            :Appendix:
:PROPERTIES:
:CUSTOM_ID: sec:appendix:morphing_cdl_spectra
:END:


- [ ] *NEED TO REREAD THIS AND FIX UP THE WORST ISSUES!*
  -> especially if this is to remain in the final thesis!

- [ ] *PLACE ME SOMEWHERE MORE IMPORTANT?*  
#+CAPTION: $\ln\mathcal{L}$ values for all the cleaned CDL cluster data against the
#+CAPTION: energy of the cluster. Left is the calculation using the old cite:krieger2018search
#+CAPTION: approach of fixed energy intervals and right the linear interpolation explained below
#+CAPTION: in this appendix. The bin wise linear interpolation clearly significantly helps to
#+CAPTION: provide a smoother description of the $\ln\mathcal{L}$ data.
#+NAME: fig:appendix:cdl_morphing_logL_vs_energy
[[~/phd/Figs/background/logL_of_CDL_vs_energy.pdf]]

This section of the (not exported) appendix contains our notes about
building and implementing the linear interpolation of CDL data. It
should give an understanding of what was considered and why the final
choice is a linear interpolation.


One problem with the current approach of utilizing the CDL data is that the
reference distributions for the different logL variables are non continuous
between two energy bins. This means that if a cluster is moved from one bin to
another it suddenly has a very different cut for each property.

It might be possible to morph CDL spectra between two energies. That is to
allow interpolation between the shape of two neighboring reference
datasets.

This is the likely cause for the sudden steps visible in the background
rate. With a fully morphed function this should hopefully disappear.

** References & ideas

Read up on morphing of different functions:
- in HEP: https://indico.cern.ch/event/507948/contributions/2028505/attachments/1262169/1866169/atlas-hcomb-morphwshop-intro-v1.pdf
- https://mathematica.stackexchange.com/questions/208990/morphing-between-two-functions
- https://mathematica.stackexchange.com/questions/209039/convert-symbolic-to-numeric-code-speed-up-morphing

Aside from morphing, the theory of optimal transport seems to be directly
related to such problems:
- https://de.wikipedia.org/wiki/Optimaler_Transport
  (funny, this can be described by topology using GR lingo; there's no English
  article on this)
- https://en.wikipedia.org/wiki/Transportation_theory_(mathematics)
  see in particular: https://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Continuous_optimal_transport.png/200px-Continuous_optimal_transport.png
This seems to imply that given some functions $f(x)$, $g(x)$ we are looking for
the transport function $T$ which maps $T: f(x) \rightarrow g(x)$ in the language
of transportation theory.

See the linked article about the Wasserstein metric:
- https://en.wikipedia.org/wiki/Wasserstein_metric
  in particular the section about its connection to the optimal transport
  problem
It describes a distance metric between two probability distributions. In that
sense the distance between two distributions to be transported is directly
related to the Wasserstein distance.

One of the major constraints of transportation theory is that the transportation
has to preserve the integral of the transported function. Technically, this is
not the case for our application to the CDL data, due to different amount of
data available for each target. However, of course we normalize the CDL data and
assume that the given data actually is a PDF. At that point each distribution is
normalized to 1 and thus each morphed function has to be normalized to 1 as
well. This is a decent check for a morph result. If a morphing technique does
*not* satisfy this property we need to renormalize the result.

On the other hand, considering the slides by Verkerke (indico.cern.ch link
above), the morphing between two functions can also be interpreted as a simple
interpolation problem.

In that sense there are multiple approaches to compute an intermediate
step of the CDL distributions.

1. visualize the CDL data as a 2D heatmap:
   - value of the logL variable is x
   - energy of each fluorescence line is y
2. linear interpolation at a specific energy $E$ based on the two
   neighboring CDL distributions (interpolation thus *only* along y axis)
3. spline interpolation over all energy ranges
4. KDE along the energy axis (only along *y*)
   Extend KDE to 2D?
5. bicubic interpolation (-> problematic, because our energies/variables are
   *not* spread on a rectilinear grid (energy is not spread evenly)
6. other distance based interpolations, i.e. KD-tree? Simply perform
   an interpolation based on all neighboring points in a certain
   distance?

Of these 2 and 4 seem to be the easiest implementations. KD-tree would
also be easy, provided I finish the implementation finally.

We will investigate different ideas in
[[file:~/CastData/ExternCode/TimepixAnalysis/Tools/cdlMorphing/cdlMorphing.nim]].

*** DONE Visualize CDL data as a 2D heatmap

In each of the following plots each distribution (= target filter
combination) is normalized to 1.

First let's visualize all of the CDL data as a scatter plot. That is
pretty simple and gives an idea where the lines are and what the shape
is roughly, fig. [[logL_scatter_lengthDivRmsTrans]].

#+begin_center
#+CAPTION: Scatter plot of the reference dataset for the length / transverse RMS
#+CAPTION: logL variable, colored by the normalized counts for each distribution
#+CAPTION: (each is normalized to 1). The distributions are drawn at the energy
#+CAPTION: of the fluorescence lines.
#+NAME: logL_scatter_lengthDivRmsTrans
[[~/phd/Figs/CDL/cdlMorphing/cdl_as_scatter_lengthDivRmsTrans.pdf]]
#+end_center

Now we can check out what the data looks like if we interpret the whole (value
of each variable, Energy) phase space as a tile map. In this way, morphing can
be interpreted as performing interpolation along the energy axis in the
resulting tile map.

Each figure contains in addition as colored lines the *start* of each energy
range as currently used. So clusters will be with the distribution defined by
the line below.

In addition the energy of each fluorescence lines is plotted in red at the
corresponding energy value. This also shows that the intervals and the energy
of the lines are highly asymmetric.

#+begin_center
#+CAPTION: Tile map of the reference dataset for the eccentricity
#+CAPTION: logL variable, colored by the normalized counts for each distribution
#+CAPTION: (each is normalized to 1 along the x axis). Each tile covers the range
#+CAPTION: from start to end interval. In addition in red the energy of each
#+CAPTION: fluorescence line is plotted.
#+NAME: logL_tile_eccentricity
[[~/phd/Figs/CDL/cdlMorphing/cdl_as_tile_eccentricity.pdf]]
#+end_center

#+begin_center
#+CAPTION: Tile map of the reference dataset for the length / transverse RMS
#+CAPTION: logL variable, colored by the normalized counts for each distribution
#+CAPTION: (each is normalized to 1 along the x axis). Each tile covers the range
#+CAPTION: from start to end interval. In addition in red the energy of each
#+CAPTION: fluorescence line is plotted.
#+NAME: logL_tile_lengthDivRmsTrans
[[~/phd/Figs/CDL/cdlMorphing/cdl_as_tile_lengthDivRmsTrans.pdf]]
#+end_center

#+begin_center
#+CAPTION: Tile map of the reference dataset for the fraction within transverse RMS
#+CAPTION: logL variable, colored by the normalized counts for each distribution
#+CAPTION: (each is normalized to 1 along the x axis). Each tile covers the range
#+CAPTION: from start to end interval. In addition in red the energy of each
#+CAPTION: fluorescence line is plotted.
#+NAME: logL_tile_fracRmsTrans
[[~/phd/Figs/CDL/cdlMorphing/cdl_as_tile_fractionInTransverseRms.pdf]]
#+end_center

*** DONE Morph by linear interpolation bin by bin

Based on the tile maps in the previous section it seems like a decent idea to
perform a linear interpolation for any point in between two intervals:
#+begin_export latex
\[f(E, x) = f_\text{low}(x) \cdot \left(1 - \frac{ |L_\text{low} - E|}{\Delta E}\right) +
            f_\text{high}(x) \cdot \left(1 - \frac{ |L_\text{high} - E|}{\Delta E}\right)  \]
#+end_export
where $f_\text{low,high}$ are the distributions below / above the given energy
$E$. $L_\text{low,high}$ corresponds to the energy of the fluorescence line
corresponding to the distribution below / above $E$. $\Delta E$ is the
difference in energy between the lower and higher fluorescence lines. $x$ is the
value of the given logL variable.

In code this is:
#+begin_src nim
  let E = ... # given as argument to function
  let lineEnergies = getXrayFluorescenceLines()
  let refLowT = df.filter(f{string -> bool: `Dset` == refLow})["Hist", float]
  let refHighT = df.filter(f{string -> bool: `Dset` == refHigh})["Hist", float]
  result = zeros[float](refLowT.size.int)
  let deltaE = abs(lineEnergies[idx] - lineEnergies[idx+offset])
  # walk over each bin and compute linear interpolation between
  for i in 0 ..< refLowT.size:
    result[i] = refLowT[i] * (1 - (abs(lineEnergies[idx] - E)) / deltaE) +
      refHighT[i] * (1 - (abs(lineEnergies[idx+offset] - E)) / deltaE)
#+end_src

Since doing this for a point between two lines is not particularly helpful,
because we do not /know/ what the distribution in between does actually look
like. Instead for validation we will now try to compute the =Cu-EPIC-0.9kV=
distribution (corresponding to the $\text{O K}_{α}$ line at
$\SI{0.525}{\kilo \electronvolt}$) based on the =C-EPIC-0.6kV= and =Cu-EPIC-2kV=
distributions.
That means the interpolate the second ridge from the first and third in the
CDL ridgeline plots.

This is shown in fig. [[linear_interpolation_morph_eccentricity]],
[[linear_interpolation_morph_lengthDivRmsTrans]] and
[[linear_interpolation_morph_fracRmsTrans]]. The real data for each distribution is
shown in red and the morphed linear bin-wise interpolation for the second ridge
is shown in red.

#+begin_center
#+CAPTION: Interpolation of the =Cu-EPIC-0.9kV= distribution for the eccentricity logL variable
#+CAPTION: using bin wise linear interpolation based on the =C-EPIC-0.6kV= and =Cu-EPIC-2kV= distributions.
#+CAPTION: The real data is shown in the second ridge in red and the morphed interpolation is
#+CAPTION: is shown in blue. The agreement is remarkable for the simplicity of the method.
#+NAME: linear_interpolation_morph_eccentricity
[[~/phd/Figs/CDL/cdlMorphing/eccentricity_ridgeline_XrayReferenceFile2018.h5_2018.pdf]]
#+end_center

#+begin_center
#+CAPTION: Interpolation of the =Cu-EPIC-0.9kV= distribution for the length / transverse RMS logL variable
#+CAPTION: using bin wise linear interpolation based on the =C-EPIC-0.6kV= and =Cu-EPIC-2kV= distributions.
#+CAPTION: The real data is shown in the second ridge in red and the morphed interpolation is
#+CAPTION: is shown in blue. The agreement is remarkable for the simplicity of the method.
#+NAME: linear_interpolation_morph_lengthDivRmsTrans
[[~/phd/Figs/CDL/cdlMorphing/lengthDivRmsTrans_ridgeline_XrayReferenceFile2018.h5_2018.pdf]]
#+end_center

#+begin_center
#+CAPTION: Interpolation of the =Cu-EPIC-0.9kV= distribution for the fraction in transverse RMS logL variable
#+CAPTION: using bin wise linear interpolation based on the =C-EPIC-0.6kV= and =Cu-EPIC-2kV= distributions.
#+CAPTION: The real data is shown in the second ridge in red and the morphed interpolation is
#+CAPTION: is shown in blue. This in particular is the problematic variable, due to the integer nature
#+CAPTION: of the data at low energies. However, even here the interpolation works extremely well.
#+NAME: linear_interpolation_morph_fracRmsTrans
[[~/phd/Figs/CDL/cdlMorphing/fractionInTransverseRms_ridgeline_XrayReferenceFile2018.h5_2018.pdf]]
#+end_center

*** DONE Compute all reference spectra from neighbors

Similar to the plots in the previous section we can now compute all
reference spectra based on the next neighboring spectras.

This is done in fig. [[linear_interpolation_morph_eccentricity_all]],
[[linear_interpolation_morph_lengthDivRmsTrans_all]], [[linear_interpolation_morph_fracRmsTrans_all]].

#+begin_center
#+CAPTION: Interpolation of all reference distribution for the eccentricity logL variable
#+CAPTION: using bin wise linear interpolation based on the neighboring distributions.
#+CAPTION: The real data is shown in red while the morphed data is shown in cyan.
#+CAPTION: In most ridges the agreement is very good.
#+NAME: linear_interpolation_morph_eccentricity_all
[[~/phd/Figs/CDL/cdlMorphing/eccentricity_ridgeline_morph_all_XrayReferenceFile2018.h5_2018.pdf]]
#+end_center

#+begin_center
#+CAPTION: Interpolation of all reference distribution for the length / transverse RMS logL variable
#+CAPTION: using bin wise linear interpolation based on the neighboring distributions.
#+CAPTION: The real data is shown in red while the morphed data is shown in cyan.
#+CAPTION: In most ridges the agreement is very good.
#+NAME: linear_interpolation_morph_lengthDivRmsTrans_all
[[~/phd/Figs/CDL/cdlMorphing/lengthDivRmsTrans_ridgeline_morph_all_XrayReferenceFile2018.h5_2018.pdf]]
#+end_center

#+begin_center
#+CAPTION: Interpolation of all reference distribution for the fraction in transverse RMS logL variable
#+CAPTION: using bin wise linear interpolation based on the neighboring distributions.
#+CAPTION: The real data is shown in red while the morphed data is shown in cyan.
#+CAPTION: In most ridges the agreement is very good.
#+NAME: linear_interpolation_morph_fracRmsTrans_all
[[~/phd/Figs/CDL/cdlMorphing/fractionInTransverseRms_ridgeline_morph_all_XrayReferenceFile2018.h5_2018.pdf]]
#+end_center

*** DONE Compute full linear interpolation between fluorescence lines

We can now apply the lessons from the last section to compute
arbitrary reference spectra. We will use this to compute a heatmap of
all possible energies in between the first and last fluorescence line.

For all three logL variables, these are shown in fig.

#+begin_center
#+CAPTION: Heatmap of a fully linear interpolated view of the energy / eccentricity
#+CAPTION: phase space in between the first and last fluorescence line.
#+NAME: linear_interpolation_raster_eccentricity
[[~/phd/Figs/CDL/cdlMorphing/cdl_as_raster_interpolated_eccentricity.pdf]]
#+end_center

#+begin_center
#+CAPTION: Heatmap of a fully linear interpolated view of the energy / lenthDivRmsTrans
#+CAPTION: phase space in between the first and last fluorescence line.
#+NAME: linear_interpolation_raster_lengthDivRmsTrans
[[~/phd/Figs/CDL/cdlMorphing/cdl_as_raster_interpolated_lengthDivRmsTrans.pdf]]
#+end_center

#+begin_center
#+CAPTION: Heatmap of a fully linear interpolated view of the energy / fracRmsTrans
#+CAPTION: phase space in between the first and last fluorescence line.
#+NAME: linear_interpolation_raster_fracRmsTrans
[[~/phd/Figs/CDL/cdlMorphing/cdl_as_raster_interpolated_fractionInTransverseRms.pdf]]
#+end_center

** KDE approach

Using a KDE is problematic, because our data is already pre-binned of
course. This leads to a very sparse phase space, which either makes
the local prediction around a known distribution good, but fails
miserably in between them (small bandwidth) or gives decent
predictions in between, but pretty bad reconstruction of the known
distributions (larger bandwidth).

There is also a strong conflict in bandwidth selection, due to the
non-linear steps in energy between the different CDL
distributions. This leads to a too large / too small bandwidth at
either end of the energy range.

Fig. [[eccentricity_ridgeline_morph_kde]],
[[fracTransRms_ridgeline_morph_kde]],
[[lengthDivRmsTrans_ridgeline_morph_kde]] show the default bandwidth
(Silverman's rule of thumb). In comparison Fig. [[eccentricity_ridgeline_morph_kde_small_bw]],
[[fracTransRms_ridgeline_morph_kde_small_bw]],
[[lengthDivRmsTrans_ridgeline_morph_kde_small_bw]] show the same plot
using a much smaller custom bandwidth of 0.3 keV. The agreement is
much better, but the actual prediction between the different
distributions becomes much worse. Compare
fig. [[eccentricity_raster_morph_kde]] (default bandwidth) to
fig. [[eccentricity_raster_morph_kde_small_bw]]. The latter has regions of
almost no counts, which is obviously wrong.

Note that also the fig. [[eccentricity_raster_morph_kde]] is
problematic. An effect of a bad KDE input is visible, namely that the
bandwidth vs. number of datapoints is such that the center region (in
energy) has higher values than the edges, due to the fact that
predictions near the boundaries see no signal (this boundary effect
could be corrected for by assuming a suitable boundary conditions,
e.g. just extending the first/last distributions in the respective
ranges. It is not clear however in what spacing such a distribution
should be placed etc.

#+begin_center
#+CAPTION: Reconstruction of the different eccentricity CDL distributions using a KDE
#+CAPTION: with the bin counts as weights with the automatically computed
#+CAPTION: bandwidth using Silverman's rule of thumb (about 1.6 keV in this case).
#+CAPTION: Bad match with real data for low energies (top ridges).
#+NAME: eccentricity_ridgeline_morph_kde
[[~/phd/Figs/CDL/cdlMorphing/eccentricity_ridgeline_morph_kde.pdf]]
#+end_center

#+begin_center
#+CAPTION: Reconstruction of the different fraction in transverse RMS CDL distributions using a KDE
#+CAPTION: with the bin counts as weights with the automatically computed
#+CAPTION: bandwidth using Silverman's rule of thumb (about 1.6 keV in this case).
#+CAPTION: Bad match with real data for low energies (top ridges).
#+NAME: fracTransRms_ridgeline_morph_kde
[[~/phd/Figs/CDL/cdlMorphing/fractionInTransverseRms_ridgeline_morph_kde.pdf]]
#+end_center

#+begin_center
#+CAPTION: Reconstruction of the different length / transverse RMS CDL distributions using a KDE
#+CAPTION: with the bin counts as weights with the automatically computed
#+CAPTION: bandwidth using Silverman's rule of thumb (about 1.6 keV in this case).
#+CAPTION: Bad match with real data for low energies (top ridges).
#+NAME: lengthDivRmsTrans_ridgeline_morph_kde
[[~/phd/Figs/CDL/cdlMorphing/lengthDivRmsTrans_ridgeline_morph_kde.pdf]]
#+end_center

\clearpage

#+begin_center
#+CAPTION: Reconstruction of the different eccentricity CDL distributions using a KDE
#+CAPTION: with the bin counts as weights with a custom bandwidth of 0.3 keV.
#+CAPTION: Bad match with real data for low energies (top ridges).
#+NAME: eccentricity_ridgeline_morph_kde_small_bw
[[~/phd/Figs/CDL/cdlMorphing/eccentricity_ridgeline_morph_kde_small_bw.pdf]]
#+end_center

#+begin_center
#+CAPTION: Reconstruction of the different fraction in transverse RMS CDL distributions using a KDE
#+CAPTION: with the bin counts as weights with a custom bandwidth of 0.3 keV.
#+CAPTION: Bad match with real data for low energies (top ridges).
#+NAME: fracTransRms_ridgeline_morph_kde_small_bw
[[~/phd/Figs/CDL/cdlMorphing/fractionInTransverseRms_ridgeline_morph_kde_small_bw.pdf]]
#+end_center

#+begin_center
#+CAPTION: Reconstruction of the different length / transverse RMS CDL distributions using a KDE
#+CAPTION: with the bin counts as weights with a custom bandwidth of 0.3 keV.
#+CAPTION: Bad match with real data for low energies (top ridges).
#+NAME: lengthDivRmsTrans_ridgeline_morph_kde_small_bw
[[~/phd/Figs/CDL/cdlMorphing/lengthDivRmsTrans_ridgeline_morph_kde_small_bw.pdf]]
#+end_center

\clearpage

#+begin_center
#+CAPTION: Raster of the KDE interpolation for the eccentricity using the automatically
#+CAPTION: determined bandwidth based on Silverman's rule of thumb (about 1.6 keV in this case).
#+CAPTION: Boundary effects are visible due to apparent more activity near the center
#+CAPTION: energies.
#+NAME: eccentricity_raster_morph_kde
[[~/phd/Figs/CDL/cdlMorphing/eccentricity_raster_kde.pdf]]
#+end_center

#+begin_center
#+CAPTION: Raster of the KDE interpolation for the eccentricity using a custom
#+CAPTION: bandwidth of 0.3 keV.
#+CAPTION: Better agreement at the different CDL target energies at the expense of
#+CAPTION: a reasonable prediction between the different regions.
#+NAME: eccentricity_raster_morph_kde_small_bw
[[~/phd/Figs/CDL/cdlMorphing/eccentricity_raster_kde_small_bw.pdf]]
#+end_center

** Spline approach

Another idea is to use a spline interpolation. This has the advantage
that the existing distributions will be correctly predicted (as for
the linear interpolation), but possibly yields better results between
distributions (or in the case of predicting a known distributions.

Fig. [[eccentricity_ridgeline_morph_spline]],
[[fracTransRms_ridgeline_morph_spline]],
[[lengthDivRmsTrans_ridgeline_morph_spline]] show the prediction using a
spline. Same as for the linear interpolation each morphed distribution
was computed by excluding that distribution from the spline definition
and then predicting the energy of the respective fluorescence line.

The result looks somewhat better in certain areas than the linear
interpolation, but has unphysical artifacts in other areas (negative
values) while also deviating quite a bit. For that reason it seems
like simpler is better in case of CDL morphing (at least if it's done
bin-wise).

#+begin_center
#+CAPTION: Reconstruction of the different eccentricity CDL distributions using a spline
#+CAPTION: interpolation (by excluding each distribution that is being predicted).
#+CAPTION: Prediction sometimes even yields negative values, highlighting the
#+CAPTION: problems of a spline in certain use cases (unphysical results).
#+NAME: eccentricity_ridgeline_morph_spline
[[~/phd/Figs/CDL/cdlMorphing/eccentricity_ridgeline_morph_spline.pdf]]
#+end_center

#+begin_center
#+CAPTION: Reconstruction of the different fraction in transverse RMS CDL distributions using a spline
#+CAPTION: interpolation (by excluding each distribution that is being predicted).
#+CAPTION: Prediction sometimes even yields negative values, highlighting the
#+CAPTION: problems of a spline in certain use cases (unphysical results).
#+NAME: fracTransRms_ridgeline_morph_spline
[[~/phd/Figs/CDL/cdlMorphing/fractionInTransverseRms_ridgeline_morph_spline.pdf]]
#+end_center

#+begin_center
#+CAPTION: Reconstruction of the different length / transverse RMS CDL distributions using a spline
#+CAPTION: interpolation (by excluding each distribution that is being predicted).
#+CAPTION: Prediction sometimes even yields negative values, highlighting the
#+CAPTION: problems of a spline in certain use cases (unphysical results).
#+NAME: lengthDivRmsTrans_ridgeline_morph_spline
[[~/phd/Figs/CDL/cdlMorphing/lengthDivRmsTrans_ridgeline_morph_spline.pdf]]
#+end_center

\clearpage

** Summary

For the time being we will use the linear interpolation method and see
where this leads us. Should definitely be a big improvement over the
current interval based option.

For the results of applying linear interpolation based morphing to the
likelihood analysis see section [[Effects of Morphing on background rate]].

** Implementation in =likelihood.nim=

Thoughts on the implememntition in =likelihood.nim= or CDL morphing.

0. Add interpolation code from =cdlMorphing.nim= in
   =private/cdl_cuts.nim=.
1. Add a field to =config.nim= that describes the morphing technique
   to be used.
2. Add an enum for the possible morphing techniques, =MorphingKind=
   with fields =mkNone=, =mkLinear=
3. In =calcCutValueTab= we currently return a =Table[string, float]=
   mapping target/filter combinations to cut values. This needs to be
   modified such that we have something that hides away input ->
   output and yields what we need.
   Define an =CutValueInterpolator= type, which is returned
   instead. It will be a variant object with case =kind:
   MorphingKind=.
   This object will allow access to cut values based on:
   - =string=: a target/filter combination.
     - =mkNone=: access internal =Table[string, float]= as done
       currently
     - =mkLinear=: raise exception, since does not make sense
   - =float=: an energy in keV:
     - =mkNone=: convert energy to a target/filter combination and
       access internal =Table=
     - =mkLinear=: access the closest energy distribution and return
       its cut value
4. in =filterClustersByLogL= replace =cutTab= name and access by
   energy of cluster instead of converted to target/filter dataset

With these steps we should have a working interpolation routine. The
code used in the =cdlMorphing.nim= test script needs to be added of
course to provide the linearly interpolated logic (see step 0).

*** Bizarre Al-Al 4kV behavior with =mkLinear=

After the first implementation we see some very bizarre behavior in
the case of linear interpolation for the logL distributions.

This is both visible with the =plotCdl.nim= as well as plotting code
in =likelihood.nim=.

See fig. [[cdl_logL_linear_bizarre_Al_Al_4kV]].

#+begin_center
#+CAPTION: LogL distribitions after implementing linear interpolation and running
#+CAPTION: with =mkLinear=. The Al-Al 4kV line is noweher near where we expect it.
#+CAPTION: The code currently recomputes the logL values by
#+CAPTION: default, in which the =mkLinear= plays a role. The bug has to be somewhere
#+CAPTION: in that part of the interpolation.
#+NAME: cdl_logL_linear_bizarre_Al_Al_4kV
[[~/phd/Figs/CDL/cdlMorphing/cdl_logl_linear_bizarre_Al_Al_4kV.pdf]]
#+end_center

*UPDATE*: The issue was a couple of bugs & design choices in the
implementation of the linear interpolation in
=likelihood_utils.nim=. In particular about the design of the DF
returned from =getInterpolatedWideDf= and a bug accessing not the sub
DF, but the actual DF in the loop.

The fixed result is shown in fig. [[cdl_logL_linear_fixed]] and in
comparison the result using no interpolation (the reference in a way)
in fig. [[cdl_logL_no_interp]].

#+begin_center
#+CAPTION: LogL distribitions after implementing linear interpolation and running
#+CAPTION: with =mkLinear= and after the above mentioned bug has been fixed.
#+CAPTION: This is the same result as for =mkNone=, see fig. [[cdl_logL_no_interp]].
#+NAME: cdl_logL_linear_fixed
[[~/phd/Figs/CDL/cdlMorphing/cdl_logl_linear_fixed.pdf]]
#+end_center

#+begin_center
#+CAPTION: LogL distribitions after implementing linear interpolation and running
#+CAPTION: with =mkLinear= and after the above mentioned bug has been fixed.
#+CAPTION: This is the same result as for =mkNone=, see fig. [[cdl_logL_no_interp]].
#+NAME: cdl_logL_no_interp
[[~/phd/Figs/CDL/cdlMorphing/cdl_logl_no_interp.pdf]]
#+end_center


* Background rates
:PROPERTIES:
:CUSTOM_ID: sec:appendix:background_rates
:END:

** FADC veto [/]
:PROPERTIES:
:CUSTOM_ID: sec:appendix:background:fadc
:END:

- [ ] INSERT PLOTS OF RISE TIME, FALL TIME, BY RUN, BY SETTING,
  SKEWNESS VS RISE TIME AND MORE!


* Solar tracking candidates [/]                                    :Appendix:
:PROPERTIES:
:CUSTOM_ID: sec:appendix:solar_candidates
:END:

- [ ] *INSERT FIGS OF THE SOLAR TRACKING CANDIDATES*
  -> the distribution over the chip w/ axion image
  -> rate background vs candidates
  -> s/b plot


* Software                                                         :Software:Appendix:
:PROPERTIES:
:CUSTOM_ID: sec:appendix:software
:END:
#+LATEX: \minitoc

Essentially we aim to move the chapter [[Software]] here and extend it to:
- give an overview of all tools we use in this thesis behind the
  scenes (means what they do, what their CLI looks like and how to use
  them in the context of generating the limit starting from raw data)

Introduce used software for analysis.

Previous code used MarlinTPC (already extended a framework for use for
gaseous detectors, focus on strips). Additional extension to use our
new detector features would be beyond the scope of the framework. 

- [ ] *POSSIBLY MAKE THIS A LONG APPENDIX* Then we can just refer to
  that appendix and at the same time then don't have to worry about
  keeping it very focused on things that "should be" in a thesis.

- [ ] *IN APPENDIX ABOUT THIS, EXPLAIN CONFIG FILE OF TPA!*  
  
** Nim

Shortly introduce Nim & why it was chosen.

Efficient, productive, gets out of my way.

** TimepixAnalysis
:PROPERTIES:
:CUSTOM_ID: sec:appendix:timepix_analysis
:END:

Framework written for data analysis.

Rewrites Timepix / InGrid related code from MarlinTPC in Nim and
extends it (e.g. supports Timepix3).

[[https://github.com/Vindaar/TimepixAnalysis]]

After the thesis is published it is possible that this repository will
become the de facto repository for the thesis and the actual analysis
code will become its own repository. We'll see.

*THESE SECTIONS MUST BE MERGED WITH THE ANALYSIS BELOW*. Maybe let
this chapter simply be a high level overview: introduce Nim and the
why. Mention the TimepixAnalysis repo only as a "this is the code for
the analysis, the details of which will be explained in the next
section? In that case one might merge the whole chapter with the next
one and simply have these as the introductory part of the chapter.

*** ~InGridDatabase~
:PROPERTIES:
:CUSTOM_ID: sec:appendix:software:ingrid_database
:END:

- explain database
- explain data structure needed to add to DB
- show how to add    



*** =raw_data_manipulation=

*LINK TO HDF5 FORMAT IF FIRST TIME MENTIONING*

The first step of the analysis pipeline is essentially just a parsing
stage of the data generated by TOS (see section [[#sec:daq:tos_output_format]]
for an explanation of it) and storing it in a compressed HDF5 data
file.

The program is fed with a directory containing a TOS run, i.e. a
single data taking period ranging typically from minutes to days in
length, or a directory that itself contains multiple TOS run directories.

All data files contained in a run directory will then be parsed in a
multithreaded way. The files are memory mapped and parsed in parallel
into a =Run= data structure, which itself contains =Event= structures.

Depending on the designated run type of a file, some slight
processing steps are performed. *WHAT?*

If FADC files are present in a directory, these will also be parsed
into =FadcEvent= structures in a similar fashion.

Each run is then written into the output HDF5 file as a group. The
meta data about each run and event are stored as attributes and
additional datasets, respectively.

An example structure of a resulting HDF5 file is shown in:
*EXAMPLE LAYOUT WITH EXAMPLE ATTRIBUTES AND DATASETS?*

In addition the tool also supports input from HDF5 files containing
the raw data from a Timepix3 detector. That data is parsed and
reprocessed into the same kind of file structure.

#+CAPTION: Usage of the =raw_data_manipulation= tool. Input is in the form of a run directory / a directory
#+CAPTION: containing multiple run directories. The parsed output is stored in compressed HDF5 files.
#+NAME: list:raw_data_manipulation_help
#+begin_src shell-session
Version: 13809be built on: 2021-05-07 at 00:53:39
  InGrid raw data manipulation.

Usage:
  raw_data_manipulation <folder> [options]
  raw_data_manipulation <folder> --runType <type> [options]
  raw_data_manipulation <folder> --out=<name> [--nofadc] \
    [--runType=<type>] [--ignoreRunList] [options]
  raw_data_manipulation <folder> --nofadc [options]
  raw_data_manipulation --tpx3 <H5File> [options]
  raw_data_manipulation --tpx3 <H5File> --runType <type> [options]
  raw_data_manipulation --tpx3 <H5File> --runType <type> --out=<name> \
    [options]
  raw_data_manipulation -h | --help
  raw_data_manipulation --version

Options:
  --tpx3 <H5File>     Convert data from a Timepix3 H5 file to TPA format
  --runType=<type>    Select run type (Calib | Back | Xray)
                      The following are parsed case insensetive:
                      Calib = {"calib", "calibration", "c"}
                      Back = {"back", "background", "b"}
                      Xray = {"xray", "xrayfinger", "x"}
  --out=<name>        Filename of output file
  --nofadc            Do not read FADC files
  --ignoreRunList     If set ignores the run list 2014/15 to indicate
                      using any rfOldTos run
  --overwrite         If set will overwrite runs already existing in the
                      file. By default runs found in the file will be skipped.
                      HOWEVER: overwriting is assumed, if you only hand a
                      run folder!
  -h --help           Show this help
  --version           Show version.
#+end_src

*** =reconstruction=
:PROPERTIES:
:CUSTOM_ID: sec:reconstruction
:END:

After the raw data has been converted to storage in HDF5, the
=reconstruction= tool is used to start the actual analysis of the
data.

As the name implies, the first stage of data analysis is in the form
of reconstructing the basic properties of each event. In this stage
all events are processed in a multithreaded way. A cluster finding
algorithm is applied to each event on each chip separately, splitting
a single event into possibly multiple clusters. Clusters are defined
based on a certain notion of distance (the details depend on the
clustering algorithm used). The multiple clusters from a single event
are then treated fully equally for the rest of the analysis. The fact
that they originate from the same event has no further relevance (with
a slight exception for one veto technique, which utilizes clustering
over multiple chips, more on that in section [[sec:septem_veto]]).

For the individual clusters geometric properties will be
computed. These are the long and short axis, the eccentricity as well
as the statistical moments up to kurtosis along the long and short
axis. The full list is shown in tab. [[tab:geometric_properties]].

#+CAPTION: Table of all the (mostly) geometric properties of a single cluster computed during the
#+CAPTION: =reconstruction= tool. All but the likelihood, charge and energy properties are computed
#+CAPTION: during the first pass of the tool.
#+NAME: tab:geometric_properties
#+ATTR_LATEX: :booktabs t
|---------------------------+------------------------------------------------------------------|
| Property                  | Meaning                                                          |
|---------------------------+------------------------------------------------------------------|
| igCenterX                 | =x= position of cluster center                                   |
| igCenterY                 | =y= position of cluster center                                   |
| igHits                    | number of pixels in cluster                                      |
| igEventNumber             | event number cluster is from                                     |
| igEccentricity            | eccentricity of the cluster                                      |
| igSkewnessLongitudinal    | skewness along long axis                                         |
| igSkewnessTransverse      | skewness along short axis                                        |
| igKurtosisLongitudinal    | kurtosis along long axis                                         |
| igKurtosisTransverse      | kurtosis along short axis                                        |
| igLength                  | size along long axis                                             |
| igWidth                   | size along short axis                                            |
| igRmsLongitudinal         | RMS along long axis                                              |
| igRmsTransverse           | RMS along short axis                                             |
| igLengthDivRmsTrans       | length divided by transverse RMS                                 |
| igRotationAngle           | rotation angle of long axis over chip coordinate system          |
| igEnergyFromCharge        | energy of cluster computed from its charge                       |
| igLikelihood              | likelihood value for cluster                                     |
| igFractionInTransverseRms | fraction of pixels within radius of transverse RMS around center |
| igTotalCharge             | integrated charge of total cluster in electrons                  |
| igNumClusters             |                                                                  |
| igFractionInHalfRadius    | fraction of pixels in half radius around center                  |
| igRadiusDivRmsTrans       | radius divided by transverse RMS                                 |
| igRadius                  | radius of cluster                                                |
| igLengthDivRadius         | length divided by radius                                         |
|---------------------------+------------------------------------------------------------------|

The properties are computed here:
https://github.com/Vindaar/TimepixAnalysis/blob/master/Analysis/ingrid/private/geometry.nim#L308-L366
and here:
https://github.com/Vindaar/TimepixAnalysis/blob/master/Analysis/ingrid/private/geometry.nim#L517-L569

*NOTE:* How should we take care of linking to our code? Of course need
tagged version that corresponds to stuff in the thesis, but beyond
that?


After all geometrical properties have been computed, the next step is
to apply the ~ToT~ calibration
(sec. [[sec:operation_calibration:tot_calibration]]) to the ~ToT~ values of
all clusters, resulting in the equivalent charge in electrons. The
charge values for all recorded pixels are then used to compute a
histogram, which roughly follows a Pólya distribution
(sec. [[sec:polya_distribution]]). From the mean value of that
distribution a value for the gas gain is obtained, which is a
necessary input to perform an energy calibration for each cluster.

Second step of analysis, performs most of the major steps.

- cluster finding
- calculation of geometric properties
- charge calibration
- gas gain computation
- energy calibration
- ...



*** =likelihood=

Apply likelihood method for background suppression.

*** =computeLimit=

Computes the limit *NOT THE ONE USED*

*** Other

**** =cdl_spectrum_creation=

Generates the data from the CDL data that is used as X-ray reference
data for the likelihood method.

**** ...

Many other tools are present. Mainly different plotting tools and
tools dealing with other data, e.g. log files of the CAST experiment.

*** =cast_log_reader=
:PROPERTIES:
:CUSTOM_ID: sec:appendix:software:cast_log_reader
:END:

- [ ] *IMPORTANT* to read log files for tracking info

- [ ] Also mention some of the other features, i.e. generating times
  the magnet was on, point of the magnet etc.

* Full data reconstruction [/]                                     :Appendix:
:PROPERTIES:
:CUSTOM_ID: sec:appendix:full_data_reconstruction
:END:

- [ ] well, actually we want to reduce all this to a single script...
- [ ] Replace all this by a call to ~runAnalysisChain~.
- [ ] This chapter may still be one that contains some other stuff
  that needs to be run. Not sure. Or it might contain the code to set
  everything up, we'll see.  

This part of the appendix contains the full set of operations to
perform the data reconstruction.

1. setup toolchain
2. compile TPA binaries
3. (chips to ingrid database (well, it's commited))
3. raw data
4. reco
5. cdl same
6. background rate
7. limit   

2017 data:
#+begin_src sh 
reconstruction ~/CastData/data/CalibrationRuns2017_Raw.h5 --out ~/CastData/data/CalibrationRuns2017_Reco_withFadc.h5
#+end_src
#+begin_src sh 
reconstruction ~/CastData/data/CalibrationRuns2017_Reco_withFadc.h5 --only_fadc
#+end_src


#+begin_src sh
reconstruction ~/CastData/data/DataRuns2017_Raw.h5 --out ~/CastData/data/DataRuns2017_Reco_withFadc.h5
#+end_src

#+begin_src sh
reconstruction ~/CastData/data/DataRuns2017_Reco_withFadc.h5 --only_fadc
#+end_src

2018 data:
#+begin_src sh 
reconstruction ~/CastData/data/CalibrationRuns2018_Raw.h5 --out ~/CastData/data/CalibrationRuns2018_Reco_withFadc.h5
#+end_src
#+begin_src sh 
reconstruction ~/CastData/data/CalibrationRuns2018_Reco_withFadc.h5 --only_fadc
#+end_src


#+begin_src sh
reconstruction ~/CastData/data/DataRuns2018_Raw.h5 --out ~/CastData/data/DataRuns2018_Reco_withFadc.h5
#+end_src

#+begin_src sh
reconstruction ~/CastData/data/DataRuns2018_Reco_withFadc.h5 --only_fadc
#+end_src



* Raytracing - where does this belong? [0/1]                       :Software:Appendix:
:PROPERTIES:
:CUSTOM_ID: sec:appendix:raytracing
:END:
#+LATEX: \minitoc

*UPDATE*: <2023-01-17 Tue 19:55> For length reasons of the thesis this
will remain a pretty short appendix after all. While writing a longer
intro to raytracing would be really neat, it's just going to make the
thesis explode in length. Also it would take significant time to write
it after all.
*UPDATE 2*: <2023-08-04 Fri 17:26>
Having thought about this now, I think it would make sense to have a
basic introduction to raytracing (just the ray equation and the like)
and then present the LLNL telescope setup and a couple of plots
showing e.g. that fraction of shells hit matches the expectation from
thesis (i.e. the opening area).
*UPDATE 3*: <2023-09-08 Fri 13:42>
Given that we have finally after all implemented the interactive
raytracer, we should write a short raytracing introduction
(i.e. camera + rays into scene + ray equation + object intersections +
Whitted algorithm + Path tracing algorithm + solving the light
... equation)



- [ ] Whitted raytracing algorithm!
- [ ] Reference modern real time raytracing features & support on GPUs
- [ ] Mention the Path Tracing algorithm


*Signal hypothesis - raytracing from Sun to detector*

(It can be a reasonably long section, I think that's fair)

An interactive raytracer for the applications of solar axion fluxes, which allows to investigate the
scene (geometry of objects) in 'visible light' as well as serve as an
X-ray raytracer is in development.
+For time reasons that development is somewhat on hold unfortunately.+
+https://github.com/Vindaar/rayTracingInOneWeekend/tree/interactiveRayTracer+
https://github.com/Vindaar/TrAXer
-> Done.


Ray tracing through the detector. Put this before limit calculation stuff?

The whole theoretical side needs to be described in the theory chapter
in [[Axion-electron flux]]. Then here we can just describe how one
implements this (in particular in the noexport section).

Raytracing in a Weekend: cite:Shirley2020RTW1

For an in depth guide to raytracing, from theoretical principles to a
pretty sophisticated raytracer, see the amazing 'Physically based
rendering' cite:pharr2016physically. 

- [ ] *INTRODUCE CONCEPT OF RAYTRACING*
- [X] *CITE PBR*
- [ ] *MAYBE SHOW RAYTRACING IN A WEEKEND PICTURE AS EXAMPLE*
  -> Could of course mention fancy realtime raytracing possible nowadays.

- [ ] *PLOT OF MIRROR SHELLS*
  -> And relation of LLNL telescope table from PhD thesis &
  reproducing the fractions in our code via the ~geom_bar~ plot of how
  many times each shell hit

- [X] *AXION ELECTRON IMAGE*
  -> Found in limit chapter

- [ ] *PRIMAKOFF IMAGE* -> If we _do_ compute it, will be placed in relevant chapter there next to flux
- [ ] *CHAMELEON IMAGE* -> If we _do_ compute it, will be placed in relevant chapter there next to flux

- [ ] *NUMBERS FOR FLUX FRACTION ENCOUNTERED WHERE*

- [ ] *LLNL telescope overview*:
  In the section where we talk about our raytracing of this
  telescope include:
  - table of parameters of the telescope
  - numbers for telescope design 'as built'
  - Wolter equation with correct radius to use
  - the multilayer recipes used  


** Related to explicit axion image [/]

- [ ] *SHOW PLOTS CHARACTERIZING USED SOLAR MODEL*
- [ ] *REFERENCE SOLAR MODEL*

** TODO Can we finish our interactive ray tracer?

Need:
- light sources (4h of work at most)
- cylinders, hyperboloids, paraboloids as objects (once we figure out
  one, the rest should be relatively easy)
- placing different telescope layers etc. (2h)

In theory this *should* be possible as an extensive weekend project!
I'd say this is definitely worth it.


** Computation of atomic processes :noexport:

Computation of atomic processes done in *CODE*. 




* List of figures and tables
:PROPERTIES:
:CUSTOM_ID: sec:appendix:list_figures_tables
:END:

Read up on =ox-extra= and the :ignore: tag that it includes to only
export this, but not the actual header.

#+LATEX: \listoffigures{}
#+LATEX: \listoftables{}

* Acknowledgments                                                     :Part5:
:PROPERTIES:
:CUSTOM_ID: sec:appendix:acknowledgments
:END:

Thanks to Klaus & group.

Thanks to Araq for building Nim.

Thanks to Nim community, and especially:
Mamy (@mratsim), Hugo (@hugogranstrom), Clonkk, Chuck (@cblake), Andrea Ferreti (alea among others), @brentp
(plotly was a *huge* help in the beginning), @Bluenote10 (NimData was
great), @yglukhov (nimpy in particular!!)

Also thanks to Theodoros (and maybe Giovanni / Konstantin / Horst ?
maybe not...)

